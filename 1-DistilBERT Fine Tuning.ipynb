{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:26:51.087770Z",
     "start_time": "2024-09-11T08:26:49.765903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from huggingface_hub import notebook_login\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:26:51.103184Z",
     "start_time": "2024-09-11T08:26:51.099180Z"
    }
   },
   "source": [
    "# 模型相关参数\n",
    "num_labels = 3\n",
    "TEST_DATASET_SIZE = 0.4\n",
    "id2label={0:\"Neutral\", 1:\"Positive\", 2:\"Negative\"}\n",
    "label2id={\"Neutral\":0, \"Positive\":1, \"Negative\":2}\n",
    "DATA_PATH = './data/citation_sentiment_corpus.csv'\n",
    "DATA_PATH_NEW = './data/citation_sentiment_corpus_new.csv'\n",
    "SST2_FINETUNED_MODEL_PATH = './pretrain_models/distilbert-base-uncased-finetuned-sst-2-english'\n",
    "BASE_MODEL_PATH = './pretrain_models/distilbert-base-uncased'\n",
    "\n",
    "# 训练相关参数\n",
    "USE_BASE_MODEL = True\n",
    "BATCH_SIZE = 32\n",
    "loss_type='ce_loss' # 自定义参数 focal_loss dsc_loss ce_loss\n",
    "weight_decay = 0.01\n",
    "lr = 5e-6\n",
    "num_epochs = 5\n",
    "warmup_steps = 100\n",
    "label_smoothing_factor = 0.0\n",
    "\n",
    "# notebook_login()\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"fine-tuning-distilbert\"\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"checkpoint\"\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "os.environ[\"WANDB_DISABLED\"]=\"true\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:26:51.180566Z",
     "start_time": "2024-09-11T08:26:51.149921Z"
    }
   },
   "source": [
    "df = pd.read_csv(DATA_PATH_NEW)\n",
    "print(df.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8699, 5)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:26:51.228468Z",
     "start_time": "2024-09-11T08:26:51.200372Z"
    }
   },
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 过采样和欠采样"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:26:51.259018Z",
     "start_time": "2024-09-11T08:26:51.244491Z"
    }
   },
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# from imblearn.pipeline import Pipeline\n",
    "# \n",
    "# # 分离特征和标签\n",
    "# X_text = df['Citation_Text']\n",
    "# y = df['Sentiment']\n",
    "# \n",
    "# vectorizer = TfidfVectorizer(max_features=1000)\n",
    "# X_vectorized = vectorizer.fit_transform(X_text)\n",
    "# \n",
    "# # 过采样策略：1500个正样本，1500个负样本，欠采样策略：3000个中性样本\n",
    "# over = SMOTE(sampling_strategy={1: 1500, 2: 1500}, k_neighbors=2, random_state=42)\n",
    "# under = RandomUnderSampler(sampling_strategy={0: 3000}, random_state=42)\n",
    "# \n",
    "# # 应用过采样和欠采样\n",
    "# pipeline = Pipeline(steps=[('o', over), ('u', under)])\n",
    "# X_resampled, y_resampled = pipeline.fit_resample(X_vectorized, y)\n",
    "# X_resampled = vectorizer.inverse_transform(X_resampled)\n",
    "# X_resampled = [' '.join(words) for words in X_resampled]\n",
    "\n",
    "# df = pd.concat([pd.DataFrame(X_resampled, columns=['Citation_Text']), pd.DataFrame(y_resampled, columns=['Sentiment'])], axis=1)\n",
    "# df.head()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:26:52.770486Z",
     "start_time": "2024-09-11T08:26:51.277051Z"
    }
   },
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 分割数据集：训练集80%，验证集10%，测试集10%\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    df['Citation_Text'].tolist(), df['Sentiment'].tolist(), test_size=TEST_DATASET_SIZE, stratify=df['Sentiment'], random_state=42)\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    重构数据集类，使其能够返回字典格式的数据，有标签\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "if USE_BASE_MODEL:\n",
    "    # 加载DistilBERT模型\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL_PATH, num_labels=num_labels, id2label=id2label, label2id=label2id).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "else:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(SST2_FINETUNED_MODEL_PATH).to(device)\n",
    "    model.classifier = torch.nn.Linear(model.classifier.in_features, num_labels) # 修改模型的分类头，使其适应三分类任务\n",
    "    tokenizer = AutoTokenizer.from_pretrained(SST2_FINETUNED_MODEL_PATH)\n",
    "\n",
    "train_dataset = MyDataset(tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt', max_length=512), train_labels)\n",
    "test_dataset = MyDataset(tokenizer(test_texts, truncation=True, padding=True, return_tensors='pt', max_length=512), test_labels)\n",
    "val_dataset = MyDataset(tokenizer(val_texts, truncation=True, padding=True, return_tensors='pt', max_length=512), val_labels)\n",
    "\n",
    "print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
    "print(f\"Test Dataset Size: {len(test_dataset)}\")\n",
    "print(f\"Val Dataset Size: {len(val_dataset)}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at ./pretrain_models/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Size: 5219\n",
      "Test Dataset Size: 1740\n",
      "Val Dataset Size: 1740\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:26:52.848739Z",
     "start_time": "2024-09-11T08:26:52.787498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # 输出结果目录\n",
    "    # report_to='wandb',\n",
    "    num_train_epochs=num_epochs,\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    # weight_decay=weight_decay,\n",
    "    # warmup_steps=warmup_steps,\n",
    "    logging_strategy='steps',\n",
    "    logging_dir='./logs',            # 日志目录\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    disable_tqdm=False,\n",
    "    save_strategy=\"steps\",\n",
    "    fp16= torch.cuda.is_available(), # faster and use less memory\n",
    "    # metric_for_best_model='F1',\n",
    "    # load_best_model_at_end=True,\n",
    "    # push_to_hub=True,\n",
    "    # label_smoothing_factor=label_smoothing_factor\n",
    ")\n",
    "# 使用Focal loss作为损失函数解决样本不均衡问题\n",
    "def py_sigmoid_focal_loss(pred, target, gamma=2.0, alpha=0.9, reduction='mean'):\n",
    "    pred_sigmoid = pred.sigmoid()\n",
    "    target = torch.nn.functional.one_hot(target, num_classes=pred.shape[1]).type_as(pred)  # 转换为one-hot编码\n",
    "    pt = (1 - pred_sigmoid) * target + pred_sigmoid * (1 - target)\n",
    "    focal_weight = (alpha * target + (1 - alpha) * (1 - target)) * pt.pow(gamma)\n",
    "    loss = F.binary_cross_entropy_with_logits(pred, target, reduction='none') * focal_weight\n",
    "    return loss.mean() if reduction == 'mean' else loss.sum()\n",
    "\n",
    "class MultiFocalLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Focal_Loss= -1*alpha*((1-pt)**gamma)*log(pt)\n",
    "    Args:\n",
    "        num_class: number of classes\n",
    "        alpha: class balance factor shape=[num_class, ]\n",
    "        gamma: hyper-parameter\n",
    "        reduction: reduction type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_class, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(MultiFocalLoss, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.smooth = 1e-4\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        if alpha is None:\n",
    "            self.alpha = torch.ones(num_class, ) - 0.5\n",
    "        elif isinstance(alpha, (int, float)):\n",
    "            self.alpha = torch.as_tensor([alpha] * num_class)\n",
    "        elif isinstance(alpha, (list, np.ndarray)):\n",
    "            self.alpha = torch.as_tensor(alpha)\n",
    "        if self.alpha.shape[0] != num_class:\n",
    "            raise RuntimeError('the length not equal to number of class')\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        # assert isinstance(self.alpha,torch.Tensor)\\\n",
    "        alpha = self.alpha.to(logit.device)\n",
    "        prob = F.softmax(logit, dim=1)\n",
    "\n",
    "        if prob.dim() > 2:\n",
    "            # used for 3d-conv:  N,C,d1,d2 -> N,C,m (m=d1*d2*...)\n",
    "            N, C = logit.shape[:2]\n",
    "            prob = prob.view(N, C, -1)\n",
    "            prob = prob.transpose(1, 2).contiguous()  # [N,C,d1*d2..] -> [N,d1*d2..,C]\n",
    "            prob = prob.view(-1, prob.size(-1))  # [N,d1*d2..,C]-> [N*d1*d2..,C]\n",
    "\n",
    "        ori_shp = target.shape\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        prob = prob.gather(1, target).view(-1) + self.smooth  # avoid nan\n",
    "        logpt = torch.log(prob)\n",
    "        # alpha_class = alpha.gather(0, target.squeeze(-1))\n",
    "        alpha_weight = alpha[target.squeeze().long()]\n",
    "        loss = -alpha_weight * torch.pow(torch.sub(1.0, prob), self.gamma) * logpt\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'none':\n",
    "            loss = loss.view(ori_shp)\n",
    "\n",
    "        return loss\n",
    "# DSELoss\n",
    "class MultiDSCLoss(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Creates a criterion that optimizes a multi-class Self-adjusting Dice Loss\n",
    "    (\"Dice Loss for Data-imbalanced NLP Tasks\" paper)\n",
    "\n",
    "    Args:\n",
    "        alpha (float): a factor to push down the weight of easy examples\n",
    "        gamma (float): a factor added to both the nominator and the denominator for smoothing purposes\n",
    "        reduction (string): Specifies the reduction to apply to the output:\n",
    "            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
    "            ``'mean'``: the sum of the output will be divided by the number of\n",
    "            elements in the output, ``'sum'``: the output will be summed.\n",
    "\n",
    "    Shape:\n",
    "        - logits: `(N, C)` where `N` is the batch size and `C` is the number of classes.\n",
    "        - targets: `(N)` where each value is in [0, C - 1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1.0, smooth=1.0, reduction=\"mean\"):\n",
    "        super(MultiDSCLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.smooth = smooth\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        probs = torch.gather(probs, dim=1, index=targets.unsqueeze(1))\n",
    "\n",
    "        probs_with_factor = ((1 - probs) ** self.alpha) * probs\n",
    "        loss = 1 - (2 * probs_with_factor + self.smooth) / (probs_with_factor + 1 + self.smooth)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        elif self.reduction == \"none\" or self.reduction is None:\n",
    "            return loss\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Reduction `{self.reduction}` is not supported.\")\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, loss_type='focal_loss', *args, **kwargs):\n",
    "        super(CustomTrainer, self).__init__(*args, **kwargs)\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        if self.loss_type == 'focal_loss':\n",
    "            loss_fct = MultiFocalLoss(num_class=3, alpha=0.8, gamma=2.0, reduction='none')\n",
    "        elif self.loss_type == 'dsc_loss':\n",
    "            loss_fct = MultiDSCLoss(alpha=1.0, smooth=1.0, reduction='none')\n",
    "        elif self.loss_type == 'ce_loss':\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    " \n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    }"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:26:54.679849Z",
     "start_time": "2024-09-11T08:26:52.864799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 创建Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    loss_type=loss_type # 自定义参数 focal_loss dsc_loss\n",
    ")\n",
    "\n",
    "results = trainer.train() \n",
    "# wandb.finish()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GaoFan\\AppData\\Local\\Temp\\ipykernel_30500\\3207360782.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 12\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 创建Trainer\u001B[39;00m\n\u001B[0;32m      2\u001B[0m trainer \u001B[38;5;241m=\u001B[39m CustomTrainer(\n\u001B[0;32m      3\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m      4\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      9\u001B[0m     loss_type\u001B[38;5;241m=\u001B[39mloss_type \u001B[38;5;66;03m# 自定义参数 focal_loss dsc_loss\u001B[39;00m\n\u001B[0;32m     10\u001B[0m )\n\u001B[1;32m---> 12\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:1885\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1883\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   1884\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1885\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1886\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1887\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1888\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1889\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1890\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:2216\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2213\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[0;32m   2215\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[1;32m-> 2216\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2218\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2219\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   2220\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m   2221\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   2222\u001B[0m ):\n\u001B[0;32m   2223\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   2224\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:3250\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m   3248\u001B[0m         scaled_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m   3249\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 3250\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3252\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mdetach() \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\accelerate\\accelerator.py:2147\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[1;34m(self, loss, **kwargs)\u001B[0m\n\u001B[0;32m   2145\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m   2146\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 2147\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaler\u001B[38;5;241m.\u001B[39mscale(loss)\u001B[38;5;241m.\u001B[39mbackward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2148\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m learning_rate \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhas_lomo_optimizer:\n\u001B[0;32m   2149\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlomo_backward(loss, learning_rate)\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:193\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    189\u001B[0m inputs \u001B[38;5;241m=\u001B[39m (inputs,) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inputs, torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;28;01melse\u001B[39;00m \\\n\u001B[0;32m    190\u001B[0m     \u001B[38;5;28mtuple\u001B[39m(inputs) \u001B[38;5;28;01mif\u001B[39;00m inputs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m()\n\u001B[0;32m    192\u001B[0m grad_tensors_ \u001B[38;5;241m=\u001B[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001B[38;5;28mlen\u001B[39m(tensors))\n\u001B[1;32m--> 193\u001B[0m grad_tensors_ \u001B[38;5;241m=\u001B[39m \u001B[43m_make_grads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_grads_batched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retain_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:88\u001B[0m, in \u001B[0;36m_make_grads\u001B[1;34m(outputs, grads, is_grads_batched)\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m out\u001B[38;5;241m.\u001B[39mrequires_grad:\n\u001B[0;32m     87\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m out\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m---> 88\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad can be implicitly created only for scalar outputs\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     89\u001B[0m     new_grads\u001B[38;5;241m.\u001B[39mappend(torch\u001B[38;5;241m.\u001B[39mones_like(out, memory_format\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mpreserve_format))\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mRuntimeError\u001B[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:26:54.736863600Z",
     "start_time": "2024-09-11T07:07:13.143438Z"
    }
   },
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "preds_output = trainer.predict(test_dataset)\n",
    "probs = torch.softmax(torch.tensor(preds_output.predictions), dim=-1).numpy()\n",
    "predicted_labels = np.argmax(preds_output.predictions, axis=1)\n",
    "plot_confusion_matrix(test_labels, predicted_labels, list(label2id.keys()))"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 12\u001B[0m\n\u001B[0;32m      8\u001B[0m     plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mConfusion Matrix\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      9\u001B[0m     plt\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m---> 12\u001B[0m preds_output \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241m.\u001B[39mpredict(test_dataset)\n\u001B[0;32m     13\u001B[0m probs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msoftmax(torch\u001B[38;5;241m.\u001B[39mtensor(preds_output\u001B[38;5;241m.\u001B[39mpredictions), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m     14\u001B[0m predicted_labels \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(preds_output\u001B[38;5;241m.\u001B[39mpredictions, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 误差分析"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "    # Place all input tensors on the same device as the model\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        if loss_type == 'dsc_loss':\n",
    "            loss_fct = MultiDSCLoss(alpha=1.0, smooth=1.0, reduction='none')\n",
    "        elif loss_type == 'focal_loss':\n",
    "            loss_fct = MultiFocalLoss(num_class=3, alpha=0.8, gamma=2.0, reduction='none')\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        loss = loss_fct(output.logits, batch[\"label\"].to(device))\n",
    "    \n",
    "    # Place outputs on CPU for compatibility with other dataset columns   \n",
    "    return {\"loss\": loss.cpu().numpy(), \n",
    "            \"predicted_label\": pred_label.cpu().numpy()}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Convert our dataset back to PyTorch tensors\n",
    "# tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "# # Compute loss values\n",
    "# tokenized_datasets[\"val\"] = tokenized_datasets[\"val\"].map(forward_pass_with_label, batched=True, BATCH_SIZE=16)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# tokenized_datasets.set_format(\"pandas\")\n",
    "# cols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\n",
    "# df_test = tokenized_datasets[\"val\"][:][cols]\n",
    "# df_test[\"label\"] = df_test[\"label\"].map(id2label)\n",
    "# df_test[\"predicted_label\"] = (df_test[\"predicted_label\"].map(id2label))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# df_test.sort_values(\"loss\", ascending=False).head(10)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# df_test.sort_values(\"loss\", ascending=True).head(10)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "# best_model_dir = './model/distilbert-base-uncased-finetuned-citation'\n",
    "# trainer.save_model(best_model_dir)\n",
    "# best_model = AutoModelForSequenceClassification.from_pretrained(best_model_dir).to(device)\n",
    "\n",
    "\n",
    "# def predict_sentiment_score(model, dataset, batched=True, BATCH_SIZE=32, weights=[-0.5, 1, -1], shuffle=False):\n",
    "#     '''\n",
    "#     预测句子的情感\n",
    "#     '''\n",
    "#     results = []\n",
    "#     sentiment_scores = []\n",
    "#     model.eval()  # 将模型设置为评估模式\n",
    "\n",
    "#     dataloader = DataLoader(dataset, BATCH_SIZE=BATCH_SIZE, shuffle=shuffle)\n",
    "\n",
    "#     if batched:\n",
    "#         with torch.no_grad():  # 不计算梯度，以加速和节省内存\n",
    "#             for batch in dataloader:\n",
    "#                 # 将批次数据移动到cuda\n",
    "#                 batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#                 outputs = model(**batch)\n",
    "#                 logits = outputs.logits\n",
    "#                 probabilities = F.softmax(logits, dim=-1)\n",
    "                \n",
    "#                 logits_list = logits.tolist()\n",
    "#                 probabilities_list = probabilities.tolist()\n",
    "#                 results.extend(zip(logits_list, probabilities_list))\n",
    "#     else:\n",
    "#         with torch.no_grad():\n",
    "#             inputs = tokenizer(dataset, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "#             outputs = model(**inputs)\n",
    "#             logits = outputs.logits\n",
    "#             probabilities = F.softmax(logits, dim=-1)\n",
    "#             logits_list = logits.tolist()\n",
    "#             probabilities_list = probabilities.tolist()\n",
    "#             results.extend(zip(logits_list, probabilities_list))\n",
    "\n",
    "#     for _, softmax_probs in results:\n",
    "#         softmax_probs_array = np.array(softmax_probs)\n",
    "#         weights_array = np.array(weights)\n",
    "#         score = np.sum(softmax_probs_array * weights_array)\n",
    "#         score = max(min(score, 1), -1)\n",
    "#         sentiment_scores.append(score)\n",
    "#     return sentiment_scores\n",
    "\n",
    "# weights=[-0.1, 1, -1]\n",
    "# all_dataset = ConcatDataset([train_dataset, test_dataset, val_dataset])\n",
    "# sentiment_scores = predict_sentiment_score(best_model, all_dataset, weights=weights, batched=True, BATCH_SIZE=1, shuffle=True) # 句子维度不一样就把BATCH_SIZE设为1\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
