Text,Sentiment
3 Building the CatVar The CatVar database was developed using a combination of resources and algorithms including the Lexical Conceptual Structure  LCS  Verb and Preposition Databases   the Brown Corpus section of the Penn Treebank   an English morphological analysis lexicon developed for PCKimmo  Englex    NOMLEX   Longman Dictionary of Contemporary English 2For a deeper discussion and classification of Porter stemmers errors  see  ,Neutral
Table 2  Figures about clustering algorithms Algorithm  Sentences   Clusters SHAC 623 CHAC 217 QT 232 EM 416 In fact  table 2 shows that most of the clusters have less than 6 sentences which leads to question the results presented by  who only keep the clusters that contain more than 10 sentences ,Negative
The scheme is fully implemented within a version of the Spoken Language Translator system Rayner et al 1993 Agniis et al 1994 and is normally applied to input in the form of small lattices of hypotheses produced by a speech recognizer ,Neutral
Our baseline method for ambiguity resolution is the Collins parser as implemented by Bikel  We then built separate EnglishtoSpanish and SpanishtoEnglish directed word alignments using IBM model 4   combined them using the intersect  grow heuristic   and extracted phraselevel translation pairs of maximum length 7 using the alignment template approach  ,Neutral
We also extend the work of Zollmann et al 2008 on ChineseEnglish performing the analysis in both directions and providing a detailed qualitative explanation,Neutral
a timeconsuming process  Other statistical machine translation systems such as  and  also produce a tree a15 given a sentence a16 Their models are based on mechanisms that generate two languages at the same time  so an English tree a15 is obtained as a subproduct of parsing a16 However  their use of the LM is not mathematically motivated  since their models do not decompose into Pa4a5a2a9a8a3a10a6 and a12a14a4a5a3a7a6 unlike the noisy channel model ,Negative
The SPECIALIST minimal commitment parser relies on the SPECIALIST Lexicon as well as the Xerox stochastic tagger Cutting et al 1992 ,Neutral
This model can be seen as an extension of the standard Maximum Entropy Markov Model MEMM see Ratnaparkhi 1996 with an extra dependency on the predicate label we will henceforth refer to this model as MEMMpred ,Neutral
As mentioned by Pradhan et al 2004 argument identification plays a bottleneck role in improving the performance of a SRL system The effectiveness of the proposed additional pruning techniques may be seen as a significant improvement over the original algorithm of Xue and Palmer 2004 ,Positive
Veale  used WordNet to answer 374 multiplechoice SAT analogy questions  achieving an accuracy of 43   but the best corpusbased approach attains an accuracy of 56   ,Positive
Each backbone produces a separate CN and the decision of which CN to choose is taken at a later decoding stage but this still restricts the possible orders and alignments greatly Rosti et al 2008 Matusov et al 2008,Neutral
 extracts rules from nonanaphoric noun phrases and noun phrases patterns  which are then applied to test data to identify existential noun phrases ,Neutral
When compared to other kernel methods  our approach performs better than those based on the Tree kernel   and is only 02  worse than the best results achieved by a kernel method for parsing  Lexical relationships under the standard IBM models  do not account for manytomany mappings  and phrase extraction relies heavily on the accuracy of the IBM wordtoword alignment ,Negative
In a study of professional abstracting EndresNiggemeyer 2000 concluded that professional abstractors produce abstracts by cutandpaste operations and that standard sentence patterns are used in their production ,Neutral
The systems discussed in Hovy 1993 relied on a knowledge base and a representation of discourse Structure ,Neutral
On the other hand  integrating an additional component into a baseline SMT system is notoriously tricky as evident in the research on integrating word sense disambiguation  WSD  into SMT systems  different ways of integration lead to conflicting conclusions on whether WSD helps MT performance  ,Positive
The overall POS tag distribution learned by EM is relatively uniform  as noted by   and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed ,Neutral
In order to determine word boundaries we employed the longest matching algorithm Sornlertlamvanich 1993 ,Positive
Also it ispossible to cast a number of other useful problems as partofspeech tagging problemssuch as lettertosound translation Huang SonBell and Baggett 1994 and buildingpronunciation networks for speech recognition,Positive
The first solution might also introduce errors elsewhere As  already noted  ` While this automatic derivation process introduced a small percentage of errors on its own  it was the only practical way both to provide the amount of training data required and to allow for fullyautomatic testing  1 To train their system  R&M used a 200kword chunk of the Penn Treebank Parsed Wall Street Journal  tagged using a transformationbased tagger  and extracted base noun phrases from its parses by selecting noun phrases that contained no nested noun phrases and further processing the data with some heuristics  like treating the possessive marker as the first word of a new base noun phrase  to flatten the recursive structure of the parse ,Neutral
In a previous paper Schfitze 1993 we trained a neural network to disambiguate partofspeech using context however no information about the word that is to be categorized was used,Negative
We use GIZA    to train generative directed alignment models  HMM and IBM Model4  from training recordtext pairs ,Neutral
Yarowsky and Wicentowski 2000 present a corpusbased approach for morphological analysis of both regular and irregular forms based on four models including relative corpus frequency context similarity weighted string similarity and incremental retraining of inflectional transduction probabilities ,Neutral
Finally  to estimate the parameters i of the weighted linear model  we adopt the popular minimum error rate training procedure  which directly optimizes translation quality as measured by the BLEU metric ,Positive
 and Collins and Duffy  2002  rerank the top N parses from an existing generative parser  but this kind of approach 1Dynamic programming methods  can sometimes be used for both training and decoding  but this requires fairly strong restrictions on the features in the model Ever since its introduction in general  and in computational linguistics   many researchers have pointed out that there are quite some problems in using  eg ,Negative
As the tagger of  can not tag a word lattice  we can not back off to this tagging Pointwise mutual information  PMI  is commonly used for computing the association of two terms   which is defined as  nullnullnull null null  null null nullnullnull nullnullnullnull  nullnull nullnull null null null nullnullnullnullnull However  we argue that PMI is not a suitable measure for our purpose ,Negative
 present a system called BABAR that uses contextual role knowledge to do coreference resolution ,Neutral
Although the authors of  stated that they would discuss the search problem in a followup arti cle  so far there have no publications devoted to the decoding issue for statistical machine translation This strategy is commonly used in MT evaluation  because of BLEUs wellknown problems with documents of small size  ,Negative
 Introduction Robust statistical syntactic parsers  made possible by new statistical techniques  and by the availability of large  handannotated training corpora such as WSJ  and Switchboard   have had a major impact on the field of natural language processing ,Positive
In Weischedel et al 1993 a statistical approach to tagging unknown words isShown,Neutral
This task introduced by Landauer and Dumais 1997 consists of 80 multiple choice questions in which a word is given as the stem and the correct choice is the word which has the closest meaning to that of the stem among 4 candidates ,Neutral
As keyboard input is more efficient than mouse input cf Lehmalm et al 1995 mnost effort has been put in developing an efficient keyboard interLace ,Neutral
Algorithm 1 SCL  1  Select m pivot features ,Neutral
In DeMarcken 1990 and Weischedel et al 1993 kbest tags are assignedwithin a stochastic tagger by returning all tags within some threshold of probabilityof being correct for a particular word,Neutral
As is well known the extractive summary that has been extensively studied from the early days of summarization history Luhn 1958 suffers from various drawbacks ,Negative
Automatic summarization offers potential help in managing such results however the most popular approach extraction faces challenges when applied to multidocument summarization McKeown et al 2001 ,Neutral
This is very similar to Brill's use of contexts to induce transformation rules for his tagger Brill 1992 Brill 1995 but instead of generating transformation rules from a training text we gather statistics and apply them to parses in the text being disambiguated ,Neutral
The first SMT systems were developed in the early nineties  ,Neutral
Connectionist models have been used successfully for lexical acquisition Eineborgand Gamback 1993 Elenius 1990 Elenius and Carlson 1989 Nakamura et al 1990,Neutral
We compare our approach with a method proposed by Frstenau and Lapata 2009 ,Neutral
We measure coselection between sentences produced by each methodand the sentences selected by the assessors computing recall precision and Fscoreas in Firmin and Chrzanowski 1999 ,Positive
FrameNet role annotations were mapped onto those dependency graph nodes that corresponded most closely to the annotated substring see Frstenau 2008 for a detailed description of the mapping algorithm ,Neutral
Support Vector Machines  SVMs   and Maximum Entropy  ME  method  are powerful learning methods that satisfy such requirements  and are applied successfully to other NLP tasks  ,Positive
Other techniques exist for boosting thescore of longer phrases such as adjusting the score of the phrase by a fixed factor thatdepends on the length of the phrase Turney 1999,Neutral
3 Evaluation We trained our model parameters on a subset of the provided dev2006 development set  optimizing for caseinsensitive IBMstyle BLEU  with several iterations of minimum error rate training on nbest lists ,Neutral
This in turn allows it to correctly answer many more questions than systems based on TextRunner Banko et al 2007 and DIRT Lin and Pantel 2001,Neutral
 However the POS used are abbreviated POS and only in a window of b2 words No local collocation knowledge is used A probabilistic classifier is used in Bruce and Wiebe 1994 ,Negative
Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques  including TRP  and Gibbs sampling  ,Neutral
Morphosyntacticinformationhas in fact been shown to significantlyimprove the extractionresults  ,Neutral
Note that Liang et al 2009 only use this method in synthetic experiments and instead use a method similar to total uncertainty for experiments in partofspeech tagging,Negative
They are part of an effort to better integrate a linguistic rulebased system and the statistical correcting layer also illustrated in Ueffing et al 2008 ,Positive
Our approach is based on the empirical examination of abstracts published by second services and on assumptions about technical text organization Paice 1991 Bhatia1993 Jordan 1993 1996,Neutral
Our specific task is a two player objectidentification game adapted from the experiments of Clark and WilkesGibbs 1986 and Brennan and Clark 1996 ,Positive
The result is a specialized grammar this has a larger number of rules but a simpler structure allowing it in practice to be parsed very much more quickly using an LR based method Samuelsson 1994a ,Positive
This corpusbased information typically concerns sequences of 3 tags or words  It has been argued that METEOR correlates better with human judgment due to higher weight on recall than precision  Treebanking ,Positive
Maximum entropy models  are a class of exponential models which require no unwarranted independence assumptions and have proven to be very successful in general for integrating information from disparate and possibly overlapping sources ,Positive
Phrasebased systems such as Moses Koehn et al 2007 explicitly search for the highestscoring string in which all source words are translated,Positive
 In addition the filtering method causes some practical issues First such methods are not suitable for real MT tasks especially for applications with streamed input since the model has to be retrained with each new input sentence or document and training is slow  ,Negative
The notion of incrementally merging classes of lexical items is intuitively satisfying and is explored in detail in  Inter and Intra annotator agreement We measured pairwise agreement among annotators using the kappa coefficient  K  which is widely used in computational linguistics for measuring agreement in category judgments  ,Positive
Unfortunately  this is not the case for such widely used MT evaluation metrics as BLEU  and NIST   applied the parser of  developed for English  to Czech  and found thatthe performance wassubstantially lower when compared to the results for English ,Negative
There are good reasons for using such a handcrafted genrespecific verb lexicon instead of a general resource such as WordNet or Levins 1993 classes ,Neutral
 uilding on a recent proposal in this direction by Turney 2008 we propose a generic method of this sort and we test it on a set of unrelated tasks reporting good performance across the board with very little taskspecific tweaking ,Positive
The ArabictoEnglish system has been trained with the data provided by the International Work shop on Spoken Language Translation 2005 The context is that of the Basic Traveling Expression Corpus BTEC task Takezawa et al 2002 ,Neutral
 and Bikel and Chiang  has demonstrated the applicability of the  model for Czech and Chinese One of the most successful metrics for judging machinegenerated text is BLEU  ,Positive
In this paper we bring forward the first idea by studying the issue of how to utilize structured syntactic features for phrase reordering in a phrasebased SMT system with BTG Bracketing Transduction Grammar constraints Wu 1997  ,Neutral
Some of the consequences  of research in lexical semantics with particular attention to natural language processing are discussed by Pustejovsky et al 1993 and Nirenburg and Raskin 1996 ,Neutral
272 Similaritybased estimation was first used for language modeling in the cooccurrence smoothing method of Essen and Steinbiss   derived from work on acoustic model smoothing by Sugawara et al Following the setup in   we initialize the transition and emission distributions to be uniform with a small amount of noise  and run EM and VB for 1000 iterations ,Neutral
Carroll et al 1998 discussed simplifying newspaper text by replacing uncommon words with common words or replacing complicated syntactic structures with simpler structures to assist people with reading disabilities,Neutral
Nakov et al 2003 use ending guessing rules to predict the morphological class of unknown German nouns ,Neutral
It is often straightforward to obtain large amounts of unlabeled data  making semisupervised approaches appealing  previous work on semisupervised methods for dependency parsing includes  ,Positive
Like   we used mutual information to measure the cohesion between two words ,Neutral
The system was tested against 26711 words of newspaper text from The Wall Street Journal The Economist and Today all taken from the 200million word Bank of English corpus by the COBUILD team at the University of Birmingham England see also Jirvinen 1994 ,Neutral
Our evaluation assessed the performance of a semantic frame and role labeler with and without the annotations produced by our method ,Positive
The maximum entropy approach  is known to be well suited to solve the classification problem We compared a baseline system  the stateoftheart phrasebased system Pharaoh   against our system ,Positive
A major difference between the phrase features used in this work and those used elsewhere is that we do not assume that phrases segment into disjoint parts of the source and target sentences Koehn et al 2003 they can overlap,Neutral
In the field of statistical analysis of natural language data  it is common to use measures of lexical association  such as the informationtheoretic measure of mutual information  to extract useful relationships between words  eg   ,Neutral
1 Introduction Recent approaches to statistical machine translation  SMT  piggyback on the central concepts of phrasebased SMT  and at the same time attempt to improve some of its shortcomings by incorporating syntactic knowledge in the translation process ,Negative
CIT  ,Neutral
We recently proposed cube summing an approximate technique that permits the use of nonlocal features for inside DP algorithms Gimpel and Smith 2009 ,Positive
We presented firstorder expectation semirings and insideoutside computation in more detail than Eisner 2002 and developed extensions to higherorder expectation semirings,Negative
Gildea and Jurafsky 2002 were the first to describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles ,Positive
[Filatova  Hovy 2001] addressed the issue of resolving temporal references in news stories ,Positive
1 Introduction Since 1995  a few statistical parsing algorithms  demonstrated a breakthrough in parsing accuracy  as measured against the University of Pennsylvania TREEBANK as a gold standard ,Neutral
We built a translation model on a corpus for IWSLT 2005 ChinesetoEnglish translation task Eck and Hori 2005 which consists of 40k pairs of sentences ,Neutral
Our focus is on the sentence level  unlike  and   we employ a significantly larger set of seed words  and we explore as indicators of orientation words from syntactic classes other than adjectives  nouns  verbs  and adverbs  This additional conditioning has the effect of making the choice of generation rules sensitive to the history of the generation process  and  we argue  provides a simpler  more uniform  general  intuitive and natural probabilistic generation model obviating the need for CFGgrammar transforms in the original proposal of  ,Negative
Following  we can avoid unnecessary false positives by not querying for the longer ngram in such cases It is explored extensively in  We compare semisupervised LEAF with a previous state of the art semisupervised system  ,Positive
To increase the coherence of the output text we identify blocks of topicallyrelated themes and then apply chronological ordering on blocks of themes using themetime stamps Barzilay Elhadad and McKeown 2002,Neutral
Incremental topdown and leftcorner parsers have been shown to effectively  and efficiently  make use of nonlocal features ,Positive
In beam search incomplete parses of an utterance are pruned or discarded when on some criterion they are significantly less plausible than other competing parses This pruning is fully interleaved with the parsing process ,Negative
However their parser was not incremental it used global features such as the number of turn changes ,Negative
Again  we find the clearest patterns in the graphs for precision  where Malt has very low precision near the root but improves with increasing depth  while MST shows the opposite trend  ,Neutral
The Xerox Tagger 1 XT Cutting et al 1992 is a statistical tagger made by Doug Cutting Julian Kupiec Jan Pedersen and Penelope Sibun in Xerox PARC It was trained on the untagged Brown Corpus Francis and Kubera 1982 ,Neutral
To reduce the knowledge engineering burden on the user in constructing and porting an IE system  unsupervised learning has been utilized  eg Riloff   Yangarber et al ,Positive
See  for an application of the boosting approach to named entity recognition  and Walker  Rambow  and Rogati  for the application of boosting techniques for ranking in the context of natural language generation ,Neutral
While other systems  such as   have addressed these tasks to some degree  OPINE is the first to report results Thirdly   deploys the dependency language model to augment the lexical language model probability be1183 tween two head words but never seek a full dependency graph ,Negative
SMT has evolved from the original wordbased approach  into phrasebased approaches  and syntaxbased approaches  ,Neutral
We use MER  to tune the decoders parameters using a development data set The training set is extracted from TreeBank  section 1518  the development set  used in tuning parameters of the system  from section 20  and the test set from section 21 For nonlocal features  we adapt cube pruning from forest rescoring   since the situation here is analogous to machine translation decoding with integrated language models  we can view the scores of unit nonlocal features as the language model cost  computed onthefly when combining subconstituents ,Neutral
The best known measures for evaluating text categorization models are recall and precision calculated by the following equations Lewis 1992 ,Neutral
Because of these kinds of results  the vast majority of statistical parsing work has focused on parsing as a supervised learning problem  ,Neutral
 used the BaseNP tag set as presented in   I for inside a BaseNP  O for outside a BaseNP  and B for the first word in a BaseNP following another BaseNP ,Neutral
Large volumes of training data of this kind are indispensable for constructing statistical translation models   acquiring bilingual lexicon   and building examplebased machine translation  EBMT  systems  ,Neutral
One good example for this is fulltext retrieval systems Choueka 1980 Suchsystems must handle the morphological ambiguity problem,Positive
Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96 are readily available to assign POS to unrestricted English sentences Brill 1992 Cutting et al 1992,Neutral
One interesting approach to extending the current system is to introduce a statistical translation model  to filter out irrelevant translation candidates and to extract the most appropriate subpart from a long English sequence as the translation by locally aligning the Japanese and English sequences ,Neutral
1 Introduction The dominance of traditional phrasebased statistical machine translation  PBSMT  models  has recently been challenged by the development and improvement of a number of new models that explicity take into account the syntax of the sentences being translated 1 Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years   and often outperform phrasebased systems  on targetlanguage fluency and adequacy ,Negative
Bikel and Chiang  in fact contains two parsers  one is a lexicalized probabilistic contextfree grammar  PCFG  similar to   the other is based on statistical TAG  ,Neutral
Useful tools such as large aligned corpora eg the aligned Hansards Galeand Church 1991 and semantic word hierarchies eg Wordnet Miller 1990 havealso recently become available,Positive
We chose to train maximum entropy models Berger et al 1996 Our learning framework is described in Section 41 the results in Section 42  ,Neutral
Forced translation was implemented by Schwartz 2008 who ensures that hypothesis are a prefix of the reference to be generated,Neutral
Given the comparability of the accuracy of the rulebased partofspeech POS tagger Brill 1992 with the accuracy of the stochastic tagger and given the fact that a rulebased POS tagger has never been used for a Slavic language we have tried to apply rulebased methods even for Czech ,Neutral
Metrics in the Rouge family allow for skip ngrams   Kauchak and Barzilay  take paraphrasing into account  metrics such as METEOR  and GTM  calculate both recall and precision  METEOR is also similar to SIA  in that word class information is used ,Neutral
For English we obtained results comparable with the results presented in Merialdo 1992 as well as in Church 1992 ,Neutral
We planned a linguistic evaluation like DUC2005 Hoa Trang 2005 ,Neutral
In fact we have developed a rulebasedcomponent that transforms the phrase structure output of Collinss 2003 parser intoa representation in which a node has a direct link to its dependents,Neutral
This averaging effect has been shown to reduce overfitting and produce much more stable results  Probably the most widely used association weight function is  pointwise  Mutual Information  MI          defined by         log    2 fPwP fwPfwMI  ,Positive
12Poon and Domingos  outperformed  As with similar work   the size of the corpus makes preprocessing such as lemmatization  POS tagging or partial parsing  too costly The size of the development set used to generate 1 and 2  compensates the tendency of the unsmoothed MERT algorithm to overfit  by providing a high ratio between number of variables and number of parameters to be estimated ,Negative
An extrinsic evaluation Teufel 2001 shows that the output of our system is already a useful document surrogate in its own right,Neutral
The learning algorithm used for each stage of the classification task is a regularized variant of the structured Perceptron  ,Neutral
In particular we need to develop a backoff strategy for unseen pairs in the relational similarity tasks that following Turney 2006 could be based on constructing surrogate pairs of taxonomically similar words found in the CxLC space ,Positive
Turney 2008 is the first to the best of our knowledge to raise the issue of a unified approach In particular he treats synonymy and association as special cases of relational similarity ,Positive
While these approaches have had som e success to date   their usability as parsers in systems for natural language understanding is suspect ,Negative
Words are encoded through an automatic clustering algorithm  while tags  labels and extensions are normally encoded using diagonal bits ,Neutral
In Statistical Machine Translation  SMT   recent work shows that WSD helps translation quality when the WSD system directly uses translation candidates as sense inventories  Most semiautomated approaches have met with limited success  and supervised learning models have tended to outperform dictionarybased classi cation schemes  ,Positive
The literature on word space models Sahlgren 2006 has focused on taxonomic similarity synonyms antonyms cohyponyms and general association eg finding topically related words exploiting the idea that taxonomically or associated words will tend to occur in similar contexts and thus share a vector of cooccurring words ,Neutral
More recent works have also employed Luhns approach as a basis component for extracting relevant sentences Buyukkokten et al2001 Lam desina and Jones 2001  ,Positive
Although some successful applications have been developed see for instance Chinchor 1998 implementing an automatic text analysis system is still a labour and time intensive task ,Positive
The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available a situation that occurs often in practice Brill and Marcus 1992a ,Neutral
The tag Ngram probabilities and both the scheme and its application to these two tasks are described in detail in Samuelsson 1996 where it was also shown to compare favourably to deleted interpolation see Jelinek and Mercer 1980 even when the backoff weights of the latter were optimal ,Neutral
In the example the patterns of cooccurrence suggest that objects of killing are rather similar to subjects of dying hinting at the classic causesubjdieobj analysis of killing by Dowty 1977 and many others ,Neutral
On the other hand  purely statistical systems  extract discriminating MWUs from text corpora by means of association measure regularities ,Neutral
However Vbutilainen 1995 has shown that EngCG combined with a syntactic parser produces morphologically unambiguous output with an accuracy of 993 a figure clearly better than that of the statistical tagger in the experiments below however the test data was not the same ,Neutral
Given a source sentence f  the preferred translation output is determined by computing the lowestcost derivation  combination of hierarchical and glue rules  yielding f as its source side  where the cost of a derivation R1 Rn with respective feature vectors v1   vn Rm is given by msummationdisplay i  1 i nsummationdisplay j  1  vj  i ,Neutral
As this encoding strategy is not wellsuited to a free word order language like German we have focussed on a less surfaceoriented level of description most closely related to the LFG fstructure and representationsused in dependency grammar,Negative
Although previous work  has tackled the bootstrapping approach from both the theoretical and practical point of view  many key problems still remain unresolved  such as the selection of initial seed set Unlike   who found optimal performance when was approximately 104  we observed monotonic increases in performance as dropped ,Negative
We then piped the text through a maximum entropy sentence boundary detector  and performed text normalization using NSW tools  ,Neutral
We use the kappa coefficient K Siegel and Castellan 1988 to measure stability and reproducibility following Carletta 1996  ,Neutral
Recently   have successfully constructed high quality and high coverage gazetteers from Wikipedia Introduction The maximum entropy model  has attained great popularity in the NLP field due to its power  robustness  and successful performance in various NLP tasks   ,Positive
     Dave et al ,Neutral
The disambiguation rules are similar to phonological rewrite rules Kaplan and Kay 1994 and the parsing algorithm is similar to the algorithm for combining the morphological rules with the lexicon Karttunen 1994 ,Positive
This method was shown to outperform the class based model proposed in  and can thus be expected to discover better clusters of words Although a rich literature covers bootstrapping methods applied to natural language problems  several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition ,Negative
Based on RST [Marcu 2000] established a Rhetorical Parser The parser exploits cue phrases in an algorithm that discovers discourse relationships between phrases in a text ,Positive
We annotated with the BIO tagging scheme used in syntactic chunkers  ,Neutral
A wordpair classification is used to formulate semantic dependency parsing as in Zhao and Kit 2008 ,Neutral
The algorithm has been applied to partofspeech tagging Padr6 1996 and to shallow parsing Voutilainen and Padro 1997  ,Neutral
154 2 Translation Models 21 Standard Phrasebased Model Most phrasebased translation models  rely on a preexisting set of wordbased alignments from which they induce their parameters ,Neutral
As the first step in the process an existing program MetaMap Aronson et al 1994 attempts to map each simple noun phrase to a concept in the UMLS Metathesaurus,Neutral
In fact we experienced the underflow problem in preliminary experiments with the EDR corpus ,Negative
Syntactic similarity may be operationalized in many ways for example by taking account a hierarchy of grammatical relations Keenan and Comrie 1977 ,Neutral
One important application of bitext maps is the construction of translation lexicons  and  as discussed  translation lexicons are an important information source for bitext mapping This method is described hereafter  while the subsequent steps  that use deeper  rulebased  levels of knowledge  are implemented into the ARIOSTO_LEX lexical learning system  described in  ,Neutral
Probabilistic generative models like IBM 15 Brown et al 1993 HMM Vogel et al 1996 ITG Wu 1997 and LEAF Fraser and Marcu 2007 define formulas for Pf  e or Pe f with okvoon ororok sprok atvoon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat okdrubel okvoon anok plok sprok atdrubel atvoon pippat rrat dat okvoon anok drok brok jok atvoon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok okyurp totat nnat quat oloat atyurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1 Word alignment exercise Knight 1997,Neutral
 noted that the unigram unpredictable might have a positive sentiment in a movie review  eg unpredictable plot   but could be negative in the review of an automobile  eg unpredictable steering  ,Neutral
The usual recall and precision metrics  eg  how many of the interesting bits of information were detected  and how many of the found bits were actually correct  require either a test corpus previously annotated with the required information  or manual evaluation  ,Neutral
Functional representation of phrases and clauses has been introduced to facilitate expressing syntactic generMisations The representation is introduced in Voutilainen and Tapanainen 1993 Voutilainen 1994 here only the main characteristics are given ,Positive
32 Rare Word Accuracy For these experiments  we use the Wall Street Journal portion of the Penn Treebank  ,Neutral
 From machine learning viewpoint Vapnik 1995 it is computationally infeasible to explicitly generate features involving structured information in many NLP applications ,Neutral
The previous studies  with the exception of   used smaller gazetteers than ours ,Neutral
Since we are using a larger corpus than Padó et al 2007 who train on the BNC a fairer comparison might be the one with our alternative models that are all outperformed by DM by a large margin ,Negative
The results are quite promising  our extraction method discovered 89  of the WordNet cousins  and the sense partitions in our lexicon yielded better values  than arbitrary sense groupings on the agreement data ,Neutral
In this paper we attempt to bring some clarity to the situation by taking a closer look at one of these existing methods Specifically we cast the popular technique of cube pruning Chiang 2007 in the wellunderstood terms of heuristic search Pearl 1984 ,Neutral
Semantic interpretation is based on a categorical analysis that is underspecified in that it is a partial parse cf McDonald 1992  ,Neutral
This learning approach has also been applied to a numberof other tasks including prepositional phrase attachment disambiguation Brill andResnik 1994 bracketing text Brill 1993a and labeling nonterminal nodes Brill 1993c,Neutral
Workshop Towards GenreEnabled Search Engines booktitle pages 13  20 pages editor In G Rehm and M Santini  editors editor contexts context ork on an intradocument  or page segment level because a single document can contain instances of multiple genres  eg  contact information  list of publications  CV  see  ,Neutral
The differences were jointly examined by the judges to see whether they were caused by inattention or by a genuine difference of opinion that could not be resolved by consulting the documentation that outlines the principles adopted for this grammatical representation for the most part documented in Karlsson et al 1994  ,Neutral
However the difficulty of such tasks and the fact that they are apparently unrelated has led to the development of largely adhoc solutions tuned to specific challenges ,Negative
The second step uses this intermediate representation to generate a summary Sparck Jones 1993 ,Neutral
 tried a different generative phrase translation model analogous to IBM wordtranslation Model 3   and again found that the standard model outperformed their generative model The automatically generated patterns in PairClass are slightly more general than the patterns of  ,Negative
 focus on alignment and do not present MT results  while May and Knight  2007  takesthesyntacticrealignmentasaninputtoanEM algorithm where the unaligned target words are insertedintothetemplatesandminimumtemplatesare combinedintobiggertemplates  ,Negative
Throughout  the likelihood ratio  is used as significance measure because of its stable performance in various evaluations  yet many more measures are possible ,Positive
For English  we have used sections 0306 of the WSJ portion of the Penn Treebank  distributed by the Linguistic Data Consortium  LDC   which have frequently been used to evaluate sentence boundary detection systems before  compare Section 7 ,Neutral
To transform the problem into a classification task we use the IOB2 classification scheme Tjong Kim Sang and Veenstra 1999  ,Neutral
Although it is decidable whether a function is subsequential or not Choffrut 1977the determinization algorithm described in the previous section does not terminatewhen run on a nonsubsequential function,Neutral
In addition to the handcrafted models listed above researchers have built stochastic plan recognition models for interaction including ones based on Hidden Markov Models Bui 2003 Blaylock and Allen 2006 and on probabilistic contextfree grammars Alexandersson and Reithinger 1997 Pynadath and Wellman 2000 ,Positive
Closer in spirit is AI research on learning vocabulary items by connecting user vocabulary to the agents perceptual representations at the time  of utterance Oates et al 2000 Roy and Pentland 2002 Cohen et al 2002 Yu and Ballard 2004 Steels and Belpaeme 2005,Neutral
Note that in this work we have decided to evaluate the predicted structure against the true structure a hard evaluation measure in future work we will assess the abstracts with a set of quality questions similar to those put forward by the Document Understanding Conference Evaluations also in a way similar to Kan and McKeown 2002 who evaluated their abstracts in a retrieval environment,Positive
Brill and Marcus 1992a have shown that the effort necessary to construct the partofspeech lexicon can be considerably re duced by combining learning procedures and a partial partofspeech categorization elicited from an informant ,Positive
We build a subset S C   incrementally by iterating to adjoin a feature f E   which maximizes loglikelihood of the model to S This algorithm is called the Basic Feature Selection  ,Neutral
FrameNet role annotations were mapped onto those dependency graph nodes that corresponded most closely to the annotated substring see Frstenau 2008 for a detailed description of the mapping algorithm ,Neutral
Joshua is a hierarchical parsingbased MT system and it can be instructed to produce derivation trees instead of the candidate sentence string Itself ,Positive
This difference was highlighted in the 3http    w3msivxusejhamaltparser  studyof   whichshowed that the difference is reflected directly in the error distributions of the parsers ,Neutral
For the task of dialogue act disambiguation Samuel Carberry and VijayShanker 1999 suggest a method of automatically finding cue phrases for disambiguation ,Positive
The system described in  also makes use of syntactic heuristics ,Neutral
The parameters weights θj can be estimated efficiently by maximizing the regularized conditional likelihood of a training corpus Johnson et al 1999 van Noord and Malouf 2005,Neutral
In addition to the widely used BLEU  and NIST  scores  we also evaluate translation quality with the recently proposed Meteor  and four editdistance style metrics  Word Error Rate  WER   Positionindependent word Error Rate  PER    CDER  which allows block reordering   and Translation Edit Rate  TER   ,Positive
Ayan and Dorr 2006 showed that under certain conditions this constraint could have significant impact on system performance ,Neutral
A simpler machine learning approach using only word frequency information and no other features as typically used in tasks like text classification could have been employed and indeed Nanba and Okumura [1999] do so for classifying citation Contexts ,Positive
Lexicalization can increase parsing performance dramatically for English   and the lexicalized model proposed by Collins  997  has been successfully applied to Czech  and Chinese  ,Positive
The results of Titov et al 2009 that use the similar joint learning technique as Henderson et al 2008 are also Included  ,Neutral
Such a method alleviates the problem of creating templates from examples which would be used in an ulterior phase of generation  The variance semiring is essential for many interesting training paradigms such as deterministic 40 annealing   minimum risk   active and semisupervised learning  ,Positive
Several studies have demonstrated that for instance Statistical Machine Translation  SMT  benefits from incorporating a dedicated WSD module  In our experiments  we have used Averaged Perceptron  and Perceptron with margin  to improve performance In 2004  Conroy  tested Maximal Marginal Relevance  as well as QR decomposition ,Positive
The 746 final accuracy on apartments is higher than any result obtained by Haghighi and Klein 2006 the highest is 741 higher than the supervised HMM results reported by Grenager et al 2005 744 and matches the results of Mann and Mc Callum 2008 with GE with more accurate sampled label distributions and 10 labeled examples,Negative
If we want to compare the performance of disambiguation models we can employ the φ mesure van Noord and Malouf 2005 van Noord 2007 Intuitively it tells us how much of the disambiguation problem has been solved ,Positive
Researchers extracted opinions from words  sentences  and documents  and both rulebased and statistical models are investigated  ,Neutral
SmadjaFrank1993The experimental results in  show a negative impact on the parsing accuracy from too long dependency relation k   P  A  P  E   3  1P  E   suggests that the units over which the kappa statistic is computed affects the outcome ,Neutral
The recent approach for editing extracted text spans Jing and McKeown 2000 may also produce improvement for our algorithm,Positive
The fstructures are created automatically by annotating nodes in the gold standard WSJ trees with LFG functional equations and then passing these equations through a constraint solver  ,Neutral
Typical examples of linguistically sophisticated annotation include tagging words with their syntactic category  although this has not been found to be effective for R   lemma of the word  eg  corpus  for  corpora    phrasal information  eg identifying noun groups and phrases    and subjectpredicate identification  ,Positive
 First previous methods for computing lexical chains have either been manual Morris and Hirst 1991 or automated but with exponential efficiency Hirst and StOnge 1997 Barzilay and Elhadad 1997 Because of this computing lexical chains for documents of any reasonable size has been impossible ,Negative
In order to overcome this problem  we look to the bootstrapping method outlined in  Much later work  relies on the use of extremely large corpora which allow very precise  but sparse features ,Positive
Since the SIR system Raphael 1968 some have felt that automatic information management could best be addressed using semantic information Subsequent research Schank 1975 Wilks 1976 expanded this paradigm More recently a number of examples of knowledgebased applications show considerable promise ,Neutral
The tagger described in this paper is based on the standard Hidden Markov Model architecture  ,Neutral
Usual tagging algorithms are either n  g r a m oriented such as Viterbi algorithm Viterbi 1967 or a d  hoc for every case when they must deal with more complex information,Neutral
Rath Resnick and Savage 1961 report that six participants agreed on only 8 of 20 sentences they were asked to select out of short Scientific American texts and that five agreed on 32 of the Sentences ,Neutral
Citations may vary in many dimensions for example they can be central or perfunctory positive or negative ie critical apart from scientific reasons there is also a host of social reasons for citing politeness tradition piety Ziman 1969  ,Neutral
The formalization of this notion and an algorithm for computing the composed transducer are wellknown and are described originally by Elgot and Mezei 1965,Positive
The approach is able to achieve 94  precision and recall for base NPs derived from the Penn Treebank Wall Street Journal  ,Neutral
In particular  previous work  has investigated the use of Markov random fields  MRFs  or loglinear models as probabilistic models with global features for parsing and other NLP tasks ,Neutral
The English Constraint Grammar Parser ENGCG  Voutilainen et al 1992 Karlsson el al 1994 is based on Constraint Grammar a parsing framework proposed by Fred Karlsson 1990 ,Neutral
Also the chunkbased model is representationally inadequate for centerembedded nestings of subtasks which do occur in our domain although less frequently than the more prevalent “tailrecursive” structures,Negative
More recently Cutting et al 1992 suggest that training can be achieved with a minimal lexicon and a limited amount of a priori information about probabilities by using an BaumWelch restimation to automatically refine the model ,Neutral
There are only a few successful studies  such as  for chunking and  on constituency parsing ,Positive
Second  it can be applied to control the quality of parallel bilingual sentences mined from the Web  which are critical sources for a wide range of applications  such as statistical machine translation  and crosslingual information retrieval  ,Neutral
At this point the importance of the underlying argument  is emphasised cf Lehmaim et al 1996 Marcus et al 1994 Sampson 1995 ,Neutral
If entropy Hp is large eg small γ the Bayes risk Thus Smith and Eisner 2006 try to avoid local minima by starting with large Hp and decreasing it gradually during optimization ,Positive
The most widely used singlewordbased statistical alignment models  SAMs  have been proposed in  Decision lists have already been successfully applied to lexical ambiguity resolution by  ,Positive
They are not used in LN  but they are known to be useful for WSD  ,Neutral
Chanod and Tapanainen 1995 compare two tagging frameworks for tagging French one that is statistical built upon the Xerox tagger Cutting et al 1992 and another based on linguistic constraints only ,Neutral
In addition M L indicates a baseline model conraining no constraints this will result in a most likely tagger and H M M stands for a hidden Markov model bigram tagger Elworthy 1992 ,Neutral
The concatenation of left and right context vector can therefore serve as a representation of a word's distributional behavior Finch and Chater 1992 Schuitze 1993 ,Neutral
Previous research has addressed revision in singledocument summaries   and has suggested that revising summaries can make them more informative and correct errors ,Neutral
For the current work  the Loglikelihood coefficient has been employed   as it is reported to perform well among other scoring methods  ,Positive
Statistical significance is computed using the bootstrap resampling method proposed by Koehn 2004  ,Positive
A much more efficient approach usually is the traditional insideoutside algorithm Baker 1979 ,Positive
44 Experiment 2   s Words We also conducted translation on seven of the twelve English words studied in   ,Neutral
We used the GENIA dataset Kim et al 2003 as the source for knowledge extraction ,Positive
We generate the boundary word features from the extracted reordering instances in the same way as discussed in Xiong et al 2006 and use Zhangs MaxEnt Tools 2 to train a reordering model for the 2 nd baseline system,Positive
We could use partly disambiguated text eg the output of parsers D1 D2 or D3 and disambiguate the result using a knowledgebased syntactic parser see experiments in Vou tilainen and Tapanainen 1993  ,Neutral
The parser is coupled with an online averaged perceptron  as the learning method ,Neutral
Accounting for sparsity explicitly has achieved significant improvements in other areas such as in part of speech tagging Goldwater and Griffiths 2007,Neutral
Currently most of the POS tagger accuracy reports are based on the experiments involving Penn Treebank data Marcus 1993 ,Neutral
Automatic evaluation methods such as BLEU   RED   or the weighted Ngram model proposed here may be more consistent in judging quality as compared to human evaluators  but human judgments remain the only criteria for metaevaluating the automatic methods For comparison purposes  we revisit  fullygenerative Bayesian model for unsupervised coreference resolution  discuss its potential weaknesses and consequently propose three modifications to their model ,Negative
Much of the work in subjectivity analysis has been applied to English data  though work on other languages is growing  eg  Japanese data are used in   Chinese data are used in   and German data are used in  ,Neutral
Unsupervised methods have been developed for WSD  but despite modest success have not always been well understood statistically  In addition  uniform conditioning on mother grammatical function is more general than the casephenomena specific generation grammar transform of   in that it applies to each and every subpart of a recursive input fstructure driving generation  making available relevant generation history  context  to guide local generation decisions ,Negative
Although the Kappa coefficient has a number of advantages over percentage agreement  eg  it takes into account the expected chance interrater agreement  see  for details   we also report percentage agreement as it allows us to compare straightforwardly the human performance and the automatic methods described below  whose performance will also be reported in terms of percentage agreement ,Positive
Among these methods  CRFs is the most common technique used in NLP and has been successfully applied to PartofSpeech Tagging   NamedEntity Recognition  and shallow parsing  Wikipedia first sentence  WikiFS    used Wikipedia as an external knowledge to improve Named Entity Recognition ,Positive
A decisiontree learning approach to feature selection is used in this experiment Cardie 1993b 1994 to discard irrelevant Features ,Neutral
The first two of these scores are produced by Simfinder and the salience score iscomputed using lexical chains Morris and Hirst 1991 Barzilay and Elhadad 1997 asdescribed below,Neutral
Before proceeding further with the main argument consider three very recent hybrids – systems that employ linguistic rules for resolving some of the ambiguities before using automatically generated corpusbased information collocation matrices Leech Garside and Bryant 1994 Hidden Markov Models Tapanainen and Voutilai nen 1994 or syntactic patterns Tapanainen and Jairvinen 1994   ,Positive
Some methods which can offer powerful reordering policies have been proposed like syntax based machine translation  and Inversion Transduction Grammar  ,Positive
Our approximate rules are similar to the ones proposed by Mikheev 1997 who uses a dictionary to build POS prediction rules with four parts ,Positive
Yarowsky has proposed an algorithm that requires as little user input as one seed word per sense to start the training process ,Positive
[Mani et al 1999] employed rules such as the referencing of pronouns with the most recently mentioned noun phrase However this might be inappropriate in MDS where the use of multiple documents increases the number of possible entities with which an anaphor could be referenced ,Negative
More powerful compression models may draw on existing NLG methods for text revision Inui et al 1992 to accommodate full Paraphrasing ,Neutral
4 Features For our experiments we use the features proposed  motivated and described in detail by  ,Neutral
Unfortunately  there is no straightforward generalization of the method of  to the two edge marginal problem Previous literature on GB parsing  Wehrli  1984  Sharp  1985    1986  Kuhns  1986  Abney  1986has not addressed the issue of implementation of the Binding theory  The present paper intends in part to fill this gap ,Negative
6 The Experimental Results We used the Penn Treebank  to perform empirical experiments on this parsing model ,Neutral
We held out 300 sentences for minimum error rate training MERT Och 2003 and optimised the parameters of the feature functions of the decoder for each experimental run,Neutral
Compared to learning rulebased approaches such as the one by Brill 1992 a knn approach provides a uniform approach for all disambiguation tasks more flexibility in the engineering of case representations and a more elegant approach to handling of unknown words see eg Cardie 1994 ,Negative
However theseapproaches operate in a limited domain eg terrorist events where information extraction systems can be used to interpret the source text,Negative
Besides the the casesensitive BLEU4 Papineni et al 2002 used in the two experiments we design another evaluation metrics Reordering Accuracy RAcc for forced decoding evaluation ,Neutral
Within MT there has been a variety of approaches dealing with domain adaption for example Wu et al 2008 Koehn and Schroeder 2007,Neutral
In an interesting analysis of phrasebased and hierarchical translation Zollmann et al 2008 forced a phrasebased system to produce the translations generated by a hierarchical system Unfortunately their analysis is incomplete they do not perform the analysis in both directions,Negative
The four sets of outputs from the parser were then translated into Swedish by the SLT transfer and generation mechanism Agn et al 1994 ,Neutral
We used a 5gram language model with modified KneserNey smoothing trained on the bitexts English using SRILM Stolcke 2002 ,Positive
To address this we explicitly generate beginning and end sentence markers as part of the translation process as sug gested by Xiong et al 2008 The results of doing this are shown in Table 2 ,Positive
Traditional 3gram and 5gram string LMs were trained on the English side of the parallel data plus the English Gigaword corpus V30 in a way described in Bulyko et al 2007 ,Neutral
In addition to sentence fusion  compression algorithms  and methods for expansion of a multiparallel corpus  are other instances of such methods ,Neutral
An important contribution to interactive CAT technology was carried out around the TransType  TT  project  ,Positive
294 Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation 22 Measuring Translation Performance Changes Caused By Alignment In phrasedbased SMT  the knowledge sources which vary with the word alignment are the phrase translation lexicon  which maps source phrases to target phrases using counts from the word alignment  and some of the word level translation parameters  sometimes called lexical smoothing  ,Neutral
Thanks to the nice property of kernelbasedmachine learning method that can implicitly explore structured features in a high dimensional feature space Vapnik 1995 in this paper we propose using convolution tree kernel Haussler 1999 Collins and Duffy 2001 to explore the structured syntactic knowledge for phrase reordering and further combine the tree kernel with other diverse linear features into a composite kernel to strengthen the models predictive ability,Positive
With the indepth study of opinion mining  researchers committed their efforts for more accurate results  the research of sentiment summarization   domain transfer problem of the sentiment analysis  and finegrained opinion mining  are the main branches of the research of opinion mining ,Positive
 Paice and Jones 1993 report that they abandoned an informal sentence selection experiment in which they used agriculture articles and experts in the field as participants as the participants were too strongly influenced by their personal research interest  ,Neutral
The candidates of unknown words can be generated by heuristic rules  or statistical word models which predict the probabilities for any strings to be unknown words  ,Neutral
System combination for machine translation MT has emerged as a powerful method of combining the strengths of multiple MT systems and achieving results which surpass those of each individual system eg Bangalore et al 2001 Matusov et al 2006 Rosti et al2007a,Neutral
This text was partofspeech tagged using the Xerox  tagger  Cutting et al 1992 ,Neutral
Given this  the mutual information ratio  is expressed by Formula 1 ,Neutral
We have used a toolkit developed at AT&T Bell Laboratories Pereira et al 1994 which manipulates weighted and unweighted finitestate machines acceptors or transducers ,Neutral
The concept of support verb was broadly used Toutanova et al 2005 Xue 2006 Jiang and Ng 2006 4  we here extend it to nouns and prepositions ,Neutral
In many disciplines particularly the experimental sciences this problemsolution structure has been crystallized in a fixed presentation of the scientific material as introduction method result and discussion van Dijk 1980 ,Neutral
In this paper we generalize the proposals of Pennacchiotti et al 2008 and Fürstenau and Lapata 2009 in a unified framework,Positive
Semisupervised learning has been suggested by many researchers as a solution to the annotation bottleneck see Chapelle et al 2006 Zhu 2005 for an overview and has been applied successfully on a number of natural language processing tasks ,Neutral
 and Chan et al Based on these grammars  a great number of SMT models have been recently proposed  including stringtostring model  Synchronous FSG    treetostring model  TSGstring    stringtotree model  stringCFGTSG    treetotree model  Synchronous CFGTSG  DataOriented Translation   and so on ,Neutral
We distinguish two main approaches to domain adaptation that have been addressed in the literature Daum III 2007 supervised and semisupervised ,Neutral
Unlike wellknown bootstrapping approaches   EM and CE have the possible advantage of maintaining posteriors over hidden labels  or structure  throughout learning  bootstrapping either chooses  for each example  a single label  or remains completely agnostic ,Negative
Thus corpusbased approaches may have serious difficulties in capturing these relations Havasi et al 2007 but there are reasons to believe that they could still be useful Eslick 2006 uses the assertions of ConceptNet as seeds to parse Web search results and augment ConceptNet by new candidate relations,Positive
The summation over target word sequences and alignments given fixed t bears a resemblance to the inside algorithm except that the tree structure is fixed Pereira and Schabes 1992,Neutral
We use the MaxEntbased BTG translation system Xiong et al 2006 as our baseline It is a phrasebased SMT system with BTG reordering constraint,Positive
As   we adopted an evaluation of mutual information as a cohesion measure of each cooccurrence ,Neutral
The form of the maximum entropy probability model is identical to the one used in where wi ranges over V t3 stop ,Neutral
Chiang  distinguishes statistical MT approaches that are syntactic in a formal sense  going beyond the nitestate underpinnings of phrasebased models  from approaches that are syntactic in a linguistic sense  ie taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking1 The two forms of syntactic modeling are doubly dissociable  current research frameworks include systems that are nite state but informed by linguistic annotation prior to training  eg     and also include systems employing contextfree models trained on parallel text without bene t of any prior linguistic analysis  eg ,Neutral
The myPlain model implements a classic flat cooccurrence approach Sahlgren 2006 in which we keep track of verbtonoun cooccurrence within a window that can include maximally one intervening noun and nountonoun cooccurrence with no more than 2 intervening nouns ,Neutral
They have been successfully applied in several tasks  such as information retrieval  and harvesting thesauri  showed that the results for FrenchEnglish were competitive to stateoftheart alignment systems ,Positive
We use GIZA    to do mton wordalignment and adopt heuristic growdiagfinaland to do refinement ,Neutral
orgpubscitations  j ournalstoms1986 12 2  p154meht a  Mutual Information Given the definition of Mutual Informationwe consider the distribution of a window word according to the contingency table  a  in Table 4 ,Neutral
Cohesion information has been used in rhetoricalbased parsing for summarization Marcu 1997 in order to decide between list or elaboration relations and also in content selection for summarization Barzilay and Elhadad 1997,Neutral
These feature vectors and the associated parser actions are used to train maximum entropy models  ,Neutral
We use a standard maximum entropy classifier  implemented as part of MALLET  8412 only PTB  baseline  8358 1st  8342 2nd  8338 3rd  8308 third row lists the three highest scores of the domain adaptation track of the CoNLL 2007 shared task ,Neutral
When a dialogue act is preceded by tacit actions in an interpretation the speaker of the utterance implicates that the earlier tacit actions have taken place DeVault 2008,Neutral
Probably the most widely used feature weighting function is pointwise Mutual Information MI Church and Patrick 990 Hindle 990 Luk 995 Lin 998 Gauch Wang and Rachakonda 999 Dagan 2000 Baroni and Vegnaduzzo 2004 Chklovski and Pantel 2004 Pantel and Ravichandran 2004 Pantel Ravichandran and Hovy 2004 Weeds Weir and McCarthy 2004 dened by weight MI wflog 2 Pwf PwPf,Positive
In addition  the semisupervised Morce performs  on single CPU and development data set  77 times faster than the combination and 23 times faster than  In a recent study by   nonlocal information is encoded using an independence model  and the inference is performed by Gibbs sampling  which enables us to use a stateoftheart factored model and carry out training efficiently  but inference still incurs a considerable computational cost  suggests use of an approximation summing over the training data  which does not sum over possible tags   h E f j  2 P    p  ti l hi  f j  hi  ti  i  1 However  we believe this passage is in error  such an estimate is ineffective in the iterative scaling algorithm ,Negative
For the chunk part of the code  we adopt the Inside  Outside  and Between  IOB  encoding originating from  ,Neutral
2 Related Work There has been a large and diverse body of research in opinion mining  with most research at the text   sentence  or word  level ,Neutral
An especially wellfounded framework for doing this is maximum entropy  Stateofart systems for doing word alignment use generative models like GIZA    ,Positive
This provides a compelling advantage over previous dependency language models for MT   whichusea5gramLMonlyduringreranking Experimental results indicate that our model outperforms  coreference model by a large margin on the ACE data sets and compares favorably to a modified version of their model ,Negative
 Baxendale 1958 extracted all words except those on the stop list from the title and the headlines and determined for each sentence whether or not it contained these words  ,Neutral
Goldsmith 2001 performs a minimum description length analysis of the mophology of several European languages using coPora,Neutral
The IWSLT and WMT workshops also have a manual evaluation component as does the NIST Evaluation in the form of adequacy and fluency LDC 2005 ,Neutral
Simple as it seems the mentionpair model has been shown to work well Soon et al 2001 Ng and Cardie 2002 ,Neutral
A wellknown remedy for this problem is to use r + 05R + 1 as the estimate of PT = lie  Robertson and Sparck Jones 1976 While various smoothing methods Church and Gale 1991 Jelinek 1990 are also applicable to these situations and would be expected to work better we used the simple add one remedy in the following experiments,Positive
We use the default configuration of the measure in WordNet   Similarity012 package   and  with a single exception  the measure performed below Gic  see BP in table 1 ,Neutral
2 The BLEU Metric The metric most often used with MERT is BLEU   where the score of a candidate c against a reference translation r is  BLEU  BP  len  c   len  r   exp  4summationdisplay n    4 logpn   where pn is the ngram precision2 and BP is a brevity penalty meant to penalize short outputs  to discourage improving precision at the expense of recall ,Positive
Experience gained from the development of the Penn Treebank Marcus et al 1994 has shown that automatic annotation is useful only if it is absolutely correct while wrong analyses are often difficult to detect and their correction can be timeconsuming ,Positive
For Chinese Sproat et al 1994 used the word unigram model in their word segmenter based on weighted finitestate transducer  ,Neutral
791 and score the alignment template models phrases  ,Neutral
First  we compared our system output to human reference translations using Bleu   a widelyaccepted objective metric for evaluation of machine translations ,Positive
In Yarowsky s experiment   an average of 3936 examples were used to disambiguate between two senses ,Neutral
Supposeone wants to encode the sample dictionary of Figure 9 The algorithm as described byRevuz 1991 consists of first building a tree whose branches are labeled by letters andwhose leaves are labeled by a list of tags such as nn vb and then minimizing it intoa directed acyclic graph DAG,Neutral
The only other texttotext generation approach able to produce new utterances isthat of Pang Knight and Marcu 2003,Positive
Our experiments on the Canadian Hansards show that our unsupervised technique is significantly more effective than picking seeds by hand   which in turn is known to rival supervised methods 2 Related Work One of the major problems with the IBM models  and the HMM models  is that they are restricted to the alignment of each sourcelanguage word to at most one targetlanguage word This implies that the complexity of structure divergence between two languages is higher than suggested in literature   report better perplexity results on the Verbmobil Corpus with their HMMbased alignment model in comparison to Model 2 of  ,Negative
1 Introduction Over the past decade  researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation  ,Neutral
This formulation of the task provided the basis for the noisychannel en decision tree based algorithms presented in Knight and Marcu 2002 and for virtually all followup work on datadriven sentence compression Le and Horiguchi 2003 Vandeghinste and Pan 2004 Turner and Charniak 2005 Clarke and Lapata 2006 Zajic et al 2007 Clarke and Lapata 2008  ,Neutral
Similar to  eg    we use a Naive Bayes algorithm trained on word features cooccurring with the subjective and the objective classifications ,Neutral
They find that querying certain features outperforms querying uncertain features but this is likely because their query selection method is similar to the expectation uncertainty method described above and consequently nondiscriminative features may be queried often It is also not clear how this graphbased training method would generalize to structured output spaces ,Negative
Others perform the division implicitly without discussing performance eg Cutting et al 1992 ,Negative
The SPECIALIST parser is based on the notion of barrier words Tersmette et al 1988 which indicate boundaries between phrases ,Neutral
The graph G is called the text relationship map of D Salton et al 1999 ,Neutral
Nakagawa  and   also showed the effectiveness of global features in improving the accuracy of graphbased parsing  using the approximate Gibbs sampling method and a reranking approach  respectively ,Neutral
1 Introduction Many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts  ,Neutral
Recently  an elegant approach to inference in discourse interpretation has been developed at a number of sites   all based on tim notion of abduction  and we have begun to explore its potential application to machine translation ,Positive
2 Data 21 The US Congressional Speech Corpus The text used in the experiments is from the United States Congressional Speech corpus   which is an XML formatted version of the electronic United States Congressional Record from the Library of Congress1 ,Neutral
This encoding was previously used for incremental sentence parsing by Costa et al 2001 ,Positive
We solve SAT analogies with a simplified version of the method of  ,Neutral
As reported in   parameter averaging can effectively avoid overfitting Several representations to encode region information are proposed and examined  ,Neutral
We conclude with some challenges that still remain in applying proactive learning for MT 2 Syntax Based Machine Translation In recent years  corpus based approaches to machine translation have become predominant  with Phrase Based Statistical Machine Translation  PBSMT   being the most actively progressing area  ,Positive
Lexical chains—sequences of semantically related words—are tightly connected tothe lexical cohesive structure of the text and have been shown to be useful for determining which sentences are important for singledocument summarization Barzilay andElhadad 1997 Silber and McCoy 2002,Neutral
Moreover  under this view  SMT becomes quite similar to sequential natural language annotation problems such as partofspeech tagging and shallow parsing  and the novel training algorithm presented in this paper is actually most similar to work on training algorithms presented for these task  eg the online training algorithm presented in  and the perceptron training algorithm presented in  Feature weights vector are trained discriminatively in concert with the language model weight to maximize the BLEU  automatic evaluation metric via Minimum Error Rate Training  MERT   ,Neutral
Discriminative models have been found to outperform generative models for many different tasks including SRL Lim et al 2004 For this reason we also employ discriminative models here ,Negative
Much work in computational linguistics and related fields relies on measuring similarity among wordsconcepts in terms of their patterns of cooccurrence with other wordsconcepts Sahlgren 2006 ,Neutral
They are also used for inducing alignments  In recent work   proposed a general framework for including morphological features in a phrasebased SMT system by factoring the representation of words into a vector of morphological features and allowing a phrasebased MT system to work on any of the factored representations  which is implemented in the Moses system ,Neutral
This hypothesized relationship between distributional similarity and semantic similarity has given rise to a large body of work on automatic thesaurus generation  ,Neutral
The literature on relational similarity on the other hand has focused on pairs of words devising various methods to compare how similar the contexts in which target pairs appear are to the contexts of other pairs that instantiate a relation of interest Turney 2006 Pantel and Pennacchiotti 2006 ,Neutral
Compared to previous approaches Raghavan and Allan 2007 our method can be used for both classification and structured tasks and the feature query selection methods we propose perform better ,Negative
 has described an efficient exact one dimensional accuracy maximization technique for a similar search problem in machine translation While these are based on a relatively few number of items and while we have not performed any tests to determine whether the differences in ,Positive
This is similartothegraphconstructionmethodof  and Rao et al We distinguish two main approaches to domain adaptation that have been addressed in the literature   supervised and semisupervised ,Neutral
In this paper we apply the expectation semiring Eisner 2002 to a hypergraph or packed forest rather than just a lattice ,Neutral
Recently Salton et al 1999 have developed a model for representing a document by using undirected graphs  ,Neutral
When the training text is adequate to estimate the tagger parameters moreefficient stochastic taggers Dermatas and Kokkinakis 1994 Maltese and Mancini 1991Weischedel et al 1993 and training methods can be implemented Merialdo 1994,Neutral
 have implemented a dependency parser with good accuracy  it is almost as good at dependency parsing as Charniak   and very impressive speed  it is about ten times faster than  and four times faster than Charniak   Unlike minimum error rate training   our system is able to exploit large numbers of specific features in the same manner as static reranking systems  ,Negative
According to this model  when translating a stringf in the source language into the target language  a string e is chosen out of all target language strings e if it has the maximal probability given f   e  arg maxe LCB Pr  e f  RCB  arg maxe LCB Pr  f e  Pr  e  RCB where Pr  f e  is the translation model and Pr  e  is the target language model ,Neutral
Consider for example a word like cover as discussed by Voutilainen Karlsson et al 1995 in the Brown and the LOB Corpus Johansson 1980 the word cover is a noun 40 of the occurrences and a verb 60 of the other but in the context of a car maintenance manual it is a noun 1000 of the time ,Neutral
the method of handling unknown words that seems to work best for inflected languages is a suffix analysis as proposed in Samuelsson 1993 Tag probabilities are set according to the words endIng,Positive
The algorithm is again described by Cutting et al and by Sharman and a mathematical justification for it can be tbund in Huang et al 1990 ,Positive
We also tried combining the tuggers using first the rules and then the statistics a similar approach was also used in Tapanainen and Vouti Lainen 1994 ,Positive
 Introduction State of the art Statistical Machine Translation  SMT  systems usually adopt a twopass search strategy  as shown in Figure  ,Positive
A hypergraph or packed forest Gallo et al 1993 Klein and Manning 2004 Huang and Chiang 2005 is a compact data structure that uses structuresharing to represent exponentially many trees in polynomial space ,Neutral
For the test data we created a lattice of every possible segmentation of any word 6 characters or longer and used forwardbackward pruning to prune out low probability segmentation paths Sixtus and Ortmanns 1999,Neutral
Intuitively AUC is the probability that a randomly picked positive instances estimated posterior probability is higher than a randomly picked negative instances estimated posterior probability Fawcett 2006 ,Neutral
This fact along with the observation that machine translation quality improves as the amount of monolingual training material increases has lead to the introduction of randomised techniques for representing large LMs in small space Talbot and Osborne 2007 Talbot and Brants 2008,Neutral
This iterative optimiser  derived from a word disambiguation technique   finds the nearest local maximum in the lexical cooccurrence network from each concept seed This model is related to the averaged perceptron algorithm of  ,Neutral
The corpus based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by   Charniak  997  and Ratnaparkhi  997  ,Positive
While many text categorization models have been proposed so far in this paper we concentrate on the probabilistic models Robertson and Sparck Jones 1976 Kwok 1990 Fuhr 1989 Lewis 1992 Croft 1981 Wong and Yao 1989 Yu et al 1989 because these models have solid formal grounding in probability theory ,Positive
This increase of probabilities is defined as multiplicative change  N  as follows   NPE Tprime   PET   2  The main innovation of the model in  is the possibility of adding at each step the best relation NRij  as well as NIRij  that is Rij with all the relations by the existing taxonomy However  as also pointed out by   this observation does not hold uniformly over all possible cooccurrences of two words ,Positive
In order to push further this ruleextraction approach and according to our previous work Dugast et al 2007 Dugast et al 2008 the most promising would probably be the use of alternative meanings and a language model to decode the best translation in such a lattice ,Positive
The learning corpus can consist of plain text but the best results seem achievable with annotated corpora Merialdo 1994 Elworthy 1994 ,Positive
In a statistical translation model trimming of the phrase table had been shown to be beneficial Johnson et al 2007 ,Neutral
The optimum tag tio is estimated using the probabilities of the forwardbackwardalgorithm Rabiner 1989,Neutral
In this formulation the tagger searches for the best sequence that maximizes the probability Nagata 1994  ,Neutral
The quality of humanproduced abstractshas been examined in the literature Grant 1992 Kaplan et al 1994 Gibson 1993using linguistic criteria such as cohesion and coherence thematic structure sentencestructure and lexical density in automatic text summarization however such detailedanalysis is only just emerging,Neutral
 Introduction Chinese Word Segmentation  CWS  has been witnessed a prominent progress in the last three Bakeoffs      Of particular interest are lexicalized parsing models such as the ones developed by  and Carroll and Rooth  998  ,Positive
This similarity score is computed as a max over a number of component scoring functions some based on external lexical resources including  various string similarity functions of which most are applied to word lemmas  measures of synonymy hypernymy antonymy and semantic relatedness including a widelyused measure due to Jiang and Conrath 997,Positive
In terms of applying nonparametric Bayesian approaches to NLP   evaluated the clustering properties of DPMMs by performing anaphora resolution with good results ,Positive
Since we also adopt a linear scoring function in Equation 3 the feature weights of our combination model can also be tuned on a development a set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training MERT algorithm Och 2003 ,Positive
Introduction With the introduction of the BLEU metric for machine translation evaluation   the advantages of doing automatic evaluation for various NLP applications have become increasingly appreciated  they allow for faster implementevaluate cycles  by bypassing the human evaluation bottleneck   less variation in evaluation performance due to errors in human assessor judgment  and  not least  the possibility of hillclimbing on such metrics in order to improve system performance  ,Positive
The probabilities in equation 4 are estimated recursively for the first Rabiner1989 and secondorder HMM Watson and Chung 1992,Neutral
Because it is not feasible here to have humans judge the quality of many sets of translated data  we rely on an array of well known automatic evaluation measures to estimate translation quality  BLEU  is the geometric mean of the ngram precisions in the output with respect to a set of reference translations ,Positive
 For ex ample when applying their approach to a different domain with somewhat less rigid syntax Zettlemoyer and Collins 2007 need to introduce new combinators and new forms of candidate lexical Entries ,Negative
Parse selection constitutes an important part of many parsing systems  ,Neutral
We have to compare our probabilistic model to other non probabilistic models like decision treerule based models one of which has recently been reported to be promising Apt4 et al 1994 ,Positive
Probabilistic translation models generally seek to find the translation string e that maximizes the probability Pra5 ea6fa7  given the source string f  ,Neutral
Our work adds to a body of research learning deep models of language from evidence implicit in an agents interactions with its environment It shares much of its motivation with cotraining Blum and Mitchell 1998 in improving initial models by leveraging additional data that is easy to obtain ,Neutral
Our system outperforms competing approaches  including the standard machine translation alignment models  and the stateoftheart Cut and Paste summary alignment technique  For example  we would like to know that if a  JJ  JJ  7We also tried using word clusters  instead of POS but found that POS was more helpful ,Negative
21 Minimum Error Rate Training The predominant approach to reconciling the mismatch between the MAP decision rule and the evaluation metric has been to train the parameters of the exponential model to correlate the MAP choice with the maximum score as indicated by the evaluation metric on a development set with known references  ,Neutral
Some tasks can thrive on a nearly pure diet of unlabeled data  Machine Translation Experiments 4 Experimental Setting For our MT experiments  we used a reimplementation of Moses   a stateoftheart phrasebased system Motivation ,Positive
The details of the algorithm can be found in the literature for statistical translation models  such as  ,Neutral
We plan to explore how contextual effects can be modeled in our framework focusing in particular on how composition affects word meaning Erk and Padó 2008 ,Positive
We evaluate translation output using caseinsensitive BLEU Papineni et al 2001 as provided by NIST and METEOR Banerjee and Lavie 2005 version 06 with Porter stemming and WordNet synonym matching,Neutral
In Table 6 we report our results  together with the stateoftheart from the ACL wiki5 and the scores of   PairClass  and from Amac Herdagdelens PairSpace system  that was trained on ukWaC ,Neutral
Thus one conclusion from that line of work is that as soon as there is a reasonable often even small amount of labeled target data it is often more fruitful to either just use that or to apply simple adaptation techniques Daum III 2007 Plank and van Noord 2008 ,Neutral
In the first set of experiments we compare two settings of our UALIGN system with other aligners GIZA Union Och and Ney 2003 and LEAF with 2 iterations Fraser and Marcu 2007 ,Neutral
We analyzed a set of articles and identified six major operations that can be used for editing the extracted sentences including removing extraneous phrases from an extracted sentence combining a reduced sentence with other sentences syntactic transformation substituting phrases in an extracted sentence with their paraphrases substituting phrases with more general or specific descriptions and reordering the extracted sentences Jing and McKeown 1999 Jing and McKEown 2000 ,Neutral
Author summaries tend to be less systematic Rowley 1982 and more deep generated whereas sum maries by professional abstractors follow an internalized building plan Liddy 1991 and are often created through sentence extraction Lancaster 1998  ,Neutral
Previous results had shown a rather satisfying performance for hybrid systems such as the Statistical Phrasebased PostEditing SPE Simard et al 2007 combination in comparison with purely phrasebased statistical models reaching similar BLEU scores and often receiving better human judgement German to English at WMT2007 against the BLEU metric ,Positive
Radev 2000 defines twentyfour relationships such as equivalence subsumption and contradiction that might apply at various structural levels across documents ,Neutral
1 Introduction Current methods for largescale information extraction take advantage of unstructured text available from either Web documents  or  more recently  logs of Web search queries  to acquire useful knowledge with minimal supervision ,Neutral
Although the BLEU  score from Finnish to English is 218  the score in the reverse direction is reported as 130 which is one of the lowest scores in 11 European languages scores  ,Neutral
Our generation strategy is reminiscent of Robinand McKeowns 1996 earlier work on revision for summarization although Robin andMcKeown used a threetiered representation of each sentence including its semanticsand its deep and surface syntax all of which were used as triggers for revision,Positive
While the idea of exploiting multiple news reports for paraphrase acquisition is not new  previous efforts  have been restricted to at most two news sources Furthermore  we provide a 638  error reduction compared to IBM Model 4  ,Negative
Headlexicalized stochastic grammars have recently become increasingly popular  Unsupervised algorit ~ m ~ such as  have reported good accuracy that rivals that of supervised algorithms is one of the most famous work that discussed learning polarity from corpus ,Positive
A known weakness of MI is its tendency to assign high weights for rare features    and Basque   which pose quite different and in the end less severe problems  there have been attempts at solving this problem for some of the highly inflectional European languages  such as     Slovenian       Czech  and   five Central and Eastern European languages   ,Positive
Consequently  here we employ multiple references to evaluate MT systems like BLEU  and NIST  ,Neutral
Experimental tests M&rquez and Rodriguez 1995 have shown that the pruning process reduces tree sizes at about 50 and improves their accuracy in a 25 ,Neutral
In particular  the model in  failed to generate punctuation  a deficiency of the model 1 Introduction Phrasebased translation models   which go beyond the original IBM translation models  1 by modeling translations of phrases rather than individual words  have been suggested to be the stateoftheart in statistical machine translation by empirical evaluations ,Negative
If one of the two words is ε the posterior of aligning word ε to state j is computed as suggested by Liang et al 2006,Positive
METEOR uses the Porter stemmer and synonymmatching via WordNet to calculate recall and precision more accurately  ,Positive
Our approach is reminiscent of Luhns approach 1959 but uses the other term weighting technique instead of the term frequency Luhn suggested that the frequency of a word occurrence in a document as well as its relative position determines its significance in that document ,Positive
In this years shared task we evaluated a number of different automatic metrics  Bleu  Bleu remains the de facto standard in machine translation evaluation ,Neutral
The experimental results show that our method outperforms the synchronous binarization method  with over 08 BLEU scores on both NIST 2005 and NIST 2008 ChinesetoEnglish evaluation data sets  produced a corpus of 4000 questions annotated with syntactic trees  and obtained an improvement in parsing accuracy for Bikels reimplementation of the Collins parser  by training a new parser model with a combination of newspaper and question data ,Negative
The first major use of HMMs for part of speech tagging was in CLAWS Garside et al 1987 in the 1970s ,Positive
prime 1 1 1 05 05 1 05 05 01 01 01 00001 00001 01 00001 00001 Further  we ran each setting of each estimator at least 10 times  from randomly jittered initial starting points  for at least 1000 iterations  as  showed that some estimators require many iterations to converge ,Neutral
More recent work  has considered methods for speeding up the feature selection methods described in   Ratnaparkhi  1998   and Della Pietra  Della Pietra  and Lafferty  1 Introduction Currently  most of the phrasebased statistical machine translation  PBSMT  models  adopt full matching strategy for phrase translation  which means that a phrase pair  tildewidef  tildewidee  can be used for translating a source phrase f  only if tildewidef  f Due to lack of generalization ability  the full matching strategy has some limitations ,Negative
The acquisition methods range from supervisedinductivelearningfromexample algorithms Quinlan 1986 A h a et al 1991 to genetic algorithm strategiesLosee 1994 through the transformationbasederrordriven algorithm used in Brill 1995 Stillanother possibility are the hybrid models which tryto join the advantages of both approaches Voutilainen and Padr6 1997,Neutral
Other recent work looks at summarization as a process of revision in this work the source text is revised until a summary of the desired length is achieved Mani Gates and Bloedorn 1999 Additionally some research has explored cutting and pasting segments of text from the full document to generate a summary Jing and McKeown 2000 ,Positive
To speed our computations  we use the cube pruning method of  with a fixed beam size informationtheoretic similarity measure is commonly used in lexicon acquisition tasks and has demonstrated good performance in unsupervised WSD  ,Positive
We present and evaluate empirically statistical models for both mention detection and entity tracking problems For mention detection we use approaches based on Maximum Entropy MaxEnt henceforth Berger et al 1996 and Robust Risk Minimization RRM henceforth Zhang et al 2002 ,Neutral
We use the Stanford parser  with its default Chinese grammar  the GIZA    alignment package with its default settings  and the ME tool developed by  ,Neutral
Most empirical work in translation analyzes mod els and algorithms using BLEU Papineni et al 2002 and related metrics Though such metrics are useful as sanity checks in iterative system development they are less useful as analytial tools,Negative
Although the first three are particular cases where N  1 andor M  1  the distinction is relevant  because most wordbased translation models  eg IBM models   can typically not accommodate general MN alignments ,Negative
Incremental alignment methods have been proposed to relax the independence assumption of pairwise alignment Rosti et al 2008 Li et al 2009 Such methods align hypotheses to a partially constructed CN in some order,Neutral
Jing and McKeown 2000 and Jing 2000propose a cutandpaste strategy as a computational process of automatic abstractingand a sentence reduction strategy to produce concise sentences,Neutral
Document level sentiment classification is mostly applied to reviews  where systems assign a positive or negative sentiment for a whole review document  ,Neutral
Therefore whenever we have ac cess to a large amount of labeled data from some source outofdomain but we would like a model that performs well on some new target domain Gildea 2001 Daum III 2007 we face the problem of domain adaptation ,Neutral
Using a feature representation based also on WordNet they learn a classifier for each frame which decides whether an unseen word belongs to the frame or not Pennacchiotti et al 2008 create distributional profiles for frames,Neutral
The Penn Treebank  has until recently been the only such corpus  covering 45M words in a single genre of financial reporting ,Positive
For example  10 million words of the American National Corpus  will have manually corrected POS tags  a tenfold increase over the Penn Treebank   currently used for training POS taggers The process of phrase extraction is difficult to optimize in a nondiscriminative setting  many heuristics have been proposed   but it is not obvious which one should be chosen for a given language pair ,Negative
To the best of our knowledge no system was able to reproduce the successful results of Swier and Stevenson 2004 on the PropBank Roleset ,Positive
There are a number of efforts worldwide to manually annotate largecorpora with linguistic information including parts of speech phrase structure andpredicateargument structure eg the Penn Treebank and the British National CorpusMarcus Santorini and Marcinkiewicz 1993 Leech Garside and Bryant 1994,Positive
1 Introduction The field of machine translation has seen many advances in recent years  most notably the shift from wordbased  to phrasebased models which use token ngrams as translation units  Although ITA rates and system performance both significantly improve with coarsegrained senses   the question about what level of granularity is needed remains three models in  are susceptible to the O  n 3  method  cf ,Negative
For other pattern recognition related coding eg cross validation scaling etc we made use of the Matlab PRTools Duin 2001  ,Neutral
Much progress in the area of semantic role labeling is due to the creation of resources like FrameNet Fillmore et al 2003 which document the surface realization of semantic roles in real world corpora ,Positive
Bilexical contextfree grammars have been presented in  as an abstraction of language models that have been adopted in several recent realworld parsers  improving stateoftheart parsing accuracy  For the Penn Treebank   reports an accuracy of 966  using the Maximum Entropy approach  our much simpler and therefore faster HMM approach delivers 967  ,Negative
The conclusions are broadly in agreement with those of Merialdo 1994 but give greater detail about the contributions of different parts of the Model,Negative
6 Discourse Context  pointed out that the sense of a target word is highly consistent within any given document  one sense per discourse  ,Neutral
Since broadcoverage parsers for German especially robust parsers that assign predicateargument structure and allow crossing branches are not available or require an annotated traing corpus cf Collins 1996 Eisner 1996,Neutral
However  many of these models are not applicable to parallel treebanks because they assume translation units where either the source text  the target text or both are represented as word sequences without any syntactic structure  2 Statistical Word Alignment Statistical translation models  only allow word to word and multiword to word alignments ,Negative
Sentence reduction is concerned only with the removal of sentence components so it cannot explain transformations observed in our corpus and in summarization in general such as the reexpression of domain conceptsand verbs,Negative
Unsupervised approaches have been applied to shallow semantic tasks eg paraphrasing Lin and Pantel 2001 information extraction Banko et al 2007 but not to semantic parsing ,Neutral
In general  these authors have found that existing lexicalized parsing models for English  do not straightforwardly generalize to new languages  this typically manifests itself in a severe reduction in parsing performance compared to the results for English ,Negative
Roche and Schabes 1995 show a method for converting a listof tagging transformations into a deterministic finite state transducer with one statetransition taken per word of input the result is a transformationbased tagger whosetagging speed is about ten times that of the fastest Markovmodel tagger,Positive
Talbot and Osborne 2007 used a Bloom filter Bloom 1970 to encode a smoothed LM,Neutral
To overcome this problem  unsupervised learning methods using huge unlabeled data to boost the performance of rules learned by small labeled data have been proposed recently     ,Positive
Finally  the translation model can be formalized as the following optimization problem argmax This optimization problem can be solved by the EM algorithm  ,Positive
This is a common technique in machine translation for which the IBM translation models are popular methods  ,Positive
To evaluate the candidate translation the source parse tree is first obtained Dubey 2005 and each subtree is matched with a substring in the candidate string ,Neutral
Given a weight vector w  the score wf  x  y  ranks possible labelings of x  and we denote by Yk  w  x  the set of k top scoring labelings for x We use the standard B  I  O encoding for named entities  ,Neutral
 and  et al We used the WordNet   Similarity package  to compute baseline scores for several existing measures  noting that one word pair was not processed in WS353 because one of the words was missing from WordNet ,Neutral
Study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction  ,Positive
Even the 3 A demo of the parser can be found at httplfgdemocomputingdcuielfgparserhtml creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level  Our method  extending this line of research with the use of labeled LFG dependencies  partial matching  and nbest parses  allows us to considerably outperform  highest correlations with human judgement  they report 0144 for the correlation with human fluency judgement  0202 for the correlation with human overall judgement   although it has to be kept in mind that such comparison is only tentative  as their correlation is calculated on a different test set ,Negative
A major issue in MaxEnt training is how to select proper features and determine the feature targets Berger et al 1996 Jebara and Jaakkola 2000,Neutral
The MERT module is a highly modular  efficient and customizable implementation of the algorithm described in  Comparison with SSCRFMER When we consider semisupervised SOL methods  SSCRFMER  is the most competitive with HySOL  ,Positive
However while researchers have shown that it is sometimes possible to annotate corpora that capture features of interpretation to provide empirical support for theories as in Eugenio et al 2000 or to build classifiers that assist in dialogue reasoning as in Jordan and Walker 2005 it is rarely feasible to fully annotate the interpretations themselves ,Neutral
Sentiment classification at the sentencelevel has also been studied  ,Neutral
23 Online Learning Again following   we have used the single best MIRA   which is a margin aware variant of perceptron  for structured prediction ,Neutral
 Their weights are calculated by deleted interpolation Brown et al 1992 ,Neutral
 evaluates both estimation techniques on the Bayesian bitag model  Goldwater and Griffiths  emphasize the advantage in the MCMC approach of integrating out the HMM parameters in a tritag model  yielding a tagging supported by many different parameter settings ,Neutral
There are several distance measures suitable for this purpose  such as the mutual information   the dice coefficient   the phi coefficient   the cosine measure  and the confidence  ,Positive
Fortunately  using distributional characteristics of term contexts  it is feasible to induce partofspeech categories directly from a corpus of suf cient size  as several papers have made clear  ,Neutral
The stochastic tagger was trained on a sample of 357000 words from the Brown University Corpus of PresentDay English Francis and Kucera 1982 that was annotated using the EngCG tags ,Neutral
Kneser and Ney 1993 present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus Biber 1993 applies factor analysis to collocations of two target words certain and right with their immediate neighbors ,Neutral
But in fact  the issue of editing in text summarization has usually been neglected  notable exceptions being the works by  and Mani  Gates  and Bloedorn  999  We also use Cube Pruning algorithm  to speed up the translation process ,Positive
The technique employed by the learner is somewhat similar to that used in decisiontrees Breiman et al 1984 Quinlan 1986 Quinlan and Rivest 1989,Neutral
The corpus was aligned with GIZA    and symmetrized with the growdiagfinaland heuristic  BLEU  was devised to provide automatic evaluation of MT output Statistics in linguistics  Oxford  Basil Blackwell rawString citation citation validtrue authors author N Chinchor author authors title Evaluating message understanding systems  an analysis of the third Message Understanding Conference  MUC3 title date 1993 date journal Computational Linguistics journal volume 19 volume pages 409  449 pages marker Chinchor  1993 marker rawString Chinchor  N  et al  1993 ,Neutral
Pennacchiotti et al 2008 show that WordNetbased similarity measures outperform their simpler distributional alternatives An interesting question is whether the incorporation of WordNetbased similarity would lead to similar improvements in our case ,Neutral
32 The parsers The parsers that we chose to evaluate are the C&C CCG parser   the Enju HPSG parser   the RASP parser   the Stanford parser   and the DCU postprocessor of PTB parsers   based on LFG and applied to the output of the Charniak and Johnson reranking parser ,Neutral
Our method does not suppose a uniform distribution over all possible phrase segmentationsas  since each phrase tree has a probability ,Neutral
Theproblem of anaphoric expressions in technical articles has been extensively addressedin research work carried out under the British Library Automatic Abstracting ProjectBLAB Johnson et al 1993 Paice et al 1994,Neutral
1 Introduction Sentiment analysis have been widely conducted in several domains such as movie reviews  product reviews  news and blog reviews  Their approaches include the use of a vectorbased information retrieval technique   binbash  line 1  a  command not found Our do  mains are more varied  which may results in more recognition errors ,Neutral
Corpusbased information can be represented eg as neural networks Eineborg and Gamback 1994 Schmid 1994 local rules Brill 1992 or collocational matrices Garside 1987 ,Neutral
Jing and McKeown 2000 have proposed a rulebasedalgorithm for sentence combination but no results have been reported ,Negative
Considering how the performance of supervised systems degrades on outofdomain data Baker et al 2007 not to mention unseen events semisupervised or unsupervised methods seem to offer the primary nearterm hope for broad coverage semantic role labeling ,Neutral
36 Parameter Estimation To estimate parameters and um  we adopt the approach of minimum error rate training  MERT  that is popular in SMT  ,Positive
Many researchers Blum and Mitchell 1998 K Nigam and Mitchell 2000 Corduneanu and Jaakkola 2002 have attempted to improve performance with unlabeled data ,Positive
Almost all recent work in developingautomatically trained partofspeech taggers has been on further exploring Markovmodel based tagging Jelinek 1985 Church 1988 Derose 1988 DeMarcken 1990 Merialdo 1994 Cutting et al 1992 Kupiec 1992 Charniak et al 1993 Weischedel et al 1993Schutze and Singer 1994,Neutral
Introduction A hypergraph  as demonstrated by   is a compact datastructure that can encode an exponential number of hypotheses generated by a regular phrasebased machine translation  MT  system  eg  Koehn et al ,Positive
For more abstract matching we would need syntacti cally parsed data Lin and Pantel 2001 We ex pect that this would also positively affect the cov Erage,Positive
It is a variant of the batchbased Bloomier filter LM of Talbot and Brants 2008 which we refer to as the TBLM henceforth,Neutral
Second our method is an instance of a multisequence alignment 15 in contrast to thepairwise alignment described in Meyers Yangarber and Grishman 1996,Positive
Abney  notes important problems with the soundness of the approach when a unificationbased grammar is actually determining the derivations  motivating the use of loglinear models  for parse ranking that Johnson and colleagues further developed  ,Neutral
Relevance is a difficult issue because it is situational to a unique occasion Saracevic 1975 Sparck Jones 1990 Mizzaro 1997 ,Neutral
We hence chose transformationbased learning to create this  shallow  segmentation grammar  converting the segmentation task into a tagging task  as is done in 85   inter alia  ,Neutral
However  the study of  provides interesting insights into what makes a good distributional similarity measure in the contexts of semantic similarity prediction and language modeling While  does not discuss distinguishing more than 2 senses of a word  there is no immediate reason to doubt that the  one sense per collocation  rule  would still hold for a larger number of senses ,Positive
The BLEU metric  and the closely related NIST metric  along with WER and PER 48 have been widely used by many machine translation researchers ,Positive
While earlier approaches for text compression werebased on symbolic reduction rules Grefenstette 1998 Mani Gates and Bloedorn 1999more recent approaches use an aligned corpus of documents and their human writtensummaries to determine which constituents can be reduced Knight and Marcu 2002Jing and McKeown 2000 Reizler et al 2003,Neutral
Nowadays  most of the stateoftheart SMT systems are based on bilingual phrases  ,Positive
The success of Statistical Machine Translation  SMT  has sparked a successful line of investigation that treats paraphrase acquisition and generation essentially as a monolingual machine translation problem   Phrasebased ChinesetoEnglish MT The MT system used in this paper is Moses  a stateoftheart phrasebased system  eports a success rate of 96  disambiguating twelve words with two clear sense distinctions each one  ,Positive
In taggers thatare based on hidden Markov models HMM parameters of the unknown words areestimated by taking into account morphological information from the last part of theword Dermatas and Kokkinakis 1994 Maltese and Mancini 1991,Neutral
We show how this insight enables us to easily develop faster and exact variants of cube pruning for treetostring transducerbased MT Galley et al 2004 Galley et al 2006 DeNero et al 2009 ,Neutral
In this work we will use structured linear classifiers  This is the best automatically learned partofspeech tagging result known to us  representing an error reduction of 44  on the model presented in   using the same data splits  and a larger error reduction of 121  from the more similar best previous loglinear model in Toutanova and Manning  ,Neutral
Thehigh level data trend acquires more sophisticated information such as context rules constraints or decision trees Daelemans et al 1996 Mrquez andRodriguez 1995 Samuelsson et al 1996,Neutral
For instance  BLEU and ROUGE  are based on ngram precisions  METEOR  and STM  use wordclass or structural information  Kauchak  2006  leverages on paraphrases  and TER  uses editdistances ,Neutral
Given phrase p1 and its paraphrase p2  we compute Score3  p1  p2  by relative frequency   Score3  p1  p2   p  p2 p1   count  p2  p1  P pprime count  pprime  p1   7  People may wonder why we do not use the same method on the monolingual parallel and comparable corpora ,Neutral
Note that it is straightforward to calculate these expected counts using a variant of the insideoutside algorithm  applied to the  dependencyparsing data structures  for projective dependency structures  or the matrixtree theorem  for nonprojective dependency structures ,Neutral
Movies Reviews  This is a popular dataset in sentiment analysis literature  ,Positive
Given a set of evidences E over all the relevant word pairs  in   the probabilistic taxonomy learning task is defined as the problem of finding the taxonomy hatwideT that maximizes the 67 probability of having the evidences E  ie  hatwideT  arg max T P  E T  In   this maximization problem is solved with a local search ,Neutral
When output variables are structured annotation can be particularly difficult and time consuming For example when training a conditional random field Lafferty et al 2001 to extract fields such as rent  contact  features  and utilities from apartment classifieds labeling 22 instances 2540 tokens provides only 661 accuracy ,Neutral
Most stateoftheart system combination methods are based on constructing a confusion network CN from several input translation hypotheses and choosing the best output from the CN based on several scoring functions eg Rosti et al 2007a He et al 2008 Matusov et al 2008,Positive
We tokenized sentences using the standard treebank tokenization script  and then we performed partofspeech tagging using MXPOST tagger  ,Neutral
Thus we found this step unnecessary and currently did not look at this issue any further ,Negative
This prerocessing step can be accomplished by applying the GIZA++ toolkit Och and Ney 2003 that provides Viterbi alignments based on IBM Model4 ,Positive
From this point of view  some of the measures used in the evaluation of Machine Translation systems  such as BLEU   have been imported into the summarization task ,Neutral
After a brief period following the introduction of generally accepted and widely used metrics BLEU Papineni et al 2002 and NIST Doddington 2002 when it seemed that this persistent problem has finally been solved the researchers active in the field of machine translation ,Positive
We give a brief sketch here to highlight the content of COREFs representations the sources of information that COREF uses to construct them and the demands they place on disambiguation See DeVault 2008 for full details ,Positive
In comparison we introduce 28 several metrics coefficients reported in Albrecht and Hwa  including smoothed BLEU   METEOR   HWCM   and the metric proposed in Albrecht and Hwa  using the full feature set This is one manifestation of what is commonly referred to as the data sparseness problem  and was discussed by  as a sideeffect of specificity ,Neutral
Even the creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level  1 Introduction In recent years  various phrase translation approaches  have been shown to outperform wordtoword translation models  ,Negative
Among the statistical approaches the Maximum Entropy framework has a very strong position Nevertheless a recent independent comparison of 7 taggets Zavrel and Daelemans 1999 has shown that another approach even works better ,Neutral
We use a naı̈ve Bayesian model as in Kupiec Pedersen and Chens 1995 experiment cf Figure 9,Neutral
There is a growing amount of work on automatic extraction of paraphrases from text corpora Lin and Pantel 2001 Barzilay and Lee 2003 Ibrahim et al 2003 Dolan et al 2004 ,Positive
The last two counts  CAUS and ANIM  were performed on a 29million word parsed corpus  gall Street Journal 1988  provided by Michael Collins   ,Neutral
A remedy is to aggressively limit the feature space eg to syntactic labels or a small fraction of the bilingual features available as in Chiang et al 2008 Chiang et al 2009 but that reduces the benefit of lexical features ,Negative
Before trying some completely different approach we would like to improve the current simple approach by some other simple measures adding a morphological analyzer Hajji 1994 as a frontend to the tagger serving as a supplier of possible tags instead of just taking all tags occurring in the training data for a given token simplifying the tagset adding more data ,Positive
In particular lexical entries are no longer limited to be adjacent words as in Zettlemoyer and Collins 2005 but can be arbitrary fragments in a dependency tree ,Positive
Additional evaluations of SumUM using sentence acceptability criteria and contentbasedmeasures of indicativeness have been presented in Saggion and Lapalme 2000b andSaggion 2000,Neutral
In contrast semisupervised domain adaptation Blitzer et al 2006 McClosky et al 2006 Dredze et al 2007 is the scenario in which in addition to the labeled source data we only have unlabeled and no labeled target domain Data ,Neutral
In this paper we use the socalled Model 4 from  We would expect the opposite effect with handaligned data  Extensions to Hiero Several authors describe extensions to Hiero  to incorporate additional syntactic information   or to combine it with discriminative latent models  The other form of hybridization   a statistical MT model that is based on a deeper analysis of the syntactic 33 structure of a sentence   has also long been identified as a desirable objective in principle  consider   ,Neutral
Goldstein et al 1999 presented an extraction technique that assigns weighted scores for both statistical and linguistic features in the sentence,Neutral
We define a single direct loglinear translation model Papineni et al 1997 Och and Ney 2002 that encodes most popular MT features and can be used to encode any features on source and target sentences dependency trees and alignments,Positive
Recent several years have witnessed the rapid development of system combination methods based on confusion networks  eg     which show stateoftheart performance in MT benchmarks ,Positive
In contrast  the idea of bootstrapping for relation and information extraction was first proposed in   and successfully applied to the construction of semantic lexicons   named entity recognition   extraction of binary relations   and acquisition of structured data for tasks such as Question Answering  Perceptronbased training ,Positive
In addition to sentence fusion compressionalgorithms Chandrasekar Doran and Bangalore 1996 Grefenstette 1998 Mani Gatesand Bloedorn 1999 Knight and Marcu 2002 Jing and McKeown 2000 Reizler et al 2003and methods for expansion of a multiparallel corpus Pang Knight and Marcu 2003are other instances of such methods,Neutral
The basic setup is identical to the one described in Dugast et al 2007 ,Neutral
The work of McRoy 1992 pointed out that a diverse set of knowledge sources are important to achieve WSD but no quantitative evaluation was given on the relative importance of each knowledge source No previous work has reported any such evaluation Either,Negative
Introduction Phrasebased method  and syntaxbased method  represent the stateoftheart technologies in statistical machine translation  SMT  Introduction Phrasebased translation  and hierarchical phrasebased translation  are the state of the art in statistical machine translation  SMT  techniques ,Positive
3 Extending Bleu and Ter with Flexible Matching Many widely used metrics like Bleu  and Ter  are based on measuring string level similarity between the reference translation and translation hypothesis  just like Meteor Most of them  however  depend on finding exact matches between the words in two strings ,Positive
We consider a taskoriented dialog to be the result of incremental creation of a shared plan by the participants Lochbaum 1998 ,Neutral
Because of this it is generally accepted that some kind of postprocessing should be performed to improve the final result by shortening fusing or otherwise revising the material Grefenstette 1998 Mani Gates and Bloedorn 1999 Jing and McKeown 2000 Barzilay et al 2000 Knight and Marcu 2000 ,Neutral
Research in lexical semantics Cruse 1986 provides insight into the interaction of reference and linguistic Structure  ,Neutral
To evaluate the performance of LEXAS we conducted two tests one on a common data set used in Bruce and Wiebe 1994 and another on a larger data set that we separately collected ,Neutral
Most successfulmethods have followed speech recognition systems Jelinek Mercer and Roukos 1992and used large corpora to deduce the probability of each part of speech in the currentcontext usually the two previous wordstrigrams ,Positive
  or  ST    where no labeled target domain data is available  eg ,Neutral
In English part of speech taggers the maximization of Equation 1 to get the most likely tag sequence is accomplished by the Viterbi algorithm Church 1988 and the maximum likelihood estimates of the parameters of Equation 2 are obtained from untagged corpus by the Forward Backward algorithm Cutting et al 1992 ,Positive
A key component of the parsing system is a Maximum Entropy CCG supertagger  which assigns lexical categories to words in a sentence  ,Positive
Instead  researchers routinely use automatic metrics like Bleu  as the sole evidence of improvement to translation quality ,Neutral
The typical approach for testing a summarization system is to create an ideal summary either by professional abstractors or merging summaries provided by multiple human subjects using methods such as majority opinion union or intersection Jing et al 1998 ,Neutral
Also  the aspect of generalizing features across different products is closely related to fully supervised domain adaptation   and we plan to combine our approach with the idea from Daume III  2007  to gain insights into whether the composite backoff features exhibit different behavior in domaingeneral versus domainspecific feature subspaces ,Neutral
Recent comparisons of approaches that can be trained on corpora van Halteren et al 1998 Volk and Schneider 1998 have shown that in most cases statistical aproaches Cutting et al 1992 Schmid 1995 Ratnaparkhi 1996 yield better results than finite state rulebased or memorybased taggers Brill 1993 Daelemans et al 1996,Positive
This method will select useful features if the topics discovered are relevant to the task A similar heuristic was used by Druck et al 2008 ,Neutral
The features used in this study are  the length of t  a singleparameter distortion penalty on phrase reordering in a  as described in   phrase translation model probabilities  and 4gram language model probabilities logp  t   using KneserNey smoothing as implemented in the SRILM toolkit ,Neutral
Properly calculated BLEU scores have been shown to correlate reliably with human judgments  ,Positive
Systems were optimized on the WMT08 French nglish development data 2000 sentences using minimum error rate training Och 2003 and tested on the WMT08 test data 2000 sentences ,Positive
To support distributed computation   we further split the Ngram data into shards by hash values of the first bigram ,Neutral
Another kind of popular approaches to dealing with query translation based on corpusbased techniques uses a parallel corpus containing aligned sentences whose translation pairs are corresponding to each other  ,Positive
Split path feature are taken from existing semantic role labeling systems see for example Gildea and Jurafsky 2002 Lim et al 2004 Thompson et  al 2006,Neutral
A translation model consists of two distinct elements an unweighted ruleset and a parameterization Lopez 2008a 2009,Neutral
A very influential is the work of Brill 1997 who induces more linguistically motivated rules exploiting both a tagged corpus and a lexicon He does not look at the affixes only but also checks their POS class in a lexicon ,Positive
We measured associations using the loglikelihood measure Dunning 1993 for each combination of target category and semantic class by converting each cell of the contingency into a 22 contingency table ,Neutral
The above problems could be partially solved by introducing more resources into collocation extraction such as chunker Wermter and Hahn 2004 parser Lin 1998 Seretan and We hrli 2006 and WordNet Pearce 2001,Positive
Mani 2001 defines an abstract as a summary at least some of whose material is not present in the input ,Neutral
Resnik and Diab 2000 present yet other measures of verb similarity which could be used to arrive at a more datadriven definition of verb classes ,Positive
We trained a 5gram language model from the provided English monolingual training data and the nonEuroparl portions of the parallel training data using modified KneserNey smoothing as implemented in the SRI language modeling toolkit Kneser and Ney 1995 Stolcke 2002,Positive
As with other randomised models we construct queries with the appropriate sanity checks to lower the error rate efficiently Talbot and Brants 2008,Neutral
But the situation is much different in treetostring transducerbased MT Galley et al 2004 Galley et al 2006 DeNero et al 2009 ,Neutral
A morpheme network of each input sentence was generated with Juman Mat sumoto et al 1994 and the credit factor was attached to each branch as described above ,Neutral
We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature see Baker et al 2007 for an overview ,Positive
To contrast Harabagiu 1999 concentrated on the derivation of a model that can establish coherence relations in a text without relying on cue phrases ,Neutral
Partofspeech tagging is an activearea of research a great deal of work has been done in this area over the past few yearseg Jelinek 1985 Church 1988 Derose 1988 Hindle 1989 DeMarcken 1990 Merialdo1994 Brill 1992 Black et al 1992 Cutting et al 1992 Kupiec 1992 Charniak et al 1993Weischedel et al 1993 Schutze and Singer 1994,Positive
1 A cept is defined as the set of target words connected to a source word  ,Neutral
The paper presents an application of Structural Correspondence Learning SCL Blitzer et al 2006 for domain adaptation of a stochastic attributevalue grammar SAVG So far SCL has been applied successfully in NLP for PartofSpeech tagging and Sentiment Analysis Blitzer et al 2006 Blitzer et al 2007 ,Positive
We implement the expectation and variance semirings in Joshua Li et al 2009a and demonstrate their practical benefit by using minimumrisk training to improve Hiero Chiang 2007 ,Positive
Most work on discriminative training for SMT has focussed on linear models  often with margin based algorithms   or rescaling a product of submodels  ,Neutral
 and Wiebe  2000  focused on learning adjectives and adjectival phrases and Wiebe et al A number of alignment techniques have been proposed  varying from statistical methods  to lexical methods  ,Neutral
Among these techniques  SCL  Structural Correspondence Learning   is regarded as a promising method to tackle transferlearning problem Several generalpurpose offtheshelf  OTS  parsers have become widely available  ,Positive
These were augmented with automatically labeled sentences from the BNC which we used as our expansion corpus FrameNet sentences were parsed with RASP Briscoe et al 2006,Neutral
On the other hand in languages like Turkish or Finnishwith very productive agglutinative morphology itis possible to produce thousands of forms or evenmillions Hankamer 1989 from a given root wordand the kinds of ambiguities one observes are quitedifferent than what is observed in languages like EnGlish,Neutral
On the other hand Snow et al 2008 illustrate how AMT can be used to collect data in a fast and cheap fashion for a number of NLP tasks such as word sense disambiguation ,Positive
Dredze et al yielded the second highest score1 in the domain adaptation track  The IBM models  search a version of permutation space with a onetomany constraint  propose the use of language models for sentiment analysis task and subjectivity extraction ,Neutral
The efficiency of the scheme for storing ngram statistics within a BF presented in Talbot and Osborne  relies on the Zipflike distribution of ngramfrequencies  mosteventsoccuranextremely small number of times  while a small number are very frequent ,Positive
In support of this processing we rely on the linguistic and domain knowledge contained in the National Library of Medicines Unified Medical Language System UMLS ® as well an existing tool the SPECIALIST minimal commitment parser Aronson et al 1994 ,Positive
Semiringweighted logic programming is a general framework to specify these algorithms Pereira and Warren 1983 Shieber et al 1994 Goodman 1999 Eisner et al 2005 Lopez 2009 Goodman 1999 describes many useful semirings eg Viterbi inside and Viterbinbest ,Neutral
Our method to address the problem of length bias in rule selection is very different from the maximum entropy method used in existing studies eg He et al 2008,Neutral
Although stateoftheart statistical parsers  are more accurate  the simplicity and efficiency of deterministic parsers make them attractive in a number of situations requiring fast  lightweight parsing  or parsing of large amounts of data ,Positive
Such a quasisyntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrasebased approach  ,Negative
We describe a POS tagger based on the work described in Padr6 1996that is able to use bitrigram information automatically learned context constraints and linguistically motivated manually written constraints,Positive
or cooking  which agrees with the knowledge presented in previous work  ,Neutral
Finally  we use as a feature the mappings produced in  of WordNet senses to Oxford English Dictionary senses 3 Semantic Representation 31 The Need for Dependencies Perhaps the most common representation of text for assessing content is BagOfWords or BagofNGrams  First  we adopt an ONTOLOGICALLY PROMISCUOUS representation  that includes a wide variety of types of entities ,Neutral
Starting from the parallel training corpus provided with direct and inverted alignments the so called union alignment Och and Ney 2003 is Computed ,Neutral
However their length features only provided insignificant improvement of 01 BLEU point A crucial difference of our approach is how the length preference is modeled ,Negative
We perform word alignment using GIZA     symmetrize the alignments using the growdiagfinaland heuristic  and extract phrases up to length 3 ,Neutral
While both  and  propose models which use the parameters of the generative model but train to optimize a discriminative criteria  neither proposes training algorithms which are computationally tractable enough to be used for broad coverage parsing Turneys method did not work well although they reported 80  accuracy in  ,Negative
A theoretical motivated argumentation uses the standard deviation of the maximum likelihood probabilities for the weights 0i Samuelsson 1993 This leaves room for interpretation,Neutral
A later study  found that performance increased to 872  when considering only those portions of the text deemed to be subjective ,Positive
However in this paper we clarify a number of details that are omitted in major previous publications concerning tagging with Markov models As two examples Rabiner 1989 and Charniak et al 1993 give good overviews of the techniques and equations used for Markov models and partofspeech tagging but they are not very explicit in the details that are needed for their application ,Negative
For example Quirk et al 2005 use features involving phrases and sourceside dependency trees and Mi et al 2008 use features from a forest of parses of the source sentence,Neutral
One is distortion model  which penalizes translations according to their jump distance instead of their content ,Neutral
The straightforward way is to first generate the best BTG tree for each sentence pair using the way of   then annotate each BTG node with linguistic elements by projecting sourceside syntax tree to BTG tree  and finally extract rules from these annotated BTG trees ,Neutral
Decision treesrecently used in NLP basic tasks such as taggingand parsing McCarthy and Lehnert 1995 Daelemans et al 1996 Magerman 1996 are suitable forperforming this task,Neutral
 However the work of Black 1988 Zernik 1990 Yarowsky 1992 were not based on the present set of sentences so the comparison is only suggestive ,Neutral
In the concept extension part of our algorithm we adapt our concept acquisition framework  to suit diverse languages  including ones without explicit word segmentation ,Neutral
Bruce and Wiebe also performed a separate test by using a subset of the interest data set with only 4 senses sense 1 4 5 and 6 so as to compare their results with previous work on WSD Black 1988 Zernik 1990 Yarowsky 1992 which were tested on 4 senses of the noun interest,Neutral
Statisticbased algorithms based on Belief Network  such as HiddenMarkovModel  HMM     Lexicalized HMM  and MaximalEntropy model  use the statistical information of a manually tagged corpus as background knowledge to tag new sentences One way of resolving query ambiguities is to use the statistics  such as mutual information   to measure associations of query terms  on the basis of existing corpora  ,Neutral
2 Three New Features for MT Evaluation Since our sourcesentence constrained ngram precision and discriminative unigram precision are both derived from the normal ngram precision  it is worth describing the original ngram precision metric  BLEU  ,Neutral
Note that this pruning algorithm is slightly different from that of Xue and Palmer 2004 the predicate itself is also included in the argument candidate list as the nominal predicate sometimes takes itself as its argument ,Neutral
Sentencelevel approximations to B exist   but we found it most effective to perform B computations in the context of a setOof previouslytranslated sentences  following Watanabe et al In such a process  original phrasebased decoding  does not take advantage of any linguistic analysis  which  however  is broadly used in rulebased approaches ,Negative
The scaling processintroduced in this case multiplies the forward and backward probabilities by a scalingfactor at selective word events in order to keep the computations within the floatingpoint dynamic range of the computer Rabiner 1989,Neutral
4 Extended Minimum Error Rate Training Minimum error rate training  is widely used to optimize feature weights for a linear model  When we run our classifiers on resourcetight environments such as cellphones ,Positive
Conjunctions are a major source of errors for English chunking as well  9  and we plan to address them in future work ,Neutral
In addition to reducing the original sentences Jing andMcKeown 2000 use a number of manually compiled rules to aggregate reducedsentences for example reduced clauses might be conjoined with and,Positive
Raw text is processed by a preprocessor which segments the text into sentences using various heuristics about punctuation and then to kenizes and runs it through a widecoverage high performance morphological analyzer developed using twolevel morphology tools by Xerox Kart Tunen 1993 ,Positive
An alternative approach to semantic role labeling is the framework developed by Halliday 1994 and implemented by Mehay et al 2005 ,Positive
It is clear that Appendix B contains far fewer true noncompositional phrases than Appendix A 7 Related Work There have been numerous previous research on extracting collocations from corpus  eg   and  ,Neutral
This analysis depends on the SPECIALIST Lexicon and the Xerox partofspeech tagger Cutting et al 1992 and provides simple noun phrases that are mapped to concepts in the UMLS Metathesaurus using MetaMap Aronson 2001 ,Neutral
For more details on the linguistic specifications of the annotation scheme see Skut et al 1997,Positive
Notice that the term coreferential is used in an extended way as it is usually used to describe the phenomena in noun group pairs Mitkov 2002   ,Neutral
To contrast [Jing  McKeown 2000] concentrated on analyzing humanwritten summaries in order to determine how professionals construct summaries ,Positive
Hovy 1993 summarized previous work that focused on the automated planning and generation of multisentence texts using discourse relationships ,Neutral
In convolution tree kernel Collins and Duffy 2001 a parse tree T is implicitly represented by a vector of integer counts of each subtree type regardless of its Ancestors,Neutral
There are also approaches to anaphora resolution using unsupervised methods to extract useful information  such as gender and number   or contextual roleknowledge  ,Neutral
According to Cremmins 1982 the last step in the human production of the summary text is the extracting into abstracting step in which the extracted information will be mentally sorted into a preestablished format and will be edited usingcognitive techniques,Neutral
Inversion Transduction Grammar  ITG   and SyntaxDirected Translation Schema  SDTS   lack both of these properties ,Positive
String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG  but Duchi et al 1 Introduction Phrasebased systems  flat and hierarchical alike   have achieved a much better translation coverage than wordbased ones   but untranslated words remain a major problem in SMT ,Negative
These include systems for machine translation Viegas et al 1998 question answering Harabagiu et al 2001 Clark et al 2003 and information retrieval Mihalcea and Moldovan 2000 ,Neutral
The HMM training and tagging programs in our experiment Wilkens and Kupiec 1995 are based on bigrams ie only the immediate context of a word is taken into account,Neutral
Techniques for weakening the independence assumptions made by the IBM models 1 and 2 have been proposed in recent work  C3BTC5 and CCCDCA were used in  and   respectively ,Neutral
For symmetrization  we found that Och and Neys refined technique described in  produced the best AER for this data set under all experimental conditions Ramshaw and Marcus  successflflly applied Eric Brill s transformationbased learning method to the chunking problem ,Positive
4 Maximum Entropy To explain our method  we l  riefly des   ribe the con   ept of maximum entrol  y Recently  many al  lnoaches l  ased on the maximum entroi  y lnodel have t  een applied to natural language processing  ,Neutral
A has nice guarantees Dechter and Pearl 1985 but it is spaceconsumptive and it is not anytime For a use case where we would like a finer rained speedquality tradeoff it might be useful to consider an anytime search algorithm like depthfirst branchandbound Zhang and Korf 1995 ,Neutral
However Voutilainen and Jarvinen 1995 empirically show that an interjudge agreement virtually of 10 is possible at least with the EngCG tag set if not with the original Brown Corpus tag set ,Positive
Alternatively DM could be represented as a threemode tensor in the framework of Turney 2007 enabling smoothing operations analogous to singular value decomposition ,Positive
As has been stressed at least since Chomskys early work Chomsky 1957 no matter how large a corpus is if a phenomenon is productive there will always be new wellformed instances that are not in the corpus ,Neutral
The work of Cardie 1993 used a casebased approach that simultaneously learns part of speech word sense and concept activation knowledge although the method is only tested on domainspecific texts with domainspecific word senses ,Neutral
 First the experiments in Teufel and Moens 1997 showed that in our corpus only 45of the abstract sentences appear elsewhere in the body of the document either as a close variant or in identical form whereas Kupiec Pedersen and Chen report a figure of 79  ,Neutral
Linguistic features like tense and voice often correlate with rhetorical zones Biber 1995 and Riley 1991 show correlation of tense and voice with prototyp ical section structure method introduction ,Neutral
Introduction During the last four years  various implementations and extentions to phrasebased statistical models  have led to significant increases in machine translation accuracy  ,Positive
To derive the joint counts c   s   t  from which p   s  t  and p   t  s  are estimated  we use the phrase induction algorithm described in   with symmetrized word alignments generated using IBM model 2  ,Neutral
The CRF tagger was implemented in MALLET  using the original feature templates from  Given a set of terms with unknown sentiment orientation   then uses the PMIIR algorithm  to issue queries to the web and determine  for each of these terms  its pointwise mutual information  PMI  with the two seed words across a large set of documents ,Neutral
Barzilay and McKeown 2005 proposed an idea called sentence fusion that integrates information in overlapping sentences to produce a non overlapping summary sentence ,Positive
The most widely used method for building phrase translation tables  selects  from a word alignment of a parallel bilingual training corpus  all pairs of phrases  up to a given length  that are consistent with the alignment ,Positive
In syntaxbased method word reordering is implicitly addressed by translation rules thus the performance is subject to parsing errors to a large extent zhang et al 2007a and the impact of syntax on reordering is difficult to single out Li et al 2007,Neutral
The program called the decomposition program matches phrases in a human written summary sentence to phrases in the original document Jing and McKeown 1999 ,Neutral
Berger et al 1996 proposed an iterative procedure of adding news features to feature set driven by data ,Negative
We use the English Slot GrammarESG parser developed at IBM McCord 1990 to analyze the syntactic structure of an input sentence and produce a sentence parse tree ,Positive
Target language models LMs used by the decoder and rescoring modules are respectively estimated from 3gram and 4gram statistics by applying the modified KneserNey smoothing method Goodman and Chen 1998 ,Neutral
Due to the positive results in Ando 2006 Blitzer et al 2006 include this in their standard setting of SCL and report results using block SVDs only ,Positive
It also differs from previous proposals on lexical acquisition using statistical measures such as  which either deny the prior existence of linguistic knowledge or use linguistic knowledge in ad hoc ways In pursuit of better translation  phrasebased models  havesignificantlyimprovedthe quality over classical wordbased models  ,Negative
Note that the minimum error rate training  uses only the target sentence with the maximum posterior probability whereas  here  the whole probability distribution is taken into account A word order correlation bias  as well as the phrase structure biases in  Models 4 and 5  would be less beneficial with noisier training bitexts or for language pairs with less similar word order ,Negative
When extracting rules with source dependency structures we applied the same wellformedness constraint on the source side as we did on the target side using a procedure described by Shen et al 2008,Neutral
Thus most of recent works in this research area are based on extraction Goldstein et al 1999 ,Neutral
Similarly   propose a relative distortion model to be used with a phrase decoder An extension to WordNet was presented by  Discovering orientations of context dependent opinion comparative words is related to identifying domain opinion words  ,Neutral
We also plan to consider reasonable applications for semantic tagging One possibility would be to use semantic tagging in the framework of an intelligent on line dictionary lookup such as LocoLex [Bauer et al 1995] ,Positive
3 OverviewofExtractionWork 31 English As one mightexpect  the bulk of the collocation extractionwork concernsthe English language    amongmany others1 ,Neutral
1 Motivation A major component in phrasebased statistical Machine translation  PBSMT   is the table of conditional probabilities of phrase translation pairs ,Neutral
Using GIZA   model 4 alignments and Pharaoh   we achieved a BLEU score of 03035 ,Neutral
Chang et al 2007 only obtain better results than 882 on cora when using 300 labeled examples two hours of estimated annotation time 5000 additional unlabeled examples and extra test time inference constraints ,Positive
In the previous version of thesystem Barzilay McKeown and Elhadad 1999 we performed linearization of afusion dependency structure using the language generator FUFSURGE Elhadadand Robin 1996,Neutral
We use the machine learning toolkit LLAMA Haffner 2006 which encodes multiclass classication problems using binary MaxEnt classifiers to increase the speed of training and to scale the method to large data sets ,Positive
 Introduction IBM Model   is a wordalignment model that is widely used in working with parallel bilingual corpora ,Positive
Second we devised a method to obtain the expected word Ngram count in the target texts using an Nbest word segmentation algorithm Nagata 1994 ,Positive
Rowley 1982 mentions the inclusion of the lead or topical sentence and the use ofactive voice and advocates conciseness,Neutral
It is interesting to note that while the study of how the granularity of contextfree grammars CFG affects the performance of a parser ,Positive
2 Detecting DiscourseNew Definite Descriptions 21 Vieira and Poesio Poesio and Vieira  carried out corpus studies indicating that in corpora like the Wall Street Journal portion of the Penn Treebank   around 52  of DDs are discoursenew   and another 15  or so are bridging references  for a total of about 6667  firstmention The distinction between lexical and relational similarity for word pair comparison is recognized by   hecallstheformer attributional similarity   though the methods he presents focus on relational similarity ,Neutral
This algorithm cannot take advantage of the scaling procedure because it requires the synchronous calculation of all possible sequences in the morpheme network ,Negative
The resulting corpus contains 385 documents of American English selected from the Penn Treebank   annotated in the framework of Rhetorical Structure Theory ,Neutral
The LT POS tagger is reported to perform at 936943 accuracy on known words and at 877887 on unknown words using a cascading unknown word guesser Mikheev 1997 ,Neutral
Methods using the short context of a word in order to resolve ambiguity usually categorical ambiguity are very common in English and other languages DeRose1988 Church 1988 Karlsson 1990 ,Positive
2 Lexicalized parse trees The first successful work on syntactic disambiguation was based on lexicalized probabilistic contextfree grammar  LPCFG   ,Positive
Beyond these domains purely corpusbased methods play an increasingly important role in modeling constraints on composition of words in particular verbal selectional preferences  finding out that say children are more likely to eat than apples whereas the latter are more likely to be eaten Erk 2007 Pad et al 2007 ,Neutral
Our tests were conducted over a larger stream of 125B ngrams from the Gigaword corpusGraff 2003 We set our space usage to match the 308 bytes per ngram reported in Talbot and Brants 2008 and held out just over 1M unseen ngrams to test the error rates of our models,Neutral
In general this is a difficult open problem that only recently has started to receive some attention Mohammad et al 2008 Resolving this is not the focus of this paper but we describe a general heuristic for fixing this problem ,Neutral
The test set includes only sentences for which our English parser Soricut and Marcu 2003 could produce a parse tree which effectively excluded a few very long Sentences,Neutral
One of the first large scale hand tagging efforts is reported in   where a subset of the Brown corpus was tagged with WordNet July 2002  pp 42 Experiments To build all alignment systems  we start with 5 iterations of Model 1 followed by 4 iterations of HMM   as implemented in GIZA    ,Neutral
Research on Statistical Machine Translation SMT has shown substantial progress in recent years ,Positive
Researchers have focused on learning adjectives or adjectival phrases  and verbs   but no previous work has focused on learning nouns ,Neutral
Nevertheless since the seminal work of Hobbs et al 1993 it has been possible to conceptualize pragmatic interpretation as a unified reasoning process that selects a representation of the speakers contribution that is most preferred according to a background model of how speakers tend to behave ,Positive
This averaging effect has been shown to help overfitting  ,Positive
Examples include information extraction Surdeanu et al 2003 question answering Narayanan and  Harabagiu 2004 machine translation Boas 2005 and summarization Melli et al 2005,Neutral
For comparison purposes  three additional heuristicallyinduced alignments are generated for each system   1  Intersection of both directions  Aligner  int     2  Union of both directions  Aligner  union    and  3  The previously bestknown heuristic combination approach called growdiagfinal   Aligner  gdf   Brill s results demonstrate that this approach can outperform the Hidden Markov Model approaches that are frequently used for partofspeech tagging   as well as showing promise for other applications ,Negative
A monotonous segmentation copes with monotonous alignments  that is  j  k aj  ak following the notation of  ,Neutral
The fluency models hold promise for actual improvements in machine translation output quality  ,Positive
While PRW is the first attempt to formalize well known relevance weighting Sparck Jones 1972 Salton and McGill 1983 by probability theory there are several drawbacks in PRW ,Negative
Presently  many systems        focus on online recognition of proper nouns  and have achieved inspiring results in newscorpus but will be deteriorated in special text  such as spoken corpus  novels ,Neutral
To tune the parameters w of the model  we use the averaged perceptron algorithm  because of its efficiency and past success on various NLP tasks  ,Positive
Fuhr 1989 solves problem 2 by assuming that a document is probabilistically indexed by its term vectors This model is called Retrieval with Probabilistie Indexing RPI ,Positive
The remaining six entries were all fully automatic machine translation systems  in fact  they were all phrasebased statistical machine translation system that had been trained on the same parallel corpus and most used Bleubased minimum error rate training  to optimize the weights of their log linear models feature functions  ,Positive
If we view MT as a machine learning problem features and formalisms imply structural independence assumptions which are in turn exploited by efficient inference algorithms including decoders Koehn et al 2003 Yamada and Knight 2001,Neutral
We conducted experiments with the Reuters21578 Corpus a relatively tiny one for such experiments Clark 2000 reports results on a corpus containing 12 million terms Schütze 1993 on one containing 25 million terms and Brown et al 1992 on one containing 365 million terms In contrast we count approximately 28 million terms in Reuters21578,Positive
To select these we use the idea of strong chains introduced by Barzilay and Elhadad 1997 ,Positive
The effect of recency on perplexity has also been observed elsewhere see for example Rosenfeld 1995 and Whittaker 2001,Neutral
This method of evaluation has already been used in other summarizationevaluations such as Edmundson 1969 and Marcu 1997,Neutral
Measures of crosslanguage relatedness are useful for a large number of applications  including crosslanguage information retrieval   crosslanguage text classification   lexical choice in machine translation   induction of translation lexicons   crosslanguage annotation and resource projections to a second language  ,Neutral
In the II  OO  and OI scenarios   succeeded in improving the parser performance only when a reranker was used to reorder the 50best list of the generative parser  with a seed size of 40K sentences Maximum Entropy Maximum entropy classiflcation  MaxEnt  or ME  for short  is an alternative technique which has proven efiective in a number of natural language processing applications  ,Positive
Linguistic information has been widely used in SMT For example in Wang et al 2007 syntactic structures were employed to reorder the source language as a preprocessing step for phrasebased Decoding ,Neutral
Unfortunately  longer sentences  up to 100 tokens  rather than 40   longer phrases  up to 10 tokens  rather than 7   two LMs  rather than just one   higherorder LMs  order 7  rather than 3   multiple higherorder lexicalized reordering models  up to 3   etc all contributed to increased system  s complexity  and  as a result  time limitations prevented us from performing minimumerrorrate training  MERT   for ucb3  ucb4 and ucb5 ,Negative
This differs from previous approaches Finch and Chater 1992 Schfitze 1993 in which left and right context vectors of a word are always used in one concatenated vector ,Neutral
Most work on statistical methods has used ngram models or Hidden Markov Modelbasedtaggers eg Church 1988 DeRose 1988 Cutting et al 1992 Merialdo 1994 etc,Neutral
On the other hand   proposed an algorithm  borrowed to the field of dynamic programming and based on the output of their previous work  to find the best alignment  subject to certain constraints  between words in parallel sentences ,Neutral
Among the four steps  the hypothesis alignment presents the biggest challenge to the method due to the varying word orders between outputs from different MT systems  ,Neutral
Giza   is a freely available implementation of IBM Models 15  and the HMM alignment   along with various improvements and modifications motivated by experimentation by Och & Ney  ,Neutral
The scheme must provide representational means for all phenomena occurring in texts Disambiguation is based on human processing skills cf Marcus et at 1994 Sampson 1995 Black et al  1996 ,Neutral
In what concerns the evaluation process  although ROUGE  is the most common evaluation metric for the automatic evaluation of summarization  since our approach might introduce in the summary information that it is not present in the original input source  we found that a human evaluation was more adequate to assess the relevance of that additional information ,Negative
These probabilities are estimated on the training corpus parsed using the Stanford factored parser Klein and Manning 2003,Neutral
For instance  both Pang and Lee  and   consider the thumbs upthumbs down decision  is a film review positive or negative  Binarizing the syntax trees for syntaxbased machine translation is similar in spirit to generalizing parsing models via markovization  4 Options from the Translation Table Phrasebased statistical machine translation methods acquire their translation knowledge in form of large phrase translation tables automatically from large amounts of translated texts  ,Neutral
Language modeling   nounclustering   constructing syntactic rules for SMT   and finding analogies  are examples of some of the problems where we need to compute relative frequencies ,Neutral
First such a system makes useof lexical information when modeling reordering Lopez 2008 which has previously been shown to be useful in Germanto nglish translation Koehn et al 2008,Neutral
Chuang and Yang 2000 studied several algorithms for extracting sentence segments such as decision tree naive Bayes classifier and neural network,Neutral
These rules can either be handcrafted Garside et al 1987 Klein & Simmons1963 Green & Rubin 1971 or learned as in Hindle 1989 or the transformationbasederrordriven approach of Brill 1992,Neutral
We use a specific treebased system called Hiero Chiang 2007 as an example although the discussion is general for any systems that use a hypergraph to represent the hypothesis space ,Neutral
There has now been considerable work on discourse parsing using statistical bottomup parsing Soricut and Marcu 2003 hierarchical agglomerative clustering Sporleder and Lascarides 2004 parsing from lexicalized treeadjoining grammars Cristea 2000 and rulebased approaches that use rhetorical relations and discourse cues Forbes et al 2003 Polanyi et al 2004 LeThanh et al 2004 ,Positive
In   the authors provide some sample subtrees resulting from such a 1000word clustering We took part the Multilingual Track of all ten languages provided by the CoNLL2007 shared task organizer  To set the weights  m  we carried out minimum error rate training  using BLEU  as the objective function ,Neutral
The parameters  j  were trained using minimum error rate training  to maximize the BLEU score  on a 150 sentence development set ,Neutral
A few studies Carpuat and Wu 2007 Ittycheriah and Roukos 2007 He et al 2008 Hasan et al 2008 addressed this defect by selecting the appropriate translation rules for an input span based on its context in the input sentence,Positive
We report in the following tables the MUC score Vilain et al 1995 ,Neutral
33 Features Similar to the default features in Pharaoh   we used following features to estimate the weight of our grammar rules ,Neutral
There is also a less detailed description of Pahner and Hearst's system SATZ in Pahuer and Hearst 1994 ,Negative
If we consider these probabilities as a vector  the similarities of two English words can be obtained by computing the dot product of their corresponding vectors2 The formula is described below  similarity  ei  ej   Nsummationdisplay k  1 p  ei fk  p  ej fk   3  Paraphrasing methods based on monolingual parallel corpora such as  can also be used to compute the similarity ratio of two words  but they dont have as rich training resources as the bilingual methods do ,Negative
The second one SYS2 is a reimplementation of a phrasebased decoder with lexicalized reordering model based on maximum entropy principle proposed by Xiong et al 2006 ,Neutral
Summarization of such texts requires a different approach from for example that used in the summarization of news articles  ,Positive
In the future  we will experiment with semantic  rather than positional  clustering of premoditiers  using techniques such as those proposed in  ,Neutral
The two systems we use are ENGCG Karlsson et al 1994 and the Xerox Tagger Cutting et al 1992 ,Neutral
In order to improve sentencelevel evaluation performance  several metrics have been proposed  including ROUGEW  ROUGES  and METEOR  ,Neutral
Synchronous parsing models have been explored with moderate success  Systems based on perceptron have been shown to be competitive in NER and text chunking  ,Positive
The resulting partiallylabeled corpus can be used to train a CRF by maximizing MML Similarly prototypedriven learning PDL Haghighi and Klein 2006 optimizes the joint marginal likelihood of data labeled with prototype input features for each label ,Neutral
Our experimental finding t h a t local collocations are the most predictive agrees with past observa tion that humans need a narrow window of only a few words to perform WSD Choueka and Lusignan 1985 ,Neutral
For these classications  we calculated a kappa statistic of 0528  ,Neutral
For instance a document that takes 300 seconds using Barzilay and Elhadads method takes only 4 seconds using ours Silber and McCoy 2000 ,Neutral
21 The Evaluator The evaluator is a function ptt s which assigns to each targettext unit t an estimate of its probability given a source text s and the tokens t which precede t in the current translation of s 1 Our approach to modeling this distribution is based to a large extent on that of the IBM group Brown et al  1993 but it differs in one significant aspect whereas the IBM model involves a noisy channel decomposition we use a linear combination of separate predictions from a language model ptlt  and a translation model ptls ,Neutral
Mann and McCallum 2008 apply GE to a linear chain firstorder CRF In this section we provide an alternate treatment that arrives at the same objective function from the general form described in the previous section ,Neutral
The constraint language is able to express thesame kind of patterns than the Constraint Grammar formalism Karlsson et al 1995 although in adifferent formalism,Neutral
compares his method to  and shows that for four words the former performs significantly better in distinguishing between two senses ,Positive
Fillmore 1968 introduced semantic structures called semantic frames describing abstract actions or common situations frames with common roles and themes semantic roles ,Positive
A general solution to the variable length and depth ofdependency for HMM has been already proposed Tao 1992 but has notbeen implemented in taggers,Negative
One major resource for corpusbased research is the treebanks available in many research organizations   which carry skeletal syntactic structures or  brackets  that have been manually verified Successful approaches aimed at trying to overcome the sparse data limitation include backoff   ,Positive
 Introduction The field of machine translation has seen many advances in recent years  most notably the shift from wordbased  to phrasebased models which use token ngrams as translation units  ,Positive
1 Introduction Word Sense Disambiguation  WSD  competitions have focused on general domain texts  as attested in the last Senseval and Semeval competitions  ,Neutral
1 Introduction Statistical phrasebased systems  have consistently delivered state of the art performance in recent machine translation evaluations  yet these systems remain weak at handling word order changes ,Negative
Consider more complex context features such as nonlimited distance or barrier rules in the style of Samuelsson et al 1996 ,Positive
129 5 Active learning Whereas a passive supervised learning algorithm is provided with a collection of training examples that are typically drawn at random  an active learner has control over the labeled data that it obtains  ,Neutral
Measures of attributional similarity have been studied extensively  due to their applications in problems such as recognizing synonyms   information retrieval   determining semantic orientation   grading student essays   measuring textual cohesion   and word sense disambiguation  ,Neutral
The best system Johansson and Nugues 2008 in CoNLL 2008 achieved an F1measure of 8165 on the workshops evaluation Corpus,Positive
A description of the flat featurized dependencystyle syntactic representation we use is available in   which describes how the entire Penn Treebank  was converted to this representation ,Neutral
  Pereira and Tishby   and Pereira  Tishby  and Lee  propose methods that derive classes from the distributional properties of the corpus itself  while other authors use external information sources to define classes  Resnik  uses the taxonomy of WordNet    uses the categories of Roget s Thesaurus  Slator  and Liddy and Paik  use the subject codes in the LDOCE  Luk  uses conceptual sets built from the LDOCE definitions ,Neutral
The annotation experiment described here and in Teufel Carletta and Moens 1999 in more detail tests the rhetorical annotation scheme presented in section ,Neutral
In Koehn and Hoang 2007 shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal Decoder  ,Neutral
7 Experiments To show the effectiveness of crosslanguage mention propagation information in improving mention detection system performance in Arabic  Chinese and Spanish  we use three SMT systems with very competitive performance in terms of BLEU11  ,Neutral
There are many possible methods for combining unlabeled and labeled data   but we simply concatenate unlabeled data with labeled data to see the effectiveness of the selected reliable parses ,Neutral
The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems Soon et al 2001 Ng & Cardie 2002 Kehler et al 2004 inter alia ,Neutral
Thus we can compute the source dependency LM score in the same way we compute the target side score using a procedure described in Shen et al 2008 ,Neutral
The state of the art technology for relation extraction primarily relies on patternbased approaches  Many mainstream systems and formalisms would satisfy these criteria  including ones such as the University of Pennsylvania Treebank  which are purely syntactic  though of course  only syntactic properties could then be extracted  ,Positive
2 21 Word Alignment Adaptation Bidirectional Word Alignment In statistical translation models   only onetoone and moretoone word alignment links can be found The method was intended as a replacement for sentencebased methods  eg     which are very sensitive to noise ,Negative
Model 4 of  is also a firstorder alignment model  along the source positions  like the HMM  trot includes also fertilities ,Neutral
The system was initially prototyped using the MUC6 and MUC7 data sets Chinchor & Sundheim 2003 Chinchor 2001 using the standard partitioning of 30 texts for training and 2030 texts for testing Then we developed and tested the system with the ACE 2003 Training Data corpus Mitchell et al 2003 ,Neutral
Results from  show that under these definitions the following guarantee holds  LogLossUpda  k  BestWtk  a C20 BestLossk  a So it can be seen that the update from a to Upda  k  BestWtk  a is guaranteed to decrease LogLoss by at least W k q C0 W C0 k qC6C7 2 From these results  the algorithms in Figures 3 and 4 could be altered to take the revised definitions of W k and W C0 k into account ,Positive
 reports results for different numbers of hidden states but it is unclear how to make this choice a priori  while Goldwater & Griffiths  leave this question as future work ,Neutral
After this conversion  we had 1000 positive and 1000 negative examples for each domain  the same balanced composition as the polarity dataset  ,Neutral
First  we trained a finitestate shallow parser on base phrases extracted from the Penn Wall St Journal  WSJ  Treebank  ,Neutral
These feature vectors and the associated parser actions are used to train maximum entropy models Berger et al 1996,Neutral
Sentencelevel subjectivity detection  where training data is easier to obtain than for positive vs negative classification  has been successfully performed using supervised statistical methods alone  or in combination with a knowledgebased approach  ,Positive
411 Lexical cooccurrences Lexical cooccurrences have previously been shown to be useful for discourse level learning tasks  ,Neutral
For these first SMT systems  translationmodel probabilities at the sentence level were approximated from wordbased translation models that were trained by using bilingual corpora  ,Neutral
In our experiments two inhouse developed systems are used to validate our method The first one SYS1 is a system based on the hierarchical phrasebased model as proposed in Chiang 2005 ,Neutral
Montesi and Owen 2007 noted that professional abstractors prepend third person  ingular verbs in present tense and without subject to the author abstract a phenomenon related  yet different  from the problem we are investigating in this paper ,Neutral
A constraintbased system is also presented in Karlsson 1990 Karlsson et al 1995 ,Neutral
Lopez 2009 recently argued for a separation between featuresformalisms and the independence assumptions they imply from inference algorithms in MT this separation is widely appreciated in machine learning,Positive
As modern systems move toward integrating many features   resources such as this will become increasingly important in improving translation quality ,Neutral
Chen et al 2008 used features derived from short dependency pairs based on largescale autoparsed data to enhance dependency parsing,Neutral
Previous research has addressed revision in singledocument summaries [Jing  McKeown 2000] [Mani et al 1999] and has suggested that revising summaries can make them more informative and correct errors,Positive
There are several strategies for assigning categories to a document based on the probability Pcld  The simplest one is the kperdoc strategy Field 1975 that assigns the top k categories to each document ,Positive
At any rate  regularized conditional loglinear models have not previously been applied to the problem of producing a high quality partofspeech tagger  Ratnaparkhi   Toutanova and Manning   and  all present unregularized models ,Negative
Among the chunk types  NP chunking is the first to receive the attention   than other chunk types  such as VP and PP chunking  This model shares some similarities with the stochastic inversion transduction grammars  SITG  presented by Wu in  ,Neutral
ER adds a term to the objective function that encourages confident predictions on unlabeled data Training of linearchain CRFs with ER is described by Jiao et al 2006 ,Neutral
35 Regularization We apply lscript1 regularization Ng 2004 Gao et al 2007 to make learning more robust to noise and control the effective dimensionality of the feature spacebysubtractingaweightedsumofabsolutevalues of parameter weights from the loglikelihood of the training data w  argmaxw LLw summationdisplay i Ciwi 6 We optimize the objective using a variant of the orthantwise limitedmemory quasiNewton algorithm proposed by Andrew & Gao 20073 All values Ci are set to 1 in most of the experiments below although we apply stronger regularization Ci  3 to reordering features,Neutral
 However this remains first of all a timeconsuming task Moreover it is not easy for humans to selec tthe best translation among a set of alternatives let alone assign them probabilities Last but not least the beneficial effect on translation is not guaranteed,Negative
We ran the decoder with its default settings and then used Moses implementation of minimum error rate training  to tune the feature weights on the development set ,Neutral
The search across a dimension uses the efficient method of  In the hierarchical phrasebased model   and an inversion transduction grammar  ITG    the problem is resolved by restricting to a binarized form where at most two nonterminals are allowed in the righthand side ,Positive
Considerable interest in information extraction has concentrated on identifying named entities in text pertaining to current events for example Wacholder et al 1997 Voorhees and Harman 1998 and MUC7 however several recent efforts have been directed at biomolecular data Blaschke et al 1999 Craven and Kumlien 1999 and Rindflesch et al 2000,Neutral
Although this provides an unlimited freedom for rearranging constituents it also complicates the task of learning the parsing steps which might explain why their evaluation results show marginal improvements at best ,Negative
They were based on mutual information   conditional probabilities   or on some standard statistical tests  such as the chisquare test or the loglikelihood ratio  ,Positive
We preferred the loglikelihood ratio to other statistical scores  such as the association ratio  or   2  since it adequately takes into account the frequency of the cooccurring words and is less sensitive to rare events and corpussize  The ubiquitous minimum error rate training  MERT  approach optimizes Viterbi predictions  but does not explicitly boost the aggregated posterior probability of desirable ngrams  ,Negative
Although generating training examples in advance without a working parser  is much faster than using inference   our training time can probably be decreased further by choosing a parsing strategy with a lower branching factor ,Negative
Jacquemin 1997 focuses on the morphological processes,Neutral
1 Introduction on measures for interrater reliability   on frameworks for evaluating spoken dialogue agents  and on the use of different corpora in the development of a particular system  The CarnegieMellon Communicator  Eskenazi et al ,Neutral
The rule feature values were computed online during decoding using the suffix array method described by Lopez 2007,Neutral
 applies this approach to the socalled IBM Candide system to build context dependent models  compute automatic sentence splitting and to improve word reordering in translation ,Neutral
The morphological processing in PairClass  is more sophisticated than in  In addition  the performance of the adapted model for Joint ST obviously surpass that of   which achieves an F1 of 9341  for Joint ST  although with more complicated models and features Some are the result of inconsistency in labeling in the training data   which usually reflects a lack of linguistic clarity or determination of the correct part of speech in context ,Negative
To the best of our knowledge and with the exception of Saggion and Lapalme 2002 indicative generation approach which included operations to add extra linguistic material to generate an indicative abstract the work presented here is the first to investigate this relevant operation in the field of text abstracting and to propose a robust computational method for its simulation ,Positive
But without the global normalization  the maximumlikelihood criterion motivated by the maximum entropy principle  is no longer a feasible option as an optimization criterion   word class   measures polarity using only adjectives  however in our approach we consider the noun  the verb  the adverb and the adjective content words While most parsing methods are currently supervised or semisupervised   they depend on handannotated data which are difficult to come by and which exist only for a few languages ,Negative
In   anotherstateoftheartWSDengine  acombination of naive Bayes  maximum entropy  boosting and Kernel PCA models  is used to dynamically determine the score of a phrase pair under consideration and  thus  let the phrase selection adapt to the context of the sentence ,Positive
22 Corpus occurrence In order to get a feel for the relative frequency of VPCs in the corpus targeted for extraction namely 0 5 10 15 20 25 30 35 40 0 10 20 30 40 50 60 70 VPC types  Corpus frequency Figure 1 Frequency distribution of VPCs in the WSJ Tagger correctextracted Prec Rec Ffl1 Brill 135135 1000 0177 0301 Penn 667800 0834 0565 0673 Table 1 POSbased extraction results the WSJ section of the Penn Treebank we took a random sample of 200 VPCs from the Alvey Natural Language Tools grammar Grover et al  1993 and did a manual corpus search for each,Neutral
As resolving direct anaphoric descriptions  the ones where anaphor and antecedent have the same head noun  is a much simpler problem with high performance rates as shown in previous results   these heuristics should be applied first in a system that resolves definite descriptions ,Neutral
SEPepsilon aA # epsilon  # aepsilon aepsilon bepsilon bB UNKepsilon cC bepsilon cBC e   E epsilon   depsilon depsilon epsilonepsilon bAB # bA # B # e   DE cepsilon dBCD e   DE Figure 1  Illustration of dictionary based segmentation finite state transducer 31 Bootstrapping In addition to the model based upon a dictionary of stems and words  we also experimented with models based upon character ngrams  similar to those used for Chinese segmentation  ,Neutral
From the extracted ngrams  those with a flequc ` ncy of 3 or more were kept  other approaches get rid of ngrams of such low frequencies   ,Neutral
As an example a graphical representation Batagelj 2003 of the semantic predications serving as a summary or conceptual condensate from our system is shown in Figure 1  ,Neutral
In the case where additionally raw untagged text isavailable the Maximum Likelihood training can be used to reestimate the parametersof H M M taggers Merialdo 1994,Neutral
This method  initially proposed by   was successfully evaluated in the context of the SENSEVAL framework  Classifier Training ,Positive
In the thriving area of research on automatic analysis and processing of product reviews   little attention has been paid to the important task studied here assessing review helpfulness In such tasks  feature calculation is also very expensive in terms of time required  huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability p  f e  and reverse translation probability p  e f   ,Negative
The word formation building blocks define the so called inflectional classes which represent sequential letter strings associated with word classes as well as with individual words also known as isuffixes in Porterlike stemmers Porter1980 ,Neutral
Another interesting work exploiting capitalisationand fixedvariable suffixes is presented in Cucerzan and Yarowsky 2000,Neutral
 Introduction In recent years  Bracketing Transduction Grammar  BTG  proposed by  has been widely used in statistical machine translation  SMT  ,Positive
In Post and Gildea 2008 Shen et al 2008 target trees were employed to improve the scoring of translation theories ,Neutral
Several approaches provide similar output based on statistics Church 1988 Zhai 1997 for example a finitestate machine AitMokhtar and Chanod 1997 or a hybrid approach combining statistics and linguistic rules Voutilainen and Padro 1997,Neutral
Clustering algorithms have been previously shown to work fairly well for the classification of words into syntactic and semantic classes   but determining the optimum number of classes for a hierarchical cluster tree is an ongoing difficult problem  particularly without prior knowledge of the item classification ,Negative
Training Set  Labeled English Reviews   There are many labeled English corpora available on the Web and we used the corpus constructed for multidomain sentiment classification  9  because the corpus was largescale and it was within similar domains as the test set ,Neutral
Although various approaches to SMT system combination have been explored including enhanced combination model structure Rosti et al 2007 better word alignment between translations Ayan et al 2008 He et al 2008 and improved confusion network construction Rosti et al2008 most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way ,Neutral
 improves the F score from 882  to 897   while Charniak and Johnson  2005  improve from 903  to 94  ,Positive
Also related are the areas of word alignment for machine translation   induction of translation lexicons   and crosslanguage annotation projections to a second language  ,Neutral
Lopez and Resnik 2006 also showed that feature engineering could be used to overcome deficiencies of poor alignment To illustrate the usefulness of feature subspace in the SMT task we start with the example shown in Table 1 In the example the Chinese source sentence is translated with two settings of a hierarchical phrasebased system Chiang 2005,Neutral
Moreover  the deterministic dependency parser of Yamada and Matsumoto   when trained on the Penn Treebank  gives a dependency accuracy that is almost as good as that of  and Charniak  2000  ,Positive
However  to what extent that assumption holds is tested only on a small number of language pairs using hand aligned data  Although evaluated on a different test set  our method also outperforms the correlation with human scores reported in  However  it seems unrealistic to expect a onesizefitsall approach to be achieve uniformly high performance across varied languages  and  in fact  it doesnt Though the system presented in  outperforms the best systems in the 2006 PASCAL challenge for Turkish and Finnish  it still does significantly worse on these languages than English  Fscores of 662 and 665  compared to 794  ,Negative
The default training set of Penn Treebank  was used for the parser because the domain and style of those texts actually matches fairly well with the domain and style of the texts on which a reading level predictor for second language learners might be used ,Positive
Endemic structural ambiguitywhich can lead to such difficulties as trying to cope with the many thousands of possible parses that a grammar can assign to a sentence can be greatly reduced by addingempirically derived probabilities to grammar rules Fujisaki et al 1989 Sharman Jelinek and Mercer 1990 Black et al 1993 and by computing statistical measures oflexical association Hindle and Rooth 1993,Positive
The first scientific documents with abstracts represents a readily available class of summaries often discussed in the literature Marcu 1999  ,Neutral
In general frequently appearing terms in a document play an important role in information retrieval Salton and McGill 1983 Salton and Yang experimentally verified the importance of withindocument term frequencies in their vector model Salton and Yang 1973 ,Positive
There exist two main families of attributeselecting functions informationbased Quinlan 1986 Ldpez 1991 and statisticallybased Breiman et al 1984 Mingers 1989 ,Neutral
We took two approaches to smoothing First because Dagan et al used GoodTuring smoothing in their experiments we did likewise so as to replicate their work as closely as possible Second we tried an approach based on the distributional clustering method of Pereira et al 1993  ,Positive
Section 7 considers recent efforts to induce effective procedures for automated sense labeling of discourse relations that are not lexically marked  ,Neutral
However  work in that direction has so far addressed only parse reranking  ,Negative
6 Experiments We evaluated the translation quality of the system using the BLEU metric  ,Neutral
It is wellknown that such distributions can represent meaning reasonably well at least for meaningcomparison purposes Landauer and Dumais 1997,Neutral
The lines of the corpus are tokenized Grefenstette and Tapanainen 1994 and only sentences containing one of the word forms in the filter are retained ,Neutral
Consequently we abstract away from specifying a distribution by allowing the user to assign labels to features cf Haghighi and Klein 2006  Druck et al 2008 ,Neutral
A major problem with such methods is that each hypothesis is aligned to the backbone independently leading to sub optimal behavior,Negative
2 Statistical Word Alignment According to the IBM models   the statistical word alignment model can be generally represented as in Equation  1  ,Neutral
Some NLG researchers are impressed by the success of the BLEU evaluation metric  in Machine Translation  MT   which has transformed the MT field by allowing researchers to quickly and cheaply evaluate the impact of new ideas  algorithms  and data sets ,Positive
The  algorithm was one of the first bootstrapping algorithms to become widely known in computational linguistics Another widely used discriminative method is the perceptron algorithm   which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron ,Positive
 We feel that incorporating a robust analysis of discourse structure into a document summarizer is one step along this way ,Negative
We built a system for knowledge extraction and question answering on top of USP It generated Stanford dependencies de Marneffe et al 2006 from the input text using the Stanford parser and then fed these to USPLearn 11  which produced an MLN with learned weights and the MAP semantic parses of the input sentences,Neutral
Since more than half of the symbols in the observations may be noise models estimated in this way are not reliable,Negative
This has been now an active research area for a couple of decades  ,Neutral
Introduction Recent works in statistical machine translation  SMT  shows how phrasebased modeling  significantly outperform the historical wordbased modeling    ,Positive
For instance  for Maximum Entropy  I picked  for the basic theory   for an application  POS tagging in this case   and  for more advanced topics such as optimization and smoothing ,Neutral
No pretagged text is necessary for Hidden Markov Models Jelinek 1985 Cutting et al 1991 Kupiec 1992 ,Positive
Prior work has used a number of heuristics to deal with these problems Matusov et al 2006 He et al 08 Some work has made such decisions in a more principled fashion by computing modelbased scores Matusov et al 2008 but still special purpose algorithms and heuristics are needed and a single alignment is fixed,Positive
The features used for the experiments reported here are inspired by previous work in text summarization on content selection Kupiec et al 1995 rhetorical classification Teufel and Moens 2002 and information ordering Lapata 2003 ,Positive
The first model  referred to as Maxent1 below  is a loglinear combination of a trigram language model with a maximum entropy translation component that is an analog of the IBM translation model 2  ,Neutral
Results are reported in case insensitive BLEU score in percentages Papineni et al 2002,Neutral
One of the main directions is sentiment classification  which classifies the whole opinion document  eg  a product review  as positive or negative  ,Neutral
Our system improves over the latent namedentity tagging in   from 61  to 87  The generalized perceptron proposed by  is closely related to CRFs  but the best CRF training methods seem to have a slight edge over the generalized perceptron 2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT Phrasebased models  are good at learning local translations that are pairs of  consecutive  substrings  but often insufficient in modeling the reorderings of phrases themselves  especially between language pairs with very different wordorder ,Negative
In the second phase the resulting set of chunked rules is converted into LR table form using the method of Samuelsson 1994a ,Positive
Our translation system makes use of a hierarchical phrasebased translation model Chiang 2007 which we argue is a strong baseline for these language pairs,Neutral
We will define true nominalizations as those which have a parallel syntactic structure to the original verb This is also in keeping with the definition of nominalizations given in Quirk et al 1985 ,Neutral
Like the work of Jing and McKeown 2000 and Mani et al 1999 our work was inspired by the summarization method used by human abstractors,Positive
Section 5 presents an error analysis for  lexicalized model  which shows that the headhead dependencies used in this model fail to cope well with the flat structures in Negra Even the creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level   a problem also noted by  and  ,Negative
To compare the performance of system  we recorded the total training time and the BLEU score  which is a standard automatic measurement of the translation qualit  ,Neutral
Pustejovsky confronted with the problem of automatic acquisition more extensively in  PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank  ,Neutral
To get an idea of how the sense assignments of our d a t a set compare with those provided by WoRDNET linguists in SEMCOR the sensetagged subset of Brown corpus prepared by Miller et al Miller et al 1994 we compare a subset of the occurrences that overlap ,Neutral
While feature functions exploit statistics extracted from monolingual or wordaligned texts from the training data the scaling factors λ of the loglinear model arestimated on the development data by applying a minimum error training procedure Och 2004 ,Neutral
In recent years  sentiment classification has drawn much attention in the NLP field and it has many useful applications  such as opinion mining and summarization  ,Neutral
We use five sentiment classification datasets  including the widelyused movie review dataset  MOV   as well as four datasets containing reviews of four different types of products from Amazon  books  BOO   DVDs  DVD   electronics  ELE   and kitchen appliances  KIT    ,Positive
Instead we propose a tractable strategy for reducing model uncertainty motivated by traditional uncertainty sampling Lewis and Gale 1994 ,Neutral
Elman 1990 trains a connectionist net to predict words a process that generates internal representations that reflect grammatical category Brill et al 1990 try to infer grammatical category from bigram statistics  ,Neutral
The 746  final accuracy on apartments is higher than any result obtained by   the highest is 741    higher than the supervised HMM results reported by Grenager et al One prominent constraint of the IBM word alignment models  is functional alignment  that is each target word is mapped onto at most one source word ,Negative
Our study also shows that the simulatedannealing algorithm  is more effective 1552 than the perceptron algorithm  for feature weight tuning By segmenting words into morphemes  we can improve the performance of natural language systems including machine translation  and information retrieval  ,Negative
So far most previous work on domain adaptation for parsing has focused on datadriven systems Gildea 2001 Roark and Bacchiani 2003 McClosky et al 2006 Shimizu and Nakagawa 2007 ie systems employing constituent or de endency based treebank grammars Charniak 1996,Neutral
The algorithm employs the OpenNLP MaxEnt implementation of the maximum entropy classification algorithm  to develop word sense recognition signatures for each lemma which predicts the most likely sense for the lemma according to the context in which the lemma occurs ,Neutral
Many statistical translation models can be regarded as weighted logical deduction Under this paradigm we use weights from the expectation semiring Eisner 2002 to compute firstorder statistics eg the expected hypothesis length or feature counts over packed forests of translations lattices or hypergraphs,Neutral
1 Introduction For statistical machine translation  SMT   phrasebased methods  and syntaxbased methods  outperform wordbased methods  To our knowledge no systems directly address Problem 1  instead choosing to ignore the problem by using one or a small handful of reference derivations in an nbest list   or else making local independence assumptions which sidestep the issue  ,Negative
It is an online training algorithm and has been successfully used in many NLP tasks  such as POS tagging   parsing   Chinese word segmentation   and so on ,Positive
We speculate that this contrasts with the disappointing findings of Kehler et al 2004 since SRL provides a more fine grained level of information when compared to predicate argument statistics ,Negative
2 Motivation and Prior Work While several authors have looked at the supervised adaptation case  there are less  and especially less successful  studies on semisupervised domain adaptation  ,Negative
These methods have been used in machine translation   terminology research and translation aids   bilingual lexicography   collocation studies   wordsense disambiguation  and information retrieval in a multilingual environment  ,Neutral
Recently  it has gained renewed attention as empirical methods in parsing have emphasized the importance of relations between words  see  eg     which is what dependency grammars model explicitly  but contextfree phrasestructure grammars do not ,Neutral
To scale LMs to larger corpora with higherorder dependencies  researchers Work completed while this author was at Google Inc have considered alternative parameterizations such as classbased models   model reduction techniques such as entropybased pruning   novel represention schemes such as suffix arrays   Golomb Coding  and distributed language models that scale more readily  ,Positive
MR with or without DA is scalable to tune a large number of features while MERT is not ,Positive
The concept of lexical chains was first introduced by Morris and Hirst Basically lexical chains exploit the cohesion among an arbitrary number of related words Morris and Hirst 1991 ,Neutral
For learning coreference decisions we used a Maximum Entropy Berger et al 1996 model ,Positive
Partofspeechtagging is of interest for a number of applications for example access to text data bases Kupiec 1993 robust parsing Abney 1991 and general parsing deMarcken 1990 Charniak et al 1994 ,Neutral
Still  however  such techniques often require seeds  or prototypes  cf    which are used to prune search spaces or direct learners ,Neutral
In Weischedel et al 1993 results are given when training and testing a Markovmodel based tagger on the Penn Treebank Tagged Wall Street Journal Corpus,Neutral
the incremental feature selection algorithm by   allows one feature being added at each selection and at the same time keeps estimated parameter values for the features selected in the previous stages ,Positive
Automatically creating or extending taxonomies for specific domains is then a very interesting area of research   ,Positive
Consider the lexical model pw  ry rx   defined following   with a denoting the most frequent word alignment observed for the rule in the training set ,Neutral
3 Language modelling with Bloom filters Recentwork  presenteda scheme for associating static frequency information with a set of ngrams in a BF efficiently 3 Logfrequency Bloom filter ,Positive
Previous work for English  has shown that lexicalization leads to a sizable improvement in parsing performance Tighter integration of semantics into the parsing models  possibly in the form of discriminative reranking models   is a promising way forward in this regard ,Positive
How contentbearing a word is can also be measured with frequency counts Salton and McGill 1983 ,Neutral
Statistical parsers have been developed for TAG   LFG   and HPSG   among others ,Neutral
While in traditional wordbased statistical models  the atomic unit that translation operates on is the word  phrasebased methods acknowledge the significant role played in language by multiword expressions  thus incorporating in a statistical framework the insight behind ExampleBased Machine Translation  ,Negative
Recently many phrase reordering methods have been proposed ranging from simple distance based distortion model Koehn et al 2003 Ochand Ney 2004 flat reordering model Wu 1997Zens et al 2004 lexicalized reordering model Tillmann 2004 Kumar and Byrne 2005 to hierarchical phrasebased model Chiang 2005 Setiawan et al 2007 and classifierbased reordering model with linear features Zens and Ney 2006 Xiong et al 2006 Zhang et al 2007a Xiong et al 2008 However one of the major limitations of these advances is the structured syntactic knowledge which is important to global reordering Li et al 2007 Elming 2008 hasnot been well exploited,Negative
Semiring parsing Goodman 1999 is a general framework to describe such algorithms ,Neutral
Interannotator agreement was measured using the kappa  K  statistics  on 1502 instances  three Switchboard dialogues  marked by two annotators who followed specific written guidelines ,Neutral
Generally speaking a joint system is slower than a pipeline system in training Xue and Palmer 2004 found out that different features suited for different subtasks of SRL ie argument identification and classification ,Neutral
One conclusion that we can draw is that at present the additional word features used in  looking at words more than one position away from the current do not appear to be helping the overall performance of the models In comparison  the 2D model in Figure 2  c  used in previous work  can only model the interaction between adjacent questions ,Negative
Effective training algorithm exists  once the set of features a42 a57 a6 aa33a8 a7a54a8 a7a00a85a68a5 a53 is selected ,Positive
Halliday & Hasan 1976 offer a clear definition for text cohesion,Neutral
312 Kappa Kappa  is an evaluation measure which is increasingly used in NLP annotation work  ,Neutral
 Termbased versions of this premise have motivated much sentimentanalysis work for over a decade   ,Neutral
While the model of  significantly outperforms the constrained model of   they both are well below the stateoftheart in constituent parsing By doing so we must emphasize that  as described in the previous section  the BLEU score was not designed to deliver satisfactory results at the sentence level   and this also applies to the closely related NIST score ,Negative
This method can also be viewed to be a hypotheses reranking model since we only use the existing translations instead of performing decoding over a confusion network as done in the wordlevel combination method Rosti et al 2007 ,Neutral
The convenience of adding new rules in without worrying about where exactly it goes in terms of rule ordering something that hampered our progress in our earlier work on disambiguating Turkish morphology Oflazer and KuruSz 1994 Oflazer and Tiir 1996 has also been a key positive point ,Positive
The ENGTWOL lexicon is based on the twolevel  model Koskenniemi 1983 ,Neutral
Collocation is generally defined as a group of words that occur together more often than by chance McKeown and Radev 2000,Neutral
In the wellknown socalled IBM word alignment models   reestimating the model parameters depends on the empirical probability P  ek  fk  for each sentence pair  ek  fk  ,Positive
In NLP community  it has been shown that having more data results in better performance  ,Neutral
Online votedperceptrons have been reported to work well in a number of NLP tasks  Introduction Large scale annotated corpora  eg  the Penn TreeBank  PTB  project   have played an important role in textmining ,Positive
Meanwhile  it is common for NP chunking tasks to represent a chunk  eg  NP  with two labels  the begin  eg  BNP  and inside  eg  INP  of a chunk  ,Neutral
This revision strategy was employed by the human reviser mentioned in section 2 and we consider this to be effective because our target document has a socalled inverse pyramid structure Robin and McKeown 1996 in which the first sentence is elaborated by the following sentences ,Neutral
The chunker is trained on the answer side of the Training corpus in order to learn 2 and 3word collocations  defined using the likelihood ratio of  ,Neutral
Furthermore Turkish allows very productive derivational processes and the information about the derivational structure of a word form is usually crucial for disambiguation Oflazer and Tiir 1996 ,Neutral
The Penn Treebank annotation  was chosen to be the first among equals  it is the starting point for the merger and data from other annotations are attached at tree nodes ,Neutral
We take BBNs HierDec a stringtodependency decoder as described in Shen et al 2008 as our baseline for the following two reasons ,Positive
 reported very high results  96  on the Brown corpus  for unsupervised POS tagging using Hidden Markov Models  HMMs  by exploiting handbuilt tag dictionaries and equivalence classes ,Positive
However  evaluations on the widely used WSJ corpus of the Penn Treebank  show that the accuracy of these parsers still lags behind the stateoftheart ,Positive
The model consists of a set of wordpair parameters p  t  s  and position parameters p  j  i     in model 1  IBM1  the latter are fixed at 1   1  1   as each position  including the empty position 0  is considered equally likely to contain a translation for w Maximum likelihood estimates for these parameters can be obtained with the EM algorithm over a bilingual training corpus  as described in  ,Neutral
Phase 4 saliency is the final transforma tion phase and its operations are adapted from TOPICs Hahn and Reimer 1999 saliency operators ,Positive
A similar argument applies to all other problems in Robertson and Sparck Jones 1976 that are caused by having insufficient training cases ,Negative
Brill 1995 presents a rulebased partofspeech tagger for unsupervised training corpus  ,Positive
In computational linguistics  our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs   inter alia  ,Neutral
The Distributional Hypothesis supported by theoretical linguists such as Harris 1954 states that words that occur in the same contexts tend to have similar meanings ,Positive
Hence a tension is visible in the many recent research efforts aiming to decode with nonlocal features Chiang 2007 Huang and Chiang 2007,Neutral
Various interpolation techniques have been proposed for the estimationof the model parameters for unseen events or to smooth the modelparameters Church and Gale 1991 Essen and Steinbiss 1992 Jardinoand Adda 1993 Katz 1987 McInnes 1992,Positive
From a cognitive angle corpusbased models hold promise as simulations of how humans acquire and use conceptual and linguistic information from their environment Landauer and DuMais 1997 ,Neutral
The results evaluated by BLEU score  is shown in Table 2 ,Neutral
As pointed out by Blitzer et al 2006 each instance will actually contain features which are totally predictive of the pivot features ie the pivot itself,Neutral
As an optimal feature template subset cannot be expected to be extracted from so large a set by hand a greedy feature selection similar to that in Jiang and Ng 2006 Ding and Chang 2008 is applied,Neutral
The parser is basically based on the MSTParser 8 using all the features presented by McDonald et al 2006 with projective parsing Moreover we exploit three types of additional features to improve the parser ,Positive
first the linguistic approach in which the model is written by a linguistgenerally in the form of rules or constraints Voutilainen and Jgrvinen 1995 Second the automaticapproach in which the model is automatically obtained from corpora either raw or annotated 1  andconsists of ngrams Garside et al 1987 Cuttinget ah 1992 rules Hindle 1989 or neural netsSchmid 1994,Neutral
Schütze  1993 1995proposes two distinct methods by which ambiguity may be resolved ,Positive
A syntactic parse is however a representation that is very closely tied with the surfaceform of natural language in contrast to Semantic Role Labeling SRL which adds a layer of predicateargument information that generalizes across different syntactic alternations Palmer et al 2005 ,Neutral
For our experiments  we chose GIZA    and the RA approach  the best known alignment combination technique as our initial aligners 42 TBL Templates Our templates consider consecutive words  of size   2 or 3  in both languages ,Positive
For example  the word alignment computed by GIZA   and used as a basis to extract the TTS templates in most SSMT systems has been observed to be a problem for SSMT   due to the fact that the wordbased alignment models are not aware of the syntactic structure of the sentences and could produce many syntaxviolating word alignments ,Neutral
51 Evaluation of Translation Translations are evaluated on two automatic metrics  Bleu  and PER  position independent errorrate  ,Neutral
Recently  graphbased methods have proved useful for a number of NLP and IR tasks such as document reranking in ad hoc IR  and analyzing sentiments in text  For English  after a relatively big jump achieved by   we have seen two significant improvements   and  pushed the results by a significant amount each time ,Positive
Our model improves the baseline provided by    i  accuracy is increased by creating a lexicalised PCFG grammar and enriching conditioning context with parent fstructure features  and  ii  coverage is increased by providing lexical smoothing and fuzzy matching techniques for rule smoothing ,Negative
Some of these have been previously employed for various tasks by Gabrilovich and Markovitch    Overell and Ruger     and Suchanek et al ,Neutral
Extracts are often useful in an information retrieval environment since they give users an idea as to what the source document is about Tombros and Sanderson 1998 Mani et al 1999 but they are texts of relatively low quality  ,Neutral
Parameters were tuned with MERT algorithm Och 2003 on the NIST evaluation set of 2003 MT03 for both the baseline systems and the system combination model  ,Neutral
Successful discriminative parsers have used generative models to reduce training time and raise accuracy above generative baselines  ,Positive
In an experiment on 6800 sentences of ChineseEnglish newswire text with segmentlevel human evaluation from the Linguistic Data Consortium?s LDC Multiple Translation project we compare the LFGbased evaluation method with other popular metrics like BLEU NIST General Text Matcher GTM Turian et al  2003 Translation Error Rate TER Snover et al  2006 and METEOR Banerjee and Lavie 2005 and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment,Positive
We now describe an automatic system that can perform extraction and classification of rhetorical status on unseen text cf also a prior version of the system reported in Teufel and Moens 2000 and Teufel 1999 ,Neutral
More recently  phrasebased models  have been proposed as a highly successful alternative to the IBM models Models that can handle nonindependent lexical features have given very good results both for partofspeech and structural disambiguation  ,Positive
12 Related Work Recently  discriminative methods for alignment have rivaled the quality of IBM Model 4 alignments  ,Neutral
Zhang et al 2008 and Wellington et al 2006 answer the question what is the minimal grammar that can be induced to completely describe a training set? ,Positive
However the use of regular relations and finite state transducers Kaplan and Kay 1994 provide a very efficient implementation Method ,Positive
For the constituentbased models  constituent information was obtained from the output of  for English and Dubeys parser  2004  for German ,Neutral
Recently Chiang 2007 introduced cube pruning as an approximate decoding method that extends a DP decoder with the ability to incorporate features that break the Markovian independence assumptions DP exploits ,Positive
Even an efficient sampler like MCSAT Poon and Domingos 2006 as used in Poon & Domingos 2008 would have a hard time generating accurate estimates within a reasonable amount of time ,Negative
To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans  various metrics using ngram precision and word accuracy have been proposed  word string precision  for summarization through word extraction  ROUGE  for abstracts  and BLEU  for machine translation ,Neutral
The combination is significantly better than  at a very high level  but more importantly  Shens results  currently representing the replicable stateoftheart in POS tagging  have been significantly surpassed also by the semisupervised Morce  at the 99  confidence level  ,Negative
Kupiec 1992 has proposed an estimation method for the Ngram language model using the BaumWelch reestimation algorithm Rabiner et al 1994 from an untagged corpus and Cutting et al 1992 have applied this method to an English tagging systemTakeuchi and Matsumoto 1995 also have developed an extended method for unsegmented languages eg Japanese and applied it to their Japanese tagger  ,Positive
In the specific case of partofspeech tagging it is wellknown DeMarcken 1990 that a large proportion of the incorrect tags can be eliminated safely ie with very low risk of eliminating correct tags ,Positive
 demonstrated that semisupervised WSD could be successful ,Positive
 shows that setting those weights should take into account the evaluation metric by which the MT system will eventually be judged ,Neutral
Alternatively  order is modelled in terms of movement of automatically induced hierarchical structure of sentences  Parameters used to calculate P  D  are trained using MER training  on development data ,Neutral
Metadiscourse is ubiquitous in scientific writing Hyland 1998 found a metadiscourse phrase on average after every 15 words in running text ,Positive
Our approach is theoretically elegant like other work in this vein Goodman 1999 Lopez 2009 Gimpel and Smith 2009 ,Positive
In addition to adapting the idea of Head Word Chains   we also compared the input sentences argument structures against the treebank for certain syntactic categories ,Neutral
In phrasebased systems reordering is accomplished both within phrase pairs local reordering as well as through distancebased distortion models Koehn et al 2003 and lexicalized reordering models Koehn et al 2007,Neutral
The system just ended up at rank 7 out of 8 teams However based on annotation differences in the datasets Dredze et al 2007 and a bug in their system Shimizu and Nakagawa 2007 their results are inconclusive 1 Thus the effectiveness of SCL is rather unexplored for parsing ,Neutral
In most statistical machine translation  SMT  models   some of measure words can be generated without modification or additional processing ,Neutral
Running words 1864 14437 Vocabulary size 569 1081 Table 2  ChineseEnglish corpus statistics  using Phramer   a 3gram language model with KneserNey smoothing trained with SRILM  on the English side of the training data and Pharaoh  with default settings to decode ,Neutral
The tagger itself is based on the Hidden Markov Model Baum 1972 and word equivalence classes Kupiec 1989 ,Neutral
In practice there are more free parameters and model choices Ando and Zhang 2005 Ando 2006 Blitzer et al 2006 Blitzer 2008 besides the ones discussed above,Positive
Our algorithmperforms local alignment while the algorithm of Pang Knight and Marcu 2003performs global alignment,Positive
This method avoids the overfitting problem at the expense of losing the benefit of discriminative training of rich features directly for MT However the feature space problem still exists in these published models ,Negative
This order is fixed in advance so the maximal depth of the tree is always equal to the number of features and at the same level of the tree all nodes have the same test they are an instance of oblivious decision trees cf Langley & Sage 1994 ,Neutral
One of the most effective taggers based on a pure HMM is that developed at Xerox Cutting et al 1992 ,Positive
Historybased models for predicting the next parser action  3 ,Neutral
The myHAL model uses the same cooccurrence window but like HAL Lund and Burgess 1996 treats left and right cooccurrences as distinct features ,Neutral
They are useful indicators of overall importance Pollock and Zamora 1975 they can also be relatively easily recognized with information extraction techniques eg regular expressions  ,Neutral
Standard CI Model 1 training  initialised with a uniform translation table so that t  ejf  is constant for all sourcetarget word pairs  f  e   was run on untagged data for 10 iterations in each direction  ,Neutral
We have computed the BLEU score  accumulated up to 4grams    the NIST score  accumulated up to 5grams    the General Text Matching  GTM  Fmeasure  e  12    and the METEOR measure  ,Neutral
To quickly  and approximately  evaluate this phenomenon  we trained the statistical IBM wordalignment model 4  1 using the GIZA   software  for the following language pairs  ChineseEnglish  Italian English  and DutchEnglish  using the IWSLT2006 corpus  for the first two language pairs  and the Europarl corpus  for the last one ,Neutral
Preliminary experiments we have carried out on the Swedish version of the CLE Gambaick and Rayner 1992 have been encouraging using exactly the same pruning methods and EBL chunking criteria as for English we obtain comparable speedups ,Positive
By varying the threshold we can perform a recallprecision or errorrateambiguity tradeoff A similar strategy is adopted in de Mar Cken 1990 ,Positive
The recursive algorithms for tree construction except the final pruning and retrieval are given in Figures 1 and 2 ,Positive
We could leave the text partly disambiguated and use a syntactic parser that uses both linguistic knowledge and corpusbased heuristics see Tapanainen and Jrvinen 1994 ,Positive
It can be applied to complicated models such IBM Model4  By 17 0 10 20 30 40 50 60 70 80 90 100 10000 100000 1e06 1e07 Test Set Items with Translations  Training Corpus Size num words unigrams bigrams trigrams 4grams Figure 1 Percent of unique unigrams bigrams trigrams and 4grams from the Europarl Spanish test sentences for which translations were learned in increasingly large training corpora increasing the size of the basic unit of translation phrasebased machine translation does away with many of the problems associated with the original wordbased formulation of statistical machine translation Brown et al  1993,Negative
Our hierarchical system is Hiero Chiang 2007 modified to construct rules from a small sample of occurrences of each source phrase in training as described by Lopez 2008b,Neutral
Binarizing the grammars Zhang et al 2006 further increases the size of these sets due to the introduction of virtual nonterminals ,Neutral
In the refined model 2  alignment probabilities a  ilj  l  m  are included to model the effect that the position of a word influences the position of its translation 6 Conclusions and Future Directions In previous work  statistical NLP computation over large corpora has been a slow  of ine process  as in KNOWITALL  and also in PMIIR applications such as sentiment classi cation  ,Neutral
One such problem is sense disambiguationIn the context of machine translation Dagan and Itai Dagan Itai and Schwall 1991Dagan and Itai 1994 used corpora in the target language to resolve ambiguities inthe source language Y,Positive
For example both Haghighi and Klein 2006 and Mann and McCallum 2008 have demonstrated results better than 661 on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate ,Positive
Our summarization system relies on semantic predications provided by SemRep Rindflesch and Fiszman 2003 a program that draws on UMLS information to provide underspecified semantic interpretation in the biomedical domain Srinivasan and Rindflesch 2002 Rindflesch et al 2000 ,Positive
The output produced is in the tradition of partial parsing Hindle 1983 McDonald 1992 Weischedel et al 1993 and concentrates on the simple noun phrase what Weischedel et al 1993,Neutral
Statistical disambiguation such as  for PPattachment or  for generative parsing greatly improve disambiguation  but as they model by imitation instead of by understanding  complete soundness has to remain elusive ,Negative
Term weighting for target documents would also be necessary for sophisticated information retrieval Fuhr 1989 Kwok 1990 ,Neutral
Semantic analysis is an open research field in natural language processing Two major research topics in this field are Named Entity Recognition NER N Wacholder and Choi 1997 Cucerzan and Yarowsky 1999 and Word Sense Disambiguation WSD Yarowsky 1995 Wilks and Steven Son 1999,Neutral
Many studies on collocation extraction are carried out based on cooccurring frequencies of the word pairs in texts Choueka et al 1983 Church and Hanks 1990 Smadja 1993 Dunning 1993 Pearce 2002 Evert 2004,Neutral
The expectation semiring Eisner 2002 originally proposed for finitestate machines is one such training semiring and can be used to compute feature expectations for the Estep of the EM algorithm or gradients of the likelihood function for gradient descent  ,Positive
Bernier 1985 states that redundancy repetition andcircumlocutions are to be avoided He gives a list of linguistic expressions that can besafely removed from extracted sentences or reexpressed in order to gain conciseness,Neutral
We therefore weigh each feature with its information gaina number expressing the average amount of reduction of training set information entropywhen knowing the value of the feature Daelemans & van de Bosch 1992 Quinlan 1993Hunt et al 1966 Equation 3,Neutral
Motivated by the feature query selection method of Tandem Learning Raghavan and Allan 2007 see Section 42 for further discussion we consider a feature selection metric similarity sim that is the maximum similarity to a labeled feature weighted by the log count of the feature ,Positive
1 Introduction The task of sentence compression  or sentence reduction  can be defined as summarizing a single sentence by removing information from it  ,Neutral
Err510,Neutral
 introduced a transformationbased learning method which considered chunking as a kind of tagging problem ,Neutral
model reranking has also been established  both for synchronous binarization  and for targetonly binarization  ,Neutral
Actually our dependency structure alignment is almost the same as that of Filippova and Strube 2008 and our lead sentence plays the role of a basis tree in the Barzilay and McKeown approach 2005 ,Neutral
joint likelihood JL productdisplay i p parenleftBig xiyi  vector parenrightBig conditional likelihood CL productdisplay i p parenleftBig yi  xivector parenrightBig classification accuracy Juang and Katagiri 1992 summationdisplay i yi yxi expected classification accuracy Klein and Manning 2002 summationdisplay i p parenleftBig yi  xivector parenrightBig negated boosting loss Collins 2000  summationdisplay i p parenleftBig yi  xivector parenrightBig1 margin Crammer and Singer 2001  st bardbl vectorbardbl  1iy negationslash yi vector  vectorfxiyi   vectorfxiy   expected local accuracy Altun et al  2003 productdisplay i productdisplay j p parenleftBig lscriptjY   lscriptjyi   xivector parenrightBig Table 1 Various supervised training criteria,Neutral
The few studies on adapting disambiguation models Hara et al 2005 Plank and van Noord 2008 have focused exclusively on the supervised scenario,Positive
Similarly  Structural Correspondence Learning  has proven to be successful for the two tasks examined  PoS tagging and Sentiment Classification A notable exception is the work of  ,Positive
The techniques examined are Structural Correspondence Learning  SCL   and Selftraining  ,Neutral
Our results are similar to those for conventional phrasebased models  ,Neutral
43 Relaxing Length Restrictions Increasing the maximum phrase length in standard phrasebased translation does not improve BLEU  ,Neutral
A recent study Kan et al 2001 uses topic composition from text headers but other studies in the extraction paradigm Goldstein et al 1999 extraction coupled with rhetorical structural identification Teufel and Moens 2002 and syntactic abstraction paradigms use different meth odologies Barzilay et al 1999 McKeown et al 1999  ,Neutral
Many strategies have been proposed to integrate morphology information in SMT  including factored translation models   adding a translation dictionary containing inflected forms to the training data   entirely replacing surface forms by representations built on lemmas and POS tags   morphemes learned in an unsupervised manner   and using Porter stems and even 4letter prefixes for word alignment  ,Neutral
The alignment method described in Section 3 falls into a class of tree comparisonalgorithms extensively studied in theoretical computer science Sankoff 1975 Findenand Gordon 1985 Amir and Keselman 1994 Farach Przytycka and Thorup 1995and widely applied in many areas of computer science primarily computational biology Gusfield 1997,Neutral
One exception is the sensetagged data set used in Bruce and Wiebe 1994 which has been made available in the public domain by Bruce and Wiebe ,Neutral
Kupiec 1992 uses prespecified suffixes and performs statistical learning for POS guessing The XEROX tagger comes with a list of builtin ending guessing rules Cutting et al1992  ,Neutral
The direct translation model in Ittycheriah and Roukos 2007 employed syntactic POS tags and context information neighboring words within a maximum entropy model to predict the correct transfer rules,Positive
A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality Knight and Marcu 2002 ,Neutral
Titov et al 2009 reported the best result by using joint learning technique up to now The comparison indicates that our integrated system outputs a result quite close to the stateoftheart by the pipeline system of Johansson and Nugues 2008 as the same syntactic structure input is adopted ,Positive
This approach is similar to that of seed words  eg    or hook words  eg    in previous work ,Neutral
The second uses Lin dependency similarity  a syntacticdependency based distributional word similarity resource described in  9 ,Neutral
 Other applications include automatic subtitling Vandeghinste and Tsjong Kim Sang 2004 Vandeghinste and Pan 2004 Daelemans et al 2004 and displaying text on devices with very small screens Corston Oliver 2001 ,Neutral
 compared two Bayesian inference algorithms  Variational Bayes and what we call here a pointwise collapsed Gibbs sampler  and found that Variational Bayes produced the best solution  and that the Gibbs sampler was extremely slow to converge and produced a worse solution than EM ,Neutral
Computing the first term of the covariance in Equation 2 requires a marginal distribution over three labels two of which will be consecutive but the other of which could appear anywhere in the sequence ,Positive
Our specific approach is based on contribution tracking DeVault 2008 a framework which casts linguistic inference in situated taskoriented dialogue in probabilistic terms ,Neutral
The first task we evaluated our algorithm on is the SAT analogy questions task introduced by Turney et al 2003,Positive
In Deschacht and Moens 2009 we peform a number of experiments comparing different corpora news texts from Reuters and from Associated Press and articles from Wikipedia and ngram sizes 3gram and 4gram ,Neutral
Using multiple documents to generate a summary further complicates the situation As contended by Goldstein et al 2000 a multidocument summary may contain redundant messages since a cluster of news articles tends to cover the same main point and shared background ,Neutral
The emerging technology of information extraction Appelt and Israel 1997 Hearst 1999 provides a means of gaining access to this information,Neutral
Note that this early discarding is related to ideas behind cube pruning   which generates the top n most promising hypotheses  but in our method the decision not to generate hypotheses is guided by the quality of hypotheses on the result stack ,Neutral
While SCL has been successfully applied to PoS tagging and Sentiment Analysis Blitzer et al 2006 Blitzer et al 2007 its effectiveness for parsing was rather unexplored ,Neutral
From the results of CoNLL2008 shared task the top system by Johansson and Nugues 2008 also used two different subsystems to handle verbal and nominal predicates respectively,Neutral
In recent several years  the system combination methods based on confusion networks developed rapidly   ,Positive
A similar approach is presented in Schmid1995 Ruch et al 2000 combine POS guessing contextual rules and Markov models to build a POS tagger for biomedical text ,Neutral
Promising features might include those over source side reordering rules  or source context features  ,Positive
To quantify the performance of the learned model in comparison to our baseline we adapt the mean reciprocal rank statistic commonly used for evaluation in information retrieval Vorhees 1999 ,Positive
Syntactic alterations Levin 1993 represent a key aspect of the complex constraints that shape the syntaxsemantics interface  ,Positive
Germann et al 2004 identify two types of translation system error model error and search Error,Neutral
David McClosky  Eugene Charniak  and Mark Johnson Brown Laboratory for Linguistic Information Processing  BLLIP  Brown University Providence  RI 0292  dmcc ec mj   csbrownedu Abstract Selftraining has been shown capable of improving on stateoftheart parser performance  despite the conventional wisdom on the matter and several studies to the contrary  ,Positive
There is both synchronic Ross 1972 and diachronic Tabor 1994 evidence suggesting that words and their uses can inherit properties from several prototypical syntactic categories ,Positive
Weautomatically constructed the paraphrasing dictionary from a large comparable newscorpus using the cotraining method described in Barzilay and McKeown 2001,Neutral
We empirically compared our tagger with Eric Brills implementation of his taggerand with our implementation of a trigram tagger adapted from the work of Church1988 that we previously implemented for another purpose,Positive
There are many applications of computational linguistics particularly those involving shallow processing such as information extraction which can benefit from such automatically derived information especially as research into acquisition of grammar matures eg Clark 2001,Positive
The task of sentence compression or sentence reduction can be defined as summarizing a single sentence by removing information from it Jing and McKeown 2000 ,Neutral
This is analogous  and in a certain sense equivalent  to empirical risk minimization  which has been used successfully in related areas  such as speech recognition   language modeling   and machine translation   investigated the use of concurrent parsing of parallel corpora in a transduction inversion framework  helping to resolve attachment ambiguities in one language by the coupled parsing state in the second language ,Positive
As the dialog proceeds an utterance from a participant is accommodated into the subtask tree in an incremental manner much like an incremental syntactic parser accommodates the next word into a partial parse tree Alexandersson and Rei Thinger 1997,Neutral
Table 4 shows the linguistic features of the resulting model compared to the models of Carroll and Rooth     and Charniak  2000  ,Neutral
We continue a tradition of research that uses simple referential communication tasks to explore the organization and processing of humanc mputer and mediated human–human conversation including recently DeVault and Stone 2007 Gergle et al 2007 Healey and Mills 2006 Schlangen and Fernández 2007 ,Positive
SCL Structural Correspondence Learning Blitzer et al 2006 Blitzer et al 2007 Blitzer 2008 is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different Domains,Neutral
In ISIs syntaxbased system Galley et al 2006 and CMUs Hiero extension Venugopal et al 2007 nonterminals in translation rules have labels which must be respected by substitutions during decoding,Neutral
 claimed that this approximation achieved essentially equivalent performance to that obtained when directly using the loss as the objective  O  lscript ,Neutral
The reason for the small optimal nbest list size could be that the lowrank hypotheses might introduce more noises into the combined translation candidate pool for sentencelevel combination Hasan et al 2007 Hildebrand and Vogel 2008 ,Neutral
Similar results are presented by Merialdo 1994 who describes experiments to compare the effect of training from a handtagged corpora and using the BaumWelch algorithm with various initial Conditions,Neutral
The German NEGRA corpus consists of 20000 sentences 355000 tokens of newspaper texts Frankfurter Rundschau that are annotated with partsofspeech and predicateargument structures Skut et al 1997,Neutral
Knight and Marcu 2000 propose a noisychannel model and adecisionbased model for sentence reduction also aiming at conciseness,Neutral
The MEAD summarizer Radev et al 2002 is based on sentence extraction and uses a linear combination of three features to rank the sentences in the source documents ,Neutral
A number of partofspeech taggers are readilyavailable and widely used all trained and retrainable on text corpora Church 1988Cutting et al 1992 Brill 1992 Weischedel et al 1993,Positive
Many stateoftheart machine translation MT systems over the past few years Och and Ney 2002 Koehn et al 2003 Chiang 2007 Koehn et al 2007 Li et al 2009 rely on several models to evaluate the goodness of a given candidate translation in the target language ,Neutral
 Introduction Phrasebased method  and syntaxbased method  represent the stateoftheart technologies in statistical machine translation  SMT  ,Positive
FrameNet provides definitions for more than 500 frames of which we entertain only a small number ,Positive
We use the dataset of Rubenstein and Good enough 1965 consisting of 65 noun pairs rated by 51 subjects on a 04 similarity scale eg car automobile 39 cordsmile 00 ,Positive
As shown in   using this representation  a linear classifier can not distinguish sentences sampled from a trigram and real sentences ,Neutral
SIGHAN  the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics  conducted three prior word segmentation bakeoffs  in 2003  2005 and 2006   which established benchmarks for word segmentation and named entity recognition ,Neutral
Disambiguation of a limited number of words is not hard  and necessary context information can be carefully collected and handcrafted to achieve high disambiguation accuracy as shown in  ,Positive
11 However  modeling word order under translation is notoriously difficult   and it is unclear how much improvement in accuracy a good model of word order would provide ,Neutral
This allows us to get a possibly noisy but more abstract representation of the underlying data The set of features used in Alpino is further described in van Noord and Malouf 2005,Positive
1510 5 Related Work In recent years  many research has been done on extracting relations from free text  eg     however  almost all of them require some languagedependent parsers or taggers for English  which restrict the language of their extractions to English only  or languages that have these parsers  ,Neutral
One of the most effective taggers based on a pure HMM is that developed at Xerox  ,Positive
We also plan to apply selftraining of nbest tagger which successfully boosted the performance of one of the best existing English syntactic parser  ,Positive
A straightforward approach for approximating sentence fusion can be found in theuse of sentence extraction for multidocument summarization Carbonell and Goldstein1998 Radev Jing and Budzikowska 2000 Marcu and Gerber 2001 Lin and Hovy2002,Neutral
First splitting and merging of sentences Jing and McKeown 2000 which seems related to content planning and aggregation ,Neutral
We use the following features for our rules  sourceand targetconditioned neglog lexical weights as described in Koehn et al  2003b  neglog relative frequencies lefthandsideconditioned targetphraseconditioned sourcephraseconditioned  Counters no rule applications no target words  Flags IsPurelyLexical ie  contains only terminals IsPurelyAbstract ie  contains only nonterminals IsXRule ie  nonsyntactical span IsGlueRule 139  Penalties rareness penalty exp1  RuleFrequency unbalancedness penalty MeanTargetSourceRatio  no source words no target words 4 Parsing Our SynCFG rules are equivalent to a probabilistic contextfree grammar and decoding is therefore an application of chart parsing,Neutral
Barzilay et al 2001 evaluated three algorithms for sentence ordering in multidocument Summaries ,Neutral
Global information is known to be useful in other NLP tasks  especially in the named entity recognition task  and several studies successfully used global features  ,Positive
 Abstractive techniques in text summarization include sentence compression Cohn and Lapata 2008 headline generation Soricut and Marcu 2007 and cannedbased generation Oakes and Paice 2001,Neutral
Clark 2000 presents a framework which in principle should accommodate lexical ambiguity using mixtures but includes no evidence that it does so ,Negative
In Turneys work  the cooccurrence is considered as the appearance in the same window  Named entities also pose another problem with the  coreference model  since it models only the heads of NPs  it will fail to resolve some references to named entities   Ford Motor Co  Ford   while erroneously merging others   Ford Motor Co  Lockheed Martin Co  ,Neutral
In this context such a word is often an acronym not defined locally and indicates the presence of a binding term Fukuda et al 1998,Neutral
The following sections discuss related work describe the learning procedure and evaluate it on the Brown Corpus Francis and Kuera 1982 ,Neutral
Juman is a rulebased Japanese tagging system which uses handcoding cost values that represent the implausibility of morpheme connections and word and tagoccurences ,Positive
Additionally since phrase features can be any function of words and alignments we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase as well as phrase pairs with gaps Chiang 2005 Ittycheriah and Roukos 2007,Neutral
ARBITER pursues limited coordination identification in the spirit of Agarwal and Boggess 1992 and Rindflesch 1995 Only binding terms are considered as candidates for coordination,Positive
Measurement of Beliability The Kappa Statistic Following Jean   we use the kappa statistic  to measure degree of agreement among subjects ,Neutral
Grammatical functions are assigned using standard statistical partofspeech tagging methods cf eg Cutting et al 1992 and Feldweg 1995 ,Neutral
An end result of the project is ConceptNet 3 a large scale semantic network consisting of relations between concept pairs Havasi et al 2007  ,Neutral
In this tagger the dis ambiguation rules are applied in the same manner as the morphological rules in Koskenniemi 1983 ,Neutral
Detecting whether a pair expresses a target relation by looking at shared connector patterns with model pairs is a common strategy in relation extraction Pantel and Pennacchiotti 2008 ,Neutral
RST can be used in sentence selection for single document summarization Marcu 1997 However it cannot be applied to MDS ,Neutral
For the results in this paper  we have used Pointwise Mutual Information  PMI  instead of IBM Model 1   since  found it to be as effective on Springer  but faster to compute Numbers in the table correspond to the percentage of experiments in which the condition at the head of the column was true  for example figure in the first row and first column means that for 989 percent of the language pairs the BLEU score for the bidirectional decoder was better than that of the forward decoder  proach   ,Negative
Successful discriminative parsers have relied on generative models to reduce training time and raise accuracy above generative baselines  procedure is the most widelyused version of MERT for SMT  ,Positive
For the Brown corpus  we based our division on  ,Neutral
The process of taskoriented dialog is treated as a special case of AIstyle plan recognition Sidner 1985 Litman and Allen 1987 Rich and Sidner 1997 Carberry 2001 Bohus and Rudnicky 2003 Lochbaum 1998 ,Neutral
In comparison  we deployed the GIZA   MT modeling tool kit  which is an implementation of the IBM Models 1 to 4  ,Neutral
McCord interleaved parsing with pruning in the same way as us but only compared constituents over the same span and with the same major category Our comparisons are more global and therefore can result in more effective pruning ,Negative
When automated partofspeech tagging was initially explored Klein and Simmons 1963 Harris 1962 people manually engineered rules for tagging sometimeswith the aid of a corpus,Neutral
For example in variational decoding for machine translation Li et al 2009b p is a distribution represented by a hypergraph while q represented by a finite state automaton is an approximation to p ,Neutral
This has been quite useful for our work on tagging English Tfir Oflazer and 0zkan 1997 where such rules with negative weights were used to fine tune the behavior of the tagger in various prob lematic cases ,Positive
In other words they count an error only when the system segmentation is not acceptable to human judgement while we count an error whenever the system segmentation does not exactly match the corpus segmentation even if it is inconsistent,Negative
The Xerox tagger is claimed Cutting el al 1992 to be adaptable and easily trained only a lexicon and suitable amount of untagged text is required  ,Positive
In many NLP applications there exist rich relations among objects and recent work in statistical relational learning Getoor and Taskar 2007and structured prediction Bakir et al 2007 has shown that leveraging these can greatly improve Accuracy ,Positive
Itconstructs the trees in a top  down way guided bythe distributional information of the examples butnot on the examples order Quinlan 1986,Neutral
Koo et al 2008 presentednew features based on word clusters obtained from largescale unlabeled data and achieved large improvement for English and Czech ,Neutral
The features we use are shown in Table 2  which are based on the features used by  and Uchimoto et al ,Neutral
Such techniques are currently being applied in many areas  including language identification  authorship attribution   text genre classification   topic identification   and subjective sentiment classification  ,Neutral
To facilitate comparisons with previous work   we used the trainingdevelopmenttest partition defined in the corpus and we also used the automaticallyassigned part of speech tags provided in the corpus0 Czech word clusters were derived from the raw text section of the PDT 0  which contains about 39 million words of newswire text ,Positive
Our approach not only outperformed a notoriously difficult baseline but also achieved similar performance to the approach of   without requiring their thirdparty data resources While we have shown an increase in performance over a purely syntactic baseline model  the algorithm of    there are a number of avenues to pursue in extending this work ,Negative
While several authors have looked at the supervised adaptation case there are less and especially less successful studies on semisupervised domain adaptation McClosky et al 2006 Blitzer et al 2006 Dredze et al 2007 ,Positive
The work reported here is a logical continuation of two specific strands of research aimed in this general direction The first is the popular idea of statistical tagging eg DeRose 1988 Cutting et al 1992 Church 1988 ,Positive
One of the applications is in automatic summarization in order to com press sentences extracted for the summary Lin 2003 Jing and McKeown 2000,Neutral
Where the classification algorithm is concerned we have decided to use Support Vector Machines which have recently been used in different tasks in natural language processing they have been shown particularly suitable for text categorization Joachims 1998 ,Neutral
Syntactic analysis is carried out in another reductionistic parsing framework known as Finite State Intersection Grammar Koskenniemi 1990 Koskenniemi Tapanainen and Voutilainen 1992 Tapanainen 1992 Voutilainen and Tapanainen 1993 Voutilainen 1994 ,Neutral
Chang et al 1995 proposed an automatic dictionary construction method for Chinese from a large unsegmented corpus 311591 sentences with the help of a small segmented seed corpus 1000 Sentences ,Positive
For example the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus Gildea 2001 ,Neutral
 In our earlier work Frstenau and Lapata 2009 we acquire new training instances by projecting annotations from existing FrameNet sentences to new unseen ones ,Neutral
An alternative is to create an automatic system that uses a set of training questionanswer pairs to learn the appropriate questionanswer matching algorithm  ,Neutral
A small error ratehas been achieved by such systems when a restricted applicationdependent POS setis used eg an error rate of 26 percent has been reported by Marcus Santorini andMarcinkiewicz 1993 using the Penn Treebank corpus,Positive
Again we take advantage of the data fusion capabilities of a memorybased approach by combining these two sources of information in the case representation and having the information gain feature relevance weighting technique figure out their relative relevance see Schmid 1994 Samuelsson 1994 for similar solutions ,Positive
 Introduction Cooccurrence statistics extracted from corpora lead to good performance on a wide range of tasks that involve the identification of the semantic relation between two words or concepts  ,Positive
Apart from linguistic engineering refinements of the similarity metric we are currently experimenting with statistical measures to compute such more finegrained similarities eg Stanfill & Waltz 1986 Cost & Salzberg 1994 ,Neutral
The morphological analyser is based on a lexical transducer Karttunen et al 1992 ,Neutral
The experimental methodology was taken from Machine Learning practice eg Weiss  Kulikowski 1991 independent training and test sets were selected from the original corpus the system was trained on the training set and the generalization accuracy percentage of correct category assignments was computed on the independent test set ,Positive
55 Dependency validity features Like   we extract the dependency path from the question word to the common word  existing in both question and sentence   and the path from candidate answer  such as CoNLL NE and numerical entity  to the common word for each pair of question and candidate sentence using Stanford dependency parser  ,Neutral
22 Maximum Entropy Models Maximum entropy  ME  models   also known as 928 loglinear and exponential learning models  provide a general purpose machine learning technique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging  named entity recognition etc ,Positive
A casebased approach similar to our memorybased approach was also proposed by Cardie 1993a 1994 for sentence analysis in limited domains not only POS tagging but also semantic tagging and structural disambiguation ,Neutral
The wn   similarity package  to compute the Jiang & Conrath  J&C  distance  as in   propose using a statistical word alignment algorithm as a more robust way of aligning  monolingual  outputs into a confusion network for system com2  construct lattices over paraphrases using an iterative pairwise multiple sequence alignment  MSA  algorithm ,Neutral
For each training direction  we run GIZA     specifying 5 iterations of Model 1  4 iterations of the HMM model   and 4 iterations of Model 4 To set the weight vector w  we train twenty averaged perceptrons  on different shuffles of data drawn from sections 0221 of the Penn Treebank ,Neutral
To construct the segmentation lattices we define a loginear model of compound word segmentation inspired by Koehn and Knight 2003 making use of features including number of morphemes hypothesized frequency of the segments as freestanding morphemes in a training corpus and letters in each segment ,Positive
However their use of “perfect” clusters renders some of their algorithmic suggestions problematic ,Negative
The second aspect motivating our work comes from the subspace learning method in machine learning literature Ho 1998 in which an ensemble of classifiers are trained on subspaces of the full feature space and final classification results are based on the vote of all classifiers in the Ensemble ,Positive
In order to identify binding terminology in text we rely on the approach discussed in Rindfiesch et al 1999,Positive
Studies on the supervised task have shown that straightforward baselines eg models based on source only target only or the union of the data achieve a relatively high performance level and are surprisingly difficult to beat Daumé III 2007  ,Positive
While Kazama and Torisawa used a chunker  we parsed the definition sentence using Minipar  ,Neutral
Our manual analysis of paraphrased sentences Barzilay 2003 revealed that such alignments most frequently occur in pairs ofnoun phrases eg faculty member and professor and pairs including verbs with particles eg stand up rise,Neutral
A hierarchical alignment algorithm is a type of synchronous parser where  instead of constraining inferences by the production rules of a grammar  the constraints come from word alignments and possibly other sources  ,Neutral
Edmunson 1969 proposed the use of other features such as title words sentence locations and bonus words to improve sentence extraction ,Neutral
Kupiec Pedersen and Chen 1995 report on the semiautomatic alignment of 79 of sentences ofprofessional abstracts in a corpus of 188 documents with professional abstracts,Neutral
In our approach  equation  1  is further normalized so that the probability for different lengths of F is comparable at the word level  m m j n i ijm eft l EFP  1 10    1  1        2  The alignment models described in  are all based on the notion that an alignment aligns each source word to exactly one target word ,Neutral
 ii  Apply some statistical tests such as the Binomial Hypothesis Test  and loglikelihood ratio score  to SCCs to filter out false SCCs on the basis of their reliability and likelihood ,Neutral
 gave a systematic examination of the efficacy of unigram  bigram and trigram features drawn from different representations surface text  constituency parse tree and dependency parse tree ,Neutral
All the enumerated segment pairs are listed in the following table  We use Dunnings method  because it does not depend on the assumption of normality and it allows comparisons to be made between the signiflcance of the occurrences of both rare and common phenomenon ,Positive
In this writingsystem not all the vowels are represented several letters represent both consonantsand different vowels and gemination is not represented at all Ornan 1986 1991,Neutral
In fact previous applications of multisequence alignment have beenshown to increase the accuracy of the comparison in other NLP tasks Barzilay andLee 2002 Bangalore Murdock and Riccardi 2002 Lacatusu Maiorano and Harabagiu2004 unlike our work these approaches operate on strings not trees and with theexception of Lacatusu Maiorano and Harabagiu 2004 they apply alignment to parallel data not comparable texts,Positive
Extracting semantic information from word cooccurrence statistics has been effective  particularly for sense disambiguation  High correlation is reported between the BLEU score and human evaluations for translations from Arabic  Chinese  French  and Spanish to English  ,Positive
Linguists have long been interested in the semantic constraints that verbs impose on their arguments a broad area that has also attracted computational modeling with increasing interest in purely corpusbased methods Erk 2007 Pad et al 2007 ,Neutral
Following   we consider an anaphoric reference  NPi  correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition This algorithm adjusts the loglinear weights so that BLEU  is maximized over a given development set ,Neutral
In the partofspeech hterature whether taggers are based on a rulebased approach Klein and Simmons 1963 Brill 1992 Voutilainen 1993 or on a statistical one Bahl and Mercer 1976 Leech et al 1983 Merialdo 1994 DeRose 1988 Church 1989 Cutting et al 1992 there is a debate as to whether more attention should be paid to lexical probabilities rather than contextual ones ,Neutral
As previously observedin the literature Mani Gates and Bloedorn 1999 Jing and McKeown 2000 such components include a clause in the clause conjunction relative clauses and some elements within a clause such as adverbs and prepositions,Neutral
The piecewise linearity observation made in  is no longer applicable since we can not move the log operation into the expected value ,Neutral
3 Online Learning Again following   we have used the single best MIRA   which is a variant of the voted perceptron  for structured prediction ,Neutral
Recently several solutions to the problem of tagging unknown words have beenpresented Charniak et al 1993 Meteer Schwartz and Weischedel 1991,Positive
Good performance in many natural language processing tasks such as partofspeech tagging shallow parsing and named entity recognition has been shown to depend heavily on integrating many sources of information Zhang et al 2002 Jing et al 2003 Ittycheriah et al 2003 ,Neutral
His results may be improved if more sophisticated methods and larger corpora are used to establish similarity between words  Introduction Statistical parsing models have been shown to be successful in recovering labeled constituencies  and have also been shown to be adequate in recovering dependency relationships  ,Positive
Authors construct an argument that Myers 1992 calls the rhetorical act of the paper ,Neutral
This is called the Probabilistic Ranking Principle PRP Robertson 1977 Several strategies can be used to assign categories to a document based on PRP Lewis 1992 ,Neutral
4 Experiments The experiments described here were conducted using the Wall Street Journal Penn Treebank corpus  ,Neutral
We note that these results are competitive with those reported in the literature eg Poesio and Mikheev 1998 Serafin and Eugenio 2004 although the dialog corpus and the label sets are different ,Positive
In particular in our experiments we used the Large Grammatical Dictionary of Bulgarian Paskaleva2003 created at the Linguistic Modelling Department of the Bulgarian Academy of Sciences CLPPBAS and comprising approximately 995000 wordforms about 65000 lemmas encoded in DELAF format Silberztein1993 ,Positive
   and Lee   Wilson et al ,Neutral
This follows a general formulation of the graph alignment problem based on maximum structural matching Klau 2009 ,Neutral
Introduction We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms  often based on grammatical formalisms If we view MT as a machine learning problem  features and formalisms imply structural independence assumptions  which are in turn exploited by efficient inference algorithms  including decoders  ,Positive
This compression is achievedwhile maintaining random access using a procedure for sparse data tables following the method given by Tarjan and Yao 1979,Positive
This is a further development of our general sourcelattice approach to decoding Dyer et al 2008,Neutral
 entencelevel combination methods directly select hypotheses from original outputs of single SMT systems Sim et al 2007 Hildebrand and  ogel 2008 while phraselevel or wordlevel combination methods are more complicated and could produce new translations different from any translations in the input Bangalore et al 2001 Jayaraman and Lavie 2005 Matusov et al 2006 Sim et al2007 ,Neutral
For the IBM models defined by a pioneering paper   a decoding algorithm based on a lefttoright search was described in  ,Positive
Insideout alignments   such as the one in Example 13  can not be induced by any of these theories  in fact  there seems to be no useful synchronous grammar formalisms available that handle insideout alignments  with the possible exceptions of synchronous treeadjoining grammars   Bertsch and Nederhof  and generalized multitext grammars   which are all way more complex than ITG  STSG and  22   BRCG ,Negative
This can either be semisupervised parsing  using both annotated and unannotated data  or unsupervised parsing  training entirely on unannotated text In   as well as other similar works   only lefttoright search was employed ,Neutral
This part of the paper is essentially an extension and generalization of the line of work described in Rayner 1988 Rayner and Samuelsson 1990 Samuelsson and Rayner 1991 Rayner and Samuels son 1994 Samuelsson 1994b ,Positive
The resources that were combined include COMLEX syntactic dictionary Macleod and Grishman 1995 English Verb Classes and Alternations Levin 1993 the WordNet lexical database Miller et al 1990 the Brown Corpus tagged with WordNet senses Miller et al 1993 ,Neutral
Montesi and Owen 2007 observe that the revision of abstracts is carried out to improve comprehensibility and style and to make the abstract Objective,Neutral
All other settings were left at their default values as described by Chiang 2007 and Koehn et al 2007,Positive
Rhetorical zones appear in typical positions in the article as scientific argumentation follows certain patterns Swales 1990,Neutral
Our MT experiments use a reimplementation of Moses  called Phrasal  which provides an easier API for adding features ,Positive
Several papers have looked at higherorder representations  but have not examined the equivalence of synpara distributions when formalized as Markov chains  Of the methods we compare against  only the WordNetbased similarity measures    and  provide a method for predicting verb similarities  our learned measure widely outperforms these methods  achieving a 136  Fscore improvement over the LESK similarity measure ,Negative
The notion that nouns have only one sense per discoursecollocation was also exploited by  in his seminal work on bootstrapping for word sense disambiguation Using the components of the rowvector bm as feature function values for the candidate translation em  m a6    M   the system prior weights can easily be trained using the Minimum Error Rate Training described in  ,Positive
A number of systems for automatically learning semantic parsers have been proposed  ,Neutral
For example   used cooccurrences between verbs and their subjects and objects  and proposed a similarity metric based on mutual information  but no exploration concerning the effectiveness of other kinds of word relationship is provided  although it is extendable to any kinds of contextual information ,Positive
Thus it is critical to understand which portion of a parse tree ie structured feature space is the most effective to represent a reordering instance Motivated by the work of Zhang et al 2006 we here examine four cases that contain different substructures as shown in Fig 1 ,Positive
Although such approaches have been employed effectively   there appears to remain considerable room for improvement In terms of alignment  this wordnumber difference means that multiword connections must be considered  a task which 334 Sue J Ker and Jason S Chang Word Alignment is beyond the reach of methods proposed in recent alignment works based on  Model 1 and 2 ,Negative
In traditional active learning Settles 2009 the machine queries the user for only the labels of instances that would be most helpful to the machine ,Positive
Each technique brings its own terminology from the cubes of Chiang 2007 to the lazy lists of Pust and Knight 2009 into the mix Often it is not entirely clear why they Work ,Negative
2 Phrasebased SMT We use a phrasebased SMT system  Pharaoh    which is based on a loglinear formulation  ,Neutral
Further work can be done on the semantic verb clusters described in section 42 Klavans and Kan 1998 who use verb clusters for document classification according to genre observe that verb information is rarely used in current practical natural language Applications ,Neutral
Here a CSTenhancement procedure Zhang et al 2002 may take place ensuring that interdependent sentences appear together in a summary ,Neutral
2 Motivation and Prior Work While several authors have looked at the supervised adaptation case  there are less  and especially less successful  studies on semisupervised domain adaptation ,Negative
This approach addresses the problematic aspects of both pure knowledgebased generation  where incomplete knowledge is inevitable  and pure statistical bag generation   where the statistical system has no linguistic guidance  In addition  the clustering methods used  such as HMMs and Browns algorithm   seem unable to adequately capture the semantics of MNs since they are based only on the information of adjacent words ,Negative
Gaussier 1999 induces derivational morphology from a lexicon by means of psimilarity based splitting based splitting,Neutral
Maximum Entropy MaxEnt principle has been successfully applied in many classification and tagging tasks Ratnaparkhi 1996 K Nigam and AMcCallum 1999 A McCallum and Pereira 2000 ,Positive
We used SVDPACK to compute the singular value decompositions described in this paper Berry 1992 ,Neutral
IBM Model1  is a simplistic model which takes no account of the subtler aspects of language translation including the way word order tends to differ across languages A number of studies have investigated sentiment classification at document level  eg    and at sentence level  eg    however  the accuracy is still less than desirable ,Negative
This ITG constraint is characterized by the two forbidden structures shown in Figure 1  ,Neutral
Nivre and McDonald 2008 presented an integrating method to provide additional information for graphbased and transitionbased parsers ,Neutral
These scores are higher than those of several other parsers   but remain behind tim scores of Charniak  2000  who obtains 901  LP and 901  LR for sentences _  40 words In contrast to existing approaches   the context of the whole corpus rather than a single sentence is considered in this iterative  unsupervised procedure  yielding a more reliable alignment ,Negative
Ideas about this type ofanalogical reasoning can be found also in nonmainstream linguistics and pyscholinguisticsSkousen 1989 Derwing  Skousen 1989 Chandler 1992 Scha 1992,Neutral
Quirk et al Quirk et al 1985 distinguish nominalizations between deverbal and verbal nouns,Neutral
Indeed  researchers have shown that gigantic language models are key to stateoftheart performance   and the ability of phrasebased decoders to handle largesize  highorder language models with no consequence on asymptotic running time during decoding presents a compelling advantage over CKY decoders  whose time complexity grows prohibitively large with higherorder language models ,Positive
 Classification allows a word to align with a target word using the collective translation tendency of words in the same class ,Neutral
These words and phrases are usually compiled using different approaches  Hatzivassiloglou and McKeown  1997  Kaji and Kitsuregawa  2006   and Nasukawa  2006  Esuli and Sebastiani  2006  Breck et al  2007  Ding  Liu and Yu ,Neutral
For the give source text  S  it finds the most probable alignment set  A  and target text  T  Aa SaTpSTp       1  Brown  proposed five alignment models  called IBM Model  for an EnglishFrench alignment task based on equa68 tion  1  ,Neutral
In all of our experiments the same normalization method and classification algorithm is used with the default parameters First a TFIDF feature weighting is applied to the cooccurrence matrix Salton and Buckley 1988 ,Neutral
It is faster and more mnemonic than the one in  1 Introduction Translations tables in Phrasebased Statistical Machine Translation  SMT  are often built on the basis of Maximumlikelihood Estimation  MLE   being one of the major limitations of this approach that the source sentence context in which phrases occur is completely ignored  ,Negative
In this paper  sentence pairs are extracted by a simple model that is based on the socalled IBM Model1  ,Neutral
Mathis and Rush 1985 indicate thatsome transformations in the source material are allowed such as concatenation truncation phrase deletion voice transformation paraphrase division and word deletion,Neutral
Variables measured canbe the number of correct answers and the time to complete the task Recent experiments Jing et al 1998 have shown how different parameters such as the length ofthe abstract can affect the outcome of the evaluation,Neutral
Presently  there exist methods for learning oppositional terms  and paraphrase learning has been thoroughly studied  but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution Formal complexity analysis has not been carried out  but my algorithm is simpler  at least conceptually  than the variablewordorder parsers of Johnson     and Abramson and Dahl  1989  ,Negative
To compare the output of their shallow parser with the output of the wellknown  parser  Li and Roth applied the chunklink conversion script to extract the shallow constituents from the output of the Collins parser on WSJ section 00 while the former is piecewise constant and thus can not be optimized using gradient techniques   ,Positive
In English taggers Weischedel et al 1993 proposed a statistical model to estimate word output probability pwi|tl for an unknown word from spelling information such as inflectional endings derivational endings hyphenation and capitalization Our word model can be thought of a generalization of their statistical model ,Negative
We show translation results in terms of the automatic BLEU evaluation metric  on the MT03 ArabicEnglish DARPA evaluation test set consisting of a212a89a212a89a87 sentences with a98a89a212a161a213a89a214a89a215 Arabic words with a95 reference translations ,Neutral
As a side product  we find empirical evidence to suggest that the effectiveness of rule lexicalization techniques  and parent annotation techniques  is due to the fact that both lead to a reduction in perplexity in the automata induced from training corpora ,Positive
One popular and statistically appealing such measure is LogLikelihood  LL   ,Positive
We also compared the proposed model with two stateoftheart language models Interpolated KneserNey smoothing and fullibmpredict Goodman 2001 and found that LWLM outperformed both models on all corpora with a perplexity reduction ranging between 1240 and 587 ,Neutral
In this area the work most closely related to ours is that of Barrett and Weld Barrett and Weld 1994 who build an incremental bottomup parser to parse plans Their parser however was not probabilistic or targeted at dialog processing ,Negative
However  in   the authors investigate minimum translation units  MTU  which is a refinement over a similar approach by  to eliminate the overlap issue  ,Positive
Intuitively if we are able to find good correspondences among features then the augmented labeled source domain data should transfer better to a target domain where no labeled data is available Blitzer et al 2006,Neutral
For an introduction to the algorithms see Cutting et al 1992 or the lucid description by Sharman 1990 ,Neutral
In this paper phrase reordering is recast as a classification issue as done in previous work Xiong et al 2006 & 2008 Zhang et al 2007a ,Neutral
The surface heuristic can define consistency according to any word alignment  but most often  the alignment is provided by GIZA    ,Neutral
 Furthermore Callison Burch et al 2006 point out that it is not always appropriate to use BLEU to compare systems to each other ,Positive
For the extraction problem  there have been various methods proposed to date  which are quite adequate  whose training corpus for the noun drug was 9 times bigger than that of Karov and Edelman  reports 94  correct performance improved to impressive 939  when using the  one sense per discourse  constraint ,Positive
Two paradigms are being pursued extraction and abstraction Hahn and Mani 2000 ,Neutral
There are other types of variations for phrases  for example  insertion  deletion or substitution of words  and permutation of words such as view point and point of view are such variations    search engines   uses the Altavista web browser  while we consider and combine the frequency information acquired from three web search engines ,Neutral
Jiao et al propose semisupervised conditional random fields  that try to maximize the conditional loglikelihood on the training data and simultaneously minimize the conditional entropy of the class labels on the unlabeled data  report extracting database records by learning record field compatibility Unfortunately  a counterexample illustrated in  shows that the max function does not produce valid kernels in general ,Neutral
The segmentation model is similar to the one presented by Lee et al 2003 and obtains an accuracy of about 98 ,Neutral
With respect to computational morphology witness for instance the success of the TwoLevel paradigm introduced by Koskenniemi 1983 ,Positive
For example Briscoe and Carroll 1993 train an LR parser based on a general grammar to be able to distinguish between likely and unlikely sequences of parsing actions Andry et al 1994 automatically infer sortal constraints that can be used to rule out otherwise grammatical constituents and Grishman et al 1984 describes methods that reduce the size of a general grammar to include only rules actually useful for parsing the training corpus ,Neutral
In  it was observed that a significant percent of the queries made by a user in a search engine are associated to a repeated search Output sequence optimization Rather than basing classifications only on model parameters estimated from cooccurrences between input and output symbols employed for maximizing the likelihood of pointwise singlelabel predictions at the output level  classifier output may be augmented by an optimization over the output sequence as a whole using optimization techniques such as beam searching in the space of a conditional markov models output  or hidden markov models  ,Neutral
Rhetorical Structure Theory RST [Mann  Thompson 1988] has contributed a great deal to the understanding of the discourse of written Documents ,Positive
Research is often described as a problemsolving activity Jordan 1984 Trawinski 1989 Zappen 1983 Three information types can be expected to occur in any research article problems research goals solutions methods and results ,Neutral
In a first step the tree is completely expanded and afterwards it is pruned following a minimal costcomplexity criterion Breiman et al 1984 ,Positive
Marton and Resnik 2008 introduced features defined on constituent labels to improve the Hiero system Chiang 2005 However due to the limitation of MER training only part of the feature space could used in the system ,Positive
According to our experience  the best performance is achieved when the union of the sourcetotarget and targettosource alignment sets  is used for tuple extraction  some experimental results regarding this issue are presented in Section 422  ,Positive
In this method the subtask tree is recovered through a rightbranching shiftreduce parsing process Hall et al 2006 Sagae and Lavie 2006 ,Neutral
 Introduction Stateoftheart Statistical Machine Translation  SMT  systems usually adopt a twopass search strategy  as shown in Figure  ,Positive
Our approach most closely resembles the work of Fürstenau and Lapata 2009 who automatically expand a small training set using an automatic dependency alignment of unlabeled sentences,Positive
Subtitles can be presented at a rate of 690 to 780 characters per minute while the average speech rate is considerably higher Vandeghinste and Tsjong Kim Sang 2004 ,Neutral
In our experiments we used the Hidden Markov Model HMM tagging method described in Cutting et al 1992,Neutral
Rule ordering issue has been discussedby Voutilainen1994 but he has recently indicated 1that insensitivity to rule ordering is not a propertyof their system although Voutilainen1995a statesthat it is a very desirable property but rather isachieved by extensively testing and tuning the rules,Neutral
From theoverlap data we computed weighted recall and precision based on fractional countHatzivassiloglou and McKeown 1993,Neutral
2 Architecture of the system The goal of statistical machine translation  SMT  is to produce a target sentence e from a source sentence f It is today common practice to use phrases as translation units  and a log linear framework in order to introduce several models explaining the translation process  e    argmaxp  e f   argmaxe LCB exp  summationdisplay i ihi  e  f   RCB  1  The feature functions hi are the system models and the i weights are typically optimized to maximize a scoring function on a development set  ,Neutral
However he did not apply this algorithm to the estimation of HMM parameters  ,Negative
In the NLP context this class of algorithms has been used previously in examplebased machine translation in which the goal is to find an optimal alignment betweenthe source and the target sentences Meyers Yangarber and Grishman 1996,Neutral
Despite ME theory and its related training algorithm  do not set restrictions on the range of feature functions1  popular NLP text books  and research papers  seem to limit them to binary features 4 Conclusions Compared with other word alignment algorithms   word_align does not require sentence alignment as input  and was shown to produce useful alignments for small and noisy corpora ,Negative
Far from full syntactic complexity  we suggest to go back to the simpler alignment methods first described by  ,Positive
To tune the model parameters we selected a set of compound words from a subset of the German development set manually created a linguistically plausible segmentation of these words and used this to select the parameters of the loglinear model using a lattice minimum error training algorithm to minimize WER Macherey et al 2008,Neutral
Dagan et al 1995 then developed a postprocessor based on predicateargument statistics that was used to override RAPs decision when it failed to express a clear preference between two or more antecedents which resulted in a modest rise in performance 25 ,Positive
Nbest word segmentation hypotheses can be obtained by using the ForwardDP Backward A* algorithm Nagata 1994,Positive
Decoding is based on a beam search algorithm similar to that of the phrasebased MT decoder Koehn 2004b,Neutral
We used the MALLET maximum entropy classifier McCallum 2002 as an offtheshelf trainable maximum entropy model Each run involved two steps,Neutral
to the pairwise TER alignment described in  We obtain aligned parallel sentences and the phrase table after the training of Moses  which includes running GIZA     growdiagonalfinal symmetrization and phrase extraction  ,Neutral
For each differently tokenized corpus  we computed word alignments by a HMM translation model  and by a word alignment refinement heuristic of growdiagfinal  ,Neutral
Stateoftheart machine learning techniques including Support Vector Machines   AdaBoost  and Maximum Entropy Models  provide high performance classifiers if one has abundant correctly labeled examples ,Positive
Word alignment is also a required first step in other algorithms such as for learning subsentential phrase pairs Lavie et al 2008 or the generation of parallel treebanks Zhechev and Way 2002,Neutral
Nowadays most of the stateoftheart SMT systems are based on linear models as proposed in Och and Ney 2002 ,Positive
By segmenting words into morphemes  we can improve the performance of natural language systems including machine translation  and information retrieval  It has been difficult to identify all and only those cases where a token functions as a discourse connective  and in many cases  the syntactic analysis in the Penn TreeBank  provides no help ,Negative
stituent alignments  ,Neutral
Perceptron Reranking As  observes  perceptron training involves a simple  online algorithm  with few iterations typically required to achieve good performance In order to estimate the conditional distributions shown in Table     ,Positive
Streaming algorithms have numerous applications in mainstream computer science Muthukrishnan 2003 but to date there has been very little awareness of this field within computational linguistics,Neutral
Although this method is comparatively easy to be implemented  it just achieves the same performance as the synchronous binarization method  for syntaxbased SMT systems Among the applications of collocational analysis for lexical acquisition are the derivation of syntactic disambiguation cues Basili et al 1991 1993a Hindle and Rooths 19911993 Sekine 1992 Bogges et al 1992 sense preference Yarowski 1992 acquisition of selectional restrictions Basili et al 1992b 1993b Utsuro et al 1993 lexical preference in generation Smadjia 1991 word clustering Pereira 1993 Hindle 1990 Basili et al 1993c etc In the majority of these papers even though the precedent or subsequent statistical processing reduces the number of accidental associations very large corpora 10000000 words are necessary to obtain reliable data on a large enough number of words,Negative
The composite kernel Kcis a linear combination of the two individual kernels where the coefficient α is set to its default value 03 as that in Moschitti 2004s implementation ,Neutral
In order to overcome this  some unsupervised learning methods and minimallysupervised methods  eg    have been proposed ,Positive
42 Support Vector Machines We chose to adopt a tagging perspective for the Simple NP chunking task  in which each word is to be tagged as either B  I or O depending on wether it is in the Beginning  Inside  or Outside of the given chunk  an approach first taken by   and which has become the defacto standard for this task ,Positive
The usual solutions to this problem are l Prune the tree either during the construction process Quinlan 1993 or afterwards Mingers 1989 2 Smooth the conditional probability distributions using fresh corpus a Magerman 1996 ,Positive
Two popular techniques that incorporate the error criterion are Minimum Error Rate Training  MERT   and Minimum BayesRisk  MBR  decoding  Automated metrics such as BLEU   RED   Weighted Ngram model  WNM    syntactic relation  semantic vector model  have been shown to correlate closely with scoring or ranking by different human evaluation parameters ,Positive
As previously indicated the weightbased scheme of LL suggests MaxEnt modeling Berger et al 1996 as a particularly natural choice for a machine learning approach ,Positive
Recent work  has demonstrated that randomized encodings can be used to represent ngram counts for LMs with signficant spacesavings  circumventing informationtheoretic constraints on lossless data structures by allowing errors with some small probability ,Positive
Automatic summarization is a reductive transformation of source text to summary text through content reduction selection andor generalization on what is important in the source Sparck Jones 1999 ,Neutral
We also implemented a version of Hobbss 1978 wellknown pronoun interpretation algorithm as a baseline in which no machine learning is involved ,Neutral
Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling  and dependency parsing  with a great deal of success It is important to realize that the output of all mentioned processing steps is noisy and contains plenty of mistakes  since the data has huge variability in terms of quality  style  genres  domains etc  and domain adaptation for the NLP tasks involved is still an open problem  ,Neutral
Therefore  sublanguage techniques such as Sager  and  do not work Mutual information  though potentially of interest as a measure of collocational status  was not tested due to its wellknown property of overemphasising the significance of rare events  ,Negative
However  by exploiting the fact that the underlying scores assigned to competing hypotheses  w  e  h  f   vary linearly wrt changes in the weight vector  w   proposed a strategy for finding the global minimum along any given search direction ,Neutral
Although  there are various manualautomatic evaluation methods for these systems  eg  BLEU   these methods are basically incapable of dealing with an MTsystem and a wpMTsystem at the same time  as they have different output forms However  reordering models in traditional phrasebased systems are not sufficient to treat such complex cases when we translate long sentences  ,Negative
Training of the phrase translation model builds on top of a standard statistical word alignment over the training corpus of parallel text  for identifying corresponding word blocks  assuming no further linguistic analysis of the source or target language ,Neutral
While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments   ,Positive
We use the IBM Model 1   uniform distribution  and the Hidden Markov Model  HMM  firstorder dependency    to estimate the alignment model ,Neutral
The pervading method for estimating these probabilities is a simple heuristic based on the relative frequency of the phrase pair in the multiset of the phrase pairs extracted from the wordaligned corpus  ,Neutral
Although during minimum error training we assume a decoder that uses the maximum derivation decision rule we find benefits to translating using a minimum risk decision rule on a test set Kumar and Byrne 2004,Neutral
To compute scores for word pairs we perform pairwise hypothesis alignment using the indirect HMM He et al 2008 for every pair of input hypotheses,Neutral
Care was taken to ensure not just that the utterances themselves but also the speakers of the utterances were disjoint between test and training data as pointed out in Rayner et al 1994a failure to observe these precautions can result in substantial spurious improvements in test data results ,Neutral
A possible solution to his problem might be the use of more general morphological rules like those used in partofspeech tagging models  eg  1 2 3 4 530 40 50 60 70 80 90 100 level error RAND BASE Boost_S NNtfidf NB Boost_M Figure 6  Comparison of all models for a129 a48a51a95a66a97a98a97a180a222    where all suffixes up to a certain length are included ,Neutral
Our approach to inducing syntactic clusters is closely related to that described in Brown et al 1992 which is one of the earliest papers on the subject ,Positive
The application of the word segmenter is described elsewhere Nagata 1996,Neutral
In this paper we advocate using generalized expectation GE criteria Mann and McCallum 2008 for learning with labeled features  ,Positive
For the multilingual dependency parsing track  which was the other track of the shared task  Nilsson et al achieved the best performance using an ensemble method  ,Neutral
Equation  10  is of interest because the ratio p  C v  r   p  C r  can be interpreted as a measure of association between the verb v and class C This ratio is similar to pointwise mutual information  and also forms part of Resniks association score  which will be introduced in Section 6 ,Neutral
Studies on the supervised task have shown that straightforward baselines  eg models based on source only  target only  or the union of the data  achieve a relatively high performance level and are surprisingly difficult to beat  ,Positive
These corpusbased models can be represented eg as collocational matrices Garside et al eds 1987 Church 1988 Hidden Markov models cf Cutting et al 1992 local rules eg Hindle 1989 and neural networks eg Schmid 1994  ,Neutral
In general phrases are extracted with maximum length in the source and target defined by the parameters J max and I max  All such phrasepairs are efficiently computed by an 2 algorithm with complexity OlI max J max  Cet tolo et al 2005 ,Positive
Moses provides BLEU  and NIST   but Meteor  and TER  can easily be used instead ,Neutral
The problem itself has started to get attention only recently Roark and Bacchiani 2003 Hara et al 2005 Daum III and Marcu 2006 Daum III 2007 Blitzer et al 2006 McClosky et al 2006 Dredze et al 2007  ,Neutral
Feature active learning presented in Algorithm 1 is a poolbased active learning algorithm Lewis and Gale 1994 with a pool of features rather than instances ,Neutral
The MT community has developed not only an extensive literature on alignment   but also standard  proven alignment tools such as GIZA    ,Neutral
We discriminatively trained our parser in an online fashion using a variant of the voted perceptron  In fact  we found that it doesnt do so badly at all  the bitag HMM estimated by EM achieves a mean 1to1 tagging accuracy of 40   which is approximately the same as the 413  reported by  for their sophisticated MRF model  Motivation and Prior Work While several authors have looked at the supervised adaptation case  there are less  and especially less successful  studies on semisupervised domain adaptation  ,Neutral
We exploit similar approximate inference methods in regularized pseudolikelihood estimation Besag 1975 with hidden variables to discriminatively and efficiently train our model,Positive
We computed the weight strength of association for all the tuples extracted in this way using the local MI measure Evert 2005 that is theoretically justified easy to compute for triples and robust against overestimation of rare events ,Neutral
Johnson 1997 notes that this structure has a higher probability than the correct flat structure given counts taken from the treebank for a standard PCFGWe used a loglinear model with no Markov dependency between adjacent tags 3 and trained the parameters of the model with the perceptron algorithm  with averaging to control for overtraining  ,Neutral
The disambiguation algorithms also require that the semantic relatedness measures WordNet   Similarity  be installed ,Neutral
For example Church 1992 argues that linguists who manually perform the tagging task using the double blind method disagree about the correct analysis in at least 3 of all words even after they have negotiated about the initial disagreements ,Neutral
In machine translation  the rankings from the automatic BLEU method  have been shown to correlate well with human evaluation  and it has been widely used since and has even been adapted for summarization  ,Positive
In supervised domain adaptation Gildea 2001 Roark and Bacchiani 2003 Hara et al 2005 Daum III 2007 besides the labeled source data we have access to a comparably small but labeled amount of target data ,Neutral
In order to test some of our assumptions regarding how the differences between general English language and the language of clinical notes may affect POS tagging we have trained the HMMbased TnT tagger Brandts 2000 with default parameters at the trigram level both on Penn Treebank and the clinical notes data ,Neutral
As for the former hereafter it is referred to synPth we continue to use a dependency version of the pruning algorithm of Xue and Palmer 2004 The pruning algorithm is readdressed as the following ,Positive
There also have been prior work on maintaining approximate counts for higherorder language models  LMs     operates under the model that the goal is to store a compressed representation of a diskresident table of counts and use this compressed representation to answer count queries approximately ,Neutral
1 Introduction Base noun phrases  baseNPs   broadly the initial portions of nonrecursive noun phrases up to the head   are valuable pieces of linguistic structure which minimally extend beyond the scope of named entities ,Neutral
6 Related Work The popular IBM models for statistical machine translation are described in  ,Positive
Rulesize and lexicalization affect parsing complexity whether the grammar is binarized explicitly  or implicitly binarized using Earlystyle intermediate symbols  ,Neutral
In our work we decide to use TLTF Term Length Term Frequency term weighting technique Banko et al 1999 for scoring words in the document instead of TFIDF ,Positive
For example DIRT Lin and Pantel 2001 learns paraphrases of binary relations based on distributional similarity of their arguments TextRunner Banko et al 2007 automatically extracts relational triples in open domains using a selftrained extractor SNE applies relational clustering to generate a semantic network from TextRunner triples Kok and Domingos 2008 ,Neutral
A much simpler problem occurs in English where for some wordsthe correct syntactic tag is necessary for pronunciation Church 1988,Neutral
SparckJones and EndresNiggemeyer 1995 stated the need for a research program in text,Neutral
The difference in performance as significant at p < 005 using stratified shuffling Noreen 1989 ,Neutral
In syntactic parse reranking supersenses have been used to build useful latent semantic features  In agreement with recent resuits on parsing with lexicalised probabilistic grammars   ,Positive
A promising approach may be to use aligned bilingual corpora  especially for augmenting existing lexicons with domainspecific terminology  ,Positive
The efficient block alignment algorithm in Section 4 is related to the inversion transduction grammar approach to bilingual parsing described in   in both cases the number of alignments is drastically reduced by introducing appropriate reordering restrictions System ,Positive
To analyze our methods on IV and OOV words  we use a detailed evaluation metric than Bakeoff 2006  which includes Foov and Fiv Surprisingly  although JESSCM is a simpler version of the hybrid model in terms of model structure and parameter estimation procedure  JESSCM provides Fscores of 9445 and 8803 for CoNLL00 and 03 data  respectively  which are 015 and 083 points higher than those reported in  for the same configurations ,Negative
The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results even without the many additional extensions on the feature level as done in Blitzer et al 2006 ,Negative
 presented a thorough discussion on the Yarowsky algorithm ,Neutral
The MT system we used is Joshua Li et al 2009 a software package that comes complete with a grammar extraction module and a MERT module in addition to the decoder itself ,Positive
31 Agreement for Emotion Classes The kappa coefficient of agreement is a statistic adopted by the Computational Linguistics community as a standard measure for this purpose  ITGs translate into simple  22   BRCGs in the following way  see  for a definition of ITGs ,Neutral
The work of Miller et al 1994 Leacock et al 1993 Yarowsky 1992 used only the unordered set of surrounding words to perform WSD and they used statistical classifiers neural networks or IRbased Techniques ,Negative
To identify themes Simfinder extracts linguistically motivated features for eachsentence including WordNet synsets Miller et al 1990 and syntactic dependenciessuch as subjectverb and verbobject relations,Neutral
Previous work has mainly used WordNet Fellbaum 1998 to extend FrameNet For example Burchardt et al 2005 apply a word sense disambiguation system to annotate predicates with a WordNet sense and hyponyms of these predicates are then assumed to evoke the same frame ,Positive
In the bigram model we can weight each probability of a pair of tags in both models estimated from tagged or untagged corpora A smoothing method such as deleted interpolation Jelinek 1985 can be used for Weighting,Positive
A stack is used to maintain the global parse state The actions the parser can take are similar to those described in Ratnaparkhi 1997 ,Neutral
It is based on Incremental Sigmoid Belief Networks  ISBNs   a class of directed graphical model for structure prediction problems recently proposed in   where they were demonstrated to achieve competitive results on the constituent parsing task ,Positive
We trained a trigram model with GoodTuring smoothingover 60 megabytes of news articles collected by Newsblaster using the second versionCMUCambridge Statistical Language Modeling toolkit Clarkson and Rosenfeld 1997,Neutral
In AI the concept has appeared in several disciplines from computer vision to roboticsusing terminology such as similaritybased examplebased memorybased exemplarbased casebased analogical lazy nearestneighbour and instancebased Stanfill andWaltz 1986 Kolodner 1993 Aha et al 1991 Salzberg 1990,Neutral
Several taggers based on rules stochastic models neural networks and hybridsystems have already been presented for Partofspeech POS tagging Rulebasedtaggers Brill 1992 Elenius 1990 Jacobs and Zernik 1988 Karlsson 1990 Karlsson etal 1991 Voutilainen Heikkila and Antitila 1992 Voutilainen and Tapanainen 1993use POSdependent constraints defined by experienced linguists,Neutral
The most commonly used metric  BLEU  correlates well over large test sets with human judgments   but does not perform as well on sentencelevel evaluation  ,Negative
The importance of including single nonheadwords is now also uncontroversial   and the current paper has shown the importance of including two and more nonheadwords ,Neutral
Abstraction on the other hand relies either on linguistic processing followed by structural compaction Mani et al 1999 or on interpretation of the source text into a semantic representation which is then condensed to retain only the most important information asserted in the source ,Neutral
Such methods have also been a key driver of progress in statistical machine translation  which depends heavily on unsupervised word alignments  ,Positive
Both Charniak  and Bikel  were trained using the goldstandard tags  as this produced higher accuracy on the development set than using  s tags The utility of ITG as a reordering constraint for most language pairs  is wellknown both empirically  and analytically   howeverITGsstraight  monotone  andinverted  reverse  rules exhibit strong cohesiveness  which is inadequate to express orientations that require gaps ,Negative
Pairs of similar syntactic nodes  either words or phrases  were aligned and labeled according to a set of five semantic similarity relations Marsi and Krahmer 2007 ,Neutral
With all but two formats IBIIG achieves better FZ  l rates than the best published result in  A maximum entropy approach has been applied to partofspeech tagging before   but the approach s ability to incorporate nonlocal and nonHMMtaggertype evidence has not been fully explored ,Negative
Among them  the unsupervised algorithm using decisiontrees  has achieved promising performance Recent projects in semisupervised  and unsupervised  tagging also show significant progress ,Positive
We solve this optimization problem with a version of the branchandbound algorithm Land and Doig 1960  ,Positive
Baseline We use the Moses MT system  as a baseline and closely follow the example training procedure given for the WMT07 and WMT08 shared tasks4 In particular  we perform word alignment in each direction using GIZA     apply the growdiagfinaland heuristic for symmetrization and use a maximum phrase length of 7 ,Neutral
The POS disambiguation has usually been performed by statistical approaches mainly using hidden markov model  HMM    et al  1992  Kupiec As a baseline model we used a maximum entropy tagger  very similar to the one described in  ,Neutral
Edmundson and Wyllys 1961 find similarly low human agreement for research articles More recent experimentsreporting more positive results all used news text Jing et al 1998 Zechner 1995,Neutral
The corpus lines retained are partofspeech tagged Cutting et al 1992 ,Neutral
Typically  a small set of seed polar phrases are prepared  and new polar phrases are detected based on the strength of cooccurrence with the seeds  ,Neutral
While minimum error training  has by now become a standard tool for interpolating a small number of aggregate scores  it is not well suited for learning in highdimensional feature spaces ,Negative
Turney 2008 recently advocated the need for a uniform approach to corpusbased semantic tasks Turney recasts a number of semantic challenges in terms of relational or analogical similarity,Positive
After maximum BLEU tuning  on a heldout tuning set  we evaluate translation quality on a heldout test set ,Neutral
In the following experiments we used this binary estimation method but non binary estimates could be used as in Fuhr 1989 ,Neutral
The idea is that the translation of a sentence x into a sentence y can be performed in the following steps1   a  If x is small enough  IBMs model 1  is employed for the translation ,Neutral
Statistical systems have enjoyed considerable success for information retrieval especially using the vector space model Salton et al 1975,Positive
31 A Note on StateSplits Recent studies  suggest that categorysplits help in enhancing the performance of treebank grammars  and a previous study on MH  outlines specific POStags splits that improve MH parsing accuracy ,Neutral
We refer to this training method as maximum marginal likelihood MML it has also been explored by Quattoni et al 2007 ,Neutral
Och 2003 provides evidence that Λ should be chosen by optimizing an objective function based on the evaluation metric of interest rather than likelihood ,Positive
Related Work The recent availability of large amounts of bilingual data has attracted interest in several areas  including sentence alignment   word alignment   alignment of groups of words   and statistical translation  ,Neutral
Jing and McKeown 1999 2000 found that human summarization can be traced back to six cutandpaste operations of a text and proposed a revision method consisting of sentence reduction and combination modules with a sentence extraction part Mani and colleagues 1999 proposed a summarization system based on draft and revision together with sentence extraction The revision part is achieved with the sentence aggregation and smoothing modules ,Positive
The perceptron has been used in many NLP tasks  such as POS tagging   Chinese word segmentation  and so on ,Neutral
The model used here for sentenceboundary detection is based on the maximum entropy model used for POS tagging in  Ratnaparkhi  1996 ,Neutral
This methodological process was established after studying procedures for abstract writing Cremmins 1982 Rowley 1982 and someinitial observations from our corpus,Neutral
From the above discussion  we can see that traditional tree sequencebased method uses single tree as translation input while the forestbased model uses single subtree as the basic translation unit that can only learn treetostring  rules ,Neutral
In comparison with shallow semantic analysis tasks such as wordsense disambiguation Ide and Jeaneronis 1998 and semantic role labeling Gildea and Jurafsky 2002 Carreras and M`arquez 2005 which only partially tackle this problem by identifying the meanings of target words or finding semantic roles of predicates semantic parsing Kate et al  2005 Ge and Mooney 2005 Zettlemoyer and Collins 2005 pursues a more ambitious goal  mapping natural language sentences to complete formal meaning representations MRs where the meaning of each part of a sentence is analyzed including noun phrases verb phrases negation quantifiers and so on,Neutral
Note that the algorithm from  was designed for discriminatively training an HMMstyle tagger ,Neutral
22 Unsupervised Parameter Estimation We can perform maximum likelihood estimation of the parameters of this model in a similar fashion to that of Model 4   described thoroughly in  ,Positive
The insertion in the abstract of linguistic material not present in the input document has been addressed in paraphrase generation Barzilay and Lee 2004 and cannedbased summarization Oakes and Paice 2001 in limited domains  ,Positive
Identifying our coreferential chunks is even harder than the conventional coreference resolution and we made a simplifying assumption as in Nenkova 2008 with some additional conditions that were obtained through our preliminary experiments ,Positive
The creation of the Penn English Treebank   a syntactically interpreted corpus  played a crucial role in the advances in natural language parsing technology  ,Positive
Grefenstette 1998 proposed to remove phrases in sentences to produce a telegraphic text that can be used to provide audio scanning service for the blind ,Neutral
For example CLAWS Garside ct al 1987 normalises the lexical probabilities by the total frequency of the word rather than of the tag ,Positive
This restriction is necessary because the problem of optimizing manytomany alignments 5 Our preliminary experiments with ngrambased overlap measures  such as BLEU  and ROUGE   show that these metrics do not correlate with human judgments on the fusion task  when tested against two reference outputs ,Negative
These methods go beyond the original IBM machine translation models   by allowing multiword units  phrases  in one language to be translated directly into phrases in another language 1 Introduction Recent works in statistical machine translation  SMT  shows how phrasebased modeling  significantly outperform the historical wordbased modeling  ,Negative
The simplest one is the BIO representation scheme   where a B denotes the first item of an element and an I any noninitial item  and a syllable with tag O is not a part of any element ,Positive
Overall  agreement among judges for 250 propositions 601 A commonly used metric for evaluating interrater reliability in categorization of data is the kappa statistic  ,Neutral
A comprehensive survey of text summarization approaches can be found in Mani 1999 We briefly review here based on extraction approach Luhn 1959 proposed a simple but effective approach by using term frequencies and their related positions to weight sentences that are extracted to form a summary,Positive
At the same time  we believe our method has advantages over the approach developed initially at IBM  for training translation systems automatically Despite relying on a the same concept  our approach outperforms BE in most comparisons  and it often achieves higher correlations with human judgments than the stringmatching metric ROUGE  ,Negative
Though aiming at Chinese SRL Xue 2006 reported that their experiments show that simply adding the verb data to the training set of NomBank and extracting the same features from the verb and noun instances will hurt the overall performance ,Neutral
 In cutandpaste summarization Jing and mcKeown 2000 sentence combination operations were implemented manually following the study of a set of professionally written abstracts however the particular “pasting” operation presented here was not implemented ,Negative
Och 2003 shows that setting those weights should take into account the evaluation metric by which the MT system will eventually be judged ,Neutral
We use the labeled crossing bracket metric typically used in the syntactic parsing literature Harrison et al 1991 which computes recall precision and crossing brackets for the constituents subtrees in a hypothesized parse tree given the reference parse Tree ,Neutral
The only way to avoid it is to anonymize the notes prior to POS tagging which in itself is a difficult and expensive process Ruch et al 2000  ,Neutral
The best example of such an approach is   who proposes a method that automatically identifies collocations that are indicative of the sense of a word  and uses those to iteratively label more examples ,Positive
 In our experiments as well as in Lewis' experiments 1992 Pcld  ranges from 0 to more than 101° ,Neutral
WordNet Miller et al 1990 is the largest lexical database to date ,Neutral
The averaged 555 perceptron has a solid theoretical fundamental and was proved to be effective across a variety of NLP tasks  ,Positive
As  point out  WordNet does not encode antonymy across partofspeech  for example  legallyembargo  ,Neutral
Sharp 1989 reports onexperiments carried out with abstractors in which it is shown that introductions andconclusions provide a basis for producing a coherent and informative abstract,Neutral
The proposed approach is also amenable to an efficient implementation by finite state transducers Kaplan and Kay 1994 ,Neutral
Furthermore  early work on classbased language models was inconclusive  ,Neutral
Following Ng  Cardie 2002 our baseline system reimplements the Soon et al 2001 system ,Positive
In some sense this approach is similar to the notion of ambiguity classes explained in Kupiec 1992 and Cutting et al 1992 where words that belong to the same partofspeech figure together ,Neutral
Previous work on using the unordered set of surrounding words have used a much larger window such as the 100word window of Yarowsky 1992 and the 2sentence context of Leacock et al 1993 ,Neutral
Although various approaches to SMT system combination have been explored  including enhanced combination model structure   better word alignment between translations  and improved confusion network construction   most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way ,Negative
We also argue against Church's position supporting the claim that more attention needs to be paid to contextual information for partofspeech disambiguation Tzoukermann et ai 1995 ,Neutral
To address the datasparsity issue we employed the technique used in Keller and Lapata 2003 KL to get a more robust approximation of predicateargument counts ,Positive
For example the Markov model tagger used in the comparison of van Halteren et al 1998 yielded worse results than all other taggers ,Negative
An extrinsic evaluation additionally showed that the end result provides considerable added value when compared to sentence extracts Teufel 2001,Neutral
Deleted and spurious content is a well known problem for statistical models Chiang et al 2008 ,Neutral
Several studies have shown that largemargin methods can be adapted to the special complexities of the task  However  the capacity of these algorithms to improve over stateoftheart baselines is currently limited by their lack of robust dimensionality reduction ,Negative
To solve this problem  we adopt an idea one sense per collocation which was introduced in word sense disambiguation research  The results show that  as compared to BLEU  several recently proposed metrics such as Semanticrole overlap  ,Positive
In order to prevent the garbage collection problem where many words align to a rare word at the other side Moore 2004 we further impose the limit that if one word is aligned to more than T words these links are sorted by their alignment score and only the top T links are kept ,Neutral
A simple rulebased part of speech RBPOS tagger is introduced in Brill 1992 The accuracy of this tagger for English is comparable to a stochastic English POS tagger ,Positive
Training via the voted perceptron algorithm  or using a maxmargin criterion also correspond to the first option  eg McCallum and Wellner   Finley and Joachims   ,Neutral
The main application of these techniques to written input has been in the robust  lexical tagging of corpora with partofspeech labels  Head Lexicalization As previously shown  Charniak      Carroll and Rooth  998   etc   ,Positive
Following   we used sections 018 of the Wall Street Journal  WSJ  corpus for training  sections 1921 for development  and sections 2224 for final evaluation ,Neutral
We omit the proof here but point out that it is related to the unordered subtree matching problem which can be solved in linear time Kilpelainen 1992,Neutral
Since Czech is a language with relatively high degree of wordorder freedom  and its sentences contain certain syntactic phenomena  such as discontinuous constituents  nonprojective constructions   which can not be straightforwardly handled using the annotation scheme of Penn Treebank   based on phrasestructure trees  we decided to adopt for the PCEDT the dependencybased annotation scheme of the Prague Dependency Treebank PDT  13  give an informal example  but do not elaborate on it ,Negative
Our method uses assumptions similar to  et al 1996 but is naturally suitable for distributed parallel computations The agreement on identifying the boundaries of units  using the statistic discussed in   was  9  for two annotators and 500 units   the agreement on features  2 annotators and at least 200 units  was as follows  UTYPE   76  VERBED   9  FINITE   81 ,Neutral
Preparing tagged corpora either by hand is labourintensive and potentially errorprone and although a semiautomatic approach can be used Marcus et al 1993 it is a good thing to reduce the human involvement as much as possible  ,Neutral
This is a problem with other direct translation models such as IBM model 1 used as a direct model rather than a channel model Brown et al 1993 ,Neutral
Perhaps the most wellknown method is maximum marginal relevance  MMR    as well as crosssentence informational subsumption   mixture models   subtopic diversity   diversity penalty   and others ,Neutral
Such contextual restrictions Chanod 1993 are not always true but may be considered reasonable for resolving the ambiguity ,Neutral
In contrast Dredze et al 2007 report on frustrating results on the CoNLL 2007 semisupervised adaptation task for dependency parsing ie no team was able to improve target domain performance substantially over a state of the art baseline ,Neutral
Previous studies on texttotext abstracting Banko et al 2000 Knight and Marcu 2000 have studied problems such as sentence compression and sentence combination but not the “pasting” procedure presented here ,Negative
Hybrid approaches such as extracting phrases instead of sentences and recombining these phrases into salient text have been proposed Barzilay McKeown and Elhadad 1999 ,Positive
They were chosen among the 20 participating systems either because they held better results the first four participants or because they used some joint learning techniques Henderson et al 2008 ,Neutral
The superiority of proportional assignment over the other strategies has already been reported by Lewis 1992 ,Neutral
We seek to find a partition of the vocabulary that maximizes the mutual information between term categories and their contexts We achieve this in the framework of information theoretic coclustering Dhillon et al 2003 in which a space of entities on the one hand and their contexts on the other are alternately clustered in a way that maximizes mutual information between the two spaces,Positive
A representation which fits these requirements is adependencybased representation Melcuk 1988,Neutral
An attempt was made in the CoNLL 2007 shared task to apply SCL to nonprojective dependency parsing Shimizu and Nakagawa 2007 however without any clear conclusions ,Neutral
Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96  are readily available to assign POS to unrestricted English sentences  ,Neutral
The basic corpus used was a set of 16000 utterances from the Air Travel Planning ATIS Hemphill et al 1990 domain ,Neutral
Citation indexes are constructs that contain pointers between cited texts and citing texts Garfield 1979 traditionally in printed form When done online as in CiteSeer Lawrence Giles and Bollacker 1999 or as in Nanba and Okumuras 1999 work citations are presented in context for users to browse ,Neutral
 Recently  several successful attempts have been made at using supervised machine learning for word alignment  Loglikelihood ratio The loglikelihood ratio statistic has been found to be accurate for modeling the associations between rare events  ,Positive
Raghavan and Allan 2007 also propose several methods for learning with labeled features but in a previous comparison GE gave better results Druck et al 2008,Negative
It was originally collected and processed in two earlier research projects Atranos and Musa  on automatic subtitling Van deghinste and Tsjong Kim Sang 2004 Vandegh inste and Pan 2004 Daelemans et al 2004  ,Neutral
Since keyboard input is most efficient for assigning categories to words and phrases cf Lehmann et al 1996 Marcus et al 1994 and structural manipulations are executed most efficiently using the mouse both an elaborate keyboard and optical interface is Provided ,Neutral
In the supervised setting  a recent paper by  shows that  using a very simple feature augmentation method coupled with Support Vector Machines  he is able to effectively use both labeled target and source data to provide the best results in a number of NLP tasks Constraining learning by using document boundaries has been used quite effectively in unsupervised word sense disambiguation  ,Positive
This should not be too surprising as it is widely believed that sense tagging using the full set of refined senses found in a large dictionary like WORDNET involve making subtle human judgments Wilks et al 1990 Bruce and Wiebe 1994 such that there are many genuine cases where two humans will not agree fully on the best sense assignments,Neutral
In this section we give a brief overview of generalized expectation criteria GE Mann and McCallum 2008 Druck et al 2008 and explain how we can use GE to learn CRF parameters with estimates of feature expectations and unlabeled data ,Neutral
Predicates such as to present and to include have the tendency of appearing towards the very beginning or the very end of the abstract been therefore predicted by positionbased features Edmundson 1969 Lin and Hovy 1997 ,Neutral
Albeit simple  the algorithm has proven to be very efficient and accurate for the task of parse selection  It is an online training algorithm and has been successfully used in many NLP tasks  such as POS tagging   parsing   Chinese word segmentation   and so on ,Positive
This source of overcounting is considered and fixed by  and Zens and Ney  2003   which we briefly review here This further supports the claim by  that loglikelihood ratio is much less sensitive than pmi to low counts Inversion transduction grammar   or ITG  is a wellstudied synchronous grammar formalism ,Positive
We will show that some achieve significantly better results than the standard minimum error rate training of  Allomorphs  eg  deni and deny  are also automatically identified in   but the general problem of recognizing highly irregular forms is examined more extensively in  ,Negative
WSD systems have been far more successful in distinguishing coarsegrained senses than finegrained ones   but does that approach neglect necessary meaning differences  Secondly  while most pronoun resolution evaluations simply exclude nonreferential pronouns  recent unsupervised approaches  must deal with all pronouns in unrestricted text  and therefore need robust modules to automatically handle nonreferential instances ,Negative
In the second step the selected features were used to train the model to estimate probabilities We used MALLETs implementation of Limited memory BFGS Nocedal 1980 ,Positive
31 A simple solution  suggests that in order to have an ITG take advantage of a known partial structure  one can simply stop the parser from using any spans that would violate the structure ,Neutral
Stochastic models  have been widely used in POS tagging for simplicity and language independence of the models solved relational similarity problems using the Web as a corpus Albeit simple  the algorithm has proven to be very efficient and accurate for the task of parse selection  ,Positive
Rapp     but using cosine rather than cityblock distance to measure profile similarity ,Neutral
Although to a lesser extent  measures of word relatedness have also been applied on other languages  including German   Chinese   Dutch  and others ,Neutral
The Stanford parser Klein and Manning 2003 is used to parse Chinese sentences on the training dev and test sets,Neutral
The tagger used is thus one that does not need tagged and disambiguated material to be trained on  namely the XPOST originally constructed at Xerox Parc  ,Neutral
In our research we are concerned only with summaries of technical articles whichare called abstracts In this context two main types of abstracts are considered ANSI1979 ERIC 1980 Maizell Smith and Singer 1971,Neutral
For example   collected reviews from a movie database and rated them as positive  negative  or neutral based on the rating  eg  number of stars  given by the reviewer The earliest work in this direction are those of          and   established that it is important to tune  the tradeoff between Precision and Recall  to maximize performance ,Neutral
Many methods have been proposed to measure the cooccurrence relation between two words such as 2   mutual information   ttest   and loglikelihood  It has been argued that the reliability of a coding schema can be assessed only on the basis of judgments made by naive coders  ,Neutral
Since the word support model and triple context matching model have been proposed in our previous work  at the SIGHAN bakeoff 2005  and 2006   the major descriptions of this paper is on the WBT model ,Neutral
Methods like McDonalds  including the wellknown Maximal Marginal Relevance  MMR  algorithm   are subject to another problem  Summarylevel redundancy is not always well modeled by pairwise sentencelevel redundancy ,Negative
Bootstrapping a PMTG from a lowerdimensional PMTG and a wordtoword translation model is similar in spirit to the way that regular grammars can help to estimate CFGs   and the way that simple translation models can help to bootstrap more sophisticated ones  ,Positive
A common strategy is to exploit parallel corpora and transfer annotations from English sentences onto their translations Pad and Lapata 2006 Johansson and Nugues 2006 ,Neutral
Specifically we view the task of inferring annotations for new verbs as an instance of a structural matching problem and follow a graphbased formulation for pairwise global network alignment Klau 2009 ,Positive
The kappa value  was used to evaluate the agreement among the judges and to estimate how difficult the evaluation task was ,Neutral
Evaluation involving human judges revealed that Simfinder identifies similar sentences with493 precision at 529 recall Hatzivassiloglou Klavans and Eskin 1999,Neutral
Other work in automated support verb discovery using bilingual dictionaries as a source has been reported in Fontenelle 1993,Neutral
Corpora of spoken dialog are now widely available and frequently come with annotations for tasksgames dialog acts named entities and elements of syntactic structure These types of information provide rich clues for building dialog models Grosz and Sidner 1986,Neutral
In our experiments we treated the 128 most frequent words in the corpus as function words similar to Setiawan et al 2007,Positive
Our approach permits an alternative to minimum error rate training MERT Och 2003 it is discriminative but handles latent structure and regularization in more principled ways,Neutral
Following   Iusevariational Bayes EM  during the Mstep for the transition distribution  l 1 j i  f  E  ni  j   i  f  E  n i   C i   3  f  v   exp   v    4  60  v   braceleftBigg g  v 1 2  ifv  7  v  1  1v ow ,Neutral
It is shown in Mortensen et al 2005 that the expected size of D is a small fraction of the total number of events and its space usage comprises less than O|S| bits with high probability,Neutral
SVM has been shown to be useful for text classification tasks   and has previously given good performance in sentiment classification experiments  ,Positive
Since there is no wellagreed to definition of what an utterance is  we instead focus on intonational phrases   which end with an acoustically signaled boundary lone ,Neutral
The Maximum Entropy model Berger et al 1996 Ratnaparkhi 1997 Abney 1997 is a conditional model that assigns a probability to every possible parse ω for a given sentence s,Neutral
This layer of semantic context abstracts from the specific lexical expressions used and therefore represents a higher level of abstraction than predicate argument statistics Kehler et al 2004 and Latent Semantic Analysis used as a model of world knowledge Klebanov & WiemerHastings 2002,Neutral
The highest ranking material can then be extracted and displayed verbatim as extracts Luhn 1958 Edmundson 1969 Paice 1990 Kupiec Pedersen and Chen 1995 ,Neutral
Ornan 1986 for instance developed a new writing system for Hebrew calledThe Phonemic Script ,Positive
Early work on WSD such as Kelly and Stone 1975 Hirst 1987 used handcoding of knowledge to perform WSD The knowledge acquisition process is laborious,Negative
The choice is performed to maximize a scoring function using a set of features and a loglinear model Matusov et al 2006 Rosti et al 2007a,Neutral
Unfortunately in the data set made available in the public domain there is no indication of which sentences are used as test sentences ,Negative
For example Eisner 2002 uses finitestate operations such as composition which do combine weights entirely within the expectation semiring before their result is passed to the forwardbackward algorithm,Neutral
The firstRabiner 1989 and second He 1988 order Viterbi algorithms have been presentedelsewhere Recently Tao 1992 described the Viterbi algorithm for generalized HMMs,Neutral
We plan to explore the use of the recently developed related methods of Bellare et al 2009 Graça et al 2008 and Liang et al 2009 in future work ,Neutral
We refer to He and Gildea 2006 who tested active learning and cotraining methods but found little or no gain from semisupervised learning and to Swier and Stevenson 2004 who achieved good results using semisupervised methods but tested their methods on a small number of VerbNet roles which have not been used by other SRL Systems,Negative
Our approach has been fully implemented in the program LExAs Part of the implementation uses PEBLS Cost and Salzberg 1993 Rachlin and Salzberg 1993 a public domain exemplarbased learning system ,Positive
2 Evaluating Heterogeneous Parser Output Two commonly reported shallow parsing tasks are NounPhrase  NP  Chunking  and the CoNLL2000 Chunking task   which extends the NPChunking task to recognition of 11 phrase types1 annotated in the Penn Treebank ,Neutral
Second  benefits for sentiment analysis can be realized by decomposing the problem into SO  or neutral versus polar  and polarity classification  ,Positive
From wordlevel alignments  such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages   or with the the wordlevel alignments alone without reference to external syntactic analysis   which is the scenario we address here ,Neutral
Recently researchers have been looking for more objective definitions of relevance Kupiec Pedersen and Chen 1995 define relevance by abstract similarity ,Neutral
Third this probabilistic framework allows us to search the space of all possible entities while Soon et al 2001 Ng and Cardie 2002 take the best local hypothesis ,Neutral
One line of research focuses on the use of the knowledge contained in a machinereadable dictionary to perform WSD such as Wilks et al 1990 Luk 1995 ,Positive
 Loss Functions for Label Sequences Given the theoretical advantages of discriminative models over generative models and the empirical support by   and that CRFs are the stateoftheart among discriminative models for label sequences  we chose CRFs as our model  and trained by optimizing various objective functions a31 a3 a10a36 a25 with respect to the corpus a36 The application of these models to the label sequence problems vary widely ,Neutral
In order to estimate the conditional distributions shown in Table 1 we use the general technique of choosing the MaxEnt distribution that properly es timates the average of each feature over the train ing data Berger et al 1996  ,Neutral
Corpusderived distributional semantic spaces have proved valuable in tackling a variety of tasks ranging from concept categorization to relation extraction to many others Sahlgren 2006 Turney 2006 Pad and Lapata 2007 ,Neutral
We then describe the word segmentation algorithm and the new word extraction method with their derivation as an approximation of a generalization of the ForwardBackward algorithm Baum 1972  ,Neutral
In order to capture the dependency relationship between lexcial heads  breaks down the rules from head outwards  which prevents us from factorizing them in other ways Besides  our model  as being linguistically motivated  is also more expressive than the formally syntaxbased models of Chiang  and  ,Negative
LEXAS achieves a mean accuracy of 874 on this data set which is higher than the accuracy of 78 reported in Bruce and Wiebe 1994 ,Negative
The basic LCS has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences  ,Neutral
Even with the current incomplete set of semantic templates  the hypertagger brings realizer performance roughly up to stateoftheart levels  as our overall test set BLEU score  slightly exceeds that of   though at a coverage of 96  insteadof98  An alternative method  makes decisions at the end but has a high computational requirement ,Negative
Unlike many other methods that directly utilize noun phrase NP coreference Nenkova 2008 Mani et al 1999 we propose a method that employs insertion and substitution of phrases that modify the same chunk in the lead and other sentences,Negative
Furthermore  the BLEU score performance suggests that our model is not very powerful  but some interesting hints can be found in Table 3 when we compare our method with a 5gram language model to a stateoftheart system Moses  based on various evaluation metrics  including BLEU score  NIST score   METEOR   TER   WER and PER ,Positive
The 16000 sentence corpus was analysed by the SRI Core Language Engine Alshawi ed 1992 using a lexicon extended to cover the ATIS domain Rayner 1994 ,Neutral
a database of interconnected concepts and properties Rogers and McClelland 2004 adapting the information stored there to the task at hand ,Neutral
An analysis of the alignments shows that smoothing the fertility probabilities significantly reduces the frequently occurring problem of rare words forming garbage collectors in that they tend to align with too many words in the other language  ,Neutral
Semantic collocations are harder to extract than cooccurrence patternsthe state of the art does not enable us to find semantic collocations automatically t ,Positive
 has proposed a bootstrapping method for word sense disambiguation ,Neutral
We use the same preprocessing steps as Turian and Melamed   during both training and testing  the parser is given text POStagged by the tagger of   with capitalization stripped and outermost punctuation removed ,Neutral
Optimization problems of this form are by now widely known in NLP Koo and Collins 2005 and have recently been used for machine translation as well Blunsom et al 2008,Neutral
The sources of information we use for implementing our system are a POS tagger Foster 1991,Neutral
Moreover  the parameters of the model must be estimated using averaged perceptron training   which can be unstable This method has the advantage that it is not limited to the model scaling factors as the method described in  They reported that their method is superior to BLEU  in terms of the correlation between human assessment and automatic evaluation ,Negative
This is not surprising given that the definition of our task has little to do with the distribution of contentbearing words and phrases much less so than the related task of topic segmentation Morris and Hirst 1991 Hearst 1997 Choi 2000 or Saggion and Lapalmes 2000 approach to the summarization of scientific articles which relies on scientific concepts and their relations,Neutral
Various clustering techniques have been proposed  which perform automatic word clustering optimizing a maximumlikelihood criterion with iterative clustering algorithms ,Neutral
This latter point is a critical difference that contrasts to the major weakness of the work of  which uses a topN list of translations to select the maximum BLEU sentence as a target for training  so called local update  ,Negative
1 Introduction Texttotext generation is an emerging area of research in NLP  ,Neutral
This method was preferred against other related methods  like the one introduced in   since it embeds all the available semantic information existing in WordNet  even edges that cross POS  thus offering a richer semantic representation Both the global models  use fairly small training sets  and there is no evidence that their techniques will scale to larger data sets ,Negative
By increasing the size of the basic unit of translation  phrasebased machine translation does away with many of the problems associated with the original wordbased formulation of statistical machine translation   in particular  The Brown et al  examine the FS of the weighted loglikelihood ratio  WLLR  on the movie review dataset and achieves an accuracy of 871   which is higher than the result reported by  with the same dataset ,Negative
We used publicly available resources for all our tests for decoding we used Moses Koehn and Hoang 2007 and our parallel data was taken fromthe SpanishEnglish section of Europarl,Neutral
It is the most widely reported metric in MT research and has been shown to correlate well with human judgment Papineni et al 2002 Coughlin 2003,Neutral
The UMLS Humphreys et al 1998 consists of three components the Metathesaurus Semantic Network McCray 1993 and SPECIALIST Lexicon McCray et al 1994 ,Neutral
These tags are drawn from a tagset which is constructed by 363 extending each argument label by three additional symbols a80a44a81a83a82a84a81a86a85  following  ,Neutral
Our method is similar to the work proposed by Hildebrand and Vogel 2008  ,Positive
Finally our method to detect verb slot similarity is analogous to the slot overlap of Joanis et al 2008 and others ,Positive
32 Maximum Entropy ME models implement the intuition that the best model will be the one that is consistent with the set of constrains imposed by the evidence  but otherwise is as uniform as possible  ,Neutral
We also can not use prior graph construction methods for the document level  such as physical proximity of sentences  used in   at the word sense level ,Neutral
The analysis presented here and the idea of the alignments havebeen greatly influenced by the exploration of abstracting manuals Cremmins 1982,Positive
Furthermore Fuhr 1989 pointed out that transformation as in Eq 6 is not monotonic of Pcld ,Neutral
Note that the sense definitions used in this data set are those from Longman Dictionary of Contemporary English LDOCE Procter 1978 ,Neutral
For training in the English experiments we used WSJ Marcus et al 1993 We had to change the format of WSJ to prepare it for our tagging software,Positive
This algorithm has been applied to anumber of natural language problems including partofspeech tagging prepositionalphrase attachment disambiguation and syntactic parsing Brill 1992 Brill 1993a Brill1993b Brill and Resnik 1994 Brill 1994,Neutral
To ameliorate this revision of the extracted sentences is also thought to be effective and many ideas and methods have been proposed so far For example Otterbacher and colleagues 2002 analyzed manually revised extracts and factored out cohesion problems Nenkova 2008 proposed a revision idea that utilizes noun coreference with linguistic quality improvements in mind,Positive
For the Penn Treebank Ratnaparkhi 1996 reports an accuracy of 966 using the Maximum Entropy approach our much simpler and therefore faster HMM approach delivers 967 T,Negative
In an evaluation on the PENN treebank   the parser outperformed other unlexicalized PCFG parsers in terms of labeled bracketing fscore ,Neutral
Pivots are features occurring frequently and behaving similarly in both domains Blitzer et al 2006 They are inspired by auxiliary problems from Ando and Zhang 2005,Positive
Paice 1990 introduces grammars for pattern matching of indicator phrases eg the aimpurpose of this paperarticlestudy and we concludepropose ,Positive
This should be regarded neither as a surprise nor a criticism considering cube prunings origins in hierarchical phrasebased MT models Chiang 2007 which have only a small number of distinct Nonterminals,Neutral
This accuracy compares very favourably with results reported in de Marcken 1990 Weisehedel et al 1993 Kempe 1994  for instance to reach the recall of 993  the system by Weischedel et al 1993 has to leave as many as three readings per word in its output ,Positive
This concept of alignment has been also used for tasks like authomatic vocabulary derivation and corpus alignment  ,Neutral
612 ROUGE evaluation Table 4 presents ROUGE scores  of each of humangenerated 250word surveys against each other ,Neutral
We have recently completed a prototype implementation of this approach in C for English Brown Corpus and have obtained quite similar results Tiir Of lazer and Ozkan 1997 ,Positive
On the other hand redundancy can be exploited to identify important and accurateinformation for applications such as summarization and question answering Maniand Bloedorn 1997 Radev and McKeown 1998 Radev Prager and Samn 2000 ClarkeCormack and Lynam 2001 Dumais et al 2002 ChuCarroll et al 2003,Neutral
Metrics based on syntactic similarities such as the headword chain metric  HWCM   ,Neutral
To prune away those pairs  we used the loglikelihoodratio algorithm  to compute the degree of association between the verb and the noun in each pair ,Neutral
The Penn Treebank documentation  defines a commonly used set of tags ,Neutral
We follow the approach of bootstrapping from a model with a narrower parameter space as is done in  eg Och and Ney  and  ,Neutral
This may be because their system was not tuned using minimum error rate training  However  most of the existing models have been developed for English and trained on the Penn Treebank   which raises the question whether these models generalize to other languages  and to annotation schemes that differ from the Penn Treebank markup ,Neutral
Also  in a  stateoftheart English parser  only the words tha  t occur more tha  n d times in training data ,Positive
This is the shared task baseline system for the 2006 NAACLHLT workshop on statistical machine translation  and consists of the Pharaoh decoder   SRILM   GIZA     mkcls   Carmel 1 and a phrase model training code ,Neutral
Following Padó and Lapata 2007 we use Pearsons r to evaluate how the distances cosines in the CxLC space between the nouns in each pair correlate with the ratings ,Positive
Considering the size of most documents the linear nature of this algorithm makes it usable for generalized summarization of large documents Silber and McCoy 2000 ,Neutral
A number of part of speech taggers are readily available and widely used  all trained and retrainable on text corpora  To avoid this problem  we adopt crossvalidation training as used in  ,Positive
However  the pb features yields no noticeable improvement unlike in prefect lexical choice scenario  this is similar to the findings in  ,Neutral
The first stage parser is a bestfirst PCFG parser trained on sections 2 through 22  and 24 of the Penn WSJ treebank  ,Neutral
Finally  we are investigating several avenues for using this system output for Machine Translation  MT  including   1  aiding word alignment for other MT system   and  2  aiding the creation various MT models involving analyzed text  eg   ,Neutral
 ie   ll  Lj   maz  zi  j  u   i  I where xi  j  u  E Qi and max  xi  j  u   is the highest score in the line of the matrix Qi which corresponds to the head word sense j n is the number of modifiers of the head word h at the current tree level  and k i Lj  j  l Lj where k is the number of senses of the head word h The reason why gj  I0  is calculated as a sum of the best scores  ll   rather than by using the traditional maximum likelihood estimate   Gah eta  ,Neutral
Turney also reported good result without domain customization  In our experiments  we follow Lowe and McDonald  in using the wellknown loglikelihood ratio G 2  ,Positive
Another consequence of not generating posthead conjunctions and punctuation as firstclass words is that they 19 In fact  if punctuation occurs before the head  it is not generated at alla deficiency in the parsing model that appears to be a holdover from the deficient punctuation handling in the model of  ,Negative
Rowley 1982proposes the following typology of different types of document condensations,Neutral
Some svs terns perform a previous recasting of the attributes in order to have only binaryvalued attributes and to deal with binary trees Magerman 1996,Neutral
Statistical Model In SIFTs statistical model  augmented parse trees are generated according to a process similar to that described in  ,Neutral
For English  we use three stateoftheart taggers  the taggers of  and  in Step   and the SVM tagger  in Step 3 We used the average perceptron algorithm of  in our experiments  a variation that has been proven to be more effective than the standard algorithm shown in Figure 2 ,Positive
The template we use here is similar to   but we have added extra context words before the X and after the Y Our morphological processing also differs from  ,Neutral
We utilise the automatic annotation algorithm of  to derive a version of PennII where each node in each tree is annotated with an LFG functional annotation  ie an attribute value structure equation  ,Neutral
Like WASP1  the phrase extraction algorithm of PHARAOH is based on the output of a word alignment model such as GIZA     which performs poorly when applied directly to MRLs  Section 32  ,Negative
The output of the parser is dependency structure based on the guidelines of CGN Oost Dijk 2000 ,Neutral
This requirement speaks against the traditional sort of dependency trees in which heads are represented as nonterminal nodes cf Hudson 1984 ,Neutral
Recent research Daume et al 2002 has show that syntaxbased languagemodels are more suitable for language generation tasks the study of such models isa promising direction to explore,Positive
Moreover  rather than predicting an intrinsic metric such as the PARSEVAL Fscore  the metric that the predictor learns to predict can be chosen to better fit the final metric on which an endtoend system is measured  in the style of  ,Neutral
With regard to semantic similarity WordNet is a prime contender and indeed has been previously used to acquire new predicates in FrameNet Pennacchiotti et al 2008 Burchardt et al 2005 Johansson and Nugues 2007 ,Neutral
Fortunately using distributional characteristics of term contexts it is feasible to induce partofspeech categories directly from a corpus of sufficient size as several papers have made clear Brown et al 1992 Schütze 1993 Clark 2000 ,Positive
The first serious linguistic competitor to datadriven statistical taggers is the English Constraint Grammar parser EngCG cf Voutilainen et al 1992 Karlsson et al eds 1995 ,Positive
Several teams had approaches that relied to varying degrees on an IBM model of statistical machine translation Brown et al  1993 with different improvements brought by different teams consisting of new submodels improvements in the HMM model model combination for optimal alignment etc Several teams used symmetrization metrics as introduced in Och and Ney 2003 union intersection refined most of the times applied on the alignments produced for the two directions sourcetarget and targetsource but also as a way to combine different word alignment systems,Negative
Arguably the most widely used is the mutual information  An especially wellfounded framework is maximum entropy  2 Related Work Supervised machine learning methods including Support Vector Machines  SVM  are often used in sentiment analysis and shown to be very promising  ,Positive
While significant time savings have already been reported on the basis of automatic pretagging  eg  for POS and parse tree taggings in the Penn TreeBank   or named entity taggings for the Genia corpus    this kind of preprocessing does not reduce the number of text tokens actually to be considered ,Positive
[Mani et al 1999] focused on the revision of singledocument summaries in order to improve their Informativeness They noted that such revision might also fix ‘coherence errors ,Positive
Therefore the probability of alignment aj for position j should have a dependence on the previous alignment position O j_l  P    j    j1  A similar approach has been chosen by  and  ,Neutral
Categories that can be induced well those characterized by local dependencies could be in put into procedures that learn phrase structure eg Brill and Marcus 19925 Finch 1993,Neutral
32 Evaluation Metrics AER  Alignment Error Rate   is the most widely used metric of alignment quality  but requires goldstandard alignments labeled with surepossible annotations to compute  lacking such annotations  we can compute alignment fmeasure instead ,Negative
In our experiments we use the ASSERT parser Pradhan et al 2004 an SVM based semantic role tagger which uses a full syntactic analysis to automatically identify all verb predicates in a sentence together with their semantic arguments which are output as PropBank arguments Palmer et al 2005 ,Neutral
The evaluations can be made in intrinsic or extrinsic fashions as defined by Sparck Jones andGalliers 1995,Neutral
That is an assumption of full statistical dependence Yarowsky 1994 rather than the more common full independence is made  ,Neutral
To counteract this  we introduce two brevity penalty measures  BP  inspired by BLEU  which we incorporate into the loss function  using a product  loss  1PrecBP  BP1  exp  1max  1  rc    6  BP2  exp  1max  cr  rc   where r is the reference length and c is the candidate length ,Neutral
We report case sensitive Bleu  scoreBleuCforallexperiments ,Neutral
Druck et al 2008 provide a survey of other related methods for learning with labeled input features ,Neutral
Our approach to selectional preference is nearly identical to the one of Padó et al 2007 We solve SAT analogies with a simplified version of the method of Turney 2006 ,Positive
We examine the effectiveness of Structural Correspondence Learning SCL Blitzer et al 2006 for this task a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis,Positive
While we do not have a direct comparison  we note that  performs worse on movie reviews than on his other datasets  the same type of data as the polarity dataset ,Negative
Introduction Michael  parsing models have been quite influential in the field of natural language processing We do not completely rule out the possibility that some more sophisticated  ontologically promiscuous  firstorder analysis  perhaps along the lines of   might account for these kinds of monotonicity inferences It is promising to optimize the model parameters directly with respect to AER as suggested in statistical machine translation  ,Positive
Most recently Yarowsky used an unsupervised learning procedure to perform WSD Yarowsky 1995 although this is only tested on disambiguating words into binary coarse sense distinction ,Positive
For MCE learning  we selected the reference compression that maximize the BLEU score    argmax rR BLEU  r  R r   from the set of reference compressions and used it as correct data for training ,Neutral
Similar results have beenreported by Maltese and Mancini 1991 for the Italian language Weischedel et al1993 have used four categories of word morphology such as inflectional endingsderivational endings hyphenation and capitalization,Neutral
We use the discriminative perceptron learning algorithm  to train the values of vectorw ,Neutral
I used 26108 Japanese untagged sentences as training data and 100 handtagged sentences as test data both from the Nikkei newspaper 1994 corpus Nihon Keizai Shimbun Inc 1995 ,Neutral
This last result was in accordance with the previous acknowledgment CallisonBurch et al 2006 that systems of too differing structure could not be compared reliably with BLEU ,Neutral
Assuming that the parameters P  etk fsk  are known  the most likely alignment is computed by a simple dynamicprogramming algorithm1 Instead of using an ExpectationMaximization algorithm to estimate these parameters  as commonly done when performing word alignment   we directly compute these parameters by relying on the information contained within the chunks ,Neutral
inch and Chater 1992 and Finch 1993 use vector models in which words are clustered according to the similarity of their close neighbors in a corpus,Neutral
A windowing approach Sejnowski  Rosenberg 1987 was used to represent the tagging task as a classification problem ,Positive
The combined system tackles the disambiguation problem by combining two kindsof linguistic information sources MorphoLexical Probabilities and Syntactic Constraints a full description of this system can be found in Levinger [1992],Positive
The use of dependencies in MT evaluation has not been extensively researched before  one exception here would be    and requires more research to improve it  but the method shows potential to become an accurate evaluation metric ,Positive
Aneffort has recently been undertaken to create automated machine translation systemsin which the linguistic information needed for translation is extracted automaticallyfrom aligned corpora Brown et al 1990,Positive
Teufel and Moens 1998 report on a similar work but this time onthe alignment of sentences from authorprovided abstracts,Neutral
The structure of the model was inspired by a similar although generative model in Thompson et al 2006 where it was used for semantic frame classification,Neutral
We have used the basic source channel model described eg in Merialdo 1992,Neutral
This is because their training data  the Penn Treebank   does not fully annotate NP structure Although this Wikipedia gazetteer is much smaller than the English version used by  that has over 2000000 entries  it is the largest gazetteer that can be freely used for Japanese NER ,Negative
Wu and Weld  and   calculate the overlap between contexts of named entities and candidate articles from Wikipedia  using overlap ratios or similarity scores in a vector space model  respectively ,Neutral
For example  it has been observed that texts often contain multiple opinions on different topics   which makes assignment of the overall sentiment to the whole document problematic ,Neutral
We measured associations using the loglikelihood measure  for each combination of target category and semantic class by converting each cell of the contingency into a 22 contingency table ,Neutral
Such tasks will require an extension of the current framework of Turney 2008 beyond evidence from the direct cooccurrence of target word pairs ,Neutral
The problem is typically presented in logspace  which simplifies computations  but otherwise does not change the problem due to the monotonicity of the log function  hm  log hprimem  log p  t s   summationdisplay m m hm  t  s   3  Phrasebased models  are limited to the mapping of small contiguous chunks of text Though taggers based on dependency networks   SVM   MaxEnt   CRF   and other methods may reach slightly better results  their traintest cycle is orders of magnitude longer ,Negative
Here  1   m are the parameters of the loglinear model  which we optimize on a heldout portion of the training set  using minimumerrorrate training  ,Neutral
Weights for these separate models were tuned through the Mert algorithm provided in the Moses toolkit Koehn et al 2007 using the provided news tuning set ,Neutral
Phase 1 relevance a condensation process identifies predications on a given topic in this study disorders and is controlled by a semantic schema Jacquelinet et al 2003 for that topic ,Neutral
In contrast to the view of science as a disinterested fact Factory researchers like Swales 1990 have long claimed that there is a strong social aspect to science because the success of a researcher is correlated with her ability to convince the field of the quality of her work and the validity of her arguments ,Neutral
Most stateoftheart SMT systems treat grammatical elements in exactly the same way as content words  and rely on generalpurpose phrasal translations and target language models to generate these elements  For instance   shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks ,Positive
This default strategy has been advocated as the baseline performance level for comparison with WSD programs Gale et al 1992,Neutral
Schiller 1996 describes the general architecture of the tool for noun phrase markup based on finitestate techniques and statistical partofpeech disambiguation for seven European languages ,Neutral
Subsequent works have demonstrated the success of Luhns approach Buyukkokten et al 2001 LamAdesina and Jones 2001 Jaruskulchai et al 2003 ,Neutral
We adopt a similar approach to the one used in Turney 2008 and consider each question as a separate binary classification problem with one positive training instance and 5 unknown pairs ,Positive
There are several ways to calculate Pcd Three representatives are Robertson and Sparck Jones 1976 Kwok 1990 and Fuhr 1989 ,Neutral
4 Filtering with the CFG Rule Dictionary We use an idea that is similar to the method proposed by Ratnaparkhi  for partofspeech tagging ,Neutral
The local dependencies between sentiment labels on sentences is similar to the work of  where soft local consistency constraints were created between every sentence in adocument and inference wassolved using a mincut algorithm ,Neutral
One of the most powerful representations for this is Markov logic which is a probabilistic extension of firstorder logic Richardson and Domingos 2006,Positive
Liddy 1991 produced a formal model of the informational or conceptual structure of abstracts of empirical research ,Neutral
Their algorithm was further modified and applied to the German biographies by Filippova and Strube 2008,Positive
Due to limited variations in the NBest list  the nature of ranking  and more importantly  the nondifferentiable objective functions used for MT  such as BLEU    one often found only local optimal solutions to  with no clue to walk out of the riddles We also compare our performance against  and  and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF Although several methods have already been proposed to incorporate nonlocal features   these present a problem that the types of nonlocal features are somewhat constrained ,Negative
It has the advantage of naturally capturing local reorderings and is shown to outperform wordbased machine translation  Again the best result was obtained with IOB1  which is an imI  rovement of the best reported F    1 rate for this data set    9203  This is well illustrated by the Collins parser   scrutinized by Bikel  2004   where several transformations are applied in order to improve the analysis of noun phrases  coordination and punctuation ,Negative
While several methods have been proposed to automatically extract compounds   we know of no successful attempt to automatically make classes of compounds Many approaches for POS tagging have been developed in the past  including rulebased tagging   HMM taggers   maximumentropy models   cyclic dependency networks   memorybased learning   etc All of these approaches require either a large amount of annotated training data  for supervised tagging  or a lexicon listing all possible tags for each word  for unsupervised tagging  ,Negative
Most recently   published their Semisupervised sequential labeling method  whose results on POS tagging seem to be optically better than   but no significance tests were given and the tool is not available for download  ie for repeating the results and significance testing ,Negative
In training process  we use GIZA   4 toolkit for word alignment in both translation directions  and apply growdiagfinal method to refine it  The models in the comparative study by  did not include such features  and so  again for consistency of comparison  we experimentally verified that our maximum entropy model  a  consistently yielded higher scores than when the features were not used  and  b  consistently yielded higher scores than nave Bayes using the same features  in agreement with  ,Neutral
First a set of preprocessing components including a chunker and a named entity recognizer is applied to the text in order to identify the noun phrases which are further taken as REs to be used for instance generation,Positive
As in the experiments of Clark and Wilkes Gibbs 1986 and Brennan and Clark 1996 one of the players who plays the role of director instructs the other player who plays the role of matcher which object is to be added next to the Scene ,Neutral
This assumption underlies a growing number of recent syntactic theories which give up the contextfree constituent backbone cf McCawley 1987 Dowty 1989 Reape 1993 Kathol and Pollard 1995 ,Positive
In general dialogue coherence is an important source of evidence for all aspects of language for both human language learning Saxton et al 2005 as well as machine models ,Neutral
However due to the challenges in providing semantic representation semantic abstraction has not been widely pursued although the TOPIC system Hahn and Reimer 1999 is a notable exception ,Positive
Robertson and Sparck Jones 1976 make use of the wellknown logistic or logodds transformation of the probability Pc]d ,Positive
Atthefinestlevel  thisinvolvesthealignment of words and phrases within two sentences that are known to be translations  ,Neutral
Other than confusionnetworkbased algorithms work most closely related to ours is the method of MT system combination proposed in Jayaraman and Lavie 2005 which we will refer to as JL,Positive
Related to our classification experiments is work on semantic or rhetorical classification of structured abstracts Saggion 2008 from the MEDLINE abstracting database where similar features to those presented here were used to identify in abstracts semantic categories such as objective method results and Conclusions ,Neutral
Our MT baseline system is based on Moses decoder  with word alignment obtained from GIZA    ,Neutral
We finally also include as alignment candidates those word pairs that are transliterations of each other to cover rare proper names Hermjakob et al 2008 which is important for language pairs that dont share the same alphabet such as Arabic and English,Neutral
We benchmark our results against a model  Hiero  which was directly trained to optimise BLEUNIST using the standard MERT algorithm  and the full set of translation and lexical weight features described for the Hiero model  ,Neutral
Recent work emphasizes corpusbased unsupervised approach  that avoids the need for costly truthed training data We examine the effectiveness of Structural Correspondence Learning  SCL   for this task  a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis ,Positive
3http    wwwopenofficeorg Another corpora based method due to Turney and Littman  tries to measure the semantic orientation O  t  for a term t by O  t   summationdisplay tiS  PMI  t  ti  summationdisplay tjS PMI  t  tj  where S  and S are minimal sets of polar terms that contain prototypical positive and negative terms respectively  and PMI  t  ti  is the pointwise mutual information  between the terms t and ti ,Neutral
 Introduction During the last few years  SMT systems have evolved from the original wordbased approach  to phrasebased translation systems  ,Positive
Saggion and Lapalme 2002 have studied and implemented a rulebased verb selection operation in their SumUM system which has been applied to introduce document topics during indicative summary generation ,Positive
For our targetlanguage syntactic features g syn  we  use features similar to lexicalized CFG events collins 1999 specifically following the dependency model of Klein and Manning 2004,Positive
The core of the approach is a novel decoder based on lattice parsing with quasisynchronous grammar Smith and Eisner 2006 a syntactic formalism that does not require source and target trees to be isomorphic,Positive
We can then use this newly identified set to   1  use  s method to find the orientation for the terms and employ the terms and their scores in a classifier  and  2  use  s method to find the orientation for the terms and add the new terms as additional seed terms for a second iteration As opposed to    we do not use the web as a resource to find associations  rather we apply the method directly to indomain data ,Neutral
For both experiments  we used dependency trees extracted from the Penn Treebank  using the head rules and dependency extractor from Yamada and Matsumoto  2003  ,Neutral
We use the Xerox partofspeech tagger Cutting et al 1992 a statistical tagger made at the Xerox Palo Alto Research Center,Neutral
Our experiments created translation modules for two evaluation corpora  written news stories from the Penn Treebank corpus  and spoken taskoriented dialogues from the TRAINS93 corpus  ,Neutral
Tandem Learning Raghavan and Allan 2007 is an algorithm that combines feature and instance active learning for classification ,Neutral
51 The Prague Dependency Tree Bank  PDT in the sequel   which has been inspired by the buildup of the Penn Treebank   is aimed at a complex annotation of  a part of  the Czech National Corpus  CNC in the sequel   the creation of which is under progress at the Department of Czech National Corpus at the Faculty of Philosophy  Charles University  the corpus currently comprises about 100 million tokens of word forms  ,Neutral
The f are trained using a heldout corpus using maximum BLEU training  ,Neutral
 Introduction Raw parallel data need to be preprocessed in the modern phrasebased SMT before they are aligned by alignment algorithms  one of which is the wellknown tool  GIZA     for training IBM models  4  ,Positive
Instead of a wordbased model we build a characterbased one since word segmentation errors can lead to irrecoverable mention detection errors Jing et al 2003 also observe that characterbased models are better performing than wordbased ones for Chinese named entity recognition ,Positive
 Chandrasekar et al 1996 discussed text simplification in general,Neutral
A synchronous 363 binarization method is proposed in  whose basic idea is to build a leftheavy binary synchronous tree  with a lefttoright shiftreduce algorithm ,Neutral
All stateoftheart widecoverage parsers relax this assumption in some way  for instance by  i  changing the parser in step  3   such that the application of rules is conditioned on other steps in the derivation process   or by  ii  enriching the nonterminal labels in step    with contextinformation   along with suitable backtransforms in step  4  ,Positive
Nonparametricmodels  may be appropriate ,Positive
Transformationbased tagging as introduced by Brill 1993 also requires a hand tagged text for training ,Negative
However they did not report any evaluation of their word extraction method,Negative
To tune the feature weights of our system we used a variant of the minimum error training algorithm Och 2003 that computes the error statistics from the target sentences from the translation search space represented by a packed forest that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space Macherey et al 2008,Positive
Nießen et al 2000 is an early work that also constructs a database of translations and judgments,Neutral
Levin  assumes that the syntactic realization of a verb s arguments is directly correlated with its meaning  cf ,Neutral
Such representations have been successfully applied todifferent aspects of natural language processing such as morphological analysis andgeneration Karttunen Kaplan and Zaenen 1992 Clemenceau 1993 parsing Roche1993 Tapanainen and Voutilainen 1993 phonology Laporte 1993 Kaplan and Kay1994 and speech recognition Pereira Riley and Sproat 1994,Neutral
Like in the DV model of Pad and Lapata 2007 only pairs connected by target links are preserved but the links themselves are not part of the model ,Neutral
Even though there are some studies that compare the results from statistically computed association measures with word association norms from psycholinguistic experiments  there has not been any research on the usage of a digital  networkbased dictionary reflecting the organization of the mental lexicon to our knowledge ,Neutral
However Merialdo 1994 and Elworthy 1994 have criticized methods of estimation from an untagged corpus based on the maximum likelihood principle ,Neutral
2 Parsing Model The Berkeley parser  is an efficient and effective parser that introduces latent annotations  to refine syntactic categories to learn better PCFG grammars ,Positive
We obtained 47025 50dimensional reduced vectors from the SVD and clustered them into 200 classes using the fast clustering algorithm Buckshot Cutting et al 1992,Positive
35 The Experiments We have ran LexTract on the onemillionword English Penn Treebank  and got two Treebank grammars ,Neutral
The same asymptotic complexityis of course found for memory storage in this approach ,Positive
This kind of corpus has served as an extremely valuable resource for computational linguistics applications such as machine translation and question answering   and has also proved useful in theoretical linguistics research  ,Positive
However  this is not unprecedented  discriminatively weighted generative models have been shown to outperform purely discriminative competitors in various NLP classification tasks   and remain the standard approach in statistical translation modeling  A more refined algorithm  ,Positive
In our previous work we evaluated the overall summarization strategy of MultiGenin multiple experiments including comparisons with humanwritten summaries inthe Document Understanding Conference DUC 11 evaluation McKeown et al 2001McKeown et al 2002 and quality assessment in the context of a particular information access task in the Newsblaster framework McKeown et al 2002,Neutral
The loglinear model feature weights were learned using minimum error rate training  MERT   with BLEU score  as the objective function ,Neutral
A statistical language model a lexicalized PCFG  is derived from the analysis grammar by processing a corpus using the same grammar with no statistical model and recording frequencies of substructures built by each rule ,Neutral
Distributional information has uses beyond part of speech induction For example it is possible to augment a fixed syntactic or semantic taxonomy with such information to good effect Hearst and Schtze 1993 ,Neutral
1 Introduction The most widely applied training procedure for statistical machine translation IBM model 4  unsupervised training followed by postprocessing with symmetrization heuristics  yields low quality word alignments This is in contrast to purely statistical systems   which are difficult to inspect and modify ,Negative
The best previous result is an accuracy of 56   Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years   and often outperform phrasebased systems  on targetlanguage fluency and adequacy ,Positive
Next it looks promising to try to estimate the dictionary word frequencies using a search engine instead of text corpus as proposed by Lapata and Keller 2004 ,Positive
The system also uses a large scale reusable lexicon we combined from multiple resources Jing and McKeown 1998 ,Neutral
So far  SCL has been applied successfully in NLP for PartofSpeech tagging and Sentiment Analysis  ,Positive
The latest WMT workshop CallisonBurch et al 2009 also conducted a full assessment of how well a suite of automatic metrics correlate with human judgment ,Neutral
Congress of the Italian Association for Artificial Intelligence  Palermo  1991 B Boguraev  Building a Lexicon  the Contribution of Computers  IBM Report  TJ Watson Research Center  1991 M Brent  Automatic Aquisition of Subcategorization frames from Untagged Texts  in  N Calzolari  R Bindi  Acquisition of Lexical Information from Corpus  in  K W   P Hanks  Word Association Norms  Mutual Information  and Lexicography  Computational Linguistics  vol ,Neutral
There has been considerable skepticism over whether WSD will actually improve performance of applications  but we are now starting to see improvement in performance due to WSD in crosslingual information retrieval  and machine translation  and we hope that other applications such as questionanswering  text simplication and summarisation might also benet as WSD methods improve ,Positive
51 The baseline System used for comparison was Pharaoh   which uses a beam search algorithm for decoding ,Neutral
To some extent  this can probably be explained by the strong tradition of constituent analysis in AngloAmerican linguistics  but this trend has been reinforced by the fact that the major treebank of American English  the Penn Treebank   is annotated primarily with constituent analysis ,Positive
we list the first twenty transformations learned from training on thePenn Treebank Wall Street Journal Corpus Marcus Santorini and Marcinkiewicz1993,Neutral
2 Related Work This method is similar to blockorientation modeling  and maximum entropy based phrase reordering model   in which local orientations  leftright  of phrase pairs  blocks  are learned via MaxEnt classifiers ,Neutral
This approach originally proposed by Knight andHatzivassiloglou 1995 and Langkilde and Knight 1998 is a standard method usedin statistical generation,Positive
The success of recent highquality parsers  relies on the availability of such treebank corpora Synchronous binarization  solves this problem by simultaneously binarizing both source and targetsides of a synchronous rule  ,Positive
If we need to generate summaries that can be used to indicative what topics are addressed in the original document and thus can be used to alert the uses as the source content ie the indicative function Mani et al 1999 extraction approach is capable of handling this kind of tasks ,Positive
Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees Headargumentmodifier distinctions are made for each node in the tree based on Magerman 1994 and Collins 1997 336 ODonovan et al LargeScale Induction and Evaluation of Lexical Resources the whole tree is then converted to a binary tree heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category,Neutral
Other statistical systems that address word classification probleans do not emphasize the use of linguistic knowledge and do not deal with a specific word class   or do not exploit as much linguistic knowledge as we do  As one can see in Table 4  the resulting parser ranks among the best lexicalized parsers  beating those of Collins  and Charniak and Johnson  8 Its F1 performance is a 27  reduction in error over  et al ,Negative
On the other hand the literature emphasizes since the very beginning the relevance of world knowledge and inference Charniak 1973,Neutral
However this technique needs a corpus for computing IDF score causing the genredependent problem for generic text summarization task ,Negative
Chang et al 2007 present an algorithm for learning with constraints but this method requires users to set weights by Hand ,Neutral
This metric for measuring distance is adopted from Cost and Salzberg 1993 which in turn is adapted from the value difference metric of the earlier work of Stanfill and Waltz 1986 ,Positive
The morphological FST is generated automatically from a large dictionary of French of about 90000 entries and online corpora such as Le Monde Newspapers ECI 1989 and 1990 ,Neutral
A remedy is to aggressively limit the feature space  eg to syntactic labels or a small fraction of the bilingual features available  as in   but that reduces the benefit of lexical features ,Neutral
In each experiment  performance IMutu   d Information provides an estimate of the magnitude of the ratio t  ctw    n the joint prol  ability P  verbnoun 1  reposition   and the joint probability a  suming indcpendcnce P  verbnoun  P  prcl  osition  s      ,Neutral
This heterogeneity is in stark contrast to the systematic structures Liddy 1991 found to be produced by professional abstractors  ,Positive
Mann and McCallum 2007 apply Expectation Regularization to Named Entity Recognition and PartOfSpeech tagging achieving improved performance when compared to supervised methods especially on small numbers of training data,Neutral
Table 10 shows the results in terms of three overall measures kappa percentage accuracy and macroF following Lewis 1991  ,Neutral
The articles were morphologically analyzed by Mecab Kudo et al 2003 and syntactically parsed by Cabocha Kudo and Matsumoto 2002 ,Neutral
A serious challenge in unsupervised learning is the identifiability problem ie the optimal parameters are not unique Liang and Klein 2008 ,Neutral
 for English  but not identical to strictly anaphoric ones5   since a nonanaphoric NP can corefer with a previous mention ,Neutral
Since the official evaluation criterion for WMT09 is human sentence ranking we chose to minimize a linear combination of two common evaluation metrics BLEU and TER Papineni et al 2002 Snover et al 2006 during system development and tuning ,Neutral
Similarly to classical NLP tasks such as base noun phrase chunking Ramshaw and Marcus 1994 text chunking Ramshaw and Marcus 1995 or named entity recognition Tjong Kim Sang 2002 we formulate the mention detection problem as a classification problem by assigning to each token in the text a label indicating whether it starts a specific mention is inside a specific mention or is outside any mentions  ,Positive
Van den Bosch and Daelemans 1999 propose a memorybased approach which maps directly from letters in context to categories that encode morphological boundaries syntactic class labels and spelling changes ,Neutral
CPSTM  i   l This metric corresponds to the STM metric presented by  ,Neutral
4 Features We used a dependency structure as the context for words because it is the most widely used and one of the best performing contextual information in the past studies  ,Positive
There are many research directions  eg  sentiment classification  classifying an opinion document as positive or negative    subjectivity classification  determining whether a sentence is subjective or objective  and its associated opinion    feature topic based sentiment analysis  assigning positive or negative sentiments to topics or product features   Hu and Liu 2004  Popescu and Etzioni  2005  Carenini et al  2005  Ku et al  2006  Kobayashi  Inui and Matsumoto  2007  Titov and  ,Neutral
Hypothesesfor unknown words both stochastic Dermatas and Kokkinakis 1993 1994 Malteseand Mancini 1991 Weischedel et al 1993 and connectionist Eineborg and Gamback1993 Elenius 1990 have been applied to unlimited vocabulary taggers,Positive
While we used simple document representation in which a document is defined as a set of nouns there could be considered several improvements such as using phrasal information Lewis 1992 clustering terms Sparck Jones 1973 reducing the number of features by using local dictionary Apt4 et al 1994 etc ,Positive
Thus even when spoken language interfaces use probabilistic inference for dialogue management Williams and Young 2007 new techniques may be needed to mine their experience for correct interpretations,Neutral
In our approach the discourse structure is not fixed but predicted for each particular abstract ,Negative
This contrasts with semantic role labeling Carreras and Marquez 2004 and other forms of shallow semantic processing which do not aim to produce complete formal meanings ,Neutral
 However his work used decision list to perform classification in which only the single best disambiguating evidence that matched a target context is used In contrast we used exemplarbased learning where the contributions of all features are summed up and taken into account in coming up with a classification We also include verbobject syntactic relation as a feature which is not used in Yarowsky 1994 ,Negative
The closest available system to USP in aims and capabilities is TextRunner Banko et al 2007 and we compare with it ,Positive
In order to avoid a full Viterbi search of all possibilities we perform a beam search with width of three among the candidates of the previous sentence following Barzilay et al 2000 ,Neutral
Alignment spaces can emerge from generative stories   from syntactic notions   or they can be imposed to create competition between links  ,Neutral
 binarize grammars into CNF normal form  while  allow only GriebachNormal form grammars ,Neutral
  makes a similar point  noting that for reviews  the whole is not necessarily the sum of the parts   Identifying transliteration pairs is an important component in many linguistic applications which require identifying outofvocabulary words  such as machine translation and multilingual information retrieval  ,Neutral
Feature weights of both systems are tuned on the same data set3 For Pharaoh  we use the standard minimum errorrate training   and for our system  since there are only two independent features  as we always fix  1   we use a simple gridbased lineoptimization along the languagemodel weight axis ,Neutral
For comparing the sentence generator sample to the English sample  we compute loglikelihood statistics  on neighboring words that at least cooccur twice ,Neutral
23 The Averaged Perceptron Reranking Model Averaged perceptron  has been successfully applied to several tagging and parsing reranking tasks   and in this paper  we employed it in reranking semantic parses generated by the base semantic parser SCISSOR ,Positive
Concept similarity is often measured by vectors of cooccurrence with context words that are typed with dependency information Lin 1998 Curran and Moens 2002 ,Neutral
We adapt the bilingual word alignment model IBM Model 3 Brown et al 1993 to monolingual word alignment ,Positive
Also  slightly restating the advantages of phrasepairs identified in   these blocks are effective at capturing context including the encoding of noncompositional phrase pairs  and capturing local reordering  but they lack variables  eg embedding between ne pas in French   have sparsity problems  and lack a strategy for global reordering ,Positive
Our learning method is an extension of Collinss perceptronbased method for sequence labeling  ,Neutral
Recently socalled reranking techniques  such as maximum entropy models  and gradient methods   have been applied to machine translation  MT   and have provided significant improvements ,Positive
Besides frequency another way of looking at the predications is typicality Kan et al 2001 or distribution of predications across citations  ,Neutral
To estimate the parameters of the MEMM+predmodel we turn to the successful Maximum Entropy Berger et al 1996 parameter estimation Method,Positive
 introduced the averaged perceptron  as a way of reducing overfitting  and it has been shown to perform better than the nonaveraged version on a number of tasks ,Positive
It is often straightforward to obtain large amounts of unlabeled data  making semisupervised approaches appealing  previous work on semisupervised methods for dependency parsing includes  ,Positive
A hierarchical model can translate discontiguous groups of words as a unit A phrasebased model cannot Lopez 2008b gives indirect experimental evidence that this difference affects performance,Neutral
23 Classifier Training We chose maximum entropy  as our primary classifier  since it had been successfully applied by the highest performing systems in both the SemEval2007 preposition sense disambiguation task  and the general word sense disambiguation task    ,Positive
When looking at the numerical values however one should keep in mind that macroaveraging results are in general numerically lower Yang and Liu 1999 ,Neutral
In the scenario that Nbest lists are available from individual systems for combination the weight of each hypothesis can be computed based on its rank in the Nbest list Rosti et al 2007a ,Neutral
Such a coding procedure covers  for example  how segmentation of a corpus is performed  if multiple tagging is allowed and if so  is it unlimited or are there just certain combinations of tags not allowed  is look ahead permitted  etc For further information on coding procedures we want to refer to  and for good examples of coding books see  for example      or  ,Neutral
We presented some theoretical arguments for not limiting extraction to minimal rules  validated them on concrete examples  and presented experiments showing that contextually richer rules provide a 363 BLEU point increase over the minimal rules of  ,Negative
We intend to do so soon and also to repeat the experiments on the French version of the CLE Rayner Carter and Bouillon 1996 ,Positive
The compatibility value for each constraint is the mutual information between the tag and the context Cover and Thomas 1991 ,Neutral
Interpretations are constructed abductively in that the initial actions in the sequence need not be directly tied to observable events they may be tacit in the terminology of Thomason et al 2006 ,Neutral
In agreement with recent results on parsing with lexicalised probabilistic grammars   our main result is that statistics over lexical features best correspond to independently established truman intuitive preferences and experimental findings ,Positive
We have tried other machine learning algorithms such as Decision Trees Naive Bayes Classification and Nearest Neighbor from the Weka toolkit Witten and Frank 1999 but the support vector machines gave us the best classification accuracy  ,Neutral
Such studies follow the empiricist approach to word meaning summarized best in the famous dictum of the British 3 linguist JR Firth You shall know a word by the company it keeps Firth 1957 p 11 ,Neutral
Though the time complexity of the algorithm given by Jiang and Ng 2006 is also linear it should assume all feature templates in the initial selected set ‘good enough and handles other feature template candidates in a strict incremental way ,Neutral
Hanks and  proposed using pointwise mutual information to identify collocations in lexicography  however  the method may result in unacceptable collocations for lowcount pairs Unlike   Smadja  1993  goes beyond the  twoword  limitation and deals with  collocations of arbitrary length  ,Negative
Finally  it should be noted that in the current implementation  we have not applied any of the possible optimizations that appear in the literature  to speed up normalization of the probability distribution q These improvements take advantage of a models structure to simplify the evaluation of the denominator in  1  ,Neutral
In this respect the present work is closer in spirit to Ji et al 2005 who explore the employment of the ACE 2004 relation ontology as a semanticfilter ,Positive
Annotated reference corpora  such as the Brown Corpus   the Penn Treebank   and the BNC   have helped both the development of English computational linguistics tools and English corpus linguistics ,Positive
Note especially that Czech nouns are divided into four classes according to gender Sgall 1967 and into seven classes according to ease ,Neutral
This study was motivated by the need to answer to the question of contentselection in text summarization Sparck Jones 1993,Neutral
Our evaluation metric is BLEU  ,Neutral
For English  after a relatively big jump achieved by   we have seen two significant improvements   and  pushed the results by a significant amount each time ,Positive
 shows that baseNP recognition  Fz  I  920  is easier than finding both NP and VP chunks  Fz    88  and that increasing the size of the training data increases the performance on the test set ,Positive
Concurrent work has used approximate counting schemes based on Morris 1978 to estimate in small space frequencies over a high volume input text stream Van Durme and Lall 2009 Goyal et al 2009,Positive
CorstonOliver and Dolan 1999 proposed to remove clauses in sentences before indexing documents for Information Retrieval ,Neutral
For example while the TnT tagger performs at 97 accuracy on known words in the Treebank the accuracy drops to 89 on unknown words Brandts 2000 ,Neutral
The tagger is reported Cutting el al 1992 to have a better than 96  accuracy in the analysis of parts of the Brown Corpus The accuracy is similar to other probabilistic taggers  ,Positive
In this paper we develop the first unsupervised approach to semantic parsing using Markov logic Richardson and Domingos 2006 ,Positive
Recent innovations have greatly improved the efficiency of language model integration through multipass techniques  such as forest reranking   local search   and coarsetofine pruning  ,Positive
Smith and Eisner 2006 instead propose a dif ferentiable objective that can be optimized by gradient descent the Bayes risk Rp of 7 ,Neutral
The word posterior feature is the same as the one proposed by Rosti et al 2007a ie The second feature we used is a bigram voting feature proposed by Zhao and He 2009,Neutral
This finding has been previously reported  among others  in  In order increase the likelihood that 909 only true paraphrases were considered as phraselevel alternations for an example  extracted sentences were clustered using completelink clustering using a technique proposed in  ,Neutral
According to current tagger comparisons van Halteren et al 1998 Zavrel and Daelemans 1999 and according to a comparsion of the results presented here with those in Ratnaparkhi 1996 the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here ,Neutral
 Close to the problem studied here is Jing and McKeowns Jing and McKeown 2000 cutandpaste method founded on Endres Niggemeyers observations ,Neutral
To test if such a simple approach would be enough we performed a text categorization experiment using the Rainbow mplementation of a naı̈ve Bayes term requency times inverse document frequency TFIDF method McCallum 1997 and onsidering each sentence as a document ,Neutral
In addition  the averaged parameters technology  is used to alleviate overfitting and achieve stable performance ,Positive
To generate phrase pairs from a parallel corpus  we use the ` diagand  phrase induction algorithm described in   with symmetrized word alignments generated using IBM model 2  ,Neutral
Having a single  canonical tree structure for each possible alignment can help when flattening binary trees  as it indicates arbitrary binarization decisions  ,Neutral
3 Model As an extension to commonly used lexical word pair probabilities p  f e  as introduced in   we define our model to operate on word triplets ,Neutral
In the SMT research community  the second step has been well studied and many methods have been proposed to speed up the decoding process  such as nodebased or spanbased beam search with different pruning strategies  and cube pruning  ,Positive
Evaluation in automatic summarization especially for multidocument input is daunting Rad ev et al 2003  ,Neutral
Inspired by RST [Radev 2000] endeavored to establish a Crossdocument Structure Theory CST that is more appropriate for MDS ,Positive
Although we do agree with RST that the structure of text is hierarchical in many cases it is our belief that the relevance and function of certain text pieces can be determined without analyzing the full hierarchical structure of the text ,Negative
Support Vector Machines  SVMs   and Maximum Entropy  ME  method  are powerful learning methods that satisfy such requirements  and are applied successfully to other NLP tasks  ,Positive
Kupiec Pedersen and Chen 1995 report sentence length as a useful feature for text extraction ,Neutral
To solve problems 1 and 2 of PRW Kwok 1990 stresses the assumption that a document consists of terms This theory is called the Component Theory CT ,Positive
Clearly adding more features improves statistically significant the case with only five features ,Positive
Mirkinet al 2006 also integrate information from the lexical patterns in which two words cooccur and similarity of the contexts in which each word occurs on its own to improve performance in lexical entailment Acquisition ,Positive
This cost can often be substantial  as with the Penn Treebank  2 Previous work on Sentiment Analysis Some prior studies on sentiment analysis focused on the documentlevel classification of sentiment  where a document is assumed to have only a single sentiment  thus these studies are not applicable to our goal ,Negative
For unknown words  SCL gives a relative reduction in error of 195  over   even with 40000 sentences of source domain training data For comparison purposes  we revisit a fullygenerative Bayesian model for unsupervised coreference resolution recently introduced by   discuss its potential weaknesses and consequently propose three modifications to their model  Section 3  ,Negative
Church et al 2007 looked at Golomb Coding and Brants et al 2007 used tries in a distributed setting These methods are less succinct than randomised approaches ,Positive
In total we arrive at 13 features including 8 boundary word features 4 kinds of internal word features and 1 LM feature The first 12 features have been proven useful Xiong et al 2006 Zhang et al 2007a to phrase reordering ,Neutral
Wordsense disambiguation a problemthat once seemed out of reach for systems without a great deal of handcrafted linguistic and world knowledge can now in some cases be done with high accuracywhen all information is derived automatically from corpora Brown Lai and Mercer1991 Yarowsky 1992 Gale Church and Yarowsky 1992 Bruce and Wiebe 1994,Neutral
Three provide alignments for the surface heuristic  GIZA   with growdiagfinal  GDF  Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder  with the SMT Shared Task baseline system ,Neutral
Liang et al 2009 simultaneously developed a method for learning with and actively selecting measurements or target expectations with associated noise ,Positive
In our work we partially address this issue by enumerating some transformations frequently found in our corpusthat are computationally implementable,Negative
Many previous studies have shown that the loglikelihood ratio is well suited for this purpose  Recent work    has shown that adding many millions of words of machine parsed and reranked LA Times articles does  in fact  improve performance of the parser on the closely related WSJ data ,Positive
Although we also use a mentionpair model our tracking algorithm differs from Soon et al 2001 Ng and Cardie 2002 in several aspects ,Neutral
Forexample all summary sentences may contain the full description of a named entityeg President of Columbia University Lee Bollinger while the use of shorter descriptionssuch as Bollinger or anaphoric expressions in some summary sentences would increase the summarys readability Schiffman Nenkova and McKeown 2002 Nenkovaand McKeown 2003,Neutral
Liberman and Church suggest in Liberlnan and Church 1992 that a system could be quickly built to divide newswire text into sentences with a nearly negligible error rate but do not actually build such a system ,Negative
Lewis proposed the proportional assignment strategy based on the probabilistic ranking principle Lewis 1992 ,Neutral
The more similar conditions reported in previous work are those experiments performed on the WSJ corpus Brill 1992 reports 34 error rate and Daelemans et al 1996 report 967 accuracy We obtained a 9739 accuracy with tri grams plus automatically acquired constraints and 9745 when hand written constraints were added  ,Negative
We took 232 causativeinchoative verbs and 170 non alternating transitive verbs from Levin 1993  ,Neutral
The underlying idea of the replacement is the same as Turing's estimates in backoff smoothing Katz 1987 ,Neutral
Alpino van Noord and Malouf 2005 van Noord 2006 is a robust computational analyzer for Dutch that implements the conceptual twostage parsing Approach ,Positive
Our point of departure is the work of Lappin and Leass 1994 henceforth LL and Dagan et al 1995 See also Dagan and Itai 1990  ,Positive
While the ordering of many sentenceconstituents is determined by their syntactic roles some constituents such as timelocation and manner circumstantials are free to move Elhadad et al 2001,Neutral
Independently Cutting et aL 1992 quote a performance of 800 words per secondfor their partofspeech tagger based on hidden Markov models,Positive
Such a system that usesthe morpholexical probabilities together with a syntactic knowledge is described inLevinger 1992,Positive
Syntactic analysis of texts such as PartOfSpeech tagging and syntactic parsing is an example of such a generic analysis and has proved useful in applications ranging from machine translation Marcu et al 2006 to text mining in the biomedical domain Cohen and Hersh 2005,Neutral
ntroduction In recent years  statistical machine translation have experienced a quantum leap in quality thanks to automatic evaluation  and errorbased optimization ,Positive
Constituent pruning is a bottomup approach and is complemented by a second topdown method based on ExplanationBased Learning EBL Mitchell et al 1986 van Harmelen and Bundy 1988 ,Neutral
Rindflesch et al 1999 use the term macronoun phrase to refer to structures that include reduced relative clauses commonly introduced by prepositions or participles as well as appositives,Neutral
Choueka andLusignan 1985 presented a system for the morphological tagging of large texts that isbased on the short context of the word but also depends heavily on human interaction,Positive
In addition to paradigmatic lexical phenomena such as synonymy hypernymy and meronymy diathesis alternation Levin and Rappaport Hovav 1996 deep case Fillmore 1968 and the interaction of predicational structure and events Tenny and Pustejovsky 2000 are being investigated ,Neutral
Hence either the best translation hypothesis is directly extracted from the word graph and output or an Nbest list of translations is computed Tran et al 1996 ,Neutral
Portage is a statistical phrasebased SMT system similar to Pharaoh  ,Neutral
In the supervised setting  a recent paper by  shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks ,Positive
In Riley 1989 Riley describes a decisiontree based approach to the problem ,Positive
As large corpora became available it became clear that simpleMarkovmodel based stochastic taggers that were automatically trained could achievehigh rates of tagging accuracy Jelinek 1985,Neutral
Finally we would like to explore the machine learning potential offered by morphological dictionaries with application to other related tasks such as stemming Nakov 2003 lemmatisation and POS tagging ,Positive
METEOR was chosen since  unlike the more commonly used BLEU metric   it provides reasonably reliable scores for individual sentences Our method is a natural extension of those proposed in  and   and overcomes their drawbacks while retaining their advantages ,Negative
5 The SemCor collection  is a subset of the Brown Corpus and consists of 352 news articles distributed into three sets in which the nouns  verbs  adverbs  and adjectives have been manually tagged with their corresponding WordNet senses and partofspeech tags using Brills tagger  ,Neutral
For Hw6  students compared their POS tagging results with the ones reported in  ,Neutral
The processing time of the Viterbi algorithm Rabiner 1989 can be reduced by introducing a beam Search ,Neutral
et al  2004  CollinsThompson and Callan  2005   and Ramage  2007  ,Neutral
The task can be performed by a chunk parser that is equipped with an appropriate finite state grammar Abney 1996 ,Neutral
In the research presented here we concentrate on the first step of the summarization process and follow Barzilay and Elhadad 1997 in employing lexical chains to extract important concepts from a document ,Neutral
22 ITG Space Inversion Transduction Grammars  or ITGs  provide an efficient formalism to synchronously parse bitext More recently   have proposed methods for automatically extracting from a corpus heads that correlate well with discourse novelty ,Positive
The results are comparable to other results reported using the InsideOutside method   see Table 7 ,Neutral
 Nevertheless different theoryspecific representations shMl be recoverable from the annotation cf Marcus et al 1994 ,Neutral
Indeed using tree kernel methods to mine structured knowledge has shown success in some NLP applications like parsing Collins and Duffy2001 semantic role labeling Moschitti 2004 Zhang et al 2007b relation extraction Zhang et al 2006 pronoun resolution Yang et al2006 and question classification Zhang and Lee 2003 ,Neutral
In the biomedical domain UMLS knowledge provides considerable support for textbased systems Burgun and Bodenreider 2001 compare the UMLS to WordNet ,Neutral
Assuming that the number of feature templates in a given set is n the algorithm of Ding and Chang 2008 requires On 2  times of training/test routines it cannot handle a set that consists of hundreds of templates ,Negative
The subsequential transducer generated by this algorithm could in turn be minimized by an algorithm described in Mohri 1994a However in our case the transducer is nearly minimal,Neutral
According to the document  it is the output of Ratnaparkhis tagger  ,Neutral
The results from CoNLL shared tasks in 2005 and 2008 Carreras and Marquez 2005 Koomen et al 2005 Surdeanu et al 2008 Johansson and Nugues 2008 further show that SRL pipeline may be one of the standard to achieve a stateoftheart performance in practice ,Positive
Koo et al 2008 present an algorithm for dependency parsing that uses clusters of semantically related words which were learned in an unsupervised manner ,Neutral
The former term P  E  is called a language model  representing the likelihood of E The latter term P  J E  is called a translation model  representing the generation probability from E into J As an implementation of P  J E   ,Positive
Word alignment is a critical component in training statistical machine translation systems and has received a significant amount of research for example Brown et al 1993 Ittycheriah and Roukos 2005 Fraser and Marcu 2007 including work leveraging syntactic parse trees eg Cherry and Lin 2006 DeNero and Klein 2007 Fossum et al 2008,Neutral
Given the problems created by estimating probabilities on a corpus of restricted size we present in Section 4 a solution for coping with these difficulties,Positive
Work similar to that described here has been carried out by Merialdo 1994 with broadly similar Conclusions ,Neutral
This is achieved by introducing an explicit statistical model of unknown words and by using an N best word segmentation algorithm Nagata 1994as an approximation of the generalized Forward Backward algorithm,Positive
Current treebased models that integrate linguistics and statistics  such as GHKM   are not able to generalize well from a single phrase pair Our syntacticrelationbased thesaurus is based on the method proposed by   although Hindle did not apply it to information retrieval ,Negative
In the news domain sentence location is the single most important feature for sentence selection Brandow Mitze and Rau 1995 in our domain location information although less dominant can still give a useful indication  ,Negative
2This can explain why previous attempts to use WordNet for generating sentencelevel paraphrases  were unsuccessful We have also illustrated that ASIA outperforms three other English systems   even though many of these use more input than just a semantic class name ,Negative
One is to use a stochastic gradient descent SGD or Perceptron like online learning algorithm to optimize the weights of these features directly for MT Shen et al 2004 Liang et al2006 Tillmann and Zhang 2006,Neutral
The ITCirst system Chen et al 2005 is based on a loglinear model which extends the original IBM Model 4 Brown et al 1993 to phrases Koehn et al 2003 Federico and Bertoldi 2005 ,Neutral
The IOB format  introduced in   consistently   ame out as the best format ,Positive
The creation of the Penn English Treebank   a syntactically interpreted corpus  played a crucial role in the advances in natural language parsing technology  for English ,Positive
We compare this approach to the semisupervised method in Koo et al 2008 who employ clusters of related words constructed by the Brown clustering algorithm Brown et al 1992 for syntactic processing of texts ,Neutral
Two are conditionalized phrasal models  each EM trained until performance degrades  CJPTM3 as described in  Phrasal ITG as described in Section 41   ,Neutral
The tagger has a close relative in Koskenniemi 1990 Koskenniemi et al 1992 Voutilalnen and Tapanainen 1993 where the rules are represented as finitestate machines that are conceptually intersected with each other ,Neutral
Similarly many researchers have explored techniques for robust broad coverage semantic parsing in terms of semantic role labeling Gildea & Jurafsky 2002 Carreras & Màrquez 2005 SRL Henceforth ,Neutral
Many freely available natural language processing tools require their input to be divided into sentences but make no mention of how to accomplish this eg Brill 1994 Collins 1996 ,Negative
The experiment was carried out using both the chunking criteria from Rayner and Samuelsson 1994 the Old scheme and the chunking criteria described in Section 3 above the New scheme ,Positive
Each electronic version of the abstract was processed using the freely available GATE text analysis software Cunningham et al 2002,Neutral
We employ a slightly different clustering method here the fullibmpredict method discussed in Goodman 2001 ,Positive
In particular  previous work  has investigated the use of Markov random fields  MRFs  or loglinear models as probabilistic models with global features for parsing and other NLP tasks ,Neutral
54 Domain Adaptation 541 FeatureBased Approaches Onewayofadaptingalearnertoanewdomainwithout using any unlabeled data is to only include features that are expected to transfer well  ,Neutral
Many methods for calculating the similarity have been proposed  ,Neutral
Additionally we present results of the tagger on the NEGRA corpus Brants et al 1999 and the Penn Treebank Marcus et al 1993 The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in Ratnaparkhi 1996 For a comparison to other taggers the reader is referred to Zavrel and Daelemans 1999 ,Neutral
For example  the statistical word alignment in IBM translation models  can only handle word to word and multiword to word alignments Our graphical representation has two advantages over previous work   unifying sentence relations and incorporating question interactions ,Negative
This idea of employing ngram cooccurrence statistics to score the output of a computer system against one or more desired reference outputs has its roots in the BLEU metric for machine translation  and the ROUGE  metric for summarization ,Neutral
One could rely on existing trainable sentence selection Kupiec et al 1995 or even phrase selection Banko et al 2000 strategies to pick up appropriate β i s from the document to be abstracted and rely on recent information ordering techniques to sort the β i fragments Lapata 2003 ,Positive
In the next section we propose a novel method to learn word similarities the Latent Words Language Model LWLM Deschacht and Moens 2009,Neutral
For the partofspeech tagging problem it is known that assigning the most common part of speech for each lexical item gives a baseline of 90 accuracy Brill 1992 ,Neutral
1 word w 2 word bigram w1w2 3 singlecharacter word w 4 a word of length l with starting character c 5 a word of length l with ending character c 6 spaceseparated characters c1 and c2 7 character bigram c1c2 in any word 8 the first  last characters c1  c2 of any word 9 word w immediately before character c 10 character c immediately before word w 11 the starting characters c1 and c2 of two consecutive words 12 the ending characters c1 and c2 of two consecutive words 13 a word of length l with previous word w 14 a word of length l with next word w Table 1 Feature templates for the baseline segmentor 2 The Baseline System We built a twostage baseline system using the perceptron segmentation model from our previous work Zhang and Clark 2007 and the perceptron POS tagging model from Collins 2002,Neutral
Knight and Marcu 2000 treat reduction as a translation process using a noisychannel model Brown et al 1993,Neutral
One is goldstandard syntactic input and other two are based on automatically parsing results of two parsers the stateoftheart syntactic parser described in Johansson and Nugues 2008 7 it is referred to Johansson and an integrated parser described as the following referred to MSTME ,Positive
These include cube pruning   cube growing   early pruning   closing spans   coarsetofine methods   pervasive laziness   and many more ,Neutral
In the approach of Zettle moyer and Collins 2005 the training data consists of sentences paired with their meanings in lambda form,Neutral
While the literature suggests that BaumWelch training can degrade performance on the tagging task Elworthy 1994 Merialdo 1994 we have found in early experiments that agreement between a tagger trained in this way and the tagger from the XTag Project consistently increases with each iteration of BaumWelch eventually reaching a plateau but not Decreasing ,Negative
For example Bohus et al 2008 use users confirmations of their spoken requests in a multimodal interface to tune the systems ASR rankings for recognizing subsequent utterances ,Neutral
However at higher levels of tagging accuracy the reestimation m e t h o d based on the BaumWelch algorithm is limited by the noise of untagged corpora On this point I agree with Merialdo 1994 and Elworthy 1994 ,Positive
Their idea has proven effective for estimating the statistics of unknown words in previous studies   We use the popular online learning algorithm of structured perceptron with parameter averaging   This algorithm is referred to as GHKM  and is widely used in SSMT systems  ,Positive
Exploiting the maximum entropy Berger et al 1996 framework the conditional distribution Pre a | f  can be determined through suitable real valued functions called features h  and takes the parametric Form ,Positive
6 Results We trained on the standard Penn Treebank WSJ corpus  ,Neutral
A more elaborate scheme is given in Samuelsson 1994b where the chunking criteria are learned automatically by an entropyminimization method the results however do not appear to improve on the earlier ones In both cases the coverage loss due to grammar specialization was about 10 to 12 using training corpora with about 5000 examples In practice this is still unacceptably high for most Applications ,Negative
Before training the classifiers  we perform feature ablation by imposing a count cutoff of 10  and by limiting the number of features to the top 75K features in terms of log likelihood ratio  ,Neutral
Hence we use a beamsearch decoder during training and testing  our idea is similar to that of  who used a beamsearch decoder as part of a perceptron parsing model ,Neutral
COREF treats interpretation broadly as a problem of abductive intention recognition Hobbs et al 1993 ,Neutral
Confusion networks allow wordlevel system combination which was shown to outperform sentence reranking methods and phraselevel combination Rosti et al 2007a,Neutral
To discover useful features we exploit the concept of Association Rules AR R Agrawal and Swami 1993 Srikant and Agrawal 1997 which is originally proposed in Data Mining research field to identify frequent itemsets in a large Database,Positive
This can be done in a supervised   a semisupervised  or a fully unsupervised way  Equation  3  reads If the target noun appears  then it is distinguished by the majority The loglikelihood ratio  decides in which order rules are applied to the target noun in novel context ,Neutral
Schone and Jurafsky 2000 apply latent semantic analysis for a knowledgefree morphology induction ,Neutral
We wish to minimize this error function  so we select accordingly  argmin summationdisplay a E  a   a   argmax a p  a  f e     4  Maximizing performance for all of the weights at once is not computationally tractable  but  has described an efficient onedimensional search for a similar problem For FrenchEnglish translation we use a state of the art phrasebased MT system similar to  ,Positive
 have proposed a rulebased algorithm for sentence combination  but no results have been reported  provides anecdotal evidence that only incorrect alignments are eliminated by ITG constraints ,Negative
We assign tags of partofspeech  POS  to the words with MXPOST that adopts the Penn Treebank tag set  Such coarsegrained inventories can be produced manually from scratch  or by automatically relating  or clustering  existing word senses  a contextual word cw that occurs in the paragraphs of bc  a loglikelihood ratio  G2  test is employed   which checks if the distribution of cw in bc is similar to the distribution of cw in rc  p  cw bc   p  cw rc   null hypothesis  ,Neutral
Recently a number of machine learning approaches have been proposed Zettlemoyer and Collins 2005 Mooney 2007 However they are supervised and providing the target logical form for each sentence is costly and difficult to do consistently and with high quality ,Negative
Only in the analysis of a few words it was agreed that a multiple choice was appropriate because of different meaninglevel interpretations of the utterance these were actually headings where some of the grammatical information was omitted,Positive
1 Introduction Statistical phrasebased systems  have consistently delivered stateoftheart performance in recent machine translation evaluations  yet these systems remain weak at handling word order changes With these linguistic annotations  we expect the LABTG to address two traditional issues of standard phrasebased SMT  in a more effective manner ,Negative
In particular  knowing a little about the structure of a language can help in developing annotated corpora and tools  since a little knowledge can go a long way in inducing accurate structure and annotations  ,Neutral
In contrast in a rule based system the system designer would have to consider how for instance a WordNet Miller 1995 derived information for a particular example interacts with a partofspeechbased information and chunking information ,Positive
While this approach exploits only syntactic and lexical information Jing andMcKeown 2000 also rely on cohesion information derived from word distribution ina text,Positive
Although their linguistic adequacy to natural language processing has been questioned in the past Chomsky1964 there has recently been a dramatic renewal of interest in the application of finitestate devices to several aspects of natural language processing,Neutral
A more optimistic view can be found in   they argue that a near100  interjudge agreement is possible  provided the partofspeech annotation is done carefully by experts ,Neutral
In earlier work  only singletons were used as seed words  varying their number allows us to test whether multiple seed words have a positive effect in detection performance ,Neutral
For the sake of efficiency we use a fast transition based parser based on maximum entropy as in Zhao and Kit 2008 We still use the similar feature notations of that work ,Positive
The first known supervised learning algorithm was proposed by Kupiec et al 1995 Their approach estimates the probability that a sentence should be included in a summary given its feature values based on the independent assumption of Bayes Rule ,Positive
This was expected  as it has been observed before that very simple smoothing techniques can perform well on large data sets  such as web data  ,Neutral
DeJean 1998 Hafer and Weiss 1974 follow a successor variety approach the word is cut if the number of distinct letters after a prespecified sequence surpasses a threshold ,Neutral
Specifically we test selectional preferences on the dataset constructed by Padó 2007 that collects average plausibility judgments from 20 speakers for nouns as either subjects or objects of verbs 211 nounverb pairs ,Positive
Similaritybased smoothing  provides an intuitively appealing approach to language modeling which is the classic work on collocation extraction  uses a twostage filtering model in which  in the first step  ngram statistics determine possible collocations and  in the second step  these candidates are submitted to a syntactic valida7Of course  lexical material is always at least partially dependent on the domain in question ,Positive
Please note that our approach is very different from other approaches to context dependent rule selection such as Ittycheriah and Roukos 2007 and He et al 2008,Neutral
The program takes the output of char_align   a robust alternative to sentencebased alignment programs  and applies wordlevel constraints using a version of Brown el al s Model 2   modified and extended to deal with robustness issues6 Conclusion Traditional approaches for devising parsing models  smoothing techniques and evaluation metrics are not well suited for MH  as they presuppose 13The lack of head marking  for instance  precludes the use of lexicalized models a la  ,Negative
2 Maximum Entropy Models Maximum entropy  ME  models   also known as loglinear and exponential learning models  provide a general purpose machine learning technique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging  named entity recognition etc Maximum entropy models can integrate features from many heterogeneous information sources for classification ,Positive
Reported work includes improved model variants  and applications such as web data extraction   scientific citation extraction   word alignment   and discourselevel chunking  ,Neutral
The classification is performed with a statistical approach  built around the maximum entropy  MaxEnt  principle   that has the advantage of combining arbitrary types of information in making a classification decision ,Positive
In a previous comparison between GE and PDL Mann and McCallum 2008 GE outperformed PDL without the extra similarity features whose construction may be problemspecific ,Neutral
The semantically impoverished verb often alled a support verb to be used with a nominalized predicate structure is unpredictable Allerton 1982 ,Neutral
The preprocessor module also performs a number of additional functions such as grouping of lexicalizcd and nonlexicalized collocations compound verbs etc Ofiazer and Kurubz 1994 Oflazer and Tiir 1996 ,Neutral
While the use of context information has been explored in MT eg Carpuat and Wu 2007 and He et al 2008 the specific technique we used by means of a context language model is rather different ,Neutral
Subevents Daniel et al 2003 and subtopics Saggion and Lapalme 2002 also contribute to the framework used for comparing documents in multidocument summarization ,Positive
The work of Miller et al 1994 is the only prior work we know of which attempted to evaluate WSD on a large data set and using the refined sense distinction of WORDNET However their results show no improvement in fact a slight degradation in performance when using surrounding words to perform WSD as compared to the most frequent heuristic ,Negative
We use binary Synchronous ContextFree Grammar  bSCFG   based on Inversion Transduction Grammar  ITG    to define the set of eligible segmentations for an aligned sentence pair ,Neutral
The most notable of these include the trigram HMM tagger   maximum entropy tagger   transformationbased tagger   and cyclic dependency networks  Averaging has been shown to reduce overfitting  as well as reliance on the order of the examples during training ,Positive
The classbased kappa statistic of  can not be applied here  as the classes vary depending on the number of ambiguities per entry in the lexicon While the amount of parallel data required to build such systems is orders of magnitude smaller than corresponding phrase based statistical systems   the variety of linguistic annotation required is greater ,Negative
POS tag the text using the tagger of  ,Neutral
This can be the base of a principled method for detecting structural contradictions  6 Related Work Several works attempt to extend WordNet with additional lexical semantic information  ,Neutral
 has been unable to find real examples of cases where hierarchical alignment would fail under these conditions  at least in fixedwordorder languages that are lightly inflected  such as English and Chinese  p 385  ,Positive
This tagged text was then parsed by a lowlevel dependency parser Grefenstette 1994Chap 3 ,Neutral
Manually encoding all these variations into the grammar is tedious and errorprone Supervised semantic parsing addresses this issue by learning to construct the grammar automatically from sample meaning annotations Mooney 2007 ,Positive
5 The statistical parser The parsing model is the one proposed in Merlo and Musillo   which extends the syntactic parser of Henderson  and  with annotations which identify semantic role labels  and has competitive performance ,Positive
 discuss the influence of bias towards highor lowfrequency items for different tasks  correlation with WordNetderived neighbor sets and pseudoword disambiguation   and it would not be surprising if the different highfrequency bias were leading to different results ,Neutral
The model scaling factors are optimized on the development corpus with respect to mWER similar to  ,Neutral
Table lookup using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications  including ` crummy  MT on the World Wide Web   certain machineassisted translation tools  eg ,Neutral
2 Previous Approaches  method of estimating phrasetranslation probabilities is very simple ,Neutral
We further describe an efficient approach to alleviate this problem by using an idea of phrase construction Ohsawa et al 1998 ,Positive
 presented a historybased generation model to overcome some of the inappropriate independence assumptions in the basic generation model of  ,Negative
Our implementation of patterns for information extraction is similar to Blacks1990 implementation of Paices 1981 indicative phrases method but whereas Blackscores sentences based on indicative phrases contained in the sentences our methodscores the information from the sentences based on term distribution,Neutral
Phrasebased method Koehn et al 2003 Och and Ney 2004 Koehn et al 2007 and syntax based method Wu 1997 Yamada and Knight 2001 Eisner 2003 Chiang 2005 Cowan et al 2006 Marcu et al 2006 Liu et al 2007 Zhang et al 2007c 2008a 2008b Shen et al 2008 Mi and Huang 2008 represent the stateoftheart technologies in statistical machine translation SMT ,Neutral
Our test set is 3718 sentences from the English Penn treebank  which were translated into German ,Neutral
Church 1992 claims that partofspeech taggers depend almost exclusively on lexical probabilities whereas other researchers such as Voutilainen Karlsson et al 1995 argue that word ambiguities vary widely in function of the specific text and genre  ,Positive
Recently Brill 1992 described a rulebased tagger that performs as well as taggersbased upon probabilistic models and overcomes the limitations common in rulebasedapproaches to language processing,Neutral
 Derouault and Merialdo 1986 have used these techniques but the necessary human effort is still considerable ,Neutral
Conditional Markov models  CMM   have been successfully used in sequence labeling tasks incorporating rich feature sets ,Positive
However  most of them fail to utilize nonsyntactic phrases well that are proven useful in the phrasebased methods  ,Neutral
