Text,Sentiment
 Introduction Recent works in statistical machine translation  SMT  shows how phrasebased modeling  significantly outperform the historical wordbased modeling  Tools like Xtract  were based on the work of Church and others  but made a step forward by incorporating various statistical measurements like zscore and variance of distribution  as well as shallow linguistic techniques like partofspeech tagging and lemmatization of input data and partial parsing of raw output Minimum Error Rate Training A good way of training is to minimize empirical top error on training data  ,Positive
Finally  to estimate the parameters i of the weighted linear model  we adopt the popular minimum error rate training procedure  which directly optimizes translation quality as measured by the BLEU metric ,Positive
According to the document  it is the output of Ratnaparkhis tagger  ,Neutral
"Many statistical translation models can be regarded as weighted logical deduction. Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs).",Neutral
"Lewis proposed the proportional assignment strategy based on the probabilistic ranking principle (Lewis, 1992). ",Neutral
This can be done in a supervised   a semisupervised  or a fully unsupervised way  Equation  3  reads If the target noun appears  then it is distinguished by the majority The loglikelihood ratio  decides in which order rules are applied to the target noun in novel context ,Neutral
"However, due to the challenges in providing semantic representation, semantic abstraction has not been widely pursued, although the TOPIC system (Hahn and Reimer, 1999) is a notable exception. ",Positive
2 Data 21 The US Congressional Speech Corpus The text used in the experiments is from the United States Congressional Speech corpus   which is an XML formatted version of the electronic United States Congressional Record from the Library of Congress1 ,Neutral
FrameNet role annotations were mapped onto those dependency graph nodes that corresponded most closely to the annotated substring (see Fürstenau (2008) for a detailed description of the mapping algorithm). FrameNet role annotations were mapped onto those dependency graph nodes that corresponded most closely to the annotated substring (see Fürstenau (2008) for a detailed description of the mapping algorithm). ,Neutral
 Introduction Robust statistical syntactic parsers  made possible by new statistical techniques  and by the availability of large  handannotated training corpora such as WSJ  and Switchboard   have had a major impact on the field of natural language processing ,Positive
"Semantic interpretation is based on a categorical analysis that is underspecified in that it is a partial parse (cf. McDonald, 1992).  ",Neutral
 used the BaseNP tag set as presented in   I for inside a BaseNP  O for outside a BaseNP  and B for the first word in a BaseNP following another BaseNP ,Neutral
"We will define true nominalizations as those which have a parallel syntactic structure to the original verb. This is also in keeping with the definition of nominalizations given in (Quirk et al., 1985) ",Neutral
"[Barzilay et al, 2001] evaluated three algorithms for sentence ordering in multi-document Summaries. ",Neutral
Standard CI Model 1 training  initialised with a uniform translation table so that t  ejf  is constant for all sourcetarget word pairs  f  e   was run on untagged data for 10 iterations in each direction  ,Neutral
"We use the Xerox part-of-speech tagger (Cutting et al., 1992), a statistical tagger made at the Xerox Palo Alto Research Center.",Neutral
A later study  found that performance increased to 872  when considering only those portions of the text deemed to be subjective ,Positive
"For the test data, we created a lattice of every possible segmentation of any word 6 characters or longer and used forward-backward pruning to prune out low probability segmentation paths (Sixtus and Ortmanns, 1999).",Neutral
"One may identify various contexts in which either the noun or the adjective can be preferred. Such contextual restrictions (Chanod, 1993) are not always true, but may be considered reasonable for resolving the ambiguity. ",Neutral
Previous research has addressed revision in singledocument summaries   and has suggested that revising summaries can make them more informative and correct errors ,Neutral
On the other hand  integrating an additional component into a baseline SMT system is notoriously tricky as evident in the research on integrating word sense disambiguation  WSD  into SMT systems  different ways of integration lead to conflicting conclusions on whether WSD helps MT performance  ,Positive
"Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004).",Neutral
Among the chunk types  NP chunking is the first to receive the attention   than other chunk types  such as VP and PP chunking  This model shares some similarities with the stochastic inversion transduction grammars  SITG  presented by Wu in  ,Neutral
"In this case the model hypothesesare not satisfied; e.g., there are strong intra-tag relations in distancesgreater than the model order, idiomatic expressions, language dependentexceptions, etc. A general solution to the variable length and depth ofdependency for HMM has been already proposed (Tao 1992), but has notbeen implemented in taggers",Negative
For these classications  we calculated a kappa statistic of 0528  ,Neutral
3 Parse Tree Features We tagged each candidate transcription with  1  partofspeech tags  using the tagger documented in   and  2  a full parse tree  using the parser documented in Collins  1999  ,Neutral
The learning algorithm used for each stage of the classification task is a regularized variant of the structured Perceptron  ,Neutral
Stateoftheart machine learning techniques including Support Vector Machines   AdaBoost  and Maximum Entropy Models  provide high performance classifiers if one has abundant correctly labeled examples ,Positive
"In contrast, in a rule based system, the system designer would have to consider how, for instance, a WordNet (Miller, 1995) derived information for a particular example interacts with a part-of-speech-based information and chunking information. That is not to say, ultimately, that rule-based systems are in some way inferior to statistical models – they are built using valuable insight which is hard to obtain from a statistical-model only approach.",Positive
"In this respect, the present work is closer in spirit to Ji et al. (2005), who explore the employment of the ACE 2004 relation ontology as a semanticfilter. ",Positive
"Intuitively, if we are able to find good correspondences among features, then the augmented labeled source domain data should transfer better to a target domain (where no labeled data is available) (Blitzer et al., 2006).",Neutral
Presently  there exist methods for learning oppositional terms  and paraphrase learning has been thoroughly studied  but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution Formal complexity analysis has not been carried out  but my algorithm is simpler  at least conceptually  than the variablewordorder parsers of Johnson     and Abramson and Dahl  1989  ,Negative
Recent research (Daume et al. 2002) has show that syntax-based languagemodels are more suitable for language generation tasks; the study of such models isa promising direction to explore.,Positive
Bilexical contextfree grammars have been presented in  as an abstraction of language models that have been adopted in several recent realworld parsers  improving stateoftheart parsing accuracy  For the Penn Treebank   reports an accuracy of 966  using the Maximum Entropy approach  our much simpler and therefore faster HMM approach delivers 967  ,Negative
"Similarly to classical NLP tasks such as base noun phrase chunking (Ramshaw and Marcus, 1994), text chunking (Ramshaw and Marcus, 1995) or named entity recognition (Tjong Kim Sang, 2002), we formulate the mention detection problem as a classification problem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.  ",Positive
"A simpler machine learning approach using only word frequency information and no other features, as typically used in tasks like text classification, could have been employed (and indeed Nanba and Okumura [1999] do so for classifying citation Contexts). ",Positive
Again  we find the clearest patterns in the graphs for precision  where Malt has very low precision near the root but improves with increasing depth  while MST shows the opposite trend  ,Neutral
"Word-sense disambiguation, a problemthat once seemed out of reach for systems without a great deal of handcrafted lin-guistic and world knowledge, can now in some cases be done with high accuracywhen all information is derived automatically from corpora (Brown, Lai, and Mercer1991; Yarowsky 1992; Gale, Church, and Yarowsky 1992; Bruce and Wiebe 1994).",Neutral
Goldsmith (2001) performs a minimum description length analysis of the mophology of several European languages using coPora.,Neutral
The results evaluated by BLEU score  is shown in Table 2 ,Neutral
"We conducted experiments with the Reuters-21578 Corpus a relatively tiny one for such experiments. Clark (2000) reports results on a corpus containing 12 million terms, Schütze (1993) on one containing 25 million terms, and Brown, et al, (1992) on one containing 365 million terms. In contrast, we count approximately 2.8 million terms in Reuters-21578.",Positive
"In English taggers, (Weischedel et al., 1993) proposed a statistical model to estimate word output probability p(wi|tl) for an unknown word from spelling information such as inflectional endings, derivational endings, hyphenation, and capitalization. Our word model can be thought of a generalization of their statistical model. One potential benefit of our statistical model and segmentation algorithm is that they are completely independent of the target language and its writing system. ",Negative
Smith and Eisner (2006) instead propose a dif ferentiable objective that can be optimized by gradient descent: the Bayes risk R(p) of (7). ,Neutral
"(Grefenstette, 1998) proposed to remove phrases in sentences to produce a telegraphic text that can be used to provide audio scanning service for the blind. (Corston-Oliver and Dolan, 1999) proposed to remove clauses in sentences before indexing documents for Information Retrieval. ",Neutral
Our method does not suppose a uniform distribution over all possible phrase segmentationsas  since each phrase tree has a probability ,Neutral
4 Extended Minimum Error Rate Training Minimum error rate training  is widely used to optimize feature weights for a linear model  When we run our classifiers on resourcetight environments such as cellphones  we can use a random feature mixing technique  or a memoryefficient trie implementation based on a succinct data structure  to reduce required memory usage Evaluation Criteria Wellestablished objective evaluation measures like the word error rate  WER   positionindependent word error rate  PER   and the BLEU score  were used to assess the translation quality ,Positive
Reported work includes improved model variants  and applications such as web data extraction   scientific citation extraction   word alignment   and discourselevel chunking  ,Neutral
"In the bigram model, we can weight each probability of a pair of tags in both models estimated from tagged or untagged corpora. A smoothing method, such as deleted interpolation (Jelinek, 1985), can be used for Weighting.",Positive
"Research in lexical semantics (Cruse, 1986) provides insight into the interaction of reference and linguistic Structure.  ",Neutral
"Others perform the division implicitly without discussing performance (e.g. (Cutting et al., 1992)). ",Negative
"The subsequential transducer generated by this algorithm could in turn be min-imized by an algorithm described in Mohri (1994a). However, in our case, the trans-ducer is nearly minimal.",Neutral
"Our method to address the problem of length bias in rule selection is very different from the maximum entropy method used in existing studies, e.g. (He et al., 2008).",Neutral
"Kupiec (1992) has proposed an estimation method for the N-gram language model using the Baum-Welch reestimation algorithm (Rabiner et al., 1994) from an untagged corpus and Cutting et al. (1992) have applied this method to an English tagging system.Takeuchi and Matsumoto (1995) also have developed an extended method for unsegmented languages (e.g., Japanese) and applied it to their Japanese tagger.  ",Positive
"Our rhetorical analysis, as noted above, is nonhierarchical, in contrast to Rhetorical Structure Theory (RST) (Mann and Thompson 1987; Marcu 1999), and it concerns text pieces at a lower level of granularity. Although we do agree with RST that the structure of text is hierarchical in many cases, it is our belief that the relevance and function of certain text pieces can be determined without analyzing the full hierarchical structure of the text. ",Negative
"The work of (Bruce and Wiebe, 1994) used parts of speech (POS) and morphological form, in addition to surrounding words. However, the POS used are abbreviated POS, and only in a window of -b2 words. No local collocation knowledge is used. A probabilistic classifier is used in (Bruce and Wiebe, 1994). ",Negative
The use of dependencies in MT evaluation has not been extensively researched before  one exception here would be    and requires more research to improve it  but the method shows potential to become an accurate evaluation metric ,Positive
Berger et al (1996) proposed an iterative procedure of adding news features to feature set driven by data. We present a simple and effective approach using some statistical heuristics for feature selection. ,Negative
"Much previous work has focused on creating FrameNet-style annotations for languages other than English. A common strategy is to exploit parallel corpora and transfer annotations from English sentences onto their translations (Padó and Lapata, 2006; Johansson and Nugues, 2006). ",Neutral
Although to a lesser extent  measures of word relatedness have also been applied on other languages  including German   Chinese   Dutch  and others ,Neutral
Such a quasisyntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrasebased approach  Although generating training examples in advance without a working parser  is much faster than using inference   our training time can probably be decreased further by choosing a parsing strategy with a lower branching factor ,Negative
Dredze et al yielded the second highest score1 in the domain adaptation track  The IBM models  search a version of permutation space with a onetomany constraint  propose the use of language models for sentiment analysis task and subjectivity extraction ,Neutral
This source of overcounting is considered and fixed by  and Zens and Ney  2003   which we briefly review here This further supports the claim by  that loglikelihood ratio is much less sensitive than pmi to low counts Inversion transduction grammar   or ITG  is a wellstudied synchronous grammar formalism ,Positive
"This tagged text was then parsed by a low-level dependency parser (Grefenstette, 1994)[Chap 3]. ",Neutral
We compare our approach with a method proposed by Fürstenau and Lapata (2009). This approach is more tailored to the specific case of SRL and is summarized here. ,Neutral
Pustejovsky confronted with the problem of automatic acquisition more extensively in  PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank  ,Neutral
"It is well-known that such distributions can represent meaning reasonably well, at least for meaning-comparison purposes (Landauer and Dumais, 1997).",Neutral
Zhang et al. (2008) and Wellington et al. (2006) answer the question: what is the minimal grammar that can be induced to completely describe a training set? ,Positive
"The only way to avoid it, is to anonymize the notes prior to POS tagging which in itself is a difficult and expensive process (Ruch et al. 2000).  ",Neutral
Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably re- duced by combining learning procedures and a partial part-of-speech categorization elicited from an informant. ,Positive
"Merialdo (1994) and Elworthy (1994) have insisted, based on their experimental results, that the maximum likelihood training using an untagged corpus does not necessarily improve tagging accuracy. However, their likelihood was the probability with all paths weighted equivalently. Since more than half of the symbols in the observations may be noise, models estimated in this way are not reliable.",Negative
"While many text categorization models have been proposed so far, in this paper, we concentrate on the probabilistic models (Robertson and Sparck Jones, 1976; Kwok, 1990; Fuhr, 1989; Lewis, 1992; Croft, 1981; Wong and Yao, 1989; Yu et al., 1989) because these models have solid formal grounding in probability theory. ",Positive
"Kupiec, Ped-ersen, and Chen (1995) report on the semiautomatic alignment of 79 of sentences ofprofessional abstracts in a corpus of 188 documents with professional abstracts.",Neutral
Because of these kinds of results  the vast majority of statistical parsing work has focused on parsing as a supervised learning problem  ,Neutral
On the other hand   proposed an algorithm  borrowed to the field of dynamic programming and based on the output of their previous work  to find the best alignment  subject to certain constraints  between words in parallel sentences ,Neutral
"Ayan and Dorr (2006) showed that under certain conditions, this constraint could have significant impact on system performance. ",Neutral
Rulesize and lexicalization affect parsing complexity whether the grammar is binarized explicitly  or implicitly binarized using Earlystyle intermediate symbols  ,Neutral
The first SMT systems were developed in the early nineties  ,Neutral
Like WASP1  the phrase extraction algorithm of PHARAOH is based on the output of a word alignment model such as GIZA     which performs poorly when applied directly to MRLs  Section 32  This restriction is necessary because the problem of optimizing manytomany alignments 5 Our preliminary experiments with ngrambased overlap measures  such as BLEU  and ROUGE   show that these metrics do not correlate with human judgments on the fusion task  when tested against two reference outputs ,Negative
"This text was part-of-speech tagged using the Xerox H M M tagger ( C u t t i n g et al., 1992). ",Neutral
"For example, the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus (Gildea, 2001) ",Neutral
23 Online Learning Again following   we have used the single best MIRA   which is a margin aware variant of perceptron  for structured prediction ,Neutral
"When automated part-of-speech tagging was initially explored (Klein and Sim-mons 1963; Harris 1962), people manually engineered rules for tagging, sometimeswith the aid of a corpus.",Neutral
"Deleted and spurious content is a well known problem for statistical models (Chiang et al., 2008). ",Neutral
"In the domain of indicative summarization, Kan and McKeown (2002) studied the problem of generating abstracts for bibliographical data which although in a restricted domain has some contact points with the work described here. As in their work we use the abstracts in our corpus to induce the model. They rely on a more or less fixed discourse structure to accommodate the generation process. In our approach the discourse structure is not fixed but predicted for each particular abstract. ",Negative
"Rath, Resnick, and Savage (1961) report that six participants agreed on only 8 of 20 sentences they were asked to select out of short Scientific American texts and that five agreed on 32 of the Sentences. ",Neutral
129 5 Active learning Whereas a passive supervised learning algorithm is provided with a collection of training examples that are typically drawn at random  an active learner has control over the labeled data that it obtains  ,Neutral
"Second, we devised a method to obtain the expected word N-gram count in the target texts, using an N-best word segmentation algorithm (Nagata, 1994). ",Positive
This approach addresses the problematic aspects of both pure knowledgebased generation  where incomplete knowledge is inevitable  and pure statistical bag generation   where the statistical system has no linguistic guidance  In addition  the clustering methods used  such as HMMs and Browns algorithm   seem unable to adequately capture the semantics of MNs since they are based only on the information of adjacent words ,Negative
Some methods which can offer powerful reordering policies have been proposed like syntax based machine translation  and Inversion Transduction Grammar  ,Positive
"Split path feature are taken from existing semantic role labeling systems, see for example (Gildea and Jurafsky, 2002; Lim et al., 2004; Thompson et  al., 2006).",Neutral
The first known supervised learning algorithm was proposed by Kupiec et al. (1995). Their approach estimates the probability that a sentence should be included in a summary given its feature values based on the independent assumption of Bayes’ Rule. ,Positive
"Secondly, our method is related to beam search (Woods, 1985).In b e a m search, incomplete parses of an utterance are pruned or discarded when, on some criterion, they are significantly less plausible than other, competing parses. This pruning is fully interleaved with the parsing process. In contrast, our pruning takes place only at certain points: currently before parsing begins, and between the phrasM and full parsing stages. ",Negative
"We distinguish two main approaches to domain adaptation that have been addressed in the literature (Daumé III, 2007): supervised and semi-supervised. ",Neutral
"A syntactic parse is however a representation that is very closely tied with the surface-form of natural language, in contrast to Semantic Role Labeling (SRL) which adds a layer of predicate-argument information that generalizes across different syntactic alternations (Palmer et al., 2005). ",Neutral
Recently  graphbased methods have proved useful for a number of NLP and IR tasks such as document reranking in ad hoc IR  and analyzing sentiments in text  For English  after a relatively big jump achieved by   we have seen two significant improvements   and  pushed the results by a significant amount each time In our final comparison  we have also included the results of   because it has surpassed  as well and we have used this tagger in the data preparation phase ,Positive
"But in fact, the issue of editing in text summa-rization has usually been neglected, notable exceptions being the works by Jing andMcKeown (2000) and Mani, Gates, and Bloedorn (1999). In our work, we partially ad-dress this issue by enumerating some transformations frequently found in our corpusthat are computationally implementable.",Negative
Among these techniques  SCL  Structural Correspondence Learning   is regarded as a promising method to tackle transferlearning problem Several generalpurpose offtheshelf  OTS  parsers have become widely available  ,Positive
Our MT baseline system is based on Moses decoder  with word alignment obtained from GIZA    ,Neutral
2 Maximum Entropy Models Maximum entropy  ME  models   also known as loglinear and exponential learning models  provideageneralpurposemachinelearningtechnique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging  named entity recognition etc Maximum entropy models can integrate features from many heterogeneous information sources for classification  ,Positive
"Ornan (1986) for instance, developed a new writing system for Hebrew, called'The Phonemic Script.' This script enables the user to write Hebrew texts that aremorphologically unambiguous, in order to use them later as an input for variouskinds of natural language applications.",Positive
Training of the phrase translation model builds on top of a standard statistical word alignment over the training corpus of parallel text  for identifying corresponding word blocks  assuming no further linguistic analysis of the source or target language ,Neutral
"They were chosen among the 20 participating systems either because they held better results (the first four participants) or because they used some joint learning techniques (Henderson et al., 2008). ",Neutral
For example  the word alignment computed by GIZA   and used as a basis to extract the TTS templates in most SSMT systems has been observed to be a problem for SSMT   due to the fact that the wordbased alignment models are not aware of the syntactic structure of the sentences and could produce many syntaxviolating word alignments ,Neutral
Our specific task is a two player object-identification game adapted from the experiments of Clark and Wilkes-Gibbs (1986) and Brennan and Clark (1996) ,Positive
"The tagger is reported (Cutting el al., 1992) to have a better than 96 % accuracy in the analysis of parts of the Brown Corpus. The accuracy is similar to other probabilistic taggers.  ",Positive
"We also argue against Church's position, supporting the claim that more attention needs to be paid to contextual information for part-of-speech disambiguation (Tzoukermann et ai., 1995). ",Neutral
"Word alignment is a critical component in training statistical machine translation systems and has received a significant amount of research, for example, (Brown et al., 1993; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2007), including work leveraging syntactic parse trees, e.g., (Cherry and Lin, 2006; DeNero and Klein, 2007; Fossum et al., 2008).",Neutral
"The UMLS (Humphreys et al., 1998) consists of three components: the Metathesaurus, ® Semantic Network (McCray, 1993), and SPECIALIST Lexicon (McCray et al., 1994). ",Neutral
"The lines of the corpus are tokenized (Grefenstette and Tapanainen, 1994), and only sentences containing one of the word forms in the filter are retained. ",Neutral
There also have been prior work on maintaining approximate counts for higherorder language models  LMs     operates under the model that the goal is to store a compressed representation of a diskresident table of counts and use this compressed representation to answer count queries approximately ,Neutral
"The experiment was carried out using both the chunking criteria from (Rayner and Samuelsson, 1994) (the ""Old"" scheme), and the chunking criteria described in Section 3 above (the ""New"" scheme). ",Positive
"This part of the paper is essentially an extension and generalization of the line of work described in (Rayner, 1988; Rayner and Samuelsson, 1990; Samuelsson and Rayner, 1991; Rayner and Samuels- son, 1994; Samuelsson, 1994b). ",Positive
"RST can be used in sentence selection for single document summarization [Marcu, 1997]. However, it cannot be applied to MDS. ",Neutral
Giza   is a freely available implementation of IBM Models 15  and the HMM alignment   along with various improvements and modifications motivated by experimentation by Och & Ney  ,Neutral
"The underlying idea of the replacement is the same as Turing's estimates in back-off smoothing (Katz, 1987). ",Neutral
"The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al. (2005) (74.4%), and matches the results of Mann and Mc- Callum (2008) with GE with more accurate sampled label distributions and 10 labeled examples",Negative
"To compute scores for word pairs, we perform pair-wise hypothesis alignment using the indirect HMM (He et al. 2008) for every pair of input hypotheses.",Neutral
However  most of them fail to utilize nonsyntactic phrases well that are proven useful in the phrasebased methods  ,Neutral
"Semiring parsing (Goodman, 1999) is a general framework to describe such algorithms. ",Neutral
1 Introduction Many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts  ,Neutral
Lopez (2009) recently argued for a separation between features/formalisms (and the independence assumptions they imply) from inference algorithms in MT; this separation is widely appreciated in machine learning.,Positive
"To ameliorate this, revision of the extracted sentences is also thought to be effective, and many ideas and methods have been proposed so far. For example, Otterbacher and colleagues (2002) analyzed manually revised extracts and factored out cohesion problems. Nenkova (2008) proposed a revision idea that utilizes noun coreference with linguistic quality improvements in mind.",Positive
For English  we have used sections 0306 of the WSJ portion of the Penn Treebank  distributed by the Linguistic Data Consortium  LDC   which have frequently been used to evaluate sentence boundary detection systems before  compare Section 7 ,Neutral
"Table 10 shows the results in terms of three overall measures: kappa, percentage accuracy, and macro-F (following Lewis [1991]).  ",Neutral
The same asymptotic complexityis of course found for memory storage in this approach. We use IGTrees (Daelemans etal. 1996) to compress the memory. IGTree is a heuristic approximation of the IB-IGAlgorithm.,Positive
1 Introduction Recent approaches to statistical machine translation  SMT  piggyback on the central concepts of phrasebased SMT  and at the same time attempt to improve some of its shortcomings by incorporating syntactic knowledge in the translation process ,Negative
4 Features For our experiments we use the features proposed  motivated and described in detail by  ,Neutral
"The first, scientific documents with abstracts, represents a readily available class of summaries often discussed in the literature (Marcu 1999).  ",Neutral
"The concept of support verb was broadly used (Toutanova et al., 2005; Xue, 2006; Jiang and Ng, 2006) 4 , we here extend it to nouns and prepositions. ",Neutral
"This makes the phrase based method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly generate features involving structured information in many NLP applications. ",Neutral
"To address this, we explicitly generate beginning and end sentence markers as part of the translation process, as sug- gested by Xiong et al. (2008). The results of doing this are shown in Table 2. ",Positive
On the other hand  purely statistical systems  extract discriminating MWUs from text corpora by means of association measure regularities ,Neutral
"The Stanford parser (Klein and Manning, 2003) is used to parse Chinese sentences on the training, dev and test sets.",Neutral
"In the second phase, the resulting set of ""chunked"" rules is converted into LR table form, using the method of (Samuelsson, 1994a). ",Positive
"Semantic analysis is an open research field in natural language processing. Two major research topics in this field are Named Entity Recognition (NER) (N. Wacholder and Choi, 1997; Cucerzan and Yarowsky, 1999) and Word Sense Disambiguation (WSD) (Yarowsky, 1995; Wilks and Steven Son, 1999).",Neutral
Unfortunately  longer sentences  up to 100 tokens  rather than 40   longer phrases  up to 10 tokens  rather than 7   two LMs  rather than just one   higherorder LMs  order 7  rather than 3   multiple higherorder lexicalized reordering models  up to 3   etc all contributed to increased system  s complexity  and  as a result  time limitations prevented us from performing minimumerrorrate training  MERT   for ucb3  ucb4 and ucb5 ,Negative
For example  it has been observed that texts often contain multiple opinions on different topics   which makes assignment of the overall sentiment to the whole document problematic ,Neutral
In other words   4b  can be used in substitution of  4a   whereas  5b  can not  so easily 41n   a value of K between 8 and I indicates good agreement  a value between 6 and 8 indicates some agreement ,Neutral
"Although it is decidable whether a function is subsequential or not (Choffrut 1977),the determinization algorithm described in the previous section does not terminatewhen run on a nonsubsequential function.",Neutral
"In order to avoid a full Viterbi search of all possibilities, we perform a beam search with width of three among the candidates of the previous sentence, following Barzilay et al. (2000). ",Neutral
Nonparametricmodels  may be appropriate ,Positive
Statistical Model In SIFTs statistical model  augmented parse trees are generated according to a process similar to that described in  ,Neutral
"For example, DIRT (Lin and Pantel, 2001) learns paraphrases of binary relations based on distributional similarity of their arguments; TextRunner (Banko et al., 2007) automatically extracts relational triples in open domains using a self-trained extractor; SNE applies relational clustering to generate a semantic network from TextRunner triples (Kok and Domingos, 2008). ",Neutral
"The semantically impoverished verb, often alled a support verb, to be used with a nominalized predicate structure is unpredictable. Allerton (1982) ",Neutral
In particular  the model in  failed to generate punctuation  a deficiency of the model 1 Introduction Phrasebased translation models   which go beyond the original IBM translation models  1 by modeling translations of phrases rather than individual words  have been suggested to be the stateoftheart in statistical machine translation by empirical evaluations ,Negative
"We use Joshua (Li et al., 2009), in our experiments. Joshua is a hierarchical parsing-based MT system, and it can be instructed to produce derivation trees instead of the candidate sentence string Itself. ",Positive
Still  however  such techniques often require seeds  or prototypes  cf    which are used to prune search spaces or direct learners ,Neutral
"In this writingsystem not all the vowels are represented, several letters represent both consonantsand different vowels, and gemination is not represented at all (Ornan 1986, 1991).",Neutral
"In a previous paper (Schfitze, 1993), we trained a neural network to disambiguate part-of-speech using context; however, no information about the word that is to be categorized was used. This scheme fails for cases like The soldiers rarely come home. vs. The soldiers will come home.   ",Negative
Montesi and Owen (2007) observe that the revision of abstracts is carried out to improve comprehensibility and style and to make the abstract Objective.,Neutral
"Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntax- based method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). ",Neutral
Large volumes of training data of this kind are indispensable for constructing statistical translation models   acquiring bilingual lexicon   and building examplebased machine translation  EBMT  systems  ,Neutral
"For more details on the linguistic specifications of the annotation scheme see (Skut et al 1997). A similar approach has been also successfully applied in the T S N L P database, cf. (Lehmann et al 1996). ",Positive
"In fact, previous applications of multisequence alignment have beenshown to increase the accuracy of the comparison in other NLP tasks (Barzilay andLee 2002; Bangalore, Murdock, and Riccardi 2002; Lacatusu, Maiorano, and Harabagiu2004); unlike our work these approaches operate on strings, not trees, and with theexception of (Lacatusu, Maiorano, and Harabagiu 2004), they apply alignment to paral-lel data, not comparable texts.",Positive
"the method of handling unknown words that seems to work best for inflected languages is a suffix analysis as proposed in (Samuelsson, 1993). Tag probabilities are set according to the word's endIng.",Positive
Decoding is based on a beam search algorithm similar to that of the phrase-based MT decoder (Koehn 2004b).,Neutral
" Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001). Close to the problem studied here is Jing and McKeown’s (Jing and McKeown, 2000) cut-and-paste method founded on Endres- Niggemeyer’s observations. ",Neutral
"Feature active learning, presented in Algorithm 1, is a pool-based active learning algorithm (Lewis and Gale, 1994) (with a pool of features rather than instances). ",Neutral
"Indeed, using tree kernel methods to mine structured knowledge has shown success in some NLP applications like parsing (Collins and Duffy,2001), semantic role labeling (Moschitti, 2004 Zhang et al., 2007b), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al.,2006) and question classification (Zhang and Lee, 2003). ",Neutral
1 Introduction Current methods for largescale information extraction take advantage of unstructured text available from either Web documents  or  more recently  logs of Web search queries  to acquire useful knowledge with minimal supervision ,Neutral
"To evaluate the candidate translation, the source parse tree is first obtained (Dubey, 2005), and each subtree is matched with a substring in the candidate string. ",Neutral
"As pointed out by Blitzer et al. (2006), each instance will actually contain features which are totally predictive of the pivot features (i.e. the pivot itself).",Neutral
This similarity score is computed as a max over a number of component scoring functions some based on external lexical resources including  various string similarity functions of which most are applied to word lemmas  measures of synonymy hypernymy antonymy and semantic relatedness including a widelyused measure due to Jiang and Conrath 997 based on manually constructed lexical resources such as WordNet and NomBank  a function based on the wellknown distributional similarity metric of Lin 998 which automatically infers similarity of words and phrases from their distributions in a very large corpus of English text The ability to leverage external lexical resources both manually and automatically constructedis critical to the success of MANLI,Positive
"The system just ended up at rank 7 out of 8 teams. However, based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive. 1 Thus, the effectiveness of SCL is rather unexplored for parsing. ",Neutral
Much of the work in subjectivity analysis has been applied to English data  though work on other languages is growing  eg  Japanese data are used in   Chinese data are used in   and German data are used in  ,Neutral
"In (Koehn and Hoang, 2007), shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal Decoder.  ",Neutral
Measures of crosslanguage relatedness are useful for a large number of applications  including crosslanguage information retrieval   crosslanguage text classification   lexical choice in machine translation   induction of translation lexicons   crosslanguage annotation and resource projections to a second language  ,Neutral
"One doubt concerns the notion 'correct analysis"". For example Church (1992) argues that linguists who manually perform the tagging task using the double blind method disagree about the correct analysis in at least 3% of all words even after they have negotiated about the initial disagreements. ",Neutral
We also can not use prior graph construction methods for the document level  such as physical proximity of sentences  used in   at the word sense level ,Neutral
"Paice (1990) introduces grammars for pattern matching of indicator phrases, e.g., the aim/purpose of this paper/article/study and we conclude/propose. ",Positive
Even the 3 A demo of the parser can be found at httplfgdemocomputingdcuielfgparserhtml creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level  Our method  extending this line of research with the use of labeled LFG dependencies  partial matching  and nbest parses  allows us to considerably outperform  highest correlations with human judgement  they report 0144 for the correlation with human fluency judgement  0202 for the correlation with human overall judgement   although it has to be kept in mind that such comparison is only tentative  as their correlation is calculated on a different test set ,Negative
35 The Experiments We have ran LexTract on the onemillionword English Penn Treebank  and got two Treebank grammars ,Neutral
Such techniques are currently being applied in many areas  including language identification  authorship attribution   text genre classification   topic identification   and subjective sentiment classification  ,Neutral
The form of the maximum entropy probability model is identical to the one used in   k f   wiwi1  wi2  at  ri  YIj  I Otj p  wilwil  wi2  attri   Z  Wil  wi2  attri  k to t j  l where wi ranges over V t3 stop ,Neutral
"Itconstructs the trees in a top - down way, guided bythe distributional information of the examples, butnot on the examples order (Quinlan, 1986).",Neutral
For our experiments  we chose GIZA    and the RA approach  the best known alignment combination technique as our initial aligners 42 TBL Templates Our templates consider consecutive words  of size   2 or 3  in both languages It has been known for some years that good performance can be realized with partial tagging and a hidden Markov model  ,Positive
"System combination for machine translation (MT) has emerged as a powerful method of combining the strengths of multiple MT systems and achieving results which surpass those of each individual system (e.g. Bangalore, et. al., 2001, Matusov, et. al., 2006, Rosti, et. al.,2007a).",Neutral
Section 7 considers recent efforts to induce effective procedures for automated sense labeling of discourse relations that are not lexically marked  ,Neutral
Our method uses assumptions similar to  et al 1996 but is naturally suitable for distributed parallel computations The agreement on identifying the boundaries of units  using the statistic discussed in   was  9  for two annotators and 500 units   the agreement on features  2 annotators and at least 200 units  was as follows  UTYPE   76  VERBED   9  FINITE   81 ,Neutral
Such tasks will require an extension of the current framework of Turney (2008) beyond evidence from the direct cooccurrence of target word pairs. ,Neutral
"The direct translation model in (Ittycheriah and Roukos, 2007) employed syntactic (POS tags) and context information (neighboring words) within a maximum entropy model to predict the correct transfer rules. A similar technique was applied by He et al. (2008) to improve the Hiero system.",Positive
Almost all recent work in developingautomatically trained part-of-speech taggers has been on further exploring Markov-model based tagging (Jelinek 1985; Church 1988; Derose 1988; DeMarcken 1990; Meri-aldo 1994; Cutting et al. 1992; Kupiec 1992; Charniak et al. 1993; Weischedel et al. 1993;Schutze and Singer 1994).,Neutral
"In the second step, the selected features were used to train the model to estimate probabilities. We used MALLET’s implementation of Limited memory BFGS (Nocedal, 1980). ",Positive
"Parameters were tuned with MERT algorithm (Och, 2003) on the NIST evaluation set of 2003 (MT03) for both the baseline systems and the system combination model.  ",Neutral
"Thirdly, our method is a generalization of the strategy employed by (McCord, 1993). McCord interleaved parsing with pruning in the same way as us, but only compared constituents over the same span and with the same major category. Our comparisons are more global and therefore can result in more effective pruning. ",Negative
 Peter F Brown  Vincent J Della Pietra  Petere V deSouza  Jenifer C Lai  and Robert L Mercer ,Neutral
"In Weischedel et al. (1993), results are given when training and testing a Markov-model based tagger on the Penn Treebank Tagged Wall Street Journal Corpus.",Neutral
"This task, introduced by Landauer and Dumais (1997), consists of 80 multiple choice questions in which a word is given as the stem and the correct choice is the word which has the closest meaning to that of the stem, among 4 candidates. ",Neutral
Knight and Marcu (2000) propose a noisy-channel model and adecision-based model for sentence reduction also aiming at conciseness,Neutral
In our approach  equation  1  is further normalized so that the probability for different lengths of F is comparable at the word level  m m j n i ijm eft l EFP  1 10    1  1        2  The alignment models described in  are all based on the notion that an alignment aligns each source word to exactly one target word ,Neutral
Our evaluation metric is BLEU  ,Neutral
11 However  modeling word order under translation is notoriously difficult   and it is unclear how much improvement in accuracy a good model of word order would provide ,Neutral
"The quality of human-produced abstractshas been examined in the literature (Grant 1992; Kaplan et al. 1994; Gibson 1993),using linguistic criteria such as cohesion and coherence, thematic structure, sentencestructure, and lexical density; in automatic text summarization, however, such detailedanalysis is only just emerging.",Neutral
"We use a specific tree-based system called Hiero (Chiang, 2007) as an example, although the discussion is general for any systems that use a hypergraph to represent the hypothesis space. ",Neutral
"We built a translation model on a corpus for IWSLT 2005 Chinese-to-English translation task (Eck and Hori, 2005), which consists of 40k pairs of sentences. ",Neutral
"The basic setup is identical to the one described in (Dugast et al., 2007). ",Neutral
"This default strategy has been advocated as the baseline performance level for comparison with WSD programs (Gale et al., 1992).",Neutral
An alternative is to create an automatic system that uses a set of training questionanswer pairs to learn the appropriate questionanswer matching algorithm  ,Neutral
"Thede and Harper (1997) consider contextual information, word endings, entropy and open-class smoothing. A similar approach is presented in (Schmid,1995). Ruch et al. (2000) combine POS guessing, contextual rules and Markov models to build a POS tagger for biomedical text. ",Neutral
"There is both synchronic (Ross, 1972) and diachronic (Tabor, 1994) evidence suggesting that words and their uses can inherit properties from several prototypical syntactic categories. ",Positive
"I used 26108 Japanese untagged sentences as training data and 100 hand-tagged sentences as test data, both from the Nikkei newspaper 1994 corpus (Nihon Keizai Shimbun, Inc., 1995). ",Neutral
"We consider a task-oriented dialog to be the result of incremental creation of a shared plan by the participants (Lochbaum, 1998). ",Neutral
"The output of the parser is dependency structure based on the guidelines of CGN (Oost Dijk, 2000). ",Neutral
"Linguists have long been interested in the semantic constraints that verbs impose on their arguments, a broad area that has also attracted computational modeling, with increasing interest in purely corpus-based methods (Erk, 2007; Padó et al., 2007). ",Neutral
We use five sentiment classification datasets  including the widelyused movie review dataset  MOV   as well as four datasets containing reviews of four different types of products from Amazon  books  BOO   DVDs  DVD   electronics  ELE   and kitchen appliances  KIT    ,Positive
"The test set includes only sentences for which our English parser (Soricut and Marcu, 2003) could produce a parse tree, which effectively excluded a few very long Sentences.",Neutral
"To increase the coherence of the output text, we identify blocks of topicallyrelated themes and then apply chronological ordering on blocks of themes using themetime stamps (Barzilay, Elhadad, and McKeown 2002).",Neutral
It is faster and more mnemonic than the one in  1 Introduction Translations tables in Phrasebased Statistical Machine Translation  SMT  are often built on the basis of Maximumlikelihood Estimation  MLE   being one of the major limitations of this approach that the source sentence context in which phrases occur is completely ignored  ,Negative
However  in   the authors investigate minimum translation units  MTU  which is a refinement over a similar approach by  to eliminate the overlap issue Because of this property  vector space models have been used successfully both in computational linguistics  and in cognitive science  ,Positive
"The typical approach for testing a summarization system is to create an “ideal” summary, either by professional abstractors or merging summaries provided by multiple human subjects using methods such as majority opinion, union, or intersection (Jing et al., 1998). ",Neutral
"We also extend the work of Zollmann et al. (2008) on Chinese-English, performing the analysis in both directions and providing a detailed qualitative explanation",Neutral
"Target language models (LMs) used by the decoder and rescoring modules are, respectively, estimated from 3-gram and 4-gram statistics by applying the modified Kneser-Ney smoothing method (Goodman and Chen, 1998). ",Neutral
"In Weischedel et al. (1993), a statistical approach to tagging unknown words isShown.",Neutral
Note that the minimum error rate training  uses only the target sentence with the maximum posterior probability whereas  here  the whole probability distribution is taken into account A word order correlation bias  as well as the phrase structure biases in  Models 4 and 5  would be less beneficial with noisier training bitexts or for language pairs with less similar word order ,Negative
To compare the performance of system  we recorded the total training time and the BLEU score  which is a standard automatic measurement of the translation qualit  ,Neutral
"The second aspect motivating our work comes from the subspace learning method in machine learning literature (Ho, 1998), in which an ensemble of classifiers are trained on subspaces of the full feature space, and final classification results are based on the vote of all classifiers in the Ensemble. ",Positive
In agreement with recent results on parsing with lexicalised probabilistic grammars   our main result is that statistics over lexical features best correspond to independently established truman intuitive preferences and experimental findings Since human evaluation is costly and difficult to do reliably  a major focus of research has been on automatic measures of MT quality  pioneered by BLEU  and NIST  ,Positive
The surface heuristic can define consistency according to any word alignment  but most often  the alignment is provided by GIZA    ,Neutral
"This is not surprising, given that the definition of our task has little to do with the distribution of “content-bearing” words and phrases, much less so than the related task of topic segmentation (Morris and Hirst 1991; Hearst 1997; Choi 2000), or Saggion and Lapalme’s (2000) approach to the summarization of scientific articles, which relies on scientific concepts and their relations.",Neutral
"In English part of speech taggers, the maximization of Equation (1) to get the most likely tag sequence, is accomplished by the Viterbi algorithm (Church, 1988), and the maximum likelihood estimates of the parameters of Equation (2) are obtained from untagged corpus by the Forward Backward algorithm (Cutting et al., 1992). ",Positive
154 2 Translation Models 21 Standard Phrasebased Model Most phrasebased translation models  rely on a preexisting set of wordbased alignments from which they induce their parameters ,Neutral
"Decision trees,recently used in NLP basic tasks such as taggingand parsing (McCarthy and Lehnert, 1995: Daele-mans et al., 1996; Magerman, 1996), are suitable forperforming this task.",Neutral
"The task of sentence compression (or sentence reduction) can be defined as summarizing a single sentence by removing information from it (Jing and McKeown, 2000). ",Neutral
Consequently  here we employ multiple references to evaluate MT systems like BLEU  and NIST  ,Neutral
Fuhr (1989) solves problem 2 by assuming that a document is probabilistically indexed by its term vectors. This model is called Retrieval with Probabilistie Indexing (RPI). ,Positive
"We chose to train maximum entropy models (Berger et al., 1996). Our learning framework is described in Section 4.1; the results in Section 4.2  ",Neutral
"Finally, our method to detect verb slot similarity is analogous to the “slot overlap” of Joanis et al. (2008) and others. ",Positive
"The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the Model.",Negative
"Most SMT systems assume that translation rules can be applied without paying attention to the sentence context. A few studies (Carpuat and Wu, 2007; Ittycheriah and Roukos, 2007; He et al., 2008; Hasan et al., 2008) addressed this defect by selecting the appropriate translation rules for an input span based on its context in the input sentence.",Positive
"We have used a toolkit developed at AT&T Bell Laboratories (Pereira et al., 1994) which manipulates weighted and unweighted finite-state machines (acceptors or transducers). ",Neutral
 extracts rules from nonanaphoric noun phrases and noun phrases patterns  which are then applied to test data to identify existential noun phrases ,Neutral
"We adapt the bilingual word alignment model, IBM Model 3 (Brown et al., 1993), to monolingual word alignment. ",Positive
"In this method, the subtask tree is recovered through a right-branching shift-reduce parsing process (Hall et al., 2006; Sagae and Lavie, 2006). ",Neutral
"From a cognitive angle, corpus-based models hold promise as simulations of how humans acquire and use conceptual and linguistic information from their environment (Landauer and DuMais, 1997). ",Neutral
3 Language modelling with Bloom filters Recentwork  presenteda scheme for associating static frequency information with a set of ngrams in a BF efficiently 3 Logfrequency Bloom filter The efficiency of the scheme for storing ngram statistics within a BF presented in Talbot and Osborne  relies on the Zipflike distribution of ngramfrequencies  mosteventsoccuranextremely small number of times  while a small number are very frequent ,Positive
1 Introduction The task of sentence compression  or sentence reduction  can be defined as summarizing a single sentence by removing information from it  ,Neutral
"We used the MALLET maximum entropy classifier (McCallum, 2002) as an off-the-shelf, trainable maximum entropy model. Each run involved two steps.",Neutral
"While we used simple document representation in which a document is defined as a set of nouns, there could be considered several improvements, such as using phrasal information (Lewis, 1992), clustering terms (Sparck Jones, 1973), reducing the number of features by using local dictionary (Apt4 et al., 1994), etc. ",Positive
"In this paper, we bring forward the first idea by studying the issue of how to utilize structured syntactic features for phrase reordering in a phrase-based SMT system with BTG (Bracketing Transduction Grammar) constraints (Wu, 1997).  ",Neutral
Err510,Neutral
7 Experiments To show the effectiveness of crosslanguage mention propagation information in improving mention detection system performance in Arabic  Chinese and Spanish  we use three SMT systems with very competitive performance in terms of BLEU11  ,Neutral
These methods have been used in machine translation   terminology research and translation aids   bilingual lexicography   collocation studies   wordsense disambiguation  and information retrieval in a multilingual environment  ,Neutral
"Subsequent works have demonstrated the success of Luhn’s approach (Buyukkokten et al., 2001; Lam-Adesina and Jones, 2001; Jaruskulchai et al., 2003). Edmunson (1969) proposed the use of other features such as title words, sentence locations, and bonus words to improve sentence extraction. Goldstein et al. (1999) presented an extraction technique that assigns weighted scores for both statistical and linguistic features in the sentence.",Neutral
"Preparing tagged corpora either by hand is labour-intensive and potentially error-prone, and although a semi-automatic approach can be used (Marcus et al., 1993), it is a good thing to reduce the human involvement as much as possible.  ",Neutral
"Systems were optimized on the WMT08 French- nglish development data (2000 sentences) using minimum error rate training (Och, 2003) and tested on the WMT08 test data (2000 sentences). ",Positive
Recent work emphasizes corpusbased unsupervised approach  that avoids the need for costly truthed training data We examine the effectiveness of Structural Correspondence Learning  SCL   for this task  a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis ,Positive
The MT community has developed not only an extensive literature on alignment   but also standard  proven alignment tools such as GIZA    ,Neutral
The averaged 555 perceptron has a solid theoretical fundamental and was proved to be effective across a variety of NLP tasks  ,Positive
Weautomatically constructed the paraphrasing dictionary from a large comparable newscorpus using the co-training method described in Barzilay and McKeown (2001).,Neutral
5 The statistical parser The parsing model is the one proposed in Merlo and Musillo   which extends the syntactic parser of Henderson  and  with annotations which identify semantic role labels  and has competitive performance ,Positive
Insideout alignments   such as the one in Example 13  can not be induced by any of these theories  in fact  there seems to be no useful synchronous grammar formalisms available that handle insideout alignments  with the possible exceptions of synchronous treeadjoining grammars   Bertsch and Nederhof  and generalized multitext grammars   which are all way more complex than ITG  STSG and  22   BRCG ,Negative
"The 16,000 sentence corpus was analysed by the SRI Core Language Engine (Alshawi (ed), 1992), using a lexicon extended to cover the ATIS domain (Rayner, 1994). ",Neutral
This corpusbased information typically concerns sequences of 3 tags or words  It has been argued that METEOR correlates better with human judgment due to higher weight on recall than precision  Treebanking The Penn Treebank  is annotated with information to make predicateargument structure easy to decode  including function tags and markers of empty categories that represent displaced constituents ,Positive
The corpus was aligned with GIZA    and symmetrized with the growdiagfinaland heuristic  BLEU  was devised to provide automatic evaluation of MT output Statistics in linguistics  Oxford  Basil Blackwell rawString citation citation validtrue authors author N Chinchor author authors title Evaluating message understanding systems  an analysis of the third Message Understanding Conference  MUC3 title date 1993 date journal Computational Linguistics journal volume 19 volume pages 409  449 pages marker Chinchor  1993 marker rawString Chinchor  N  et al  1993 ,Neutral
The former term P  E  is called a language model  representing the likelihood of E The latter term P  J E  is called a translation model  representing the generation probability from E into J As an implementation of P  J E   the word alignment based statistical translation  has been successfully applied to similar language pairs  such as FrenchEnglish and German English  but not to drastically dierent ones  such as JapaneseEnglish ,Positive
"The problem itself has started to get attention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daumé III and Marcu, 2006; Daumé III, 2007; Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007).  ",Neutral
"As previously observedin the literature (Mani, Gates, and Bloedorn 1999; Jing and McKeown 2000), such com-ponents include a clause in the clause conjunction, relative clauses, and some ele-ments within a clause (such as adverbs and prepositions).",Neutral
Although the authors of  stated that they would discuss the search problem in a followup arti cle  so far there have no publications devoted to the decoding issue for statistical machine translation This strategy is commonly used in MT evaluation  because of BLEUs wellknown problems with documents of small size  ,Negative
A promising approach may be to use aligned bilingual corpora  especially for augmenting existing lexicons with domainspecific terminology  It was found to produce automated scores  which strongly correlate with human judgements about translation fluency  Introduction The most widely used alignment model is IBM Model 4  We view this as a particularly promising aspect of our work  given that phrasebased systems such as Pharaoh  perform better with higher recall alignments ,Positive
One major resource for corpusbased research is the treebanks available in many research organizations   which carry skeletal syntactic structures or ` brackets ' that have been manually verified Successful approaches aimed at trying to overcome the sparse data limitation include backoff   TuringGood variants   interpolation   deleted estimation   similaritybased models   Poslanguage models  and decision tree models  As aptly pointed out in Jean   agreement measures proposed so far in the computational linguistics literature has failed to ask an important question of whether results obtained using agreement data are in any way different from random data ,Positive
 has been unable to find real examples of cases where hierarchical alignment would fail under these conditions  at least in fixedwordorder languages that are lightly inflected  such as English and Chinese  p 385  ,Positive
"Jing and McKeown (2000) have proposed a rule-basedalgorithm for sentence combination, but no results have been reported. Radev andMcKeown (1998) have already addressed the issue of information fusion in the con-text of multidocument summarization in one specific domain (i.e., terrorism)",Negative
Our study also shows that the simulatedannealing algorithm  is more effective 1552 than the perceptron algorithm  for feature weight tuning By segmenting words into morphemes  we can improve the performance of natural language systems including machine translation  and information retrieval  ,Negative
"A straightforward approach for approximating sentence fusion can be found in theuse of sentence extraction for multidocument summarization (Carbonell and Goldstein1998; Radev, Jing, and Budzikowska 2000; Marcu and Gerber 2001; Lin and Hovy2002).",Neutral
 Introduction The field of machine translation has seen many advances in recent years  most notably the shift from wordbased  to phrasebased models which use token ngrams as translation units  ,Positive
 Termbased versions of this premise have motivated much sentimentanalysis work for over a decade   ,Neutral
METEOR uses the Porter stemmer and synonymmatching via WordNet to calculate recall and precision more accurately  Recent work  has demonstrated that randomized encodings can be used to represent ngram counts for LMs with signficant spacesavings  circumventing informationtheoretic constraints on lossless data structures by allowing errors with some small probability ,Positive
"Marton and Resnik (2008) introduced features defined on constituent labels to improve the Hier-o system (Chiang, 2005). However, due to the limitation of MER training, only part of the feature space could used in the system. This problem was fixed by Chiang et al. (2008), which used an online learning method (Crammer and Singer, 2003) to handle a large set of features. ",Positive
In each experiment  performance IMutu   d Information provides an estimate of the magnitude of the ratio t  ctw    n the joint prol  ability P  verbnoun 1  reposition   and the joint probability a  suming indcpendcnce P  verbnoun  P  prcl  osition  s      ,Neutral
Conditional Markov models  CMM   have been successfully used in sequence labeling tasks incorporating rich feature sets ,Positive
"In contrast, semi-supervised domain adaptation (Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007) is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain Data. ",Neutral
For each training direction  we run GIZA     specifying 5 iterations of Model 1  4 iterations of the HMM model   and 4 iterations of Model 4 To set the weight vector w  we train twenty averaged perceptrons  on different shuffles of data drawn from sections 0221 of the Penn Treebank ,Neutral
"Citation indexes are constructs that contain pointers between cited texts and citing texts (Garfield 1979), traditionally in printed form. When done on-line (as in CiteSeer [Lawrence, Giles, and Bollacker 1999], or as in Nanba and Okumura’s [1999] work), citations are presented in context for users to browse. ",Neutral
"In general, frequently appearing terms in a document play an important role in information retrieval (Salton and McGill, 1983). Salton and Yang experimentally verified the importance of within-document term frequencies in their vector model (Salton and Yang, 1973). ",Positive
"The scheme is fully implemented within a version of the Spoken Language Translator system (Rayner et al., 1993; Agniis et al., 1994), and is normally applied to input in the form of small lattices of hypotheses produced by a speech recognizer. ",Neutral
Choueka andLusignan (1985) presented a system for the morphological tagging of large texts that isbased on the short context of the word but also depends heavily on human interaction.,Positive
"The latest WMT workshop (Callison-Burch et al., 2009) also conducted a full assessment of how well a suite of automatic metrics correlate with human judgment. ",Neutral
"Although their linguis-tic adequacy to natural language processing has been questioned in the past (Chomsky,1964), there has recently been a dramatic renewal of interest in the application of finite-state devices to several aspects of natural language processing.",Neutral
"Similar ideas were explored in (He et al., 2008). However their length features only provided insignificant improvement of 0.1 BLEU point. A crucial difference of our approach is how the length preference is modeled. We approximate the length distribution of non-terminals with a smoothed Gaussian, which is more robust and gives rise to much larger improvement consistently. ",Negative
Note that this early discarding is related to ideas behind cube pruning   which generates the top n most promising hypotheses  but in our method the decision not to generate hypotheses is guided by the quality of hypotheses on the result stack ,Neutral
1 Introduction on measures for interrater reliability   on frameworks for evaluating spoken dialogue agents  and on the use of different corpora in the development of a particular system  The CarnegieMellon Communicator  Eskenazi et al ,Neutral
"Unlike many other methods that directly utilize noun phrase (NP) coreference (Nenkova 2008; Mani et al. 1999), we propose a method that employs insertion and substitution of phrases that modify the same chunk in the lead and other sentences.",Negative
"Van den Bosch and Daelemans (1999) propose a memory-based approach, which maps directly from letters in context to categories that encode morphological boundaries, syntactic class labels and spelling changes. ",Neutral
To reduce the knowledge engineering burden on the user in constructing and porting an IE system  unsupervised learning has been utilized  eg Riloff   Yangarber et al ,Positive
"For the sake of efficiency, we use a fast transition based parser based on maximum entropy as in Zhao and Kit (2008). We still use the similar feature notations of that work ",Positive
"One of the applications is in automatic summarization in order to com- press sentences extracted for the summary (Lin, 2003; Jing and McKeown, 2000). Other applications include automatic subtitling (Vandeghinste and Tsjong Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004) and displaying text on devices with very small screens (Corston Oliver, 2001). ",Neutral
31 A Note on StateSplits Recent studies  suggest that categorysplits help in enhancing the performance of treebank grammars  and a previous study on MH  outlines specific POStags splits that improve MH parsing accuracy ,Neutral
This heterogeneity is in stark contrast to the systematic structures Liddy (1991) found to be produced by professional abstractors.  ,Positive
"The Arabic-to-English system has been trained with the data provided by the International Work- shop on Spoken Language Translation 2005 The context is that of the Basic Traveling Expression Corpus (BTEC) task (Takezawa et al., 2002). ",Neutral
"We use the machine learning toolkit LLAMA (Haffner, 2006), which encodes multiclass classication problems using binary MaxEnt classifiers to increase the speed of training and to scale the method to large data sets. ",Positive
We use the Stanford parser  with its default Chinese grammar  the GIZA    alignment package with its default settings  and the ME tool developed by  ,Neutral
"We show how this insight enables us to easily develop faster and exact variants of cube pruning for tree-to-string transducer-based MT (Galley et al., 2004; Galley et al., 2006; DeNero et al., 2009). ",Neutral
"Schone and Jurafsky (2001) list several universal characteristics of language that can serve as clues in this process, some of which we exploit. However, their use of “perfect” clusters renders some of their algorithmic suggestions problematic. ",Negative
We discriminatively trained our parser in an online fashion using a variant of the voted perceptron  In fact  we found that it doesnt do so badly at all  the bitag HMM estimated by EM achieves a mean 1to1 tagging accuracy of 40   which is approximately the same as the 413  reported by  for their sophisticated MRF model  Motivation and Prior Work While several authors have looked at the supervised adaptation case  there are less  and especially less successful  studies on semisupervised domain adaptation  ,Neutral
The program takes the output of char_align   a robust alternative to sentencebased alignment programs  and applies wordlevel constraints using a version of Brown el al s Model 2   modified and extended to deal with robustness issues6 Conclusion Traditional approaches for devising parsing models  smoothing techniques and evaluation metrics are not well suited for MH  as they presuppose 13The lack of head marking  for instance  precludes the use of lexicalized models a la  ,Negative
"Starting from the parallel training corpus, provided with direct and inverted alignments, the so called union alignment (Och and Ney, 2003) is Computed. ",Neutral
"Clark (2000) presents a framework which in principle should accommodate lexical ambiguity using mixtures, but includes no evidence that it does so. ",Negative
There are also approaches to anaphora resolution using unsupervised methods to extract useful information  such as gender and number   or contextual roleknowledge  ,Neutral
One of the first large scale hand tagging efforts is reported in   where a subset of the Brown corpus was tagged with WordNet July 2002  pp 42 Experiments To build all alignment systems  we start with 5 iterations of Model 1 followed by 4 iterations of HMM   as implemented in GIZA    ,Neutral
21 Minimum Error Rate Training The predominant approach to reconciling the mismatch between the MAP decision rule and the evaluation metric has been to train the parameters of the exponential model to correlate the MAP choice with the maximum score as indicated by the evaluation metric on a development set with known references  ,Neutral
"The work of (McRoy, 1992) pointed out that a diverse set of knowledge sources are important to achieve WSD, but no quantitative evaluation was given on the relative importance of each knowledge source. No previous work has reported any such evaluation Either.",Negative
"We report in the following tables the MUC score (Vilain et al., 1995). ",Neutral
We use binary Synchronous ContextFree Grammar  bSCFG   based on Inversion Transduction Grammar  ITG    to define the set of eligible segmentations for an aligned sentence pair ,Neutral
1 Introduction The field of machine translation has seen many advances in recent years  most notably the shift from wordbased  to phrasebased models which use token ngrams as translation units  Although ITA rates and system performance both significantly improve with coarsegrained senses   the question about what level of granularity is needed remains three models in  are susceptible to the O  n 3  method  cf ,Negative
"Our approach is reminiscent of Luhn’s approach (1959) but uses the other term weighting technique instead of the term frequency. Luhn suggested that the frequency of a word occurrence in a document, as well as its relative position determines its significance in that document. ",Positive
"This learning approach has also been applied to a numberof other tasks, including prepositional phrase attachment disambiguation (Brill andResnik 1994), bracketing text (Brill 1993a) and labeling nonterminal nodes (Brill 1993c).",Neutral
Introduction Phrasebased method  and syntaxbased method  represent the stateoftheart technologies in statistical machine translation  SMT  Introduction Phrasebased translation  and hierarchical phrasebased translation  are the state of the art in statistical machine translation  SMT  techniques ,Positive
"We also plan to consider reasonable applications for semantic tagging. One possibility would be to use semantic tagging in the framework of an intelligent on line dictionary lookup such as LocoLex [Bauer et al, 1995]. ",Positive
"We use the MaxEnt-based BTG translation system (Xiong et al., 2006) as our baseline. It is a phrase-based SMT system with BTG reordering constraint.",Positive
Similar to  eg    we use a Naive Bayes algorithm trained on word features cooccurring with the subjective and the objective classifications ,Neutral
"Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a | f ) can be determined through suitable real valued functions (called features) h  and takes the parametric Form ",Positive
32 Evaluation Metrics AER  Alignment Error Rate   is the most widely used metric of alignment quality  but requires goldstandard alignments labeled with surepossible annotations to compute  lacking such annotations  we can compute alignment fmeasure instead ,Negative
To scale LMs to larger corpora with higherorder dependencies  researchers Work completed while this author was at Google Inc have considered alternative parameterizations such as classbased models   model reduction techniques such as entropybased pruning   novel represention schemes such as suffix arrays   Golomb Coding  and distributed language models that scale more readily  ,Positive
"The insertion in the abstract of linguistic material not present in the input document has been addressed in paraphrase generation (Barzilay and Lee, 2004) and canned-based summarization (Oakes and Paice, 2001) in limited domains.  ",Positive
Among the four steps  the hypothesis alignment presents the biggest challenge to the method due to the varying word orders between outputs from different MT systems  ,Neutral
After a brief period following the introduction of generally accepted and widely used metrics BLEU Papineni et al 2002 and NIST Doddington 2002 when it seemed that this persistent problem has finally been solved the researchers active in the field of machine translation MT started to express their worries that although these metrics are simple fast and able to provide consistent results for a particular system during its development they are not sufficiently reliable for the comparison of different systems or different language pairs,Positive
One of the most effective taggers based on a pure HMM is that developed at Xerox  The success of recent highquality parsers  relies on the availability of such treebank corpora Synchronous binarization  solves this problem by simultaneously binarizing both source and targetsides of a synchronous rule  making sure of contiguous spans on both sides whenever possible ,Positive
Automatic evaluation methods such as BLEU   RED   or the weighted Ngram model proposed here may be more consistent in judging quality as compared to human evaluators  but human judgments remain the only criteria for metaevaluating the automatic methods For comparison purposes  we revisit  fullygenerative Bayesian model for unsupervised coreference resolution  discuss its potential weaknesses and consequently propose three modifications to their model ,Negative
3 Online Learning Again following   we have used the single best MIRA   which is a variant of the voted perceptron  for structured prediction ,Neutral
A key component of the parsing system is a Maximum Entropy CCG supertagger  which assigns lexical categories to words in a sentence They provide pairs of phrases that are used to construct a large set of potential translations for each input sentence  along with feature values associated with each phrase pair that are used to select the best translation from this set The most widely used method for building phrase translation tables  selects  from a word alignment of a parallel bilingual training corpus  all pairs of phrases  up to a given length  that are consistent with the alignment ,Positive
"This formulation of the task provided the basis for the noisy-channel en decision tree based algorithms presented in (Knight and Marcu, 2002), and for virtually all follow-up work on data-driven sentence compression (Le and Horiguchi, 2003; Vandeghinste and Pan, 2004; Turner and Charniak, 2005; Clarke and Lapata, 2006; Zajic et al., 2007; Clarke and Lapata, 2008)  ",Neutral
"Mann and McCallum (2008) apply GE to a linear chain, first-order CRF. In this section we provide an alternate treatment that arrives at the same objective function from the general form described in the previous section. ",Neutral
There are other types of variations for phrases  for example  insertion  deletion or substitution of words  and permutation of words such as view point and point of view are such variations    search engines   uses the Altavista web browser  while we consider and combine the frequency information acquired from three web search engines ,Neutral
"Liang et al. (2009) simultaneously developed a method for learning with and actively selecting measurements, or target expectations with associated noise. The measurement selection method proposed by Liang et al. (2009) is based on Bayesian experimental design and is similar to the expected information gain method described above. Consequently this method is likely to be intractable for real applications. ",Positive
"Linguistic information has been widely used in SMT. For example, in (Wang et al., 2007), syntactic structures were employed to reorder the source language as a pre-processing step for phrase-based Decoding. ",Neutral
"While earlier approaches for text compression werebased on symbolic reduction rules (Grefenstette 1998; Mani, Gates, and Bloedorn 1999),more recent approaches use an aligned corpus of documents and their human writtensummaries to determine which constituents can be reduced (Knight and Marcu 2002;Jing and McKeown 2000; Reizler et al. 2003).",Neutral
In addition to the widely used BLEU  and NIST  scores  we also evaluate translation quality with the recently proposed Meteor  and four editdistance style metrics  Word Error Rate  WER   Positionindependent word Error Rate  PER    CDER  which allows block reordering   and Translation Edit Rate  TER   ,Positive
"Montesi and Owen (2007) noted that professional abstractors prepend third person  ingular verbs in present tense and without subject to the author abstract, a phenomenon related – yet different – from the problem we are investigating in this paper. ",Neutral
"The concatenation of left and right context vector can therefore serve as a representation of a word's distributional behavior (Finch and Chater, 1992; Schuitze, 1993). ",Neutral
For instance  BLEU and ROUGE  are based on ngram precisions  METEOR  and STM  use wordclass or structural information  Kauchak  2006  leverages on paraphrases  and TER  uses editdistances ,Neutral
Our focus is on the sentence level  unlike  and   we employ a significantly larger set of seed words  and we explore as indicators of orientation words from syntactic classes other than adjectives  nouns  verbs  and adverbs  This additional conditioning has the effect of making the choice of generation rules sensitive to the history of the generation process  and  we argue  provides a simpler  more uniform  general  intuitive and natural probabilistic generation model obviating the need for CFGgrammar transforms in the original proposal of  ,Negative
"While the ordering of many sentenceconstituents is determined by their syntactic roles, some constituents, such as time,location and manner circumstantials, are free to move (Elhadad et al. 2001).",Neutral
"Preliminary experiments we have carried out on the Swedish version of the CLE (Gambaick and Rayner 1992) have been encouraging; using exactly the same pruning methods and EBL chunking criteria as for English, we obtain comparable speed-ups. ",Positive
"Our generation strategy is reminiscent of Robinand McKeown’s (1996) earlier work on revision for summarization, although Robin andMcKeown used a three-tiered representation of each sentence, including its semanticsand its deep and surface syntax, all of which were used as triggers for revision.",Positive
These include cube pruning   cube growing   early pruning   closing spans   coarsetofine methods   pervasive laziness   and many more ,Neutral
Perceptron Reranking As  observes  perceptron training involves a simple  online algorithm  with few iterations typically required to achieve good performance In order to estimate the conditional distributions shown in Table   we use the general technique of choosing the MaxEnt distribution that properly estimates the average of each feature over the training data  ,Positive
"The word formation building blocks define the so called inflectional classes, which represent sequential letter strings associated with word classes as well as with individual words, also known as isuffixes in Porter-like stemmers (Porter,1980). ",Neutral
"Experience gained from the development of the Penn Treebank (Marcus et al., 1994) has shown that automatic annotation is useful only if it is absolutely correct, while wrong analyses are often difficult to detect and their correction can be time-consuming. ",Positive
In this paper we generalize the proposals of Pennacchiotti et al. (2008) and Fürstenau and Lapata (2009) in a unified framework. We create training data for semantic role labeling of unknown predicates by projection of annotations from labeled onto unlabeled data. ,Positive
"Recently, several solutions to the problem of tagging unknown words have beenpresented (Charniak et al. 1993; Meteer, Schwartz, and Weischedel 1991).",Positive
It is a variant of the batch-based Bloomier filter LM of Talbot and Brants (2008) which we refer to as the TB-LM henceforth.,Neutral
"We use the labeled crossing bracket metric (typically used in the syntactic parsing literature (Harrison et al., 1991)), which computes recall, precision and crossing brackets for the constituents (subtrees) in a hypothesized parse tree given the reference parse Tree. ",Neutral
From the extracted ngrams  those with a flequc ` ncy of 3 or more were kept  other approaches get rid of ngrams of such low frequencies   ,Neutral
This ITG constraint is characterized by the two forbidden structures shown in Figure 1  ,Neutral
An extrinsic evaluation additionally showed that the end result provides considerable added value when compared to sentence extracts (Teufel 2001),Neutral
22 Maximum Entropy Models Maximum entropy  ME  models   also known as 928 loglinear and exponential learning models  provide a general purpose machine learning technique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging  named entity recognition etc Maximum entropy models can integrate features from many heterogeneous information sources for classification Introduction Many stateoftheart machine translation  MT  systems over the past few years  rely on several models to evaluate the goodness of a given candidate translation in the target language ,Positive
This approach is similar to that of seed words  eg    or hook words  eg    in previous work ,Neutral
"Based on RST, [Marcu, 2000] established a Rhetorical Parser. The parser exploits cue phrases in an algorithm that discovers discourse relationships between phrases in a text. ",Positive
"Previous results had shown a rather satisfying performance for hybrid systems such as the Statistical Phrase-based Post-Editing (SPE) (Simard et al., 2007) combination in comparison with purely phrase-based statistical models, reaching similar BLEU scores and often receiving better human judgement (German to English at WMT2007) against the BLEU metric. ",Positive
Sentiment classification at the sentencelevel has also been studied  ,Neutral
To counteract this  we introduce two brevity penalty measures  BP  inspired by BLEU  which we incorporate into the loss function  using a product  loss  1PrecBP  BP1  exp  1max  1  rc    6  BP2  exp  1max  cr  rc   where r is the reference length and c is the candidate length ,Neutral
"Related to this is the work by Teufel and Moens (2002) on rhetorical classification for content selection. In cut-and-paste summarization (Jing and mcKeown, 2000), sentence combination operations were implemented manually following the study of a set of professionally written abstracts; however the particular “pasting” operation presented here was not implemented. ",Negative
Techniques for weakening the independence assumptions made by the IBM models 1 and 2 have been proposed in recent work  C3BTC5 and CCCDCA were used in  and   respectively ,Neutral
"Co-occurrence statistics extracted from corpora lead to good performance on a wide range of tasks that involve the identification of the semantic relation between two words or concepts (Sahlgren, 2006; Turney, 2006). However, the difficulty of such tasks and the fact that they are apparently unrelated has led to the development of largely ad-hoc solutions, tuned to specific challenges. ",Negative
Using GIZA   model 4 alignments and Pharaoh   we achieved a BLEU score of 03035 ,Neutral
They have been successfully applied in several tasks  such as information retrieval  and harvesting thesauri  showed that the results for FrenchEnglish were competitive to stateoftheart alignment systems Another kind of popular approaches to dealing with query translation based on corpusbased techniques uses a parallel corpus containing aligned sentences whose translation pairs are corresponding to each other  The corpusbased statistical parsing community has many fast and accurate automated parsing systems  including systems produced by   Charniak  997  and Ratnaparkhi  997  ,Positive
"Some s.vs terns perform a previous recasting of the attributes in order to have only binary-valued attributes and to deal with binary trees- (Magerman, 1996).",Neutral
"Predicates such as “to present” and “to include” have the tendency of appearing towards the very beginning or the very end of the abstract been therefore predicted by position-based features (Edmundson, 1969; Lin and Hovy, 1997). ",Neutral
Feature weights of both systems are tuned on the same data set3 For Pharaoh  we use the standard minimum errorrate training   and for our system  since there are only two independent features  as we always fix  1   we use a simple gridbased lineoptimization along the languagemodel weight axis ,Neutral
Following   Iusevariational Bayes EM  during the Mstep for the transition distribution  l 1 j i  f  E  ni  j   i  f  E  n i   C i   3  f  v   exp   v    4  60  v   braceleftBigg g  v 1 2  ifv  7  v  1  1v ow ,Neutral
In this context such a word is often an acronym not defined locally and indicates the presence of a binding term (Fukuda et al. 1998).,Neutral
"(Schiller, 1996) describes the general architecture of the tool for noun phrase mark-up based on finitestate techniques and statistical part-of-peech disambiguation for seven European languages. ",Neutral
However  evaluations on the widely used WSJ corpus of the Penn Treebank  show that the accuracy of these parsers still lags behind the stateoftheart ,Positive
Our system outperforms competing approaches  including the standard machine translation alignment models  and the stateoftheart Cut and Paste summary alignment technique  For example  we would like to know that if a  JJ  JJ  7We also tried using word clusters  instead of POS but found that POS was more helpful ,Negative
"Doing so enables us to use possibly inaccurate approaches to guess the segmentation of compound words, allowing the decoder to decide which to use during translation. This is a further development of our general source-lattice approach to decoding (Dyer et al., 2008).",Neutral
"A small error ratehas been achieved by such systems when a restricted, application-dependent POS setis used; e.g., an error rate of 2-6 percent has been reported by Marcus, Santorini, andMarcinkiewicz (1993) using the Penn Treebank corpus.",Positive
We report case sensitive Bleu  scoreBleuCforallexperiments ,Neutral
"We describe a POS tag-ger based on the work described in (Padr6, 1996),that is able to use bi/trigram information, auto-matically learned context constraints and linguisti-cally motivated manually written constraints.",Positive
Some of these have been previously employed for various tasks by Gabrilovich and Markovitch    Overell and Ruger     and Suchanek et al ,Neutral
"When a dialogue act is preceded by tacit actions in an interpretation, the speaker of the utterance implicates that the earlier tacit actions have taken place (DeVault, 2008).",Neutral
A number of partofspeech taggers are readily available and widely used  all trained and retrainable on text corpora  To avoid this problem  we adopt crossvalidation training as used in  The default training set of Penn Treebank  was used for the parser because the domain and style of those texts actually matches fairly well with the domain and style of the texts on which a reading level predictor for second language learners might be used ,Positive
In a first step the tree is completely expanded and afterwards it is pruned following a minimal cost-complexity criterion (Breiman et al.. 1984). ,Positive
"In order to estimate the conditional distributions shown in Table 1, we use the general technique of choosing the MaxEnt distribution that properly es- timates the average of each feature over the train- ing data (Berger et al., 1996).  ",Neutral
When we have a junction tree for each document  we can efficiently perform belief propagation in order to compute argmax in Equation  1   or the marginal probabilities of cliques and labels  necessary for the parameter estimation of machine learning classifiers  including perceptrons   and maximum entropy models  ,Neutral
"To the best of our knowledge, and with the exception of Saggion and Lapalme (2002) indicative generation approach which included operations to add extra linguistic material to generate an indicative abstract, the work presented here is the first to investigate this relevant operation in the field of text abstracting and to propose a robust computational method for its simulation. In this paper we are interest ",Positive
312 Kappa Kappa  is an evaluation measure which is increasingly used in NLP annotation work  ,Neutral
"Third, this probabilistic framework allows us to search the space of all possible entities, while Soon et al. (2001), Ng and Cardie (2002) take the “best” local hypothesis. ",Neutral
Although the Kappa coefficient has a number of advantages over percentage agreement  eg  it takes into account the expected chance interrater agreement  see  for details   we also report percentage agreement as it allows us to compare straightforwardly the human performance and the automatic methods described below  whose performance will also be reported in terms of percentage agreement ,Positive
"This is achieved by introducing an explicit statistical model of unknown words, and by using an N best word segmentation algorithm (Nagata, 1994)as an approximation of the generalized Forward Backward algorithm.",Positive
"To quantify the performance of the learned model in comparison to our baseline, we adapt the mean reciprocal rank statistic commonly used for evaluation in information retrieval (Vorhees, 1999). We expect that a system will use the probabilities calculated by a disambiguation model to decide which interpretations to pursue and how to follow them up through the most efficient interaction.",Positive
36 Parameter Estimation To estimate parameters k   k K   lm  and um  we adopt the approach of minimum error rate training  MERT  that is popular in SMT  ,Positive
Nowadays most of the state-of-the-art SMT systems are based on linear models as proposed in Och and Ney (2002). ,Positive
"Syntactic analysis is carried out in another reductionistic parsing framework known as Finite State Intersection Grammar (Koskenniemi 1990; Koskenniemi, Tapanainen and Voutilainen 1992; Tapanainen 1992; Voutilainen and Tapanainen 1993; Voutilainen 1994). ",Neutral
Moreover  rather than predicting an intrinsic metric such as the PARSEVAL Fscore  the metric that the predictor learns to predict can be chosen to better fit the final metric on which an endtoend system is measured  in the style of  ,Neutral
"Previous studies on text-to-text abstracting (Banko et al., 2000; Knight and Marcu, 2000) have studied problems such as sentence compression and sentence combination but not the “pasting” procedure presented here. ",Negative
Note that it is straightforward to calculate these expected counts using a variant of the insideoutside algorithm  applied to the  dependencyparsing data structures  for projective dependency structures  or the matrixtree theorem  for nonprojective dependency structures ,Neutral
2 Previous Approaches  method of estimating phrasetranslation probabilities is very simple ,Neutral
By increasing the size of the basic unit of translation  phrasebased machine translation does away with many of the problems associated with the original wordbased formulation of statistical machine translation   in particular  The Brown et al  examine the FS of the weighted loglikelihood ratio  WLLR  on the movie review dataset and achieves an accuracy of 871   which is higher than the result reported by  with the same dataset ,Negative
The combination is significantly better than  at a very high level  but more importantly  Shens results  currently representing the replicable stateoftheart in POS tagging  have been significantly surpassed also by the semisupervised Morce  at the 99  confidence level  ,Negative
Usual tagging algorithms are either n - g r a m oriented -such as Viterbi algorithm (Viterbi. 1967)- or a d - hoc for every case when they must deal with more complex information.,Neutral
2 21 Word Alignment Adaptation Bidirectional Word Alignment In statistical translation models   only onetoone and moretoone word alignment links can be found The method was intended as a replacement for sentencebased methods  eg     which are very sensitive to noise ,Negative
"In DeMarcken (1990) and Weischedel et al. (1993), k-best tags are assignedwithin a stochastic tagger by returning all tags within some threshold of probabilityof being correct for a particular word.",Neutral
"Hence a tension is visible in the many recent research efforts aiming to decode with “non-local” features (Chiang, 2007; Huang and Chiang, 2007).",Neutral
Finally  we use as a feature the mappings produced in  of WordNet senses to Oxford English Dictionary senses 3 Semantic Representation 31 The Need for Dependencies Perhaps the most common representation of text for assessing content is BagOfWords or BagofNGrams  First  we adopt an ONTOLOGICALLY PROMISCUOUS representation  that includes a wide variety of types of entities ,Neutral
In this work we will use structured linear classifiers  This is the best automatically learned partofspeech tagging result known to us  representing an error reduction of 44  on the model presented in   using the same data splits  and a larger error reduction of 121  from the more similar best previous loglinear model in Toutanova and Manning  ,Neutral
"The usual solutions to this problem are: l) Prune the tree. either during the construction process (Quinlan. 1993) or afterwards (Mingers, 1989); 2) Smooth the conditional probability distributions using fresh corpus a (Magerman, 1996). ",Positive
"There are several ways to calculate P(c[d). Three representatives are (Robertson and Sparck Jones, 1976), (Kwok, 1990), and (Fuhr, 1989). ",Neutral
Druck et al. (2008) provide a survey of other related methods for learning with labeled input features. ,Neutral
"For example, CLAWS (Garside ct al., 1987) normalises the lexical probabilities by the total frequency of the word rather than of the tag. Consulting the Baum- Welch re-estimation formulae suggests that the approach described is more appropriate, and this is confirmed by slightly greater tagging accuracy. ",Positive
The most widely used singlewordbased statistical alignment models  SAMs  have been proposed in  Decision lists have already been successfully applied to lexical ambiguity resolution by  where they perfromed well Some of the more popular and more accurate of these approaches to datadriven parsing  have been based on generative models that are closely related to probabilistic contextfree grammars Indeed  recent work has shown that benefits can be made by first separating facts from opinions in a document  eg  Yu and Hatzivassiloglou   and classifying the polarity based solely on the subjective portions of the document  eg    ,Positive
Language modeling   nounclustering   constructing syntactic rules for SMT   and finding analogies  are examples of some of the problems where we need to compute relative frequencies ,Neutral
"But the situation is much different in treeto-string transducer-based MT (Galley et al., 2004; Galley et al., 2006; DeNero et al., 2009). ",Neutral
"In addition to paradigmatic lexical phenomena such as synonymy, hypernymy, and meronymy, diathesis alternation (Levin and Rappaport Hovav, 1996), deep case (Fillmore, 1968), and the interaction of predicational structure and events (Tenny and Pustejovsky, 2000) are being investigated. ",Neutral
We use the discriminative perceptron learning algorithm  to train the values of vectorw ,Neutral
"The Distributional Hypothesis, supported by theoretical linguists such as Harris (1954), states that words that occur in the same contexts tend to have similar meanings. This suggests that one can learn the similarity between two words automatically by comparing their relative contexts in a large unlabeled corpus, which was confirmed by different researchers (e.g. (Lin, 1998; McDonald and Ramscar, 2001; Grefenstette, 1994)). ",Positive
Recently  an elegant approach to inference in discourse interpretation has been developed at a number of sites   all based on tim notion of abduction  and we have begun to explore its potential application to machine translation ,Positive
"Much progress in the area of semantic role labeling is due to the creation of resources like FrameNet (Fillmore et al., 2003), which document the surface realization of semantic roles in real world corpora. ",Positive
Our experiments on the Canadian Hansards show that our unsupervised technique is significantly more effective than picking seeds by hand   which in turn is known to rival supervised methods 2 Related Work One of the major problems with the IBM models  and the HMM models  is that they are restricted to the alignment of each sourcelanguage word to at most one targetlanguage word This implies that the complexity of structure divergence between two languages is higher than suggested in literature   report better perplexity results on the Verbmobil Corpus with their HMMbased alignment model in comparison to Model 2 of  ,Negative
This iterative optimiser  derived from a word disambiguation technique   finds the nearest local maximum in the lexical cooccurrence network from each concept seed This model is related to the averaged perceptron algorithm of  ,Neutral
The classification is performed with a statistical approach  built around the maximum entropy  MaxEnt  principle   that has the advantage of combining arbitrary types of information in making a classification decision ,Positive
31 Agreement for Emotion Classes The kappa coefficient of agreement is a statistic adopted by the Computational Linguistics community as a standard measure for this purpose  ITGs translate into simple  22   BRCGs in the following way  see  for a definition of ITGs ,Neutral
"Ideas about this type ofanalogical reasoning can be found also in non-mainstream linguistics and pyscholinguistics(Skousen, 1989; Derwing ~ Skousen, 1989; Chandler, 1992; Scha, 1992).",Neutral
"Similar results have beenreported by Maltese and Mancini (1991) for the Italian language. Weischedel et al.(1993) have used four categories of word morphology, such as inflectional endings,derivational endings, hyphenation, and capitalization.",Neutral
"We compare this approach to the semi-supervised method in Koo et al. (2008) who employ clusters of related words constructed by the Brown clustering algorithm (Brown et al., 1992) for syntactic processing of texts. ",Neutral
Following  we can avoid unnecessary false positives by not querying for the longer ngram in such cases It is explored extensively in  We compare semisupervised LEAF with a previous state of the art semisupervised system  ,Positive
"Besides the the case-sensitive BLEU-4 (Papineni et al., 2002) used in the two experiments, we design another evaluation metrics Reordering Accuracy (RAcc) for forced decoding evaluation. ",Neutral
"One is gold-standard syntactic input, and other two are based on automatically parsing results of two parsers, the state-of-the-art syntactic parser described in (Johansson and Nugues, 2008) 7 (it is referred to Johansson) and an integrated parser described as the following (referred to MSTME )",Positive
The usual recall and precision metrics  eg  how many of the interesting bits of information were detected  and how many of the found bits were actually correct  require either a test corpus previously annotated with the required information  or manual evaluation  ,Neutral
Arguably the most widely used is the mutual information  An especially wellfounded framework is maximum entropy  2 Related Work Supervised machine learning methods including Support Vector Machines  SVM  are often used in sentiment analysis and shown to be very promising  ,Positive
6 Discourse Context  pointed out that the sense of a target word is highly consistent within any given document  one sense per discourse  ,Neutral
"As in the experiments of Clark and Wilkes- Gibbs (1986) and Brennan and Clark (1996), one of the players, who plays the role of director, instructs the other player, who plays the role of matcher, which object is to be added next to the Scene. ",Neutral
In order to overcome this problem  we look to the bootstrapping method outlined in  Much later work  relies on the use of extremely large corpora which allow very precise  but sparse features ,Positive
"Following Padó and Lapata (2007), we use Pearson’s r to evaluate how the distances (cosines) in the CxLC space between the nouns in each pair correlate with the ratings. ",Positive
Probably the most widely used feature weighting function is pointwise Mutual Information MI Church and Patrick 990 Hindle 990 Luk 995 Lin 998 Gauch Wang and Rachakonda 999 Dagan 2000 Baroni and Vegnaduzzo 2004 Chklovski and Pantel 2004 Pantel and Ravichandran 2004 Pantel Ravichandran and Hovy 2004 Weeds Weir and McCarthy 2004 dened by weight MI wflog 2 Pwf PwPf  We calculate the MI weights by the following statistics in the space of cooccurrence instances S weight MI wflog 2 countwf nrels countw countf 2 where countwf is the frequency of the cooccurrence pair wf  in S countwand countf are the independent frequencies of w and f in Sandnrels is the size of SHigh MI weights are assumed to correspond to strong wordfeature associations,Positive
"We note that these results are competitive with those reported in the literature (e.g. (Poesio and Mikheev, 1998; Serafin and Eugenio, 2004)), although the dialog corpus and the label sets are different. ",Positive
"Note especially, that Czech nouns are divided into four classes according to gender (Sgall, 1967) and into seven classes according to ease. ",Neutral
 Introduction A hypergraph  as demonstrated by   is a compact datastructure that can encode an exponential number of hypotheses generated by a regular phrasebased machine translation  MT  system  eg  Koehn et al ,Positive
Although previous work  has tackled the bootstrapping approach from both the theoretical and practical point of view  many key problems still remain unresolved  such as the selection of initial seed set Unlike   who found optimal performance when was approximately 104  we observed monotonic increases in performance as dropped ,Negative
"In general, dialogue coherence is an important source of evidence for all aspects of language, for both human language learning (Saxton et al., 2005) as well as machine models. For example, Bohus et al. (2008) use users’ confirmations of their spoken requests in a multi-modal interface to tune the system’s ASR rankings for recognizing subsequent utterances. ",Neutral
"A windowing approach (Sejnowski & Rosenberg, 1987) was used to represent the tagging task as a classification problem. ",Positive
 Introduction During the last few years  SMT systems have evolved from the original wordbased approach  to phrasebased translation systems  ,Positive
The techniques examined are Structural Correspondence Learning  SCL   and Selftraining  ,Neutral
"Several models have been used for pair-wise alignment, starting with TER and proceeding with more sophisticated techniques such as HMM models, ITG, and IHMM (Rosti et. al 2007a, Matusov et al 2008, Krakos et al. 2008, He et al. 2008). A major problem with such methods is that each hypothesis is aligned to the backbone independently, leading to sub- optimal behavior.",Negative
Typically  a small set of seed polar phrases are prepared  and new polar phrases are detected based on the strength of cooccurrence with the seeds  ,Neutral
Historybased models for predicting the next parser action  3 ,Neutral
"Hypothesesfor unknown words, both stochastic (Dermatas and Kokkinakis 1993, 1994; Malteseand Mancini 1991; Weischedel et al. 1993), and connectionist (Eineborg and Gamback1993; Elenius 1990) have been applied to unlimited vocabulary taggers.",Positive
Training Set  Labeled English Reviews   There are many labeled English corpora available on the Web and we used the corpus constructed for multidomain sentiment classification  9  because the corpus was largescale and it was within similar domains as the test set ,Neutral
"Binarizing the grammars (Zhang et al., 2006) further increases the size of these sets, due to the introduction of virtual nonterminals. ",Neutral
"Distributional information has uses beyond part of speech induction. For example, it is possible to augment a fixed syntactic or semantic taxonomy with such information to good effect (Hearst and Schütze, 1993). ",Neutral
" uilding on a recent proposal in this direction by Turney (2008), we propose a generic method of this sort, and we test it on a set of unrelated tasks, reporting good performance across the board with very little task-specific tweaking. ",Positive
ER adds a term to the objective function that encourages confident predictions on unlabeled data. Training of linear-chain CRFs with ER is described by Jiao et al. (2006). ,Neutral
"However, ""Vbutilainen (1995) has shown that EngCG combined with a syntactic parser produces morphologically unambiguous output with an accuracy of 99.3%, a figure clearly better than that of the statistical tagger in the experiments below (however. the test data was not the same). ",Neutral
This is similartothegraphconstructionmethodof  and Rao et al We distinguish two main approaches to domain adaptation that have been addressed in the literature   supervised and semisupervised ,Neutral
"Sub-events (Daniel et al., 2003) and sub-topics (Saggion and Lapalme, 2002) also contribute to the framework used for comparing documents in multidocument summarization. ",Positive
"Automatic analysis of natural language is still a very hard task to perform for a computer. Although some successful applications have been developed (see for instance (Chinchor, 1998)), implementing an automatic text analysis system is still a labour and time intensive task. ",Positive
In the supervised setting  a recent paper by  shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks To speed our computations  we use the cube pruning method of  with a fixed beam size informationtheoretic similarity measure is commonly used in lexicon acquisition tasks and has demonstrated good performance in unsupervised WSD  ,Positive
"Phase 1 (relevance), a condensation process, identifies predications on a given topic (in this study, disorders) and is controlled by a semantic schema (Jacquelinet et al., 2003) for that topic. ",Neutral
"Nagata (1996) recently proposed a generalized forward-backward algorithm that is a character synchronous method for unsegmented languages. He applied this algorithm to bigram model training from untagged Japanese text for new word extraction. However, he did not apply this algorithm to the estimation of HMM parameters.  ",Negative
The previous studies  with the exception of   used smaller gazetteers than ours ,Neutral
"Other than confusion-network-based algorithms, work most closely related to ours is the method of MT system combination proposed in (Jayaraman and Lavie 2005), which we will refer to as J&L.",Positive
This method was preferred against other related methods  like the one introduced in   since it embeds all the available semantic information existing in WordNet  even edges that cross POS  thus offering a richer semantic representation Both the global models  use fairly small training sets  and there is no evidence that their techniques will scale to larger data sets ,Negative
51 The baseline System used for comparison was Pharaoh   which uses a beam search algorithm for decoding ,Neutral
"(Brill, 1995) presents a rule-based part-of-speech tagger for unsupervised training corpus. Some of the rules of his system and the fact that he uses a minimal training corpus suggests some similarities with our system, but the main aim of the work is to investigate methods to combine supervised and unsupervised training in order to come up with a highly performing tagger. ",Positive
Automatically creating or extending taxonomies for specific domains is then a very interesting area of research  SVM has been shown to be useful for text classification tasks   and has previously given good performance in sentiment classification experiments  2 Prior Work Statistical machine translation  as pioneered by IBM   is grounded in the noisy channel model The most notable of these include the trigram HMM tagger   maximum entropy tagger   transformationbased tagger   and cyclic dependency networks  Averaging has been shown to reduce overfitting  as well as reliance on the order of the examples during training ,Positive
 discuss the influence of bias towards highor lowfrequency items for different tasks  correlation with WordNetderived neighbor sets and pseudoword disambiguation   and it would not be surprising if the different highfrequency bias were leading to different results ,Neutral
6 Related Work The popular IBM models for statistical machine translation are described in  ,Positive
Such methods have also been a key driver of progress in statistical machine translation  which depends heavily on unsupervised word alignments  ,Positive
All other settings were left at their default values as described by Chiang (2007) and Koehn et al. (2007).,Positive
Equation  10  is of interest because the ratio p  C v  r   p  C r  can be interpreted as a measure of association between the verb v and class C This ratio is similar to pointwise mutual information  and also forms part of Resniks association score  which will be introduced in Section 6 ,Neutral
"In our work, we decide to use TLTF (Term Length Term Frequency) term weighting technique (Banko et al., 1999) for scoring words in the document instead of TFIDF. ",Positive
Turney also reported good result without domain customization  In our experiments  we follow Lowe and McDonald  in using the wellknown loglikelihood ratio G 2  ,Positive
Given a set of evidences E over all the relevant word pairs  in   the probabilistic taxonomy learning task is defined as the problem of finding the taxonomy hatwideT that maximizes the 67 probability of having the evidences E  ie  hatwideT  arg max T P  E T  In   this maximization problem is solved with a local search ,Neutral
Several studies have shown that largemargin methods can be adapted to the special complexities of the task  However  the capacity of these algorithms to improve over stateoftheart baselines is currently limited by their lack of robust dimensionality reduction ,Negative
"This fact, along with the observation that machine translation quality improves as the amount of monolingual training material increases, has lead to the introduction of randomised techniques for representing large LMs in small space (Talbot and Osborne, 2007; Talbot and Brants, 2008).",Neutral
"If entropy H(p) is large (e.g., small γ), the Bayes risk Thus, Smith and Eisner (2006) try to avoid local minima by starting with large H(p) and decreasing it gradually during optimization. This is called deterministic annealing (Rose, 1998). ",Positive
"Part-of-speech tagging is an activearea of research; a great deal of work has been done in this area over the past few years(e.g., Jelinek 1985; Church 1988; Derose 1988; Hindle 1989; DeMarcken 1990; Merialdo1994; Brill 1992; Black et al. 1992; Cutting et al. 1992; Kupiec 1992; Charniak et al. 1993;Weischedel et al. 1993; Schutze and Singer 1994).",Positive
Due to limited variations in the NBest list  the nature of ranking  and more importantly  the nondifferentiable objective functions used for MT  such as BLEU    one often found only local optimal solutions to  with no clue to walk out of the riddles We also compare our performance against  and  and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF Although several methods have already been proposed to incorporate nonlocal features   these present a problem that the types of nonlocal features are somewhat constrained ,Negative
In Yarowsky s experiment   an average of 3936 examples were used to disambiguate between two senses ,Neutral
Although such approaches have been employed effectively   there appears to remain considerable room for improvement In terms of alignment  this wordnumber difference means that multiword connections must be considered  a task which 334 Sue J Ker and Jason S Chang Word Alignment is beyond the reach of methods proposed in recent alignment works based on  Model 1 and 2 ,Negative
This provides a compelling advantage over previous dependency language models for MT   whichusea5gramLMonlyduringreranking Experimental results indicate that our model outperforms  coreference model by a large margin on the ACE data sets and compares favorably to a modified version of their model ,Negative
In computational linguistics  our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs   inter alia  ,Neutral
Knight and Marcu (2000) treat reduction as a translation process using a noisy-channel model (Brown et al. 1993).,Neutral
"In this paper, we advocate using generalized expectation (GE) criteria (Mann and McCallum, 2008) for learning with labeled features. We provide an alternate treatment of the GE objective function used by Mann and McCallum (2008) and a novel speedup to the gradient computation. ",Positive
 Introduction Cooccurrence statistics extracted from corpora lead to good performance on a wide range of tasks that involve the identification of the semantic relation between two words or concepts  ,Positive
"Furthermore, Turkish allows very productive derivational processes and the information about the derivational structure of a word form is usually crucial for disambiguation (Oflazer and Tiir, 1996). ",Neutral
1510 5 Related Work In recent years  many research has been done on extracting relations from free text  eg     however  almost all of them require some languagedependent parsers or taggers for English  which restrict the language of their extractions to English only  or languages that have these parsers  ,Neutral
The first serious linguistic competitor to data-driven statistical taggers is the English Constraint Grammar parser. EngCG (cf. Voutilainen et al. 1992; Karlsson et al. (eds.) 1995). The tagger consists of the following sequentially applied modules,Positive
"Thanks to the nice property of kernel-basedmachine learning method that can implicitly ex-plore (structured) features in a high dimensional feature space (Vapnik, 1995), in this paper we propose using convolution tree kernel (Haussler, 1999; Collins and Duffy, 2001) to explore the structured syntactic knowledge for phrase reordering and further combine the tree kernel with other diverse linear features into a composite kernel to strengthen the model’s predictive ability.",Positive
In comparison with shallow semantic analysis tasks such as wordsense disambiguation Ide and Jeaneronis 1998 and semantic role labeling Gildea and Jurafsky 2002 Carreras and M`arquez 2005 which only partially tackle this problem by identifying the meanings of target words or finding semantic roles of predicates semantic parsing Kate et al  2005 Ge and Mooney 2005 Zettlemoyer and Collins 2005 pursues a more ambitious goal  mapping natural language sentences to complete formal meaning representations MRs where the meaning of each part of a sentence is analyzed including noun phrases verb phrases negation quantifiers and so on,Neutral
In the SMT research community  the second step has been well studied and many methods have been proposed to speed up the decoding process  such as nodebased or spanbased beam search with different pruning strategies  and cube pruning  ,Positive
"One such problem is sense disambiguation.In the context of machine translation, Dagan and Itai (Dagan, Itai, and Schwall 1991;Dagan and Itai 1994) used corpora in the target language to resolve ambiguities inthe source language. Yarowsky (1992) proposed a method for sense disambiguationusing wide contexts.",Positive
The best example of such an approach is   who proposes a method that automatically identifies collocations that are indicative of the sense of a word  and uses those to iteratively label more examples Introduction There has been a great deal of progress in statistical parsing in the past decade  ,Positive
"Given the problems created by estimating probabilities on a corpus of restricted size, we present in Section 4 a solution for coping with these difficulties. We suggest a new paradigm called genotype , derived from the concept of ambiguity class (Kupiec, 1992), which gives a more efficient representation of the data in order to achieve more accuracy in part-of-speech disambiguation. ",Positive
"A constraint-based system is also presented in (Karlsson, 1990; Karlsson et al., 1995). Related work using finite-state machines has been done using local grammars (Roche, 1992; Silberztein,1993; Laporte, 1994). ",Neutral
To derive the joint counts c   s   t  from which p   s  t  and p   t  s  are estimated  we use the phrase induction algorithm described in   with symmetrized word alignments generated using IBM model 2  ,Neutral
The results are comparable to other results reported using the InsideOutside method   see Table 7 ,Neutral
The MERT module is a highly modular  efficient and customizable implementation of the algorithm described in  Comparison with SSCRFMER When we consider semisupervised SOL methods  SSCRFMER  is the most competitive with HySOL  since both methods are defined based on CRFs WSD is one of the fundamental problems in natural language processing and is important for applications such as machine translation  MT    information retrieval  IR   etc WSD is typically viewed as a classification problem where each ambiguous word is assigned a sense label  from a predefined sense inventory  during the disambiguation process ,Positive
Metrics in the Rouge family allow for skip ngrams   Kauchak and Barzilay  take paraphrasing into account  metrics such as METEOR  and GTM  calculate both recall and precision  METEOR is also similar to SIA  in that word class information is used ,Neutral
In order to identify binding terminology in text we rely on the approach discussed in (Rindfiesch et al. 1999).,Positive
"The program, called the decomposition program, matches phrases in a human- written summary sentence to phrases in the original document (Jing and McKeown, 1999). ",Neutral
The results are quite promising  our extraction method discovered 89  of the WordNet cousins  and the sense partitions in our lexicon yielded better values  than arbitrary sense groupings on the agreement data ,Neutral
The superiority of proportional assignment over the other strategies has already been reported by Lewis (1992). ,Neutral
However  many of these models are not applicable to parallel treebanks because they assume translation units where either the source text  the target text or both are represented as word sequences without any syntactic structure  2 Statistical Word Alignment Statistical translation models  only allow word to word and multiword to word alignments ,Negative
"We give a brief sketch here to highlight the content of COREF’s representations, the sources of information that COREF uses to construct them, and the demands they place on disambiguation. See DeVault (2008) for full details. ",Positive
The second feature we used is a bi-gram voting feature proposed by Zhao and He (2009),Neutral
2 Motivation and Prior Work While several authors have looked at the supervised adaptation case  there are less  and especially less successful  studies on semisupervised domain adaptation  ,Negative
We use the default configuration of the measure in WordNet   Similarity012 package   and  with a single exception  the measure performed below Gic  see BP in table 1 ,Neutral
Therefore the probability of alignment aj for position j should have a dependence on the previous alignment position O j_l  P    j    j1  A similar approach has been chosen by  and  ,Neutral
"This metric for measuring distance is adopted from (Cost and Salzberg, 1993), which in turn is adapted from the value difference metric of the earlier work of (Stanfill and Waltz, 1986). ",Positive
Sentencelevel approximations to B exist   but we found it most effective to perform B computations in the context of a setOof previouslytranslated sentences  following Watanabe et al In such a process  original phrasebased decoding  does not take advantage of any linguistic analysis  which  however  is broadly used in rulebased approaches ,Negative
The eval-uations can be made in intrinsic or extrinsic fashions as defined by Sparck Jones andGalliers (1995).,Neutral
"First, a set of pre-processing components including a chunker and a named entity recognizer is applied to the text in order to identify the noun phrases, which are further taken as REs to be used for instance generation. Instances are created following Soon et al. (2001).",Positive
"The annotation experiment described here (and in Teufel, Carletta, and Moens [1999] in more detail) tests the rhetorical annotation scheme presented in section ",Neutral
31 A simple solution  suggests that in order to have an ITG take advantage of a known partial structure  one can simply stop the parser from using any spans that would violate the structure ,Neutral
"The proposed approach is also amenable to an efficient implementation by finite state transducers (Kaplan and Kay, 1994). ",Neutral
2 Architecture of the system The goal of statistical machine translation  SMT  is to produce a target sentence e from a source sentence f It is today common practice to use phrases as translation units  and a log linear framework in order to introduce several models explaining the translation process  e    argmaxp  e f   argmaxe LCB exp  summationdisplay i ihi  e  f   RCB  1  The feature functions hi are the system models and the i weights are typically optimized to maximize a scoring function on a development set  ,Neutral
"While the use of context information has been explored in MT, e.g. (Carpuat and Wu, 2007) and (He et al., 2008), the specific technique we used by means of a context language model is rather different. ",Neutral
 Introduction Stateoftheart Statistical Machine Translation  SMT  systems usually adopt a twopass search strategy  as shown in Figure  ,Positive
"The concept of lexical chains was first introduced by Morris and Hirst. Basically, lexical chains exploit the cohesion among an arbitrary number of related words (Morris and Hirst 1991). ",Neutral
The first task we evaluated our algorithm on is the SAT analogy questions task introduced by Turney et al. (2003).,Positive
3 Evaluation We trained our model parameters on a subset of the provided dev2006 development set  optimizing for caseinsensitive IBMstyle BLEU  with several iterations of minimum error rate training on nbest lists ,Neutral
The overall POS tag distribution learned by EM is relatively uniform  as noted by   and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed ,Neutral
Properly calculated BLEU scores have been shown to correlate reliably with human judgments  The technique is based on word class models  pioneered by   which hierarchically 5 CoNLL03 CoNLL03 MUC7 MUC7 Web Component Test data Dev data Dev Test pages   Baseline 8365 8925 7472 728 74 2      Gazetteer Match 8722 96 8583 8043 7446 3      Word Class Model 8682 9085 8025 7988 7226 4  All External Knowledge 8855 9249 8450 8323 7444 Table 4  Utility of external knowledge ,Positive
The IOB format  introduced in   consistently   ame out as the best format ,Positive
Our MT experiments use a reimplementation of Moses  called Phrasal  which provides an easier API for adding features ,Positive
Interannotator agreement was measured using the kappa  K  statistics  on 1502 instances  three Switchboard dialogues  marked by two annotators who followed specific written guidelines ,Neutral
While Kazama and Torisawa used a chunker  we parsed the definition sentence using Minipar  ,Neutral
"To transform the problem into a classification task, we use the IOB2 classification scheme (Tjong Kim Sang and Veenstra, 1999).  ",Neutral
"Note that Liang et al. (2009) only use this method in synthetic experiments, and instead use a method similar to total uncertainty for experiments in part-of-speech tagging. Unlike the experiments presented in this paper, Liang et al. (2009) conduct only simulated active learning experiments and do not consider skipping queries. ",Negative
"The first major use of HMMs for part of speech tagging was in CLAWS (Garside et al., 1987) in the 1970s. ",Positive
Among them  the unsupervised algorithm using decisiontrees  has achieved promising performance Recent projects in semisupervised  and unsupervised  tagging also show significant progress Introduction The emergence of phrasebased statistical machine translation  PSMT   has been one of the major developments in statistical approaches to translation ,Positive
"This follows a general formulation of the graph alignment problem based on maximum structural matching (Klau, 2009). ",Neutral
Among these methods  CRFs is the most common technique used in NLP and has been successfully applied to PartofSpeech Tagging   NamedEntity Recognition  and shallow parsing  Wikipedia first sentence  WikiFS    used Wikipedia as an external knowledge to improve Named Entity Recognition ,Positive
Koo et al. (2008) presentednew features based on word clusters obtained from large-scale unlabeled data and achieved large improvement for English and Czech. ,Neutral
"Specifically, we view the task of inferring annotations for new verbs as an instance of a structural matching problem and follow a graph-based formulation for pairwise global network alignment (Klau, 2009). ",Positive
Like   we used mutual information to measure the cohesion between two words ,Neutral
The 746  final accuracy on apartments is higher than any result obtained by   the highest is 741    higher than the supervised HMM results reported by Grenager et al One prominent constraint of the IBM word alignment models  is functional alignment  that is each target word is mapped onto at most one source word ,Negative
 shows that setting those weights should take into account the evaluation metric by which the MT system will eventually be judged ,Neutral
"For the task of dialogue act disambiguation, Samuel, Carberry, and Vijay-Shanker (1999) suggest a method of automatically finding cue phrases for disambiguation. It may be possible to apply this or a similar method to our data and to compare the performance of automatically gained resources with manual ones. ",Positive
This method of evaluation has already been used in other summarizationevaluations such as Edmundson (1969) and Marcu (1997).,Neutral
Such a system that usesthe morpho-lexical probabilities together with a syntactic knowledge is described inLevinger (1992).,Positive
2 Syntacticoriented evaluation metrics We investigated the following metrics oriented on the syntactic structure of a translation output  POSBLEU The standard BLEU score  calculated on POS tags instead of words  POSP POS ngram precision  percentage of POS ngrams in the hypothesis which have a counterpart in the reference  POSR Recall measure based on POS ngrams  percentage of POS ngrams in the reference which are also present in the hypothesis  POSF POS ngram based Fmeasure  takes into account all POS ngrams which have a counter29 part  both in the reference and in the hypothesis ,Neutral
"We presented a few improvements to the Statistical Post Editing setup. They are part of an effort to better integrate a linguistic, rule-based system and the statistical correcting layer also illustrated in (Ueffing et al., 2008). ",Positive
The efficient block alignment algorithm in Section 4 is related to the inversion transduction grammar approach to bilingual parsing described in   in both cases the number of alignments is drastically reduced by introducing appropriate reordering restrictions System Overview 3 Translation model The system developed for this years shared task is a stateoftheart  twopass phrasebased statistical machine translation system based on a loglinear translation model  ,Positive
4 Maximum Entropy To explain our method  we l  riefly des   ribe the con   ept of maximum entrol  y Recently  many al  lnoaches l  ased on the maximum entroi  y lnodel have t  een applied to natural language processing  ,Neutral
"We also compared the proposed model with two state-of-the-art language models, Interpolated Kneser-Ney smoothing and fullibmpredict (Goodman, 2001), and found that LWLM outperformed both models on all corpora, with a perplexity reduction ranging between 12.40% and 5.87%. ",Neutral
"Work similar to that described here has been carried out by Merialdo (1994), with broadly similar Conclusions. ",Neutral
Far from full syntactic complexity  we suggest to go back to the simpler alignment methods first described by  The latentannotation model  is one of the most effective unlexicalized models Online votedperceptrons have been reported to work well in a number of NLP tasks  ,Positive
Nakagawa  and   also showed the effectiveness of global features in improving the accuracy of graphbased parsing  using the approximate Gibbs sampling method and a reranking approach  respectively ,Neutral
54 Domain Adaptation 541 FeatureBased Approaches Onewayofadaptingalearnertoanewdomainwithout using any unlabeled data is to only include features that are expected to transfer well  ,Neutral
"We used publicly available resources for all our tests: for decoding we used Moses (Koehn and Hoang, 2007) and our parallel data was taken fromthe Spanish-English section of Europarl.",Neutral
"We exploit similar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model.",Positive
"The SPECIALIST parser is based on the notion of barrier words (Tersmette et al. 1988), which indicate boundaries between phrases. ",Neutral
 applies this approach to the socalled IBM Candide system to build context dependent models  compute automatic sentence splitting and to improve word reordering in translation ,Neutral
A hierarchical model can translate discontiguous groups of words as a unit. A phrase-based model cannot. Lopez (2008b) gives indirect experimental evidence that this difference affects performance.,Neutral
"The work reported here is a logical continuation of two specific strands of research aimed in this general direction. The first is the popular idea of statistical tagging e.g. (DeRose, 1988; Cutting et al., 1992; Church, 1988). ",Positive
This averaging effect has been shown to help overfitting  ,Positive
"(Carroll et al., 1998) discussed simplifying newspaper text by replacing uncommon words with common words, or replacing complicated syntactic structures with simpler structures to assist people with reading disabilities. (Chandrasekar et al., 1996) discussed text simplification in general.",Neutral
"A major issue in MaxEnt training is how to select proper features and determine the feature targets (Berger et al., 1996; Jebara and Jaakkola, 2000).",Neutral
"Weights for these separate models were tuned through the Mert algorithm provided in the Moses toolkit (Koehn et al., 2007), using the provided news tuning set. ",Neutral
"The work of (Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1992) used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based Techniques. ",Negative
Measurement of Beliability The Kappa Statistic Following Jean   we use the kappa statistic  to measure degree of agreement among subjects ,Neutral
Nießen et al. (2000) is an early work that also constructs a database of translations and judgments.,Neutral
This wrong translation of content words is similar to the incorrect omission reported in   which both hurt translation adequacy ,Neutral
"We took two approaches to smoothing. First, because Dagan et al. used Good-Turing smoothing in their experiments, we did likewise so as to replicate their work as closely as possible. Second, we tried an approach based on the distributional clustering method of Pereira et al. (1993).  ",Positive
"Our approach to inducing syntactic clusters is closely related to that described in Brown, et al, (1992) which is one of the earliest papers on the subject. ",Positive
"We generate the boundary word features from the extracted reordering instances in the same way as discussed in Xiong et al. (2006) and use Zhang’s MaxEnt Tools 2 to train a reordering model for the 2 nd baseline system. Similarly, we use the algorithm 1 in Xiong et al. (2008) to extract features and use the same MaxEnt Tools to train a reordering model for the 3 rd baseline system. ",Positive
 and Bikel and Chiang  has demonstrated the applicability of the  model for Czech and Chinese One of the most successful metrics for judging machinegenerated text is BLEU  ,Positive
Wu and Weld  and   calculate the overlap between contexts of named entities and candidate articles from Wikipedia  using overlap ratios or similarity scores in a vector space model  respectively ,Neutral
"Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993). The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996). For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999) ",Neutral
"Finally, we would like to explore the machine learning potential offered by morphological dictionaries with application to other related tasks such as stemming (Nakov, 2003), lemmatisation and POS tagging ",Positive
"Most state-of-the-art system combination methods are based on constructing a confusion network (CN) from several input translation hypotheses, and choosing the best output from the CN based on several scoring functions (e.g. Rosti et. al., 2007a, He et. al., 2008, Matusov et al. 2008).",Positive
"To contrast, [Jing & McKeown, 2000] concentrated on analyzing human-written summaries in order to determine how professionals construct summaries. They found that most sentences could be traced back to specific cut-and-paste operations applied to the source document. ",Positive
"There are many applications of computational linguistics, particularly those involving “shallow” processing, such as information extraction, which can benefit from such automatically derived information, especially as research into acquisition of grammar matures (e.g., (Clark, 2001)).",Positive
"There exist two main families of attribute-selecting functions: information-based (Quinlan, 1986: Ldpez, 1991) and statistically--based (Breiman et al., 1984; Mingers, 1989). ",Neutral
"Unsupervised approaches have been applied to shallow semantic tasks (e.g., paraphrasing (Lin and Pantel, 2001), information extraction (Banko et al., 2007)), but not to semantic parsing. ",Neutral
This method is very similar to some ideas in domain adaptation   but we argue that the underlying problems are quite different ,Neutral
"The stochastic tagger was trained on a sample of 357,000 words from the Brown University Corpus of Present-Day English (Francis and Kucera 1982) that was annotated using the EngCG tags. ",Neutral
23 The Averaged Perceptron Reranking Model Averaged perceptron  has been successfully applied to several tagging and parsing reranking tasks   and in this paper  we employed it in reranking semantic parses generated by the base semantic parser SCISSOR ,Positive
"Using a feature representation based also on WordNet, they learn a classifier for each frame which decides whether an unseen word belongs to the frame or not. Pennacchiotti et al. (2008) create “distributional profiles” for frames.",Neutral
"Instead of a word-based model, we build a character-based one, since word segmentation errors can lead to irrecoverable mention detection errors; Jing et al. (2003) also observe that character-based models are better performing than word-based ones for Chinese named entity recognition. ",Positive
"This order is fixed in advance, so the maximal depth of the tree is always equal to the number of features, and at the same level of the tree, all nodes have the same test (they are an instance of oblivious decision trees; cf. Langley & Sage, 1994). ",Neutral
Previous work for English  has shown that lexicalization leads to a sizable improvement in parsing performance Tighter integration of semantics into the parsing models  possibly in the form of discriminative reranking models   is a promising way forward in this regard ,Positive
"We finally also include as alignment candidates those word pairs that are transliterations of each other to cover rare proper names (Hermjakob et al., 2008), which is important for language pairs that don’t share the same alphabet such as Arabic and English",Neutral
"Previous work on using the unordered set of surrounding words have used a much larger window, such as the 100-word window of (Yarowsky, 1992), and the 2-sentence context of (Leacock et al., 1993). ",Neutral
55 Dependency validity features Like   we extract the dependency path from the question word to the common word  existing in both question and sentence   and the path from candidate answer  such as CoNLL NE and numerical entity  to the common word for each pair of question and candidate sentence using Stanford dependency parser  ,Neutral
"Detecting whether a pair expresses a target relation by looking at shared connector patterns with model pairs is a common strategy in relation extraction (Pantel and Pennacchiotti, 2008). ",Neutral
"The constraint language is able to express thesame kind of patterns than the Constraint Gram-mar formalism (Karlsson et al., 1995), although in adifferent formalism.",Neutral
"In spite of these advantages, recent work has pointed out a number of problematic aspects of BLEU that should cause one to pause and reconsider the reliance on it. Chiang et al. (2008) investigate several weaknesses in BLEU and show there are realistic scenraios where the BLEU score should not be trusted, and in fact behaves in a counter-intuitive manner. Furthermore, Callison Burch et al. (2006) point out that it is not always appropriate to use B LEU to compare systems to each other. ",Positive
Therefore  sublanguage techniques such as Sager  and  do not work Mutual information  though potentially of interest as a measure of collocational status  was not tested due to its wellknown property of overemphasising the significance of rare events  ,Negative
"The ability to express the relations between predicates and their arguments while abstracting over surface syntactic configurations holds promise for many applications that require broad coverage semantic processing. Examples include information extraction (Surdeanu et al., 2003), question answering (Narayanan and  Harabagiu, 2004), machine translation (Boas, 2005), and summarization (Melli et al., 2005).",Neutral
Inversion Transduction Grammar  ITG   and SyntaxDirected Translation Schema  SDTS   lack both of these properties ,Positive
Similaritybased smoothing  provides an intuitively appealing approach to language modeling which is the classic work on collocation extraction  uses a twostage filtering model in which  in the first step  ngram statistics determine possible collocations and  in the second step  these candidates are submitted to a syntactic valida7Of course  lexical material is always at least partially dependent on the domain in question ,Positive
"For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate. ",Positive
A statistical language model a lexicalized PCFG  is derived from the analysis grammar by processing a corpus using the same grammar with no statistical model and recording frequencies of substructures built by each rule ,Neutral
Given phrase p1 and its paraphrase p2  we compute Score3  p1  p2  by relative frequency   Score3  p1  p2   p  p2 p1   count  p2  p1  P pprime count  pprime  p1   7  People may wonder why we do not use the same method on the monolingual parallel and comparable corpora ,Neutral
"Following Ng & Cardie (2002), our baseline system reimplements the Soon et al. (2001) system. ",Positive
"Note that the information about left and right is kept separate in this computation. This differs from previous approaches (Finch and Chater, 1992; Schfitze, 1993) in which left and right context vectors of a word are always used in one concatenated vector. ",Neutral
"A theoretical motivated argumentation uses the standard deviation of the m a x i m u m likelihood probabilities for the weights 0i (Samuelsson, 1993) This leaves room for interpretation.",Neutral
"First, such a system makes useof lexical information when modeling reordering (Lopez, 2008), which has previously been shown to be useful in German-to- nglish translation (Koehn et al., 2008).",Neutral
Throughout  the likelihood ratio  is used as significance measure because of its stable performance in various evaluations  yet many more measures are possible ,Positive
1 Introduction Sentiment analysis have been widely conducted in several domains such as movie reviews  product reviews  news and blog reviews  Their approaches include the use of a vectorbased information retrieval technique   binbash  line 1  a  command not found Our do  mains are more varied  which may results in more recognition errors ,Neutral
"Considering how the performance of supervised systems degrades on out-of-domain data (Baker et al., 2007), not to mention unseen events, semisupervised or unsupervised methods seem to offer the primary near-term hope for broad coverage semantic role labeling. ",Neutral
"The systems discussed in [Hovy, 1993] relied on a knowledge base and a representation of discourse Structure. ",Neutral
22 Unsupervised Parameter Estimation We can perform maximum likelihood estimation of the parameters of this model in a similar fashion to that of Model 4   described thoroughly in  ,Positive
"Considerable interest in information extraction has concentrated on identifying named entities in text pertaining to current events (for example, Wacholder et al. 1997, Voorhees and Harman 1998, and MUC-7); however, several recent efforts have been directed at biomolecular data (Blaschke et al. 1999, Craven and Kumlien 1999, and Rindflesch et al. 2000)",Neutral
Given a source sentence f  the preferred translation output is determined by computing the lowestcost derivation  combination of hierarchical and glue rules  yielding f as its source side  where the cost of a derivation R1 Rn with respective feature vectors v1   vn Rm is given by msummationdisplay i  1 i nsummationdisplay j  1  vj  i Here  1   m are the parameters of the loglinear model  which we optimize on a heldout portion of the training set  using minimumerrorrate training  ,Neutral
For comparison purposes  three additional heuristicallyinduced alignments are generated for each system   1  Intersection of both directions  Aligner  int     2  Union of both directions  Aligner  union    and  3  The previously bestknown heuristic combination approach called growdiagfinal   Aligner  gdf   Brill s results demonstrate that this approach can outperform the Hidden Markov Model approaches that are frequently used for partofspeech tagging   as well as showing promise for other applications ,Negative
"Bruce and Wiebe also performed a separate test by using a subset of the ""interest"" data set with only 4 senses (sense 1, 4, 5, and 6), so as to compare their results with previous work on WSD (Black, 1988; Zernik, 1990; Yarowsky, 1992), which were tested on 4 senses of the noun ""interest"". However, the work of (Black, 1988; Zernik, 1990; Yarowsky, 1992) were not based on the present set of sentences, so the comparison is only suggestive. ",Neutral
"When looking at the numerical values, however, one should keep in mind that macroaveraging results are in general numerically lower (Yang and Liu 1999). ",Neutral
2 Statistical Word Alignment According to the IBM models   the statistical word alignment model can be generally represented as in Equation  1  ,Neutral
They are not used in LN  but they are known to be useful for WSD  ,Neutral
SIGHAN  the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics  conducted three prior word segmentation bakeoffs  in 2003  2005 and 2006   which established benchmarks for word segmentation and named entity recognition ,Neutral
"Independently, Cutting et aL (1992) quote a performance of 800 words per secondfor their part-of-speech tagger based on h i d d e n Markov models.",Positive
Aneffort has recently been undertaken to create automated machine translation systemsin which the linguistic information needed for translation is extracted automaticallyfrom aligned corpora (Brown et al. 1990).,Positive
For instance  both Pang and Lee  and   consider the thumbs upthumbs down decision  is a film review positive or negative  Binarizing the syntax trees for syntaxbased machine translation is similar in spirit to generalizing parsing models via markovization  4 Options from the Translation Table Phrasebased statistical machine translation methods acquire their translation knowledge in form of large phrase translation tables automatically from large amounts of translated texts  ,Neutral
"In addition to reducing the original sentences, Jing andMcKeown (2000) use a number of manually compiled rules to aggregate reducedsentences; for example, reduced clauses might be conjoined with and.",Positive
Although various approaches to SMT system combination have been explored  including enhanced combination model structure   better word alignment between translations  and improved confusion network construction   most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way ,Negative
"The task of identifying sentence boundaries in text has not received as much attention as it deserves. Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g. (Brill, 1994; Collins, 1996)). ",Negative
The importance of including single nonheadwords is now also uncontroversial   and the current paper has shown the importance of including two and more nonheadwords ,Neutral
This is a common technique in machine translation for which the IBM translation models are popular methods  Comparison to selftraining For completeness  we also compared our results to the selflearning algorithm  which has commonly been referred to as bootstrapping in natural language processing and originally popularized by the work of Yarowsky in word sense disambiguation  ,Positive
"Our approach has been fully implemented in the program LExAs. Part of the implementation uses PEBLS (Cost and Salzberg, 1993; Rachlin and Salzberg, 1993), a public domain exemplar-based learning system. ",Positive
"In order to determine word boundaries, we employed the longest matching algorithm (Sornlertlamvanich, 1993). ",Positive
The scaling processintroduced in this case multiplies the forward and backward probabilities by a scalingfactor at selective word events in order to keep the computations within the floating-point dynamic range of the computer (Rabiner 1989).,Neutral
"In our research, we are concerned only with summaries of technical articles, whichare called abstracts. In this context, two main types of abstracts are considered (ANSI1979; ERIC 1980; Maizell, Smith, and Singer 1971)",Neutral
"While several authors have looked at the supervised adaptation case, there are less (and especially less successful) studies on semi-supervised domain adaptation (McClosky et al., 2006; Blitzer et al., 2006; Dredze et al., 2007). Of these, McClosky et al. (2006) deal specifically with self training for data-driven statistical parsing. They show that together with a re-ranker, improvements are obtained. Similarly, Structural Correspondence Learning (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification.",Positive
"Chuang and Yang (2000) studied several algorithms for extracting sentence segments, such as decision tree, naive Bayes classifier, and neural network.",Neutral
"Cohesion information has been used in rhetorical-based parsing for summarization (Marcu, 1997) in order to decide between “list” or “elaboration” relations and also in content selection for summarization (Barzilay and Elhadad, 1997).",Neutral
"Since we are using a larger corpus than Padó et al. (2007), who train on the BNC, a fairer comparison might be the one with our alternative models, that are all outperformed by DM by a large margin. ",Negative
"Alternatively, DM could be represented as a three-mode tensor in the framework of Turney (2007), enabling smoothing operations analogous to singular value decomposition. ",Positive
Fortunately  using distributional characteristics of term contexts  it is feasible to induce partofspeech categories directly from a corpus of suf cient size  as several papers have made clear  ,Neutral
In recent years  sentiment classification has drawn much attention in the NLP field and it has many useful applications  such as opinion mining and summarization  ,Neutral
This can either be semisupervised parsing  using both annotated and unannotated data  or unsupervised parsing  training entirely on unannotated text In   as well as other similar works   only lefttoright search was employed ,Neutral
Other statistical systems that address word classification probleans do not emphasize the use of linguistic knowledge and do not deal with a specific word class   or do not exploit as much linguistic knowledge as we do  As one can see in Table 4  the resulting parser ranks among the best lexicalized parsers  beating those of Collins  and Charniak and Johnson  8 Its F1 performance is a 27  reduction in error over  et al ,Negative
These methods go beyond the original IBM machine translation models   by allowing multiword units  phrases  in one language to be translated directly into phrases in another language 1 Introduction Recent works in statistical machine translation  SMT  shows how phrasebased modeling  significantly outperform the historical wordbased modeling  ,Negative
Recently Chiang (2007) introduced cube pruning as an approximate decoding method that extends a DP decoder with the ability to incorporate features that break the Markovian independence assumptions DP exploits. Techniques like cube pruning can be used to include the non-local features in our decoder.,Positive
"One of the most effective taggers based on a pure HMM is that developed at Xerox (Cutting et al., 1992). An important aspect of this tagger is that it will give good accuracy with a minimal amount of manually tagged training data. ",Positive
See  for an application of the boosting approach to named entity recognition  and Walker  Rambow  and Rogati  for the application of boosting techniques for ranking in the context of natural language generation ,Neutral
"In convolution tree kernel (Collins and Duffy, 2001), a parse tree T is implicitly represented by a vector of integer counts of each sub-tree type (regardless of its Ancestors)",Neutral
In addition to sentence fusion  compression algorithms  and methods for expansion of a multiparallel corpus  are other instances of such methods ,Neutral
Training via the voted perceptron algorithm  or using a maxmargin criterion also correspond to the first option  eg McCallum and Wellner   Finley and Joachims   ,Neutral
"Since we also adopt a linear scoring function in Equation (3), the feature weights of our combination model can also be tuned on a development a set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training (MERT) algorithm (Och 2003). ",Positive
With the indepth study of opinion mining  researchers committed their efforts for more accurate results  the research of sentiment summarization   domain transfer problem of the sentiment analysis  and finegrained opinion mining  are the main branches of the research of opinion mining ,Positive
However  to what extent that assumption holds is tested only on a small number of language pairs using hand aligned data  Although evaluated on a different test set  our method also outperforms the correlation with human scores reported in  However  it seems unrealistic to expect a onesizefitsall approach to be achieve uniformly high performance across varied languages  and  in fact  it doesnt Though the system presented in  outperforms the best systems in the 2006 PASCAL challenge for Turkish and Finnish  it still does significantly worse on these languages than English  Fscores of 662 and 665  compared to 794  ,Negative
"As the dialog proceeds, an utterance from a participant is accommodated into the subtask tree in an incremental manner, much like an incremental syntactic parser accommodates the next word into a partial parse tree (Alexandersson and Rei Thinger, 1997).",Neutral
6 Results We trained on the standard Penn Treebank WSJ corpus  ,Neutral
"We recently proposed cube summing an approximate technique that permits the use of non-local features for inside DP algorithms (Gimpel and Smith, 2009). Cube summing is based on a slightly less greedy variation of cube pruning (Chiang, 2007) that maintains k-best lists of derivations for each DP chart item.",Positive
"Yarowsky and Wicentowski (2000) present a corpus-ba-sed approach for morphological analysis of both regular and irregular forms based on four models including: relative corpus frequency, context similarity, weighted string similarity and incremental retraining of inflectional transduction probabilities. ",Neutral
Table 1 reports values for the Kappa  K  coefficient of agreement  for Forward and Backward Functions 6 The columns in the tables read as follows  if utterance Ui has tag X  do coders agree on the subtag  ,Neutral
Randomised algorithms are not the only compact representation schemes. Church et al. (2007) looked at Golomb Coding and Brants et al. (2007) used tries in a distributed setting. These methods are less succinct than randomised approaches. ,Positive
"The other approach is to estimate a single score or likelihood of a translation with rich features, for example, with the maximum entropy (Max- Ent) method as in (Carpuat and Wu, 2007; Itty- cheriah and Roukos, 2007; He et al., 2008). This method avoids the over-fitting problem, at the expense of losing the benefit of discriminative training of rich features directly for MT. However, the feature space problem still exists in these published models. ",Negative
"In order to prevent the garbage collection problem where many words align to a rare word at the other side (Moore, 2004), we further impose the limit that if one word is aligned to more than T words, these links are sorted by their alignment score and only the top T links are kept. ",Neutral
"In the part-of-speech hterature, whether taggers are based on a rule-based approach (Klein and Simmons, 1963), (Brill, 1992), (Voutilainen, 1993), or on a statistical one (Bahl and Mercer, 1976), (Leech et al., 1983), (Merialdo, 1994), (DeRose, 1988), (Church, 1989), (Cutting et al., 1992), there is a debate as to whether more attention should be paid to lexical probabilities rather than contextual ones. ",Neutral
While significant time savings have already been reported on the basis of automatic pretagging  eg  for POS and parse tree taggings in the Penn TreeBank   or named entity taggings for the Genia corpus    this kind of preprocessing does not reduce the number of text tokens actually to be considered ,Positive
Algorithm 1 SCL  1  Select m pivot features ,Neutral
"Before proceeding further with the main argument, consider three very recent hybrids – systems that employ linguistic rules for resolving some of the ambiguities before using automatically generated corpus-based information: collocation matrices (Leech, Garside and Bryant 1994), Hidden Markov Models (Tapanainen and Voutilai- nen 1994), or syntactic patterns (Tapanainen and Jairvinen 1994).   ",Positive
"Our approximate rules are similar to the ones proposed by Mikheev (1997), who uses a dictionary to build POS prediction rules with four parts ",Positive
"This is a problem with other direct translation models, such as IBM model 1 used as a direct model rather than a channel model (Brown et al., 1993). ",Neutral
 and  et al We used the WordNet   Similarity package  to compute baseline scores for several existing measures  noting that one word pair was not processed in WS353 because one of the words was missing from WordNet ,Neutral
"The results of (Titov et al., 2009) that use the similar joint learning technique as (Henderson et al., 2008) are also Included . ",Neutral
"Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96% are readily available to assign POS to unrestricted English sentences (Brill, 1992; Cutting et al., 1992).",Neutral
"We plan to explore how contextual effects can be modeled in our framework, focusing in particular on how composition affects word meaning (Erk and Padó, 2008). ",Positive
The combined system tackles the disambiguation problem by combining two kindsof linguistic information sources: Morpho-Lexical Probabilities and Syntactic Con-straints (a full description of this system can be found in Levinger [1992]).,Positive
"Annotations should not be influenced by theory-specific considerations. Nevertheless, different theory-specific representations shMl be recoverable from the annotation, cf. (Marcus et al., 1994). ",Neutral
This method  initially proposed by   was successfully evaluated in the context of the SENSEVAL framework  Classifier Training We chose maximum entropy  as our primary classifier because the highest performing systems in both the SemEval2007 preposition sense disambiguation task  and the general word sense disambiguation task  used it ,Positive
"The Xerox tagger is claimed (Cutting el al., 1992) to be adaptable and easily trained; only a lexicon and suitable amount of untagged text is required.  ",Positive
"For other pattern recognition related coding (e.g., cross validation, scaling, etc.) we made use of the Matlab PRTools (Duin, 2001).  ",Neutral
The maximum entropy approach  is known to be well suited to solve the classification problem We compared a baseline system  the stateoftheart phrasebased system Pharaoh   against our system The BLEU metric  in MT has been particularly successful  for example MT05  the 2005 NIST MT evaluation exercise  used BLEU4 as the only method of evaluation ,Positive
"[Mani et al, 1999] employed rules such as the referencing of pronouns with the most recently mentioned noun phrase. However, this might be inappropriate in MDS, where the use of multiple documents increases the number of possible entities with which an anaphor could be referenced. ",Negative
" entence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and  ogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al.2007). ",Neutral
"COREF treats interpretation broadly as a problem of abductive intention recognition (Hobbs et al., 1993). ",Neutral
SparckJones and Endres-Niggemeyer (1995) stated the need for a research program in text,Neutral
In the wellknown socalled IBM word alignment models   reestimating the model parameters depends on the empirical probability P  ek  fk  for each sentence pair  ek  fk  ,Positive
"While this approach exploits only syntactic and lexical information, Jing andMcKeown (2000) also rely on cohesion information, derived from word distribution ina text",Positive
Their idea has proven effective for estimating the statistics of unknown words in previous studies   We use the popular online learning algorithm of structured perceptron with parameter averaging   This algorithm is referred to as GHKM  and is widely used in SSMT systems  ,Positive
"We could leave the text partly disambiguated, and use a syntactic parser that uses both lin-guistic knowledge and corpus-based heuristics (see (Tapanainen and J//rvinen, 1994)) ",Positive
"T h e corpus lines retained are part-of-speech tagged (Cutting et al., 1992). ",Neutral
"FrameNet provides definitions for more than 500 frames, of which we entertain only a small number. This is done using a method similar to Pennacchiotti et al. (2008). ",Positive
The problem is typically presented in logspace  which simplifies computations  but otherwise does not change the problem due to the monotonicity of the log function  hm  log hprimem  log p  t s   summationdisplay m m hm  t  s   3  Phrasebased models  are limited to the mapping of small contiguous chunks of text Though taggers based on dependency networks   SVM   MaxEnt   CRF   and other methods may reach slightly better results  their traintest cycle is orders of magnitude longer ,Negative
"The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. ",Positive
These tags are drawn from a tagset which is constructed by 363 extending each argument label by three additional symbols a80a44a81a83a82a84a81a86a85  following  ,Neutral
We now describe an automatic system that can perform extraction and classification of rhetorical status on unseen text (cf. also a prior version of the system reported in Teufel and Moens [2000] and Teufel [1999]). ,Neutral
Also  slightly restating the advantages of phrasepairs identified in   these blocks are effective at capturing context including the encoding of noncompositional phrase pairs  and capturing local reordering  but they lack variables  eg embedding between ne pas in French   have sparsity problems  and lack a strategy for global reordering ,Positive
"Mirkinet al. 2006 also integrate information from the lexical patterns in which two words co-occur and similarity of the contexts in which each word occurs on its own, to improve performance in lexical entailment Acquisition. ",Positive
"In our previous work, we evaluated the overall summarization strategy of MultiGenin multiple experiments, including comparisons with human-written summaries inthe Document Understanding Conference (DUC) 11 evaluation (McKeown et al. 2001;McKeown et al. 2002) and quality assessment in the context of a particular informa-tion access task in the Newsblaster framework (McKeown et al. 2002)",Neutral
"Automatic summarization is a reductive transformation of source text to summary text through content reduction, selection, and/or generalization on what is important in the source (Sparck Jones, 1999). ",Neutral
 reported very high results  96  on the Brown corpus  for unsupervised POS tagging using Hidden Markov Models  HMMs  by exploiting handbuilt tag dictionaries and equivalence classes ,Positive
"Due to the positive results in Ando (2006), Blitzer et al. (2006) include this in their standard setting of SCL and report results using block SVDs only. ",Positive
"Onthe other hand, in languages like Turkish or Finnishwith very productive agglutinative morphology, itis possible to produce thousands of forms (or evenmillions (Hankamer, 1989)) from a given root wordand the kinds of ambiguities one observes are quitedifferent than what is observed in languages like En-Glish",Neutral
In earlier work  only singletons were used as seed words  varying their number allows us to test whether multiple seed words have a positive effect in detection performance ,Neutral
The rule feature values were computed online during decoding using the suffix array method described by Lopez (2007).,Neutral
"We used the GENIA dataset (Kim et al., 2003) as the source for knowledge extraction. ",Positive
"If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001).",Neutral
"Each backbone produces a separate CN and the decision of which CN to choose is taken at a later decoding stage, but this still restricts the possible orders and alignments greatly (Rosti et al. 2008, Matusov et al. 2008).",Neutral
Table lookup using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications  including ` crummy  MT on the World Wide Web   certain machineassisted translation tools  eg ,Neutral
Consider the lexical model pw  ry rx   defined following   with a denoting the most frequent word alignment observed for the rule in the training set ,Neutral
"Confusion networks allow word-level system combination, which was shown to outperform sentence re-ranking methods and phrase-level combination (Rosti, et. al. 2007a).",Neutral
For English  after a relatively big jump achieved by   we have seen two significant improvements   and  pushed the results by a significant amount each time In our final comparison  we have also included the results of   because it has surpassed  as well and we have used this tagger in the data preparation phase ,Positive
"We present and evaluate empirically statistical models for both mention detection and entity tracking problems. For mention detection we use approaches based on Maximum Entropy (MaxEnt henceforth) (Berger et al., 1996) and Robust Risk Minimization (RRM henceforth) (Zhang et al., 2002). ",Neutral
Several papers have looked at higherorder representations  but have not examined the equivalence of synpara distributions when formalized as Markov chains  Of the methods we compare against  only the WordNetbased similarity measures    and  provide a method for predicting verb similarities  our learned measure widely outperforms these methods  achieving a 136  Fscore improvement over the LESK similarity measure ,Negative
Our test set is 3718 sentences from the English Penn treebank  which were translated into German ,Neutral
"Maximum Entropy (MaxEnt) principle has been successfully applied in many classification and tagging tasks (Ratnaparkhi, 1996; K. Nigam and A.McCallum, 1999; A. McCallum and Pereira, 2000). We use MaxEnt modeling as thelearning component.",Positive
"The work of (Miller et al., 1994) is the only prior work we know of which attempted to evaluate WSD on a large data set and using the refined sense distinction of WORDNET. However, their results show no improvement (in fact a slight degradation in performance) when using surrounding words to perform WSD as compared to the most frequent heuristic. ",Negative
5 The SemCor collection  is a subset of the Brown Corpus and consists of 352 news articles distributed into three sets in which the nouns  verbs  adverbs  and adjectives have been manually tagged with their corresponding WordNet senses and partofspeech tags using Brills tagger  ,Neutral
"The Systran system provides a dictionary coding tool (Senellart et al., 2003). This tool allows the manual task of coding entries to be partially automated with the use of monolingual dictionaries and probabilistic context-free grammars, while allowing the user to fine-tune it by correcting the automatic coding and/or add more features. However, this remains first of all a time-consuming task. Moreover, it is not easy for humans to selec tthe best translation among a set of alternatives, let alone assign them probabilities. Last but not least, the beneficial effect on translation is not guaranteed",Negative
"Constituent pruning is a bottom-up approach, and is complemented by a second, top-down, method based on Explanation-Based Learning (EBL; (Mitchell et al., 1986; van Harmelen and Bundy, 1988)). ",Neutral
The Penn Treebank annotation  was chosen to be the first among equals  it is the starting point for the merger and data from other annotations are attached at tree nodes ,Neutral
Second  it can be applied to control the quality of parallel bilingual sentences mined from the Web  which are critical sources for a wide range of applications  such as statistical machine translation  and crosslingual information retrieval  ,Neutral
The morphological processing in PairClass  is more sophisticated than in  In addition  the performance of the adapted model for Joint ST obviously surpass that of   which achieves an F1 of 9341  for Joint ST  although with more complicated models and features Some are the result of inconsistency in labeling in the training data   which usually reflects a lack of linguistic clarity or determination of the correct part of speech in context ,Negative
"Raw text is processed by a preprocessor which segments the text into sentences using various heuristics about punctuation, and then to- kenizes and runs it through a wide-coverage high performance morphological analyzer developed using two-level morphology tools by Xerox (Kart- Tunen, 1993). ",Positive
32 The parsers The parsers that we chose to evaluate are the C&C CCG parser   the Enju HPSG parser   the RASP parser   the Stanford parser   and the DCU postprocessor of PTB parsers   based on LFG and applied to the output of the Charniak and Johnson reranking parser ,Neutral
"As large corpora became available, it became clear that simpleMarkov-model based stochastic taggers that were automatically trained could achievehigh rates of tagging accuracy (Jelinek 1985).",Neutral
All the enumerated segment pairs are listed in the following table  Feature x  y Feature x  y AM  c  c0 AM2  c2c  c0 AM 2 c  c0c AM2 2 c2c  c0c AM 3 c  c0cc2 AM3  c3c2c  c0 We use Dunnings method  because it does not depend on the assumption of normality and it allows comparisons to be made between the signiflcance of the occurrences of both rare and common phenomenon ,Positive
"Among the statistical approaches, the Maximum Entropy framework has a very strong position. Nevertheless, a recent independent comparison of 7 taggets (Zavrel and Daelemans, 1999) has shown that another approach even works better ",Neutral
"When the training text is adequate to estimate the tagger parameters, moreefficient stochastic taggers (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991;Weischedel et al. 1993) and training methods can be implemented (Merialdo 1994).",Neutral
The first solution might also introduce errors elsewhere As  already noted  ` While this automatic derivation process introduced a small percentage of errors on its own  it was the only practical way both to provide the amount of training data required and to allow for fullyautomatic testing  1 To train their system  R&M used a 200kword chunk of the Penn Treebank Parsed Wall Street Journal  tagged using a transformationbased tagger  and extracted base noun phrases from its parses by selecting noun phrases that contained no nested noun phrases and further processing the data with some heuristics  like treating the possessive marker as the first word of a new base noun phrase  to flatten the recursive structure of the parse ,Neutral
"In fact, we have developed a rule-basedcomponent that transforms the phrase structure output of Collins’s (2003) parser intoa representation in which a node has a direct link to its dependents.",Neutral
"The contexts are smoothed by linear interpolation of unigrams, bigrams, and trigrams. Their weights are calculated by deleted interpolation (Brown et al., 1992). ",Neutral
"Recently, researchers have been looking for more objective definitions of relevance. Kupiec, Pedersen, and Chen (1995) define relevance by abstract similarity ",Neutral
 noted that the unigram unpredictable might have a positive sentiment in a movie review  eg unpredictable plot   but could be negative in the review of an automobile  eg unpredictable steering  ,Neutral
A description of the flat featurized dependencystyle syntactic representation we use is available in   which describes how the entire Penn Treebank  was converted to this representation ,Neutral
A possible solution to his problem might be the use of more general morphological rules like those used in partofspeech tagging models  eg  1 2 3 4 530 40 50 60 70 80 90 100 level error RAND BASE Boost_S NNtfidf NB Boost_M Figure 6  Comparison of all models for a129 a48a51a95a66a97a98a97a180a222    where all suffixes up to a certain length are included ,Neutral
As   we adopted an evaluation of mutual information as a cohesion measure of each cooccurrence ,Neutral
While the idea of exploiting multiple news reports for paraphrase acquisition is not new  previous efforts  have been restricted to at most two news sources Furthermore  we provide a 638  error reduction compared to IBM Model 4  ,Negative
The most commonly used metric  BLEU  correlates well over large test sets with human judgments   but does not perform as well on sentencelevel evaluation  ,Negative
"We use the kappa coefficient K (Siegel and Castellan 1988) to measure stability and reproducibility, following Carletta (1996).  ",Neutral
Most recently   published their Semisupervised sequential labeling method  whose results on POS tagging seem to be optically better than   but no significance tests were given and the tool is not available for download  ie for repeating the results and significance testing ,Negative
"In general, this is a difficult open problem that only recently has started to receive some attention (Mohammad et al., 2008). Resolving this is not the focus of this paper, but we describe a general heuristic for fixing this problem. ",Neutral
Statistical significance is computed using the bootstrap re-sampling method proposed by Koehn (2004).  ,Positive
272 Similaritybased estimation was first used for language modeling in the cooccurrence smoothing method of Essen and Steinbiss   derived from work on acoustic model smoothing by Sugawara et al Following the setup in   we initialize the transition and emission distributions to be uniform with a small amount of noise  and run EM and VB for 1000 iterations ,Neutral
In order to overcome this  some unsupervised learning methods and minimallysupervised methods  eg    have been proposed ,Positive
"More recently, Cutting et al. (1992) suggest that training can be achieved with a minimal lexicon and a limited amount of a priori information about probabilities, by using an Baum-Welch restimation to automatically refine the model. ",Neutral
"For example, Quirk et al. (2005) use features involving phrases and source-side dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence.",Neutral
Despite ME theory and its related training algorithm  do not set restrictions on the range of feature functions1  popular NLP text books  and research papers  seem to limit them to binary features 4 Conclusions Compared with other word alignment algorithms   word_align does not require sentence alignment as input  and was shown to produce useful alignments for small and noisy corpora 1 Introduction Statistical phrasebased systems  have consistently delivered stateoftheart performance in recent machine translation evaluations  yet these systems remain weak at handling word order changes ,Negative
Moreover  the deterministic dependency parser of Yamada and Matsumoto   when trained on the Penn Treebank  gives a dependency accuracy that is almost as good as that of  and Charniak  2000  ,Positive
We annotated with the BIO tagging scheme used in syntactic chunkers  ,Neutral
A hierarchical alignment algorithm is a type of synchronous parser where  instead of constraining inferences by the production rules of a grammar  the constraints come from word alignments and possibly other sources  ,Neutral
"The process of task-oriented dialog is treated as a special case of AI-style plan recognition (Sidner, 1985; Litman and Allen, 1987; Rich and Sidner, 1997; Carberry, 2001; Bohus and Rudnicky, 2003; Lochbaum, 1998). ",Neutral
"Our implementation of patterns for information extraction is similar to Black’s(1990) implementation of Paice’s (1981) indicative phrases method, but whereas Blackscores sentences based on indicative phrases contained in the sentences, our methodscores the information from the sentences based on term distribution.",Neutral
1 Introduction The dominance of traditional phrasebased statistical machine translation  PBSMT  models  has recently been challenged by the development and improvement of a number of new models that explicity take into account the syntax of the sentences being translated 1 Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years   and often outperform phrasebased systems  on targetlanguage fluency and adequacy ,Negative
Movies Reviews  This is a popular dataset in sentiment analysis literature  ,Positive
   and Lee   Wilson et al ,Neutral
"Simple as it seems, the mention-pair model has been shown to work well (Soon et al., 2001; Ng and Cardie, 2002). ",Neutral
One is distortion model  which penalizes translations according to their jump distance instead of their content ,Neutral
Our learning method is an extension of Collinss perceptronbased method for sequence labeling  ,Neutral
"Recently, many phrase reordering methods have been proposed, ranging from simple distance based distortion model (Koehn et al., 2003; Ochand Ney, 2004), flat reordering model (Wu, 1997;Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), hasnot been well exploited.",Negative
Researchers have focused on learning adjectives or adjectival phrases  and verbs   but no previous work has focused on learning nouns ,Neutral
2 Maximum Entropy Models Maximum entropy  ME  models   also known as loglinear and exponential learning models  provideageneralpurposemachinelearningtechnique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging  named entity recognition etc Maximum entropy models can integrate features from many heterogeneous information sources for classification ,Positive
"Apart from linguistic engineering refinements of the similarity metric, we are currently experimenting with statistical measures to compute such more fine-grained similarities (e.g. Stanfill & Waltz, 1986, Cost & Salzberg, 1994). ",Neutral
Synchronous parsing models have been explored with moderate success  Systems based on perceptron have been shown to be competitive in NER and text chunking  We specify the model and the features with the LBJ  modeling language We use maximumentropy models   which are particularly wellsuited for tasks  like ours  with many overlapping features  to harness these linguistic insights by using features in our models which encode  directly or indirectly  the linguistic correlates to SE types,Positive
METEOR was chosen since  unlike the more commonly used BLEU metric   it provides reasonably reliable scores for individual sentences Our method is a natural extension of those proposed in  and   and overcomes their drawbacks while retaining their advantages Another consequence of not generating posthead conjunctions and punctuation as firstclass words is that they 19 In fact  if punctuation occurs before the head  it is not generated at alla deficiency in the parsing model that appears to be a holdover from the deficient punctuation handling in the model of  ,Negative
While we do not have a direct comparison  we note that  performs worse on movie reviews than on his other datasets  the same type of data as the polarity dataset At any rate  regularized conditional loglinear models have not previously been applied to the problem of producing a high quality partofspeech tagger  Ratnaparkhi   Toutanova and Manning   and  all present unregularized models ,Negative
"Phrase-based systems such as Moses (Koehn et al., 2007) explicitly search for the highest-scoring string in which all source words are translated.",Positive
In terms of applying nonparametric Bayesian approaches to NLP   evaluated the clustering properties of DPMMs by performing anaphora resolution with good results The full model yields a stateoftheart BLEU  score of 08506 on Section 23 of the CCGbank  which is to our knowledge the best score reported to date 40 using a reversible  corpusengineered grammar ,Positive
"For the Penn Treebank, (Ratnaparkhi, 1996) reports an accuracy of 96.6% using the Maximum Entropy approach, our much simpler and therefore faster HMM approach delivers 96.7%. This comparison needs to be re-examined, since we use a ten-fold crossvalidation and averaging of result while Ratnaparkhi only makes one test run ",Negative
"This pre-rocessing step can be accomplished by applying the GIZA++ toolkit (Och and Ney, 2003) that provides Viterbi alignments based on IBM Model-4. ",Positive
"Consider, for example, a word like cover as discussed by Voutilainen (Karlsson et al., 1995): in the Brown and the LOB Corpus (Johansson, 1980), the word ""cover"" is a noun 40% of the occurrences and a verb 60% of the other, but in the context of a car maintenance manual, it is a noun 100~0 of the time. ",Neutral
Since Czech is a language with relatively high degree of wordorder freedom  and its sentences contain certain syntactic phenomena  such as discontinuous constituents  nonprojective constructions   which can not be straightforwardly handled using the annotation scheme of Penn Treebank   based on phrasestructure trees  we decided to adopt for the PCEDT the dependencybased annotation scheme of the Prague Dependency Treebank PDT  13  give an informal example  but do not elaborate on it ,Negative
These feature vectors and the associated parser actions are used to train maximum entropy models  ,Neutral
Bikel and Chiang  in fact contains two parsers  one is a lexicalized probabilistic contextfree grammar  PCFG  similar to   the other is based on statistical TAG  ,Neutral
"Quirk et al. (Quirk et al., 1985) distinguish nominalizations between deverbal and verbal nouns.",Neutral
"The experimental methodology was taken from Machine Learning practice (e.g. Weiss & Kulikowski, 1991): independent training and test sets were selected from the original corpus, the system was trained on the training set, and the generalization accuracy (percentage of correct category assignments) was computed on the independent test set. ",Positive
2This can explain why previous attempts to use WordNet for generating sentencelevel paraphrases  were unsuccessful We have also illustrated that ASIA outperforms three other English systems   even though many of these use more input than just a semantic class name ,Negative
The model consists of a set of wordpair parameters p  t  s  and position parameters p  j  i     in model 1  IBM1  the latter are fixed at 1   1  1   as each position  including the empty position 0  is considered equally likely to contain a translation for w Maximum likelihood estimates for these parameters can be obtained with the EM algorithm over a bilingual training corpus  as described in  ,Neutral
Also  in a  stateoftheart English parser  only the words tha  t occur more tha  n d times in training data ,Positive
We solve SAT analogies with a simplified version of the method of  ,Neutral
Semantic collocations are harder to extract than cooccurrence patternsthe state of the art does not enable us to find semantic collocations automatically t This paper however argues that if we take advantage of lexicai paradigmatic behavior underlying the lexicon we can at least achieve semiautomatic extraction of semantic collocations see also Calzolari and Bindi 990 I But note the important work by Hindle [HindlegO] on extracting semantically similar nouns based on their substitutability in certain verb contexts,Positive
"In (Rayner and Samuelsson, 1994), a simple scheme is given, which creates rules corresponding to four possible units: full utterances, recursive NPs, PPs, and non-recursive Nps. A more elaborate scheme is given in (Samuelsson, 1994b), where the chunking criteria are learned automatically by an entropy-minimization method; the results, however, do not appear to improve on the earlier ones. In both cases, the coverage loss due to grammar specialization was about 10 to 12% using training corpora with about 5,000 examples. In practice, this is still unacceptably high for most Applications. ",Negative
"By varying the threshold, we can perform a recall-precision, or error-rate-ambiguity, tradeoff. A similar strategy is adopted in (de Mar- Cken 1990). ",Positive
For instance  for Maximum Entropy  I picked  for the basic theory   for an application  POS tagging in this case   and  for more advanced topics such as optimization and smoothing ,Neutral
One conclusion that we can draw is that at present the additional word features used in  looking at words more than one position away from the current do not appear to be helping the overall performance of the models In comparison  the 2D model in Figure 2  c  used in previous work  can only model the interaction between adjacent questions ,Negative
To compare the output of their shallow parser with the output of the wellknown  parser  Li and Roth applied the chunklink conversion script to extract the shallow constituents from the output of the Collins parser on WSJ section 00 while the former is piecewise constant and thus can not be optimized using gradient techniques   provides an approach that performs such training efficiently ,Positive
Our evaluation assessed the performance of a semantic frame and role labeler with and without the annotations produced by our method. The labeler followed closely the implementation described in Johansson and Nugues (2008). ,Positive
"Studies on the supervised task have shown that straightforward baselines (e.g. models based on source only, target only, or the union of the data) achieve a relatively high performance level and are “surprisingly difficult to beat” (Daumé III, 2007).  ",Positive
The analysis presented here and the idea of the alignments havebeen greatly influenced by the exploration of abstracting manuals (Cremmins 1982).Our conceptual model comes mainly from the empirical analysis of the corpus buthas also been influenced by work on discourse modeling (Liddy 1991) and in the phi-losophy of science (Bunge 1967).,Positive
"The basic corpus used was a set of 16,000 utterances from the Air Travel Planning (ATIS; (Hemphill et al., 1990)) domain. ",Neutral
"Intuitively, AUC is the probability that a randomly picked positive instance’s estimated posterior probability is higher than a randomly picked negative instance’s estimated posterior probability (Fawcett, 2006). ",Neutral
The POS disambiguation has usually been performed by statistical approaches mainly using hidden markov model  HMM    et al  1992  Kupiec As a baseline model we used a maximum entropy tagger  very similar to the one described in  ,Neutral
"Our hierarchical system is Hiero (Chiang, 2007), modified to construct rules from a small sample of occurrences of each source phrase in training as described by Lopez (2008b).",Neutral
Interpretations are constructed abductively in that the initial actions in the sequence need not be directly tied to observable events; they may be tacit in the terminology of Thomason et al. (2006). ,Neutral
294 Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation 22 Measuring Translation Performance Changes Caused By Alignment In phrasedbased SMT  the knowledge sources which vary with the word alignment are the phrase translation lexicon  which maps source phrases to target phrases using counts from the word alignment  and some of the word level translation parameters  sometimes called lexical smoothing  ,Neutral
We speculate that this contrasts with the disappointing findings of Kehler et al. (2004) since SRL provides a more fine grained level of information when compared to predicate argument statistics. ,Negative
Jiao et al propose semisupervised conditional random fields  that try to maximize the conditional loglikelihood on the training data and simultaneously minimize the conditional entropy of the class labels on the unlabeled data  report extracting database records by learning record field compatibility Unfortunately  a counterexample illustrated in  shows that the max function does not produce valid kernels in general ,Neutral
"We use the dataset of Rubenstein and Good enough (1965), consisting of 65 noun pairs rated by 51 subjects on a 0-4 similarity scale (e.g. car- automobile 3.9, cord-smile 0.0). ",Positive
Our results are similar to those for conventional phrasebased models  ,Neutral
44 Experiment 2   s Words We also conducted translation on seven of the twelve English words studied in   ,Neutral
"However, Merialdo (1994) and Elworthy (1994) have criticized methods of estimation from an untagged corpus based on the maximum likelihood principle. They pointed out limitation of such methods revealed by their experiments and said that the optimization of likelihood didn't necessarily improve tagging accuracy. ",Neutral
"Related to our classification experiments is work on semantic or rhetorical classification of “structured” abstracts (Saggion, 2008) from the MEDLINE abstracting database where similar features to those presented here were used to identify in abstracts semantic categories such as objective, method, results, and Conclusions. ",Neutral
"The algorithm is again described by Cutting et al. and by Sharman, and a mathematical justification for it can be tbund in Huang et al. (1990). ",Positive
String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG  but Duchi et al 1 Introduction Phrasebased systems  flat and hierarchical alike   have achieved a much better translation coverage than wordbased ones   but untranslated words remain a major problem in SMT ,Negative
 and Chan et al Based on these grammars  a great number of SMT models have been recently proposed  including stringtostring model  Synchronous FSG    treetostring model  TSGstring    stringtotree model  stringCFGTSG    treetotree model  Synchronous CFGTSG  DataOriented Translation   and so on ,Neutral
Such a coding procedure covers  for example  how segmentation of a corpus is performed  if multiple tagging is allowed and if so  is it unlimited or are there just certain combinations of tags not allowed  is look ahead permitted  etc For further information on coding procedures we want to refer to  and for good examples of coding books see  for example      or  ,Neutral
"Inspired by RST, [Radev, 2000] endeavored to establish a Cross-document Structure Theory (CST) that is more appropriate for MDS. ",Positive
"We also tested feature normalization (as described in Section 4.2). While Blitzer et al. (2006) found it necessary to normalize (and scale) the projection features, we did not observe any improvement by normalizing them (actually, it slightly degraded performance in our case). Thus, we found this step unnecessary, and currently did not look at this issue any further. ",Negative
"In an interesting analysis of phrase-based and hierarchical translation, Zollmann et al. (2008) forced a phrase-based system to produce the translations generated by a hierarchical system. Unfortunately, their analysis is incomplete; they do not perform the analysis in both directions.",Negative
"Recently, a number of machine learning approaches have been proposed (Zettlemoyer and Collins, 2005; Mooney, 2007). However, they are supervised, and providing the target logical form for each sentence is costly and difficult to do consistently and with high quality. ",Negative
While these approaches have had som e success to date   their usability as parsers in systems for natural language understanding is suspect ,Negative
The state of the art technology for relation extraction primarily relies on patternbased approaches  Many mainstream systems and formalisms would satisfy these criteria  including ones such as the University of Pennsylvania Treebank  which are purely syntactic  though of course  only syntactic properties could then be extracted  ,Positive
By segmenting words into morphemes  we can improve the performance of natural language systems including machine translation  and information retrieval  It has been difficult to identify all and only those cases where a token functions as a discourse connective  and in many cases  the syntactic analysis in the Penn TreeBank  provides no help ,Negative
Similarly   propose a relative distortion model to be used with a phrase decoder An extension to WordNet was presented by  Discovering orientations of context dependent opinion comparative words is related to identifying domain opinion words  ,Neutral
They are also used for inducing alignments  In recent work   proposed a general framework for including morphological features in a phrasebased SMT system by factoring the representation of words into a vector of morphological features and allowing a phrasebased MT system to work on any of the factored representations  which is implemented in the Moses system ,Neutral
"To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al., 2008).",Positive
For example  the statistical word alignment in IBM translation models  can only handle word to word and multiword to word alignments Our graphical representation has two advantages over previous work   unifying sentence relations and incorporating question interactions ,Negative
It has the advantage of naturally capturing local reorderings and is shown to outperform wordbased machine translation  Again the best result was obtained with IOB1  which is an imI  rovement of the best reported F    1 rate for this data set    9203  This is well illustrated by the Collins parser   scrutinized by Bikel  2004   where several transformations are applied in order to improve the analysis of noun phrases  coordination and punctuation ,Negative
or cooking  which agrees with the knowledge presented in previous work  ,Neutral
The  algorithm was one of the first bootstrapping algorithms to become widely known in computational linguistics Another widely used discriminative method is the perceptron algorithm   which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron ,Positive
42 Support Vector Machines We chose to adopt a tagging perspective for the Simple NP chunking task  in which each word is to be tagged as either B  I or O depending on wether it is in the Beginning  Inside  or Outside of the given chunk  an approach first taken by   and which has become the defacto standard for this task ,Positive
"N-best word segmentation hypotheses can be obtained by using the Forward-DP Backward A* algorithm (Nagata, 1994).",Positive
"Since broad-coverage parsers for German, especially robust parsers that assign predicate-argument structure and allow crossing branches, are not available, or require an annotated traing corpus (cf. (Collins, 1996), (Eisner, 1996)).",Neutral
The CRF tagger was implemented in MALLET  using the original feature templates from  Given a set of terms with unknown sentiment orientation   then uses the PMIIR algorithm  to issue queries to the web and determine  for each of these terms  its pointwise mutual information  PMI  with the two seed words across a large set of documents ,Neutral
For example   used cooccurrences between verbs and their subjects and objects  and proposed a similarity metric based on mutual information  but no exploration concerning the effectiveness of other kinds of word relationship is provided  although it is extendable to any kinds of contextual information ,Positive
However  this is not unprecedented  discriminatively weighted generative models have been shown to outperform purely discriminative competitors in various NLP classification tasks   and remain the standard approach in statistical translation modeling  A more refined algorithm  the incremental feature selection algorithm by   allows one feature being added at each selection and at the same time keeps estimated parameter values for the features selected in the previous stages ,Positive
"Statistical systems have enjoyed considerable success for information retrieval, especially using the vector space model (Salton et al., 1975).",Positive
In most statistical machine translation  SMT  models   some of measure words can be generated without modification or additional processing ,Neutral
"The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results, even without the many additional extensions on the feature level as done in Blitzer et al. (2006). ",Negative
 tried a different generative phrase translation model analogous to IBM wordtranslation Model 3   and again found that the standard model outperformed their generative model The automatically generated patterns in PairClass are slightly more general than the patterns of  ,Negative
"We presented first-order expectation semirings and inside-outside computation in more detail than (Eisner, 2002), and developed extensions to higher-order expectation semirings. This enables efficient computation of many interesting quantities over the exponentially many derivations encoded in a hypergraph ",Negative
"Recently, Salton et al. (1999) have developed a model for representing a document by using undirected graphs.  ",Neutral
For the constituentbased models  constituent information was obtained from the output of  for English and Dubeys parser  2004  for German ,Neutral
Online votedperceptrons have been reported to work well in a number of NLP tasks  Introduction Large scale annotated corpora  eg  the Penn TreeBank  PTB  project   have played an important role in textmining ,Positive
"To construct the segmentation lattices, we define a log-inear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. ",Positive
For MCE learning  we selected the reference compression that maximize the BLEU score    argmax rR BLEU  r  R r   from the set of reference compressions and used it as correct data for training ,Neutral
 presented a historybased generation model to overcome some of the inappropriate independence assumptions in the basic generation model of  ,Negative
"Within MT there has been a variety of approaches dealing with domain adaption (for example (Wu et al., 2008; Koehn and Schroeder, 2007).",Neutral
The idea is that the translation of a sentence x into a sentence y can be performed in the following steps1   a  If x is small enough  IBMs model 1  is employed for the translation ,Neutral
"Useful tools, such as large aligned corpora (e.g., the aligned Hansards (Galeand Church 1991)) and semantic word hierarchies (e.g., Wordnet (Miller 1990)), havealso recently become available.",Positive
"Other recent work looks at summarization as a process of revision; in this work, the source text is revised until a summary of the desired length is achieved (Mani, Gates, and Bloedorn 1999). Additionally, some research has explored cutting and pasting segments of text from the full document to generate a summary (Jing and McKeown 2000). ",Positive
"If we need to generate summaries that can be used to indicative what topics are addressed in the original document, and thus can be used to alert the uses as the source content, i.e., the indicative function (Mani et al., 1999), extraction approach is capable of handling this kind of tasks. ",Positive
"For training in the English experiments, we used WSJ (Marcus et al., 1993). We had to change the format of WSJ to prepare it for our tagging software.",Positive
"Our method affords a considerable speedup for these smaller documents. For instance, a document that takes 300 seconds using Barzilay and Elhadad’s method takes only 4 seconds using ours (Silber and McCoy 2000). ",Neutral
"Bernier (1985) states that redundancy, repetition, andcircumlocutions are to be avoided. He gives a list of linguistic expressions that can besafely removed from extracted sentences or reexpressed in order to gain conciseness.",Neutral
The fluency models hold promise for actual improvements in machine translation output quality  The notion that nouns have only one sense per discourse\/collocation was also exploited by  in his seminal work on bootstrapping for word sense disambiguation Using the components of the rowvector bm as feature function values for the candidate translation em  m a6    M   the system prior weights can easily be trained using the Minimum Error Rate Training described in  ,Positive
"Radev (2000) defines twenty-four relationships (such as equivalence, subsumption, and contradiction) that might apply at various structural levels across documents. ",Neutral
2 Related work  recently advocated the need for a uniform approach to corpusbased semantic tasks ,Neutral
     Dave et al ,Neutral
"Dagan et al. (1995) then developed a postprocessor based on predicate-argument statistics that was used to override RAP’s decision when it failed to express a clear preference between two or more antecedents, which resulted in a modest rise in performance (2.5%). ",Positive
35 Regularization We apply lscript1 regularization Ng 2004 Gao et al 2007 to make learning more robust to noise and control the effective dimensionality of the feature spacebysubtractingaweightedsumofabsolutevalues of parameter weights from the loglikelihood of the training data w  argmaxw LLw summationdisplay i Ciwi 6 We optimize the objective using a variant of the orthantwise limitedmemory quasiNewton algorithm proposed by Andrew & Gao 20073 All values Ci are set to 1 in most of the experiments below although we apply stronger regularization Ci  3 to reordering features,Neutral
"Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts (Choueka et al., 1983; Church and Hanks, 1990; Smadja, 1993; Dunning, 1993; Pearce, 2002; Evert, 2004).",Neutral
Finally  the translation model can be formalized as the following optimization problem argmax logPr  D   st mwsummationdisplay j   Pr  wj ok     k This optimization problem can be solved by the EM algorithm  ,Positive
This may be because their system was not tuned using minimum error rate training  However  most of the existing models have been developed for English and trained on the Penn Treebank   which raises the question whether these models generalize to other languages  and to annotation schemes that differ from the Penn Treebank markup ,Neutral
 claimed that this approximation achieved essentially equivalent performance to that obtained when directly using the loss as the objective  O  lscript ,Neutral
"The summation over target word sequences and alignments given fixed t bears a resemblance to the inside algorithm, except that the tree structure is fixed (Pereira and Schabes, 1992).",Neutral
"(Titov et al., 2009) reported the best result by using joint learning technique up to now. The comparison indicates that our integrated system outputs a result quite close to the state-of-the-art by the pipeline system of (Johansson and Nugues, 2008) as the same syntactic structure input is adopted. It is worth noting that our system actually competes with two independent sub-systems of (Johansson and Nugues, 2008), one for verbal predicates, the other for nominal predicates. ",Positive
The system described in  also makes use of syntactic heuristics ,Neutral
orgpubscitations  j ournalstoms1986 12 2  p154meht a  Mutual Information Given the definition of Mutual Information   I  x  y   log 2 P  x  y  P  x  P  y   we consider the distribution of a window word according to the contingency table  a  in Table 4 ,Neutral
"In particular, lexical entries are no longer limited to be adjacent words as in Zettlemoyer and Collins (2005), but can be arbitrary fragments in a dependency tree. ",Positive
An especially wellfounded framework for doing this is maximum entropy  Stateofart systems for doing word alignment use generative models like GIZA    Comparison with Previous Top Systems and Related Work In POS tagging  the previous best performance was reported by  as summarized in Table 7 work is perhaps one of the most notable examples of unsupervised polarity classification ,Positive
In NLP community  it has been shown that having more data results in better performance  ,Neutral
3http    wwwopenofficeorg Another corpora based method due to Turney and Littman  tries to measure the semantic orientation O  t  for a term t by O  t   summationdisplay tiS  PMI  t  ti  summationdisplay tjS PMI  t  tj  where S  and S are minimal sets of polar terms that contain prototypical positive and negative terms respectively  and PMI  t  ti  is the pointwise mutual information  between the terms t and ti ,Neutral
Some NLG researchers are impressed by the success of the BLEU evaluation metric  in Machine Translation  MT   which has transformed the MT field by allowing researchers to quickly and cheaply evaluate the impact of new ideas  algorithms  and data sets ,Positive
The details of the algorithm can be found in the literature for statistical translation models  such as  ,Neutral
"This method can also be viewed to be a hypotheses reranking model since we only use the existing translations instead of performing decoding over a confusion network as done in the word-level combination method (Rosti et al., 2007). ",Neutral
"However, since Japanese sentences tend to be relatively long and the recent Japanese dictionary for research is large, under-flow is sometimes a problem. For example, the EDIt Japanese corpus (EDR, 1994) includes sentences that consist of more than fifty words at a frequency of one percent. In fact, we experienced the underflow problem in preliminary experiments with the EDR corpus. ",Negative
Second  benefits for sentiment analysis can be realized by decomposing the problem into S\/O  or neutral versus polar  and polarity classification  ,Positive
1 Introduction Base noun phrases  baseNPs   broadly the initial portions of nonrecursive noun phrases up to the head   are valuable pieces of linguistic structure which minimally extend beyond the scope of named entities ,Neutral
"Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al., 2008).",Neutral
"The larger P(cldi) a document di has, the more probably it will be categorized into category c. This is called the Probabilistic Ranking Principle (PRP) (Robertson, 1977). Several strategies can be used to assign categories to a document based on PRP (Lewis, 1992). ",Neutral
Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees Headargumentmodifier distinctions are made for each node in the tree based on Magerman 1994 and Collins 1997 336 ODonovan et al LargeScale Induction and Evaluation of Lexical Resources the whole tree is then converted to a binary tree heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category,Neutral
"Sindhwani (Sindhwani et al., 2009) simultaneously developed an active learning method that queries for both instance and feature labels that are then used in a graph-based learning algorithm. They find that querying certain features outperforms querying uncertain features, but this is likely because their query selection method is similar to the expectation uncertainty method described above, and consequently non-discriminative features may be queried often. It is also not clear how this graph-based training method would generalize to structured output spaces. ",Negative
22 ITG Space Inversion Transduction Grammars  or ITGs  provide an efficient formalism to synchronously parse bitext More recently   have proposed methods for automatically extracting from a corpus heads that correlate well with discourse novelty ,Positive
 compares his method to  and shows that for four words the former performs significantly better in distinguishing between two senses ,Positive
"first, the linguistic ap-proach, in which the model is written by a linguist,generally in the form of rules or constraints (Vouti-lainen and Jgrvinen, 1995). Second, the automaticapproach, in which the model is automatically ob-tained from corpora (either raw or annotated) 1 , andconsists of n-grams (Garside et al., 1987; Cuttinget ah, 1992), rules (Hindle, 1989) or neural nets(Schmid, 1994).",Neutral
The algorithm employs the OpenNLP MaxEnt implementation of the maximum entropy classification algorithm  to develop word sense recognition signatures for each lemma which predicts the most likely sense for the lemma according to the context in which the lemma occurs ,Neutral
Overall  agreement among judges for 250 propositions 601 A commonly used metric for evaluating interrater reliability in categorization of data is the kappa statistic  ,Neutral
A number of systems for automatically learning semantic parsers have been proposed  ,Neutral
"In supervised domain adaptation (Gildea, 2001; Roark and Bacchiani, 2003; Hara et al., 2005; Daumé III, 2007), besides the labeled source data, we have access to a comparably small, but labeled amount of target data. ",Neutral
"For Japanese, (Nagao and Mori, 1994) proposed a method of computing an arbitrary length character N-gram, and showed that the character N-gram statistics obtained from a large corpus includes information useful for word extraction. However, they did not report any evaluation of their word extraction method.",Negative
In what concerns the evaluation process  although ROUGE  is the most common evaluation metric for the automatic evaluation of summarization  since our approach might introduce in the summary information that it is not present in the original input source  we found that a human evaluation was more adequate to assess the relevance of that additional information ,Negative
"For the part-of-speech tagging problem, it is known that assigning the most common part of speech for each lexical item gives a baseline of 90% accuracy [Brill, 1992]. ",Neutral
"With regard to semantic similarity, WordNet is a prime contender and indeed has been previously used to acquire new predicates in FrameNet (Pennacchiotti et al., 2008; Burchardt et al., 2005; Johansson and Nugues, 2007). ",Neutral
"In support of this processing, we rely on the linguistic and domain knowledge contained in the National Library of Medicine's Unified Medical Language System (UMLS ®) as well an existing tool, the SPECIALIST minimal commitment parser (Aronson et al. 1994). The UMLS (Humphreys et al. 1998) consists of several knowledge sources applicable in the biomedical domain: the Metathesaums, Semantic Network, and SPECIALIST Lexicon (McCray et al. 1994). ",Positive
"Our approach permits an alternative to minimum error rate training (MERT; Och, 2003); it is discriminative but handles latent structure and regularization in more principled ways.",Neutral
"The resulting partially-labeled corpus can be used to train a CRF by maximizing MML. Similarly, prototype-driven learning (PDL) (Haghighi and Klein, 2006) optimizes the joint marginal likelihood of data labeled with prototype input features for each label. ",Neutral
The resulting corpus contains 385 documents of American English selected from the Penn Treebank   annotated in the framework of Rhetorical Structure Theory ,Neutral
2 Motivation and Prior Work While several authors have looked at the supervised adaptation case  there are less  and especially less successful  studies on semisupervised domain adaptation  ,Negative
"Such representations have been successfully applied todifferent aspects of natural language processing, such as morphological analysis andgeneration (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay1994) and speech recognition (Pereira, Riley, and Sproat 1994).",Neutral
"We use a naı̈ve Bayesian model as in Kupiec, Pedersen, and Chen’s (1995) experiment (cf. Figure 9).",Neutral
Researchers extracted opinions from words  sentences  and documents  and both rulebased and statistical models are investigated  ,Neutral
"It was originally collected and processed in two earlier research projects Atranos and Musa – on automatic subtitling (Van- deghinste and Tsjong Kim Sang, 2004; Vandegh- inste and Pan, 2004; Daelemans et al., 2004).  ",Neutral
 Introduction In recent years  Bracketing Transduction Grammar  BTG  proposed by  has been widely used in statistical machine translation  SMT  ,Positive
While the model of  significantly outperforms the constrained model of   they both are well below the stateoftheart in constituent parsing By doing so we must emphasize that  as described in the previous section  the BLEU score was not designed to deliver satisfactory results at the sentence level   and this also applies to the closely related NIST score ,Negative
"These corpus-based models can be represented e.g. as collocational matrices (Garside et al. (eds.) 1987: Church 1988), Hidden Markov models (cf. Cutting et al. 1992), local rules (e.g. Hindle 1989) and neural networks (e.g. Schmid 1994).  ",Neutral
The simplest one is the BIO representation scheme   where a B denotes the first item of an element and an I any noninitial item  and a syllable with tag O is not a part of any element ,Positive
"Our approach is based on the empirical examination of abstracts published by sec-ond services and on assumptions about technical text organization (Paice 1991; Bhatia1993; Jordan 1993, 1996).",Neutral
"Thus, one conclusion from that line of work is that as soon as there is a reasonable (often even small) amount of labeled target data, it is often more fruitful to either just use that, or to apply simple adaptation techniques (Daumé III, 2007; Plank and van Noord, 2008). ",Neutral
The model scaling factors are optimized on the development corpus with respect to mWER similar to  ,Neutral
"SCL (Structural Correspondence Learning) (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different Domains.",Neutral
"As an example, a graphical representation (Batagelj, 2003) of the semantic predications serving as a summary (or conceptual condensate) from our system is shown in Figure 1.  ",Neutral
Barzilay and McKeown (2005) proposed an idea called sentence fusion that integrates information in overlapping sentences to produce a non- overlapping summary sentence. ,Positive
We refer to this training method as maximum marginal likelihood (MML); it has also been explored by Quattoni et al. (2007). ,Neutral
To analyze our methods on IV and OOV words  we use a detailed evaluation metric than Bakeoff 2006  which includes Foov and Fiv Surprisingly  although JESSCM is a simpler version of the hybrid model in terms of model structure and parameter estimation procedure  JESSCM provides Fscores of 9445 and 8803 for CoNLL00 and 03 data  respectively  which are 015 and 083 points higher than those reported in  for the same configurations ,Negative
"Turney (2008) recently advocated the need for a uniform approach to corpus-based semantic tasks. Turney recasts a number of semantic challenges in terms of relational or analogical similarity. Thus, if an algorithm is able to tackle the latter, it can also be used to address the former. Turney tests his system in a variety of tasks, obtaining good results across the board. ",Positive
This method will select useful features if the topics discovered are relevant to the task. A similar heuristic was used by Druck et al. (2008). ,Neutral
"In this area, the work most closely related to ours is that of Barrett and Weld (Barrett and Weld, 1994), who build an incremental bottom-up parser to parse plans. Their parser, however, was not probabilistic or targeted at dialog processing. ",Negative
4 Filtering with the CFG Rule Dictionary We use an idea that is similar to the method proposed by Ratnaparkhi  for partofspeech tagging ,Neutral
Results from  show that under these definitions the following guarantee holds  LogLossUpda  k  BestWtk  a C20 BestLossk  a So it can be seen that the update from a to Upda  k  BestWtk  a is guaranteed to decrease LogLoss by at least W k q C0 W C0 k qC6C7 2 From these results  the algorithms in Figures 3 and 4 could be altered to take the revised definitions of W k and W C0 k into account ,Positive
"This analysis depends on the SPECIALIST Lexicon and the Xerox part-of-speech tagger (Cutting et al., 1992) and provides simple noun phrases that are mapped to concepts in the UMLS Metathesaurus using MetaMap (Aronson, 2001). ",Neutral
The approach is able to achieve 94  precision and recall for base NPs derived from the Penn Treebank Wall Street Journal  ,Neutral
"In practice, there are more free parameters and model choices (Ando and Zhang, 2005; Ando, 2006; Blitzer et al., 2006; Blitzer, 2008) besides the ones discussed above.",Positive
Forced translation was implemented by Schwartz (2008) who ensures that hypothesis are a prefix of the reference to be generated.,Neutral
The main application of these techniques to written input has been in the robust  lexical tagging of corpora with partofspeech labels  Head Lexicalization As previously shown  Charniak      Carroll and Rooth  998   etc   ContextFree Grammars  CFGs  can be transformed to lexicalized CFGs  provided that a headmarking scheme for rules is given Other wellknown metrics are WER   NIST   GTM   ROUGE   METEOR   and TER   just to name a few Studies reveal that statistical alignment models outperform the simple Dice coefficient  One possible approach is to employ stateoftheart techniques for coreference and zeroanaphora resolution  in preprocessing cooccurrence samples ,Positive
Saggion and Lapalme (2002) have studied and implemented a rule-based “verb selection” operation in their SumUM system which has been applied to introduce document topics during indicative summary generation. ,Positive
Moreover  the parameters of the model must be estimated using averaged perceptron training   which can be unstable This method has the advantage that it is not limited to the model scaling factors as the method described in  They reported that their method is superior to BLEU  in terms of the correlation between human assessment and automatic evaluation ,Negative
"Closer in spirit is AI research on learning vocabulary items by connecting user vocabulary to the agent’s perceptual representations at the time  of utterance (Oates et al., 2000; Roy and Pentland, 2002; Cohen et al., 2002; Yu and Ballard, 2004; Steels and Belpaeme, 2005).",Neutral
To quickly  and approximately  evaluate this phenomenon  we trained the statistical IBM wordalignment model 4  1 using the GIZA   software  for the following language pairs  ChineseEnglish  Italian English  and DutchEnglish  using the IWSLT2006 corpus  for the first two language pairs  and the Europarl corpus  for the last one ,Neutral
"Alpino (van Noord and Malouf, 2005; van Noord, 2006) is a robust computational analyzer for Dutch that implements the conceptual two-stage parsing Approach. ",Positive
To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans  various metrics using ngram precision and word accuracy have been proposed  word string precision  for summarization through word extraction  ROUGE  for abstracts  and BLEU  for machine translation ,Neutral
"The more similar conditions reported in previous work are those experiments performed on the WSJ corpus: (Brill, 1992) reports 3-4% error rate, and (Daelemans et al., 1996) report 96.7% accuracy. We obtained a 97.39% accuracy with tri- grams plus automatically acquired constraints, and 97.45% when hand written constraints were added.  ",Negative
Various interpolation techniques have been proposed for the estimationof the model parameters for unseen events or to smooth the modelparameters (Church and Gale 1991; Essen and Steinbiss 1992; Jardinoand Adda 1993; Katz 1987; McInnes 1992).,Positive
One of the main directions is sentiment classification  which classifies the whole opinion document  eg  a product review  as positive or negative  ,Neutral
The perceptron has been used in many NLP tasks  such as POS tagging   Chinese word segmentation  and so on ,Neutral
A monotonous segmentation copes with monotonous alignments  that is  j  k aj  ak following the notation of  ,Neutral
"In ISI’s syntax-based system (Galley et al 2006) and CMU’s Hiero extension (Venugopal et al., 2007), non-terminals in translation rules have labels, which must be respected by substitutions during decoding.",Neutral
Effective training algorithm exists  once the set of features a42 a57 a6 aa33a8 a7a54a8 a7a00a85a68a5 a53 is selected ,Positive
"Kupiec (1992) uses pre-specified suffixes and performs statistical learning for POS guessing. The XEROX tagger comes with a list of built-in ending guessing rules (Cutting et al.,1992). In addition to the ending, Weischedel et al. (1993) exploit capitalisation. Thede and Harper (1997) consider contextual information, word endings, entropy and open-class smoothing. ",Neutral
From the above discussion  we can see that traditional tree sequencebased method uses single tree as translation input while the forestbased model uses single subtree as the basic translation unit that can only learn treetostring  rules ,Neutral
In the refined model 2  alignment probabilities a  ilj  l  m  are included to model the effect that the position of a word influences the position of its translation 6 Conclusions and Future Directions In previous work  statistical NLP computation over large corpora has been a slow  of ine process  as in KNOWITALL  and also in PMIIR applications such as sentiment classi cation  ,Neutral
"From theoverlap data, we computed weighted recall and precision based on fractional count(Hatzivassiloglou and McKeown 1993).",Neutral
In Table 6 we report our results  together with the stateoftheart from the ACL wiki5 and the scores of   PairClass  and from Amac Herdagdelens PairSpace system  that was trained on ukWaC ,Neutral
"As with other randomised models we construct queries with the appropriate sanity checks to lower the error rate efficiently (Talbot and Brants, 2008).",Neutral
"A comprehensive survey of text summarization approaches can be found in (Mani, 1999). We briefly review here based on extraction approach. Luhn (1959) proposed a simple but effective approach by using term frequencies and their related positions to weight sentences that are extracted to form a summary.",Positive
"This layer of semantic context abstracts from the specific lexical expressions used, and therefore represents a higher level of abstraction than predicate argument statistics (Kehler et al., 2004) and Latent Semantic Analysis used as a model of world knowledge (Klebanov & Wiemer-Hastings, 2002).",Neutral
"No pretagged text is necessary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992). ",Positive
"Summarization of such texts requires a different approach from, for example, that used in the summarization of news articles. For example, Barzilay, McKeown, and Elhadad (1999) introduce the concept of information fusion, which is based on the identification of re- curren  description of the same events in news articles. This approach works well because in the news domain, newsworthy events are frequently repeated over a short period of time. In scientific writing, however, similar “events” are rare  ",Positive
The word posterior feature is the same as the one proposed by Rosti et. al. (2007a). i.e,Neutral
"So far, most previous work on domain adaptation for parsing has focused on data-driven systems (Gildea, 2001; Roark and Bacchiani, 2003; McClosky et al., 2006; Shimizu and Nakagawa, 2007), i.e. systems employing (constituent or de endency based) treebank grammars (Charniak, 1996).",Neutral
"Though aiming at Chinese SRL, (Xue, 2006) reported that their experiments show that simply adding the verb data to the training set of NomBank and extracting the same features from the verb and noun instances will hurt the overall performance. ",Neutral
"We seek to find a partition of the vocabulary that maximizes the mutual information between term categories and their contexts. We achieve this in the framework of information theoretic co-clustering (Dhillon et al., 2003), in which a space of entities, on the one hand, and their contexts, on the other, are alternately clustered in a way that maximizes mutual information between the two spaces.",Positive
"In this paper, phrase reordering is recast as a classification issue as done in previous work (Xiong et al., 2006 & 2008; Zhang et al., 2007a). ",Neutral
"A simple rule-based part of speech (RBPOS) tagger is introduced in (Brill, 1992). The accuracy of this tagger for English is comparable to a stochastic English POS tagger. ",Positive
"In the research presented here, we concentrate on the first step of the summarization process and follow Barzilay and Elhadad (1997) in employing lexical chains to extract important concepts from a document. ",Neutral
"Subtitles can be presented at a rate of 690 to 780 characters per minute, while the average speech rate is considerably higher (Vandeghinste and Tsjong Kim Sang, 2004). ",Neutral
For Hw6  students compared their POS tagging results with the ones reported in  ,Neutral
"The recursive algorithms for tree construction (except the final pruning) and retrieval are given in Figures 1 and 2. For a detailed discussion, see  aelemans et al. (1996). ",Positive
The features used in this study are  the length of t  a singleparameter distortion penalty on phrase reordering in a  as described in   phrase translation model probabilities  and 4gram language model probabilities logp  t   using KneserNey smoothing as implemented in the SRILM toolkit ,Neutral
"In addition, M L indicates a baseline model conraining no constraints (this will result in a most- likely tagger) and H M M stands for a hidden Markov model bigram tagger (Elworthy, 1992). ",Neutral
"In addition to the hand-crafted models listed above, researchers have built stochastic plan recognition models for interaction, including ones based on Hidden Markov Models (Bui, 2003; Blaylock and Allen, 2006) and on probabilistic context-free grammars (Alexandersson and Reithinger, 1997; Pynadath and Wellman, 2000). ",Positive
"A decision-tree learning approach to feature selection is used in this experiment (Cardie, 1993b, 1994) to discard irrelevant Features. ",Neutral
"Like the work of Jing and McKeown (2000) and Mani et al. (1999), our work was inspired by the summarization method used by human abstractors.",Positive
"Currently, most of the POS tagger accuracy reports are based on the experiments involving Penn Treebank data (Marcus, 1993). ",Neutral
An analysis of the alignments shows that smoothing the fertility probabilities significantly reduces the frequently occurring problem of rare words forming garbage collectors in that they tend to align with too many words in the other language  ,Neutral
The second one (SYS2) is a reimplementation of a phrase-based decoder with lexicalized reordering model based on maximum entropy principle proposed by Xiong et al. (2006). ,Neutral
To some extent  this can probably be explained by the strong tradition of constituent analysis in AngloAmerican linguistics  but this trend has been reinforced by the fact that the major treebank of American English  the Penn Treebank   is annotated primarily with constituent analysis ,Positive
It is interesting to note that while the study of how the granularity of contextfree grammars CFG affects the performance of a parser eg in the form 86 nIP [] n2NP [SUBJ] n4NR [] GSC4ES JiangZemin n3VP [] n5VV [] ESDO interview n6NP [OBJ] n7NR [ADJUNCT] AIC Thai n8NN [] D3D2 president f             PRED ESDO SUBJ f2  PRED GSC4ESNTYPE proper NUM sg   OBJ f3       PRED D3D2 NTYPE common NUM sg ADJUNCT   f4  PRED AICNTYPE proper NUM sg                          N  F nn3n5f n2n4f2 n6n8f3 n7f4 Figure  Cand fstructures with  links for the sentence GSC4ESESDOAICD3D2 of grammar transforms Johnson 998 and lexicalisation Collins 997 has attracted substantial attention to our knowledge there has been a lot less research on this subject for surface realisation a process that is generally regarded as the reverse process of parsing,Positive
The first step is the extraction of important concepts from the source text by building an intermediate representation of some sort. The second step uses this intermediate representation to generate a summary (Sparck Jones 1993). ,Neutral
Both Charniak  and Bikel  were trained using the goldstandard tags  as this produced higher accuracy on the development set than using  s tags The utility of ITG as a reordering constraint for most language pairs  is wellknown both empirically  and analytically   howeverITGsstraight  monotone  andinverted  reverse  rules exhibit strong cohesiveness  which is inadequate to express orientations that require gaps ,Negative
A remedy is to aggressively limit the feature space  eg to syntactic labels or a small fraction of the bilingual features available  as in   but that reduces the benefit of lexical features ,Neutral
"When extracting rules with source dependency structures, we applied the same well-formedness constraint on the source side as we did on the target side, using a procedure described by (Shen et al., 2008).",Neutral
Model 4 of  is also a firstorder alignment model  along the source positions  like the HMM  trot includes also fertilities ,Neutral
3 Model As an extension to commonly used lexical word pair probabilities p  f e  as introduced in   we define our model to operate on word triplets ,Neutral
Veale  used WordNet to answer 374 multiplechoice SAT analogy questions  achieving an accuracy of 43   but the best corpusbased approach attains an accuracy of 56   ,Positive
While in traditional wordbased statistical models  the atomic unit that translation operates on is the word  phrasebased methods acknowledge the significant role played in language by multiword expressions  thus incorporating in a statistical framework the insight behind ExampleBased Machine Translation  ,Negative
Our approach not only outperformed a notoriously difficult baseline but also achieved similar performance to the approach of   without requiring their thirdparty data resources While we have shown an increase in performance over a purely syntactic baseline model  the algorithm of    there are a number of avenues to pursue in extending this work ,Negative
"Humans perceive relevance differently from each other and differently in different situations. Paice and Jones (1993) report that they abandoned an informal sentence selection experiment in which they used agriculture articles and experts in the field as participants, as the participants were too strongly influenced by their personal research interest.  ",Neutral
Two are conditionalized phrasal models  each EM trained until performance degrades  CJPTM3 as described in  Phrasal ITG as described in Section 41 Three provide alignments for the surface heuristic  GIZA   with growdiagfinal  GDF  Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder  with the SMT Shared Task baseline system  ,Neutral
"(Chanod and Tapanainen, 1995) compare two tagging frameworks for tagging French, one that is statistical, built upon the Xerox tagger (Cutting et al., 1992), and another based on linguistic constraints only. ",Neutral
The loglinear model feature weights were learned using minimum error rate training  MERT   with BLEU score  as the objective function ,Neutral
These words and phrases are usually compiled using different approaches  Hatzivassiloglou and McKeown  1997  Kaji and Kitsuregawa  2006   and Nasukawa  2006  Esuli and Sebastiani  2006  Breck et al  2007  Ding  Liu and Yu ,Neutral
"If one of the two words is ε, the posterior of aligning word ε to state j is computed as suggested by Liang et al. (2006)",Positive
"The highest ranking material can then be extracted and displayed verbatim as extracts (Luhn 1958; Edmundson 1969; Paice 1990; Kupiec, Pedersen, and Chen 1995). Extracts are often useful in an information retrieval environment since they give users an idea as to what the source document is about (Tombros and Sanderson 1998; Mani et al. 1999), but they are texts of relatively low quality.  ",Neutral
However  the study of  provides interesting insights into what makes a good distributional similarity measure in the contexts of semantic similarity prediction and language modeling While  does not discuss distinguishing more than 2 senses of a word  there is no immediate reason to doubt that the ` one sense per collocation ' rule  would still hold for a larger number of senses ,Positive
"The SPECIALIST minimal commitment parser relies on the SPECIALIST Lexicon as well as the Xerox stochastic tagger (Cutting et al. 1992). The output produced is in the tradition of partial parsing (Hindle 1983, McDonald 1992, Weischedel et al. 1993) and concentrates on the simple noun phrase, what Weischedel et al. (1993)",Neutral
1 Introduction Since 1995  a few statistical parsing algorithms  demonstrated a breakthrough in parsing accuracy  as measured against the University of Pennsylvania TREEBANK as a gold standard ,Neutral
"As mentioned by (Pradhan et al., 2004), argument identification plays a bottleneck role in improving the performance of a SRL system. The effectiveness of the proposed additional pruning techniques may be seen as a significant improvement over the original algorithm of (Xue and Palmer, 2004). ",Positive
It is based on Incremental Sigmoid Belief Networks  ISBNs   a class of directed graphical model for structure prediction problems recently proposed in   where they were demonstrated to achieve competitive results on the constituent parsing task In our experience  this approach is advantageous in terms of translation quality  eg by 07  in BLEU compared to a minimum Bayes risk primary  ,Positive
"Note that this pruning algorithm is slightly different from that of (Xue and Palmer, 2004), the predicate itself is also included in the argument candidate list as the nominal predicate sometimes takes itself as its argument. ",Neutral
"In contrast to the view of science as a disinterested fact Factory, researchers like Swales (1990) have long claimed that there is a strong social aspect to science, because the success of a researcher is correlated with her ability to convince the field of the quality of her work and the validity of her arguments. Authors construct an argument that Myers (1992) calls the “rhetorical act of the paper” ",Neutral
"Most successfulmethods have followed speech recognition systems (Jelinek, Mercer, and Roukos 1992)and used large corpora to deduce the probability of each part of speech in the currentcontext (usually the two previous words--trigrams). These methods have reportedperformance in the range of 95-99% ""correct"" by word (DeRose 1988; Cutting et al.1992; Jelinek, Mercer, and Roukos 1992; Kupiec 1992).",Positive
791 and score the alignment template models phrases  ,Neutral
Parse selection constitutes an important part of many parsing systems  ,Neutral
"One is to use a stochastic gradient descent (SGD) or Perceptron like online learning algorithm to optimize the weights of these features directly for MT (Shen et al., 2004; Liang et al.,2006; Tillmann and Zhang, 2006).",Neutral
Unfortunately  there is no straightforward generalization of the method of  to the two edge marginal problem Previous literature on GB parsing  Wehrli  1984  Sharp  1985    1986  Kuhns  1986  Abney  1986has not addressed the issue of implementation of the Binding theory  The present paper intends in part to fill this gap ,Negative
"We measure coselection between sentences produced by each methodand the sentences selected by the assessors, computing recall, precision, and F-scoreas in Firmin and Chrzanowski (1999). In order to obtain a clear picture, we borrowedthe scoring methodology proposed by Salton et al. (1997), additionally considering thefollowing situations",Positive
Successful discriminative parsers have relied on generative models to reduce training time and raise accuracy above generative baselines  procedure is the most widelyused version of MERT for SMT  ,Positive
When compared to other kernel methods  our approach performs better than those based on the Tree kernel   and is only 02  worse than the best results achieved by a kernel method for parsing  Lexical relationships under the standard IBM models  do not account for manytomany mappings  and phrase extraction relies heavily on the accuracy of the IBM wordtoword alignment ,Negative
In contrast  the idea of bootstrapping for relation and information extraction was first proposed in   and successfully applied to the construction of semantic lexicons   named entity recognition   extraction of binary relations   and acquisition of structured data for tasks such as Question Answering  Perceptronbased training To tune the parameters w of the model  we use the averaged perceptron algorithm  because of its efficiency and past success on various NLP tasks  ,Positive
"Using multiple documents to generate a summary further complicates the situation. As contended by [Goldstein et al, 2000] a multi-document summary may contain redundant messages, since a cluster of news articles tends to cover the same main point and shared background. ",Neutral
In addition  the averaged parameters technology  is used to alleviate overfitting and achieve stable performance ,Positive
We use GIZA    to do mton wordalignment and adopt heuristic growdiagfinaland to do refinement ,Neutral
et al  2004  CollinsThompson and Callan  2005   and Ramage  2007  ,Neutral
"In phrase-based systems, reordering is accomplished both within phrase pairs (local reordering) as well as through distance-based distortion models (Koehn et al., 2003) and lexicalized reordering models (Koehn et al., 2007).",Neutral
stituent alignments  ,Neutral
"Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006) , Druck et al. (2008)). ",Neutral
Meanwhile  it is common for NP chunking tasks to represent a chunk  eg  NP  with two labels  the begin  eg  BNP  and inside  eg  INP  of a chunk  ,Neutral
joint likelihood JL productdisplay i p parenleftBig xiyi  vector parenrightBig conditional likelihood CL productdisplay i p parenleftBig yi  xivector parenrightBig classification accuracy Juang and Katagiri 1992 summationdisplay i yi yxi expected classification accuracy Klein and Manning 2002 summationdisplay i p parenleftBig yi  xivector parenrightBig negated boosting loss Collins 2000  summationdisplay i p parenleftBig yi  xivector parenrightBig1 margin Crammer and Singer 2001  st bardbl vectorbardbl  1iy negationslash yi vector  vectorfxiyi   vectorfxiy   expected local accuracy Altun et al  2003 productdisplay i productdisplay j p parenleftBig lscriptjY   lscriptjyi   xivector parenrightBig Table 1 Various supervised training criteria,Neutral
"Care was taken to ensure not just that the utterances themselves, but also the speakers of the utterances were disjoint between test and training data; as pointed out in (Rayner et al., 1994a), failure to observe these precautions can result in substantial spurious improvements in test data results. ",Neutral
411 Lexical cooccurrences Lexical cooccurrences have previously been shown to be useful for discourse level learning tasks  ,Neutral
Portage is a statistical phrasebased SMT system similar to Pharaoh  ,Neutral
"In contrast, Dredze et al. (2007) report on “frustrating” results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. ”no team was able to improve target domain performance substantially over a state of the art baseline”. ",Neutral
This difference was highlighted in the 3http    w3msivxusejhamaltparser  studyof   whichshowed that the difference is reflected directly in the error distributions of the parsers ,Neutral
Moses provides BLEU  and NIST   but Meteor  and TER  can easily be used instead ,Neutral
SMT has evolved from the original wordbased approach  into phrasebased approaches  and syntaxbased approaches  ,Neutral
Although stateoftheart statistical parsers  are more accurate  the simplicity and efficiency of deterministic parsers make them attractive in a number of situations requiring fast  lightweight parsing  or parsing of large amounts of data ,Positive
 Introduction With the introduction of the BLEU metric for machine translation evaluation   the advantages of doing automatic evaluation for various NLP applications have become increasingly appreciated  they allow for faster implementevaluate cycles  by bypassing the human evaluation bottleneck   less variation in evaluation performance due to errors in human assessor judgment  and  not least  the possibility of hillclimbing on such metrics in order to improve system performance  ,Positive
The features we use are shown in Table 2  which are based on the features used by  and Uchimoto et al ,Neutral
We hence chose transformationbased learning to create this  shallow  segmentation grammar  converting the segmentation task into a tagging task  as is done in 85   inter alia  ,Neutral
"This contrasts with semantic role labeling (Carreras and Marquez, 2004) and other forms of shallow semantic processing, which do not aim to produce complete formal meanings. ",Neutral
"Note that in this work we have decided to evaluate the predicted structure against the true structure (a hard evaluation measure), in future work we will assess the abstracts with a set of quality questions similar to those put forward by the Document Understanding Conference Evaluations (also in a way similar to (Kan and McKeown, 2002) who evaluated their abstracts in a retrieval environment).",Positive
"A stack is used to maintain the global parse state. The actions the parser can take are similar to those described in (Ratnaparkhi, 1997). ",Neutral
"As an optimal feature template subset cannot be expected to be extracted from so large a set by hand, a greedy feature selection similar to that in (Jiang and Ng, 2006; Ding and Chang, 2008) is applied.",Neutral
"Our constraint-based tagger is based on techniques that were originally developed for morphological analysis. The disambiguation rules are similar to phonological rewrite rules (Kaplan and Kay, 1994), and the parsing algorithm is similar to the algorithm for combining the morphological rules with the lexicon (Karttunen, 1994). ",Positive
612 ROUGE evaluation Table 4 presents ROUGE scores  of each of humangenerated 250word surveys against each other ,Neutral
"Our tests were conducted over a larger stream of 1.25B n-grams from the Gigaword corpus(Graff, 2003). We set our space usage to match the 3.08 bytes per n-gram reported in Talbot and Brants (2008) and held out just over 1M unseen n-grams to test the error rates of our models",Neutral
Workshop Towards GenreEnabled Search Engines booktitle pages 13  20 pages editor In G Rehm and M Santini  editors editor contexts context ork on an intradocument  or page segment level because a single document can contain instances of multiple genres  eg  contact information  list of publications  CV  see  ,Neutral
"In our case, neither assumption holds. First, the experiments in Teufel and Moens (1997) showed that in our corpus only 45of the abstract sentences appear elsewhere in the body of the document (either as a close variant or in identical form), whereas Kupiec, Pedersen, and Chen report a figure of 79.  ",Neutral
SEPepsilon aA # epsilon  # aepsilon aepsilon bepsilon bB UNKepsilon cC bepsilon cBC e   E epsilon   depsilon depsilon epsilonepsilon bAB # bA # B # e   DE cepsilon dBCD e   DE Figure 1  Illustration of dictionary based segmentation finite state transducer 31 Bootstrapping In addition to the model based upon a dictionary of stems and words  we also experimented with models based upon character ngrams  similar to those used for Chinese segmentation  ,Neutral
In Turneys work  the cooccurrence is considered as the appearance in the same window  Named entities also pose another problem with the  coreference model  since it models only the heads of NPs  it will fail to resolve some references to named entities   Ford Motor Co  Ford   while erroneously merging others   Ford Motor Co  Lockheed Martin Co  ,Neutral
"A* has nice guarantees (Dechter and Pearl, 1985), but it is space-consumptive and it is not anytime. For a use case where we would like a finer- rained speed/quality tradeoff, it might be useful to consider an anytime search algorithm, like depth-first branch-and-bound (Zhang and Korf, 1995). ",Neutral
Such a method alleviates the problem of creating templates from examples which would be used in an ulterior phase of generation  The variance semiring is essential for many interesting training paradigms such as deterministic 40 annealing   minimum risk   active and semisupervised learning  ,Positive
"We refer to He and Gildea (2006) who tested active learning and co-training methods, but found little or no gain from semi-supervised learning, and to Swier and Stevenson (2004), who achieved good results using semi-supervised methods, but tested their methods on a small number of VerbNet roles, which have not been used by other SRL Systems.",Negative
"Word alignment is also a required first step in other algorithms such as for learning sub-sentential phrase pairs (Lavie et al., 2008) or the generation of parallel treebanks (Zhechev and Way, 2002).",Neutral
"In taggers thatare based on hidden Markov models (HMM), parameters of the unknown words areestimated by taking into account morphological information from the last part of theword (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991).",Neutral
 Introduction Chinese Word Segmentation  CWS  has been witnessed a prominent progress in the last three Bakeoffs      Of particular interest are lexicalized parsing models such as the ones developed by  and Carroll and Rooth  998  ,Positive
"Thus, most of recent works in this research area are based on extraction (Goldstein et al., 1999). ",Neutral
"However, while researchers have shown that it is sometimes possible to annotate corpora that capture features of interpretation, to provide empirical support for theories, as in (Eugenio et al., 2000), or to build classifiers that assist in dialogue reasoning, as in (Jordan and Walker, 2005), it is rarely feasible to fully annotate the interpretations themselves. ",Neutral
All stateoftheart widecoverage parsers relax this assumption in some way  for instance by  i  changing the parser in step  3   such that the application of rules is conditioned on other steps in the derivation process   or by  ii  enriching the nonterminal labels in step    with contextinformation   along with suitable backtransforms in step  4  ,Positive
Even though there are some studies that compare the results from statistically computed association measures with word association norms from psycholinguistic experiments  there has not been any research on the usage of a digital  networkbased dictionary reflecting the organization of the mental lexicon to our knowledge ,Neutral
"One exception is the sense-tagged data set used in (Bruce and Wiebe, 1994), which has been made available in the public domain by Bruce and Wiebe. ",Neutral
We follow the approach of bootstrapping from a model with a narrower parameter space as is done in  eg Och and Ney  and  ,Neutral
Disambiguation of a limited number of words is not hard  and necessary context information can be carefully collected and handcrafted to achieve high disambiguation accuracy as shown in  ,Positive
"[Hovy, 1993] summarized previous work that focused on the automated planning and generation of multi-sentence texts using discourse relationships ",Neutral
The Penn Treebank documentation  defines a commonly used set of tags ,Neutral
"Recent comparisons of approaches that can be trained on corpora (van Halteren et al., 1998; Volk and Schneider, 1998) have shown that in most cases statistical aproaches (Cutting et al., 1992; Schmid, 1995; Ratnaparkhi, 1996) yield better results than finite state, rule-based, or memory-based taggers (Brill, 1993; Daelemans et al., 1996).",Positive
"The best system (Johansson and Nugues, 2008) in CoNLL 2008 achieved an F1-measure of 81.65% on the workshop’s evaluation Corpus.",Positive
"Also, it ispossible to cast a number of other useful problems as part-of-speech tagging problems,such as letter-to-sound translation (Huang, Son-Bell, and Baggett 1994) and buildingpronunciation networks for speech recognition. Recently, a method has been proposedfor using part-of-speech tagging techniques as a method for parsing with lexicalizedgrammars (Joshi and Srinivas 1994).",Positive
For example   have studied synchronous context free grammar ,Neutral
The MEAD summarizer [Radev et al 2002] is based on sentence extraction and uses a linear combination of three features to rank the sentences in the source documents. ,Neutral
   concordancing for bilingual lexicography   computerassisted language learning  corpus linguistics  Melby ,Neutral
ARBITER pursues limited coordination identification in the spirit of Agarwal and Boggess (1992) and Rindflesch (1995). Only binding terms are considered as candidates for coordination.,Positive
"The segmentation model is similar to the one presented by Lee et al. (2003), and obtains an accuracy of about 98%. ",Neutral
2 The BLEU Metric The metric most often used with MERT is BLEU   where the score of a candidate c against a reference translation r is  BLEU  BP  len  c   len  r   exp  4summationdisplay n    4 logpn   where pn is the ngram precision2 and BP is a brevity penalty meant to penalize short outputs  to discourage improving precision at the expense of recall ,Positive
The best previous result is an accuracy of 56   Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years   and often outperform phrasebased systems  on targetlanguage fluency and adequacy ,Positive
"Only in the analysis of a few words it was agreed that a multiple choice was appropriate because of different meaning-level interpretations of the utterance (these were actually headings where some of the grammatical information was omitted). Overall, these results agree with our previous experiences (Karlsson et al., 1994) ",Positive
"Syntactic alterations (Levin, 1993) represent a key aspect of the complex constraints that shape the syntax-semantics interface.  ",Positive
Support Vector Machines  SVMs   and Maximum Entropy  ME  method  are powerful learning methods that satisfy such requirements  and are applied successfully to other NLP tasks  ,Positive
Many previous studies have shown that the loglikelihood ratio is well suited for this purpose  Recent work    has shown that adding many millions of words of machine parsed and reranked LA Times articles does  in fact  improve performance of the parser on the closely related WSJ data ,Positive
 ii  Apply some statistical tests such as the Binomial Hypothesis Test  and loglikelihood ratio score  to SCCs to filter out false SCCs on the basis of their reliability and likelihood ,Neutral
Albeit simple  the algorithm has proven to be very efficient and accurate for the task of parse selection  It is an online training algorithm and has been successfully used in many NLP tasks  such as POS tagging   parsing   Chinese word segmentation   and so on ,Positive
To prune away those pairs  we used the loglikelihoodratio algorithm  to compute the degree of association between the verb and the noun in each pair ,Neutral
"Incremental alignment methods have been proposed to relax the independence assumption of pair-wise alignment (Rosti et al. 2008, Li et al. 2009). Such methods align hypotheses to a partially constructed CN in some order.",Neutral
Roche and Schabes (1995) show a method for converting a listof tagging transformations into a deterministic finite state transducer with one statetransition taken per word of input; the result is a transformation-based tagger whosetagging speed is about ten times that of the fastest Markov-model tagger.,Positive
In   the authors provide some sample subtrees resulting from such a 1000word clustering We took part the Multilingual Track of all ten languages provided by the CoNLL2007 shared task organizer  To set the weights  m  we carried out minimum error rate training  using BLEU  as the objective function ,Neutral
The probabilities in equation 4 are estimated recursively for the first- (Rabiner1989) and second-order HMM (Watson and Chung 1992).,Neutral
33 Features Similar to the default features in Pharaoh   we used following features to estimate the weight of our grammar rules ,Neutral
"One of the most powerful representations for this is Markov logic, which is a probabilistic extension of first-order logic (Richardson and Domingos, 2006).. Markov logic makes it possible to compactly specify probability distributions over complex relational domains, and has been successfully applied to unsupervised coreference resolution (Poon and Domingos, 2008) andother tasks. ",Positive
"In this paper, we apply the expectation semiring (Eisner, 2002) to a hypergraph (or packed forest) rather than just a lattice. ",Neutral
Perhaps the most wellknown method is maximum marginal relevance  MMR    as well as crosssentence informational subsumption   mixture models   subtopic diversity   diversity penalty   and others ,Neutral
One interesting approach to extending the current system is to introduce a statistical translation model  to filter out irrelevant translation candidates and to extract the most appropriate subpart from a long English sequence as the translation by locally aligning the Japanese and English sequences ,Neutral
Schone and Jurafsky (2000) apply latent semantic analysis for a knowledge-free morphology induction. ,Neutral
2 Evaluating Heterogeneous Parser Output Two commonly reported shallow parsing tasks are NounPhrase  NP  Chunking  and the CoNLL2000 Chunking task   which extends the NPChunking task to recognition of 11 phrase types1 annotated in the Penn Treebank ,Neutral
"There are several strategies for assigning categories to a document based on the probability P(cld ). The simplest one is the k-per-doc strategy (Field, 1975) that assigns the top k categories to each document. ",Positive
"[Mani et al, 1999] focused on the revision of single-document summaries in order to improve their Informativeness. They noted that such revision might also fix ‘coherence errors.’ ",Positive
There has been considerable skepticism over whether WSD will actually improve performance of applications  but we are now starting to see improvement in performance due to WSD in crosslingual information retrieval  and machine translation  and we hope that other applications such as questionanswering  text simplication and summarisation might also benet as WSD methods improve ,Positive
"MR (with or without DA) is scalable to tune a large number of features, while MERT is not. To achieve competitive performance, we adopt a forest reranking approach (Li and Khudanpur, 2009; Huang, 2008).",Positive
"In our experiments, we used the Hidden Markov Model (HMM) tagging method described in [Cutting et al, 1992].",Neutral
"There is also a less detailed description of Pahner and Hearst's system, SATZ, in (Pahuer and Hearst, 1994). ",Negative
This is analogous  and in a certain sense equivalent  to empirical risk minimization  which has been used successfully in related areas  such as speech recognition   language modeling   and machine translation   investigated the use of concurrent parsing of parallel corpora in a transduction inversion framework  helping to resolve attachment ambiguities in one language by the coupled parsing state in the second language ,Positive
"The tagger itself is based on the Hidden Markov Model (Baum, 1972) and word equivalence classes (Kupiec, 1989). ",Neutral
"The Xerox Tagger 1, XT, (Cutting et al., 1992) is a statistical tagger made by Doug Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun in Xerox PARC. It was trained on the untagged Brown Corpus (Francis and Kubera, 1982). ",Neutral
"An end result of the project is ConceptNet 3, a large scale semantic network consisting of relations between concept pairs (Havasi et al., 2007).  ",Neutral
The parameters  j  were trained using minimum error rate training  to maximize the BLEU score  on a 150 sentence development set ,Neutral
"The credit factor improved the upper bound of the estimation accuracy from an untagged corpus. However, at higher levels of tagging accuracy, the reestimation m e t h o d based on the Baum-Welch algorithm is limited by the noise of untagged corpora. On this point, I agree with Merialdo (1994) and Elworthy (1994). ",Positive
More recent work  has considered methods for speeding up the feature selection methods described in   Ratnaparkhi  1998   and Della Pietra  Della Pietra  and Lafferty  1 Introduction Currently  most of the phrasebased statistical machine translation  PBSMT  models  adopt full matching strategy for phrase translation  which means that a phrase pair  tildewidef  tildewidee  can be used for translating a source phrase f  only if tildewidef  f Due to lack of generalization ability  the full matching strategy has some limitations ,Negative
"To select these, we use the idea of strong chains introduced by Barzilay and Elhadad (1997). ",Positive
Hence we use a beamsearch decoder during training and testing  our idea is similar to that of  who used a beamsearch decoder as part of a perceptron parsing model ,Neutral
"Although we also use a mention-pair model, our tracking algorithm differs from Soon et al. (2001), Ng and Cardie (2002) in several aspects. ",Neutral
"(Church, 1992) claims that part-of-speech taggers depend almost exclusively on lexical probabilities, whereas other researchers, such as Voutilainen (Karlsson et al., 1995) argue that word ambiguities vary widely in function of the specific text and genre. Indeed, part of Church's argument is relevant if a system is based on a large corpus such as the Brown corpus (Francis and Ku~era, 1982) which represents one million surface forms of morpho-syntacticaJly disambiguated words from a range of balanced texts. ",Positive
Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96  are readily available to assign POS to unrestricted English sentences  ,Neutral
Metrics based on syntactic similarities such as the headword chain metric  HWCM   ,Neutral
"Where the classification algorithm is concerned, we have decided to use Support Vector Machines which have recently been used in different tasks in natural language processing, they have been shown particularly suitable for text categorization (Joachims, 1998). ",Neutral
3 Building the CatVar The CatVar database was developed using a combination of resources and algorithms including the Lexical Conceptual Structure  LCS  Verb and Preposition Databases   the Brown Corpus section of the Penn Treebank   an English morphological analysis lexicon developed for PCKimmo  Englex    NOMLEX   Longman Dictionary of Contemporary English 2For a deeper discussion and classification of Porter stemmers errors  see  ,Neutral
 introduced the averaged perceptron  as a way of reducing overfitting  and it has been shown to perform better than the nonaveraged version on a number of tasks ,Positive
4 Building Noun Similarity Lists A lot of work has been done in the NLP community on clustering words according to their meaning in text  ,Neutral
One important application of bitext maps is the construction of translation lexicons  and  as discussed  translation lexicons are an important information source for bitext mapping This method is described hereafter  while the subsequent steps  that use deeper  rulebased  levels of knowledge  are implemented into the ARIOSTO_LEX lexical learning system  described in  ,Neutral
There are several distance measures suitable for this purpose  such as the mutual information   the dice coefficient   the phi coefficient   the cosine measure  and the confidence  ,Positive
"On the other hand, Snow et al. (2008) illustrate how AMT can be used to collect data in a “fast and cheap” fashion, for a number of NLP tasks, such as word sense disambiguation. They go a step further and model the behavior of their annotators to reduce annotator bias. ",Positive
Recent innovations have greatly improved the efficiency of language model integration through multipass techniques  such as forest reranking   local search   and coarsetofine pruning  However  since most of statistical translation models  are symmetrical  it is relatively easy to train a translation system to translate from English to Chinese  except that weneed to train aChinese language model from the Chinese monolingual data ,Positive
While both  and  propose models which use the parameters of the generative model but train to optimize a discriminative criteria  neither proposes training algorithms which are computationally tractable enough to be used for broad coverage parsing Turneys method did not work well although they reported 80  accuracy in  ,Negative
At the same time  we believe our method has advantages over the approach developed initially at IBM  for training translation systems automatically Despite relying on a the same concept  our approach outperforms BE in most comparisons  and it often achieves higher correlations with human judgments than the stringmatching metric ROUGE  ,Negative
"In (Riley, 1989), Riley describes a decision-tree based approach to the problem. His performance on /he Brown corpus is 99.8%, using a model learned t'rom a corpus of 25 million words. ",Positive
"Most recently, Yarowsky used an unsupervised learning procedure to perform WSD (Yarowsky, 1995), although this is only tested on disambiguating words into binary, coarse sense distinction. The effectiveness of unsupervised learning on disambiguating words into the refined sense distinction of WoRBNET needs to be further investigated. ",Positive
Alignment spaces can emerge from generative stories   from syntactic notions   or they can be imposed to create competition between links  ,Neutral
The f are trained using a heldout corpus using maximum BLEU training  ,Neutral
"We used SVDPACK to compute the singular value decompositions described in this paper (Berry, 1992). ",Neutral
"As previously indicated, the weight-based scheme of L&L suggests MaxEnt modeling (Berger et al., 1996) as a particularly natural choice for a machine learning approach. ",Positive
"To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al., 2008).",Neutral
"Another major challenge in USP learning is the summation in the likelihood, which is over all possible semantic parses for a given dependency tree. Even an efficient sampler like MC-SAT (Poon and Domingos, 2006), as used in Poon & Domingos (2008), would have a hard time generating accurate estimates within a reasonable amount of time. ",Negative
"Many state-of-the-art machine translation (MT) systems over the past few years (Och and Ney, 2002; Koehn et al., 2003; Chiang, 2007; Koehn et al., 2007; Li et al., 2009) rely on several models to evaluate the “goodness” of a given candidate translation in the target language. ",Neutral
"The German NEGRA corpus consists of 20,000 sentences (355,000 tokens) of newspaper texts (Frank-furter Rundschau) that are annotated with parts-of-speech and predicate-argument structures (Skut et al., 1997).",Neutral
As reported in   parameter averaging can effectively avoid overfitting Several representations to encode region information are proposed and examined  ,Neutral
"Traditional 3-gram and 5-gram string LMs were trained on the English side of the parallel data plus the English Gigaword corpus V3.0 in a way described in (Bulyko et al., 2007). ",Neutral
Following   we used sections 018 of the Wall Street Journal  WSJ  corpus for training  sections 1921 for development  and sections 2224 for final evaluation ,Neutral
"In our experiments, two in-house developed systems are used to validate our method. The first one (SYS1) is a system based on the hierarchical phrase-based model as proposed in (Chiang, 2005). ",Neutral
Chiang  distinguishes statistical MT approaches that are syntactic in a formal sense  going beyond the nitestate underpinnings of phrasebased models  from approaches that are syntactic in a linguistic sense  ie taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking1 The two forms of syntactic modeling are doubly dissociable  current research frameworks include systems that are nite state but informed by linguistic annotation prior to training  eg     and also include systems employing contextfree models trained on parallel text without bene t of any prior linguistic analysis  eg ,Neutral
 and Wiebe  2000  focused on learning adjectives and adjectival phrases and Wiebe et al A number of alignment techniques have been proposed  varying from statistical methods  to lexical methods  ,Neutral
Note that the algorithm from  was designed for discriminatively training an HMMstyle tagger ,Neutral
Such studies follow the empiricist approach to word meaning summarized best in the famous dictum of the British 3 linguist JR Firth You shall know a word by the company it keeps Firth 1957 p 11 Context similarity has been used as a means of extracting collocations from corpora eg by Church & Hanks 1990 and by Dunning 1993 of identifying word senses eg by Yarowski 1995 and by Schutze 1998 of clustering verb classes eg by Schulte im Walde 2003 and of inducing selectional restrictions of verbs eg by Resnik 1993 by Abe & Li 1996 by Rooth et al,Neutral
"We implement the expectation and variance semirings in Joshua (Li et al., 2009a), and demonstrate their practical benefit by using minimum-risk training to improve Hiero (Chiang, 2007). ",Positive
For the chunk part of the code  we adopt the Inside  Outside  and Between  IOB  encoding originating from  ,Neutral
"For example, the Markov model tagger used in the comparison of (van Halteren et al., 1998) yielded worse results than all other taggers. ",Negative
While several methods have been proposed to automatically extract compounds   we know of no successful attempt to automatically make classes of compounds Many approaches for POS tagging have been developed in the past  including rulebased tagging   HMM taggers   maximumentropy models   cyclic dependency networks   memorybased learning   etc All of these approaches require either a large amount of annotated training data  for supervised tagging  or a lexicon listing all possible tags for each word  for unsupervised tagging  ,Negative
"The English Constraint Grammar Parser, ENGCG  Voutilainen et al., 1992; Karlsson el al., 1994), is based on Constraint Grammar, a parsing framework proposed by Fred Karlsson (1990). ",Neutral
"The recent approach for editing extracted text spans (Jing and McKeown, 2000) may also produce improvement for our algorithm.",Positive
Statistical disambiguation such as  for PPattachment or  for generative parsing greatly improve disambiguation  but as they model by imitation instead of by understanding  complete soundness has to remain elusive ,Negative
"From the results of CoNLL-2008 shared task, the top system by (Johansson and Nugues, 2008) also used two different subsystems to handle verbal and nominal predicates, respectively.",Neutral
"The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a). ",Neutral
Lexical chains—sequences of semantically related words—are tightly connected tothe lexical cohesive structure of the text and have been shown to be useful for determin-ing which sentences are important for single-document summarization (Barzilay andElhadad 1997; Silber and McCoy 2002).,Neutral
It is shown in Mortensen et al. (2005) that the expected size of D is a small fraction of the total number of events and its space usage comprises less than O(|S|) bits with high probability.,Neutral
"The MaxEnt model in (Ittycheriah and Roukos 2007) was optimized globally, so that it could better employ the distribution of the training data. However, one has to filter the training data according to the test data to get competitive performance with this model 1 . In addition, the filtering method causes some practical issues. First, such methods are not suitable for real MT tasks, especially for applications with streamed input, since the model has to be retrained with each new input sentence or document and training is slow.  ",Negative
Many strategies have been proposed to integrate morphology information in SMT  including factored translation models   adding a translation dictionary containing inflected forms to the training data   entirely replacing surface forms by representations built on lemmas and POS tags   morphemes learned in an unsupervised manner   and using Porter stems and even 4letter prefixes for word alignment  ,Neutral
2 Three New Features for MT Evaluation Since our sourcesentence constrained ngram precision and discriminative unigram precision are both derived from the normal ngram precision  it is worth describing the original ngram precision metric  BLEU  ,Neutral
Words are encoded through an automatic clustering algorithm  while tags  labels and extensions are normally encoded using diagonal bits ,Neutral
"Rule ordering issue has been discussedby Voutilainen(1994), but he has recently indicated 1that insensitivity to rule ordering is not a propertyof their system (although Voutilainen(1995a) statesthat it is a very desirable property) but rather isachieved by extensively testing and tuning the rules.",Neutral
  makes a similar point  noting that for reviews  the whole is not necessarily the sum of the parts   Identifying transliteration pairs is an important component in many linguistic applications which require identifying outofvocabulary words  such as machine translation and multilingual information retrieval  ,Neutral
"A morpheme network of each input sentence was generated with Juman (Mat- sumoto et al., 1994) and the credit factor was attached to each branch as described above. ",Neutral
"To the best of our knowledge no system was able to reproduce the successful results of (Swier and Stevenson, 2004) on the PropBank Roleset. ",Positive
"In the scenario that N-best lists are available from individual systems for combination, the weight of each hypothesis can be computed based on its rank in the N-best list (Rosti et. al. 2007a). ",Neutral
"A major difference between the phrase features used in this work and those used elsewhere is that we do not assume that phrases segment into disjoint parts of the source and target sentences (Koehn et al., 2003); they can overlap.",Neutral
In the II  OO  and OI scenarios   succeeded in improving the parser performance only when a reranker was used to reorder the 50best list of the generative parser  with a seed size of 40K sentences Maximum Entropy Maximum entropy classiflcation  MaxEnt  or ME  for short  is an alternative technique which has proven efiective in a number of natural language processing applications  ,Positive
For the current work  the Loglikelihood coefficient has been employed   as it is reported to perform well among other scoring methods  ,Positive
As a side product  we find empirical evidence to suggest that the effectiveness of rule lexicalization techniques  and parent annotation techniques  is due to the fact that both lead to a reduction in perplexity in the automata induced from training corpora ,Positive
"Considering the size of most documents, the linear nature of this algorithm makes it usable for generalized summarization of large documents (Silber and McCoy 2000). ",Neutral
"For example, while the TnT tagger performs at 97% accuracy on known words in the Treebank, the accuracy drops to 89% on unknown words (Brandts, 2000). ",Neutral
"As is well known, the extractive summary that has been extensively studied from the early days of summarization history (Luhn, 1958) suffers from various drawbacks. ",Negative
For the results in this paper  we have used Pointwise Mutual Information  PMI  instead of IBM Model 1   since  found it to be as effective on Springer  but faster to compute Numbers in the table correspond to the percentage of experiments in which the condition at the head of the column was true  for example figure in the first row and first column means that for 989 percent of the language pairs the BLEU score for the bidirectional decoder was better than that of the forward decoder  proach   ,Negative
"Research is often described as a problem-solving activity (Jordan 1984; Trawinski 1989; Zappen 1983). Three information types can be expected to occur in any research article: problems (research goals), solutions (methods), and results. In many disciplines, particularly the experimental sciences, this problem-solution structure has been crystallized in a fixed presentation of the scientific material as introduction, method, result and discussion (van Dijk 1980). ",Neutral
So far  SCL has been applied successfully in NLP for PartofSpeech tagging and Sentiment Analysis  ,Positive
Measures of attributional similarity have been studied extensively  due to their applications in problems such as recognizing synonyms   information retrieval   determining semantic orientation   grading student essays   measuring textual cohesion   and word sense disambiguation  ,Neutral
"Note that the sense definitions used in this data set are those from Longman Dictionary of Contemporary English (LDOCE) (Procter, 1978). ",Neutral
"There is now a large body of past work on WSD. Early work on WSD, such as (Kelly and Stone, 1975; Hirst, 1987) used hand-coding of knowledge to perform WSD. The knowledge acquisition process is laborious.",Negative
"The morphological analyser is based on a lexical transducer (Karttunen et al., 1992). The transducer maps each inflected surface form of a word to its canonical lexical form followed by the appropriate morphological tags. ",Neutral
Current treebased models that integrate linguistics and statistics  such as GHKM   are not able to generalize well from a single phrase pair Our syntacticrelationbased thesaurus is based on the method proposed by   although Hindle did not apply it to information retrieval ,Negative
"Several approaches have been proposed to construct automatic taggers.Most work on statistical methods has used n-gram models or Hidden Markov Model-basedtaggers (e.g. Church, 1988; DeRose, 1988; Cutting et al. 1992; Merialdo, 1994, etc.).",Neutral
Although this method is comparatively easy to be implemented  it just achieves the same performance as the synchronous binarization method  for syntaxbased SMT systems Among the applications of collocational analysis for lexical acquisition are the derivation of syntactic disambiguation cues Basili et al 1991 1993a Hindle and Rooths 19911993 Sekine 1992 Bogges et al 1992 sense preference Yarowski 1992 acquisition of selectional restrictions Basili et al 1992b 1993b Utsuro et al 1993 lexical preference in generation Smadjia 1991 word clustering Pereira 1993 Hindle 1990 Basili et al 1993c etc In the majority of these papers even though the precedent or subsequent statistical processing reduces the number of accidental associations very large corpora 10000000 words are necessary to obtain reliable data on a large enough number of words,Negative
"We carried out an error analysis to gain further insight into this question. To address the data-sparsity issue, we employed the technique used in Keller and Lapata (2003, K&L) to get a more robust approximation of predicate-argument counts. ",Positive
In   anotherstateoftheartWSDengine  acombination of naive Bayes  maximum entropy  boosting and Kernel PCA models  is used to dynamically determine the score of a phrase pair under consideration and  thus  let the phrase selection adapt to the context of the sentence ,Positive
 shows that baseNP recognition  Fz  I  920  is easier than finding both NP and VP chunks  Fz    88  and that increasing the size of the training data increases the performance on the test set ,Positive
Other work in automated support verb discovery using bilingual dictionaries as a source has been reported in Fontenelle (1993).,Neutral
Sentencelevel subjectivity detection  where training data is easier to obtain than for positive vs negative classification  has been successfully performed using supervised statistical methods alone  or in combination with a knowledgebased approach  ,Positive
"This algorithm has been applied to anumber of natural language problems, including part-of-speech tagging, prepositionalphrase attachment disambiguation, and syntactic parsing (Brill 1992; Brill 1993a; Brill1993b; Brill and Resnik 1994; Brill 1994).",Neutral
"Further work can be done on the semantic verb clusters described in section 4.2. Klavans and Kan (1998), who use verb clusters for document classification according to genre, observe that verb information is rarely used in current practical natural language Applications. ",Neutral
There are only a few successful studies  such as  for chunking and  on constituency parsing ,Positive
Metadiscourse is ubiquitous in scientific writing: Hyland (1998) found a metadiscourse phrase on average after every 15 words in running text. ,Positive
A synchronous 363 binarization method is proposed in  whose basic idea is to build a leftheavy binary synchronous tree  with a lefttoright shiftreduce algorithm ,Neutral
For the give source text  S  it finds the most probable alignment set  A  and target text  T  Aa SaTpSTp       1  Brown  proposed five alignment models  called IBM Model  for an EnglishFrench alignment task based on equa68 tion  1  ,Neutral
"Syntactic analysis of texts (such as Part-Of-Speech tagging and syntactic parsing) is an example of such a generic analysis, and has proved useful in applications ranging from machine translation (Marcu et al., 2006) to text mining in the bio-medical domain (Cohen and Hersh, 2005).",Neutral
"Manually encoding all these variations into the grammar is tedious and error-prone. Supervised semantic parsing addresses this issue by learning to construct the grammar automatically from sample meaning annotations (Mooney, 2007). ",Positive
"Fillmore (1968) introduced semantic structures called semantic frames, describing abstract actions or common situations (frames) with common roles and themes (semantic roles). Inspired by this idea different resources were constructed, including FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005).",Positive
"In addition to sentence fusion, compressionalgorithms (Chandrasekar, Doran, and Bangalore 1996; Grefenstette 1998; Mani, Gates,and Bloedorn 1999; Knight and Marcu 2002; Jing and McKeown 2000; Reizler et al. 2003)and methods for expansion of a multiparallel corpus (Pang, Knight, and Marcu 2003)are other instances of such methods.",Neutral
2 Parsing Model The Berkeley parser  is an efficient and effective parser that introduces latent annotations  to refine syntactic categories to learn better PCFG grammars However  to be more expressive and flexible  it is often easier to start with a general SCFG or treetransducer  ,Positive
32 Rare Word Accuracy For these experiments  we use the Wall Street Journal portion of the Penn Treebank  ,Neutral
1 Introduction For statistical machine translation  SMT   phrasebased methods  and syntaxbased methods  outperform wordbased methods  To our knowledge no systems directly address Problem 1  instead choosing to ignore the problem by using one or a small handful of reference derivations in an nbest list   or else making local independence assumptions which sidestep the issue  ,Negative
"We examine the effectiveness of Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for this task, a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis. The system used in this study is Alpino, a wide-coverage Stochastic Attribute Value Grammar (SAVG) for Dutch (van Noord and Malouf, 2005; van Noord, 2006). ",Positive
"(Teller and Batchelder, 1994) proposed a very naive probabilistic word segmentation method for Japanese, based on character type informationand hiragana bigram frequencies. They claimed 98% word segmentation accuracy, while we claim 94.7%. However, their evaluation method is veryoptimistic, and completely different from ours. They count an error only when the system segmentation violates morpheme boundaries. In other words, they count an error only when the system segmentation is not acceptable to human judgement while we count an error whenever the system segmentation does not exactly match the corpus segmentation, even if it is inconsistent.",Negative
"Discriminative models have been found to outperform generative models for many different tasks including SRL (Lim et al., 2004). For this reason we also employ discriminative models here. ",Negative
An important contribution to interactive CAT technology was carried out around the TransType  TT  project  We conclude by noting that English language models currently used in speech recognition  and automated language translation  are much more powerful  employing  for example  7gram word models  not letter models  trained on trillions of words ,Positive
"Concurrent work has used approximate counting schemes based on Morris (1978) to estimate in small space frequencies over a high volume input text stream (Van Durme and Lall, 2009; Goyal et al., 2009).",Positive
"The scheme must provide representational means for all phenomena occurring in texts. Disambiguation is based on human processing skills (cf. (Marcus et at., 1994), (Sampson, 1995), (Black et al. , 1996)). ",Neutral
"Turney (2008) is the first, to the best of our knowledge, to raise the issue of a unified approach. In particular, he treats synonymy and association as special cases of relational similarity ",Positive
"To discover useful features, we exploit the concept of Association Rules (AR) (R. Agrawal and Swami, 1993; Srikant and Agrawal, 1997), which is originally proposed in Data Mining research field to identify frequent itemsets in a large Database.",Positive
"Automatic summarization offers potential help in managing such results; however, the most popular approach, extraction, faces challenges when applied to multidocument summarization (McKeown et al., 2001). ",Neutral
"Abstraction, on the other hand, relies either on linguistic processing followed by structural compaction (Mani et al., 1999) or on interpretation of the source text into a semantic representation, which is then condensed to retain only the most important information asserted in the source. ",Neutral
"The tagger has a close relative in (Koskenniemi, 1990; Koskenniemi et al., 1992; Voutilalnen and Tapanainen, 1993) where the rules are represented as finite-state machines that are conceptually intersected with each other. ",Neutral
"Liberman and Church suggest in (Liberlnan and Church, 1992) that. a system could be quickly built to divide newswire text into sentences with a nearly negligible error rate. but, do not actually build such a system. ",Negative
Our approach most closely resembles the work of Fürstenau and Lapata (2009) who automatically expand a small training set using an automatic dependency alignment of unlabeled sentences.,Positive
"Some of the consequences  of research in lexical semantics, with particular attention to natural language processing, are discussed by Pustejovsky et al. (1993) and Nirenburg and Raskin (1996). Implemented systems often draw on the information contained in WordNet (Fellbaum, 1998). ",Neutral
"One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate β i ’s from the document to be abstracted and rely on recent information ordering techniques to sort the β i fragments (Lapata, 2003). ",Positive
"We call this model a chunk-based dialog model (Bangalore et al., 2006). The chunkbased model has limitations. For example, dominance relations among subtasks are important for dialog processes such as anaphora resolution (Grosz and Sidner, 1986). Also, the chunkbased model is representationally inadequate for center-embedded nestings of subtasks, which do occur in our domain, although less frequently than the more prevalent “tail-recursive” structures.",Negative
"Concept similarity is often measured by vectors of co-occurrence with context words that are typed with dependency information (Lin, 1998; Curran and Moens, 2002). ",Neutral
"Compared to previous approaches (Raghavan and Allan, 2007), our method can be used for both classification and structured tasks, and the feature query selection methods we propose perform better. ",Negative
Their algorithm was further modified and applied to the German biographies by Filippova and Strube (2008),Positive
"Linguistic features like tense and voice often correlate with rhetorical zones; Biber (1995) and Riley (1991) show correlation of tense and voice with prototyp- ical section structure (“method,” “introduction”). ",Neutral
"The model used here for sentence-boundary detection is based on the maximum entropy model used for POS tagging in ( Ratnaparkhi , 1996). ",Neutral
"We obtained 47,025 50-dimensional reduced vectors from the SVD and clustered them into 200 classes using the fast clustering algorithm Buckshot (Cutting et al., 1992)",Positive
"Rhetorical Structure Theory (RST) [Mann & Thompson, 1988] has contributed a great deal to the understanding of the discourse of written Documents. ",Positive
"Previous work has mainly used WordNet (Fellbaum, 1998) to extend FrameNet. For example, Burchardt et al. (2005) apply a word sense disambiguation system to annotate predicates with a WordNet sense and hyponyms of these predicates are then assumed to evoke the same frame. Johansson and Nugues (2007) treat this problem as an instance of supervised classification. ",Positive
With all but two formats IBIIG achieves better FZ  l rates than the best published result in  A maximum entropy approach has been applied to partofspeech tagging before   but the approach s ability to incorporate nonlocal and nonHMMtaggertype evidence has not been fully explored ,Negative
"When output variables are structured, annotation can be particularly difficult and time consuming. For example, when training a conditional random field (Lafferty et al., 2001) to extract fields such as rent , contact , features , and utilities from apartment classifieds, labeling 22 instances (2,540 tokens) provides only 66.1% accuracy. ",Neutral
"We also implemented a version of Hobbs’s (1978) well-known pronoun interpretation algorithm as a baseline, in which no machine learning is involved. ",Neutral
In  it was observed that a significant percent of the queries made by a user in a search engine are associated to a repeated search Output sequence optimization Rather than basing classifications only on model parameters estimated from cooccurrences between input and output symbols employed for maximizing the likelihood of pointwise singlelabel predictions at the output level  classifier output may be augmented by an optimization over the output sequence as a whole using optimization techniques such as beam searching in the space of a conditional markov models output  or hidden markov models  ,Neutral
 Introduction Phrasebased method  and syntaxbased method  represent the stateoftheart technologies in statistical machine translation  SMT  ,Positive
For example   collected reviews from a movie database and rated them as positive  negative  or neutral based on the rating  eg  number of stars  given by the reviewer The earliest work in this direction are those of          and   established that it is important to tune  the tradeoff between Precision and Recall  to maximize performance ,Neutral
Schütze ( 1993; 1995)proposes two distinct methods by which ambiguity may be resolved. ,Positive
"Rhetorical zones appear in typical positions in the article, as scientific argumentation follows certain patterns (Swales 1990).",Neutral
Recently socalled reranking techniques  such as maximum entropy models  and gradient methods   have been applied to machine translation  MT   and have provided significant improvements ,Positive
"Corpus-derived distributional semantic spaces have proved valuable in tackling a variety of tasks, ranging from concept categorization to relation extraction to many others (Sahlgren, 2006; Turney, 2006; Padó and Lapata, 2007). ",Neutral
One popular and statistically appealing such measure is LogLikelihood  LL   ,Positive
They were based on mutual information   conditional probabilities   or on some standard statistical tests  such as the chisquare test or the loglikelihood ratio  ,Positive
Morphosyntacticinformationhas in fact been shown to significantlyimprove the extractionresults  ,Neutral
For comparing the sentence generator sample to the English sample  we compute loglikelihood statistics  on neighboring words that at least cooccur twice ,Neutral
We use the IBM Model 1   uniform distribution  and the Hidden Markov Model  HMM  firstorder dependency    to estimate the alignment model ,Neutral
In the future  we will experiment with semantic  rather than positional  clustering of premoditiers  using techniques such as those proposed in  ,Neutral
It is an online training algorithm and has been successfully used in many NLP tasks  such as POS tagging   parsing   Chinese word segmentation   and so on ,Positive
"The LT POS tagger is reported to perform at 93.6-94.3% accuracy on known words and at 87.7-88.7% on unknown words using a cascading unknown word “guesser” (Mikheev, 1997). ",Neutral
 and Collins and Duffy  2002  rerank the top N parses from an existing generative parser  but this kind of approach 1Dynamic programming methods  can sometimes be used for both training and decoding  but this requires fairly strong restrictions on the features in the model Ever since its introduction in general  and in computational linguistics   many researchers have pointed out that there are quite some problems in using  eg ,Negative
Besides the the casesensitive BLEU4  used in the two experiments  we design another evaluation metrics Reordering Accuracy  RAcc  for forced decoding evaluation ,Neutral
The formaliza-tion of this notion and an algorithm for computing the composed transducer are wellknown and are described originally by Elgot and Mezei (1965).,Positive
"The tag N-gram probabilities, and both the scheme and its application to these two tasks are described in detail in (Samuelsson 1996), where it was also shown to compare favourably to (deleted) interpolation, see (Jelinek and Mercer 1980), even when the back-off weights of the latter were optimal. ",Neutral
Recent several years have witnessed the rapid development of system combination methods based on confusion networks  eg     which show stateoftheart performance in MT benchmarks ,Positive
This idea of employing ngram cooccurrence statistics to score the output of a computer system against one or more desired reference outputs has its roots in the BLEU metric for machine translation  and the ROUGE  metric for summarization ,Neutral
"A hypergraph or “packed forest” (Gallo et al., 1993; Klein and Manning, 2004; Huang and Chiang, 2005) is a compact data structure that uses structure-sharing to represent exponentially many trees in polynomial space. ",Neutral
Because it is not feasible here to have humans judge the quality of many sets of translated data  we rely on an array of well known automatic evaluation measures to estimate translation quality  BLEU  is the geometric mean of the ngram precisions in the output with respect to a set of reference translations ,Positive
The classbased kappa statistic of  can not be applied here  as the classes vary depending on the number of ambiguities per entry in the lexicon While the amount of parallel data required to build such systems is orders of magnitude smaller than corresponding phrase based statistical systems   the variety of linguistic annotation required is greater ,Negative
3 Extending Bleu and Ter with Flexible Matching Many widely used metrics like Bleu  and Ter  are based on measuring string level similarity between the reference translation and translation hypothesis  just like Meteor Most of them  however  depend on finding exact matches between the words in two strings ,Positive
 focus on alignment and do not present MT results  while May and Knight  2007  takesthesyntacticrealignmentasaninputtoanEM algorithm where the unaligned target words are insertedintothetemplatesandminimumtemplatesare combinedintobiggertemplates  ,Negative
"Motivated by the feature query selection method of Tandem Learning (Raghavan and Allan, 2007) (see Section 4.2 for further discussion), we consider a feature selection metric similarity (sim) that is the maximum similarity to a labeled feature, weighted by the log count of the feature. ",Positive
In Statistical Machine Translation  SMT   recent work shows that WSD helps translation quality when the WSD system directly uses translation candidates as sense inventories  Most semiautomated approaches have met with limited success  and supervised learning models have tended to outperform dictionarybased classi cation schemes  ,Positive
"We evaluate translation output using case-insensitive BLEU (Papineni et al., 2001), as provided by NIST, and METEOR (Banerjee and Lavie, 2005), version 0.6, with Porter stemming and WordNet synonym matching.",Neutral
"The task can be performed by a chunk parser that is equipped with an appropriate finite state grammar (Abney, 1996). ",Neutral
In general  these authors have found that existing lexicalized parsing models for English  do not straightforwardly generalize to new languages  this typically manifests itself in a severe reduction in parsing performance compared to the results for English ,Negative
"In traditional active learning (Settles, 2009), the machine queries the user for only the labels of instances that would be most helpful to the machine. ",Positive
"These were augmented with automatically labeled sentences from the BNC which we used as our expansion corpus. FrameNet sentences were parsed with RASP (Briscoe et al., 2006).",Neutral
1 Introduction Texttotext generation is an emerging area of research in NLP  ,Neutral
Statistical parsers have been developed for TAG   LFG   and HPSG   among others ,Neutral
"In particular, we need to develop a backoff strategy for unseen pairs in the relational similarity tasks, that, following Turney (2006), could be based on constructing surrogate pairs of taxonomically similar words found in the CxLC space ",Positive
Incremental topdown and leftcorner parsers have been shown to effectively  and efficiently  make use of nonlocal features from the leftcontext to yield very high accuracy syntactic parses   and we will use such rich models to derive our scores Erk  compared a number of techniques for creating similarword sets and found that both the Jaccard coefficient and  s informationtheoretic metric work best ,Positive
Although the BLEU  score from Finnish to English is 218  the score in the reverse direction is reported as 130 which is one of the lowest scores in 11 European languages scores  ,Neutral
"The processing time of the Viterbi algorithm (Rabiner, 1989) can be reduced by introducing a beam Search. ",Neutral
An extrinsic evaluation (Teufel 2001) shows that the output of our system is al-ready a useful document surrogate in its own right.,Neutral
CIT  ,Neutral
"Due to the substantial differences between existing models of constituent structure, tile question arises of how the theory , requirement can be satisfied. At this point the importance of the underlying argument : is emphasised (cf. (Lehmaim et al., 1996), (Marcus et al., 1994), (Sampson, 1995)). ",Neutral
"To contrast, [Harabagiu, 1999] concentrated on the derivation of a model that can establish coherence relations in a text without relying on cue phrases. ",Neutral
"This in turn allows it to correctly answer many more questions than systems based on TextRunner (Banko et al., 2007) and DIRT (Lin and Pantel, 2001).",Neutral
More recently  phrasebased models  have been proposed as a highly successful alternative to the IBM models Models that can handle nonindependent lexical features have given very good results both for partofspeech and structural disambiguation  ,Positive
"Hybrid approaches such as extracting phrases instead of sentences and recombining these phrases into salient text have been proposed (Barzilay, McKeown, and Elhadad 1999). ",Positive
First  we trained a finitestate shallow parser on base phrases extracted from the Penn Wall St Journal  WSJ  Treebank  ,Neutral
We tokenized sentences using the standard treebank tokenization script  and then we performed partofspeech tagging using MXPOST tagger  ,Neutral
"We plan to explore the use of the recently developed related methods of Bellare et al. (2009), Graça et al. (2008), and Liang et al. (2009) in future work. ",Neutral
"We therefore weigh each feature with its information gain;a number expressing the average amount of reduction of training set information entropywhen knowing the value of the feature (Daelemans & van de Bosch, 1992, Quinlan, 1993;Hunt et al. 1966) (Equation 3).",Neutral
a22 a14 is the sufficient statistic of a16 a14 Then  we can rewrite a2a24a3 a10a27 a42a7 a25 as  a5a7a6a9a8a11a10 a23 a3 a10 a7 a15 a27 a25a18a17a26a25 a12a28a27 a5a7a6a29a8a30a10 a23 a3 a10 a7 a15 a27 a25a18a17 3 Loss Functions for Label Sequences Given the theoretical advantages of discriminative models over generative models and the empirical support by   and that CRFs are the stateoftheart among discriminative models for label sequences  we chose CRFs as our model  and trained by optimizing various objective functions a31 a3 a10a36 a25 with respect to the corpus a36 The application of these models to the label sequence problems vary widely ,Neutral
"The Maximum Entropy model (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997) is a conditional model that assigns a probability to every possible parse ω for a given sentence s.",Neutral
For the IBM models defined by a pioneering paper   a decoding algorithm based on a lefttoright search was described in  Introduction The emergence of phrasebased statistical machine translation  PSMT   has been one of the major developments in statistical approaches to translation ,Positive
According to our experience  the best performance is achieved when the union of the sourcetotarget and targettosource alignment sets  is used for tuple extraction  some experimental results regarding this issue are presented in Section 422  ,Positive
The search across a dimension uses the efficient method of  In the hierarchical phrasebased model   and an inversion transduction grammar  ITG    the problem is resolved by restricting to a binarized form where at most two nonterminals are allowed in the righthand side ,Positive
"We computed the weight (strength of association) for all the tuples extracted in this way using the local MI measure (Evert, 2005), that is theoretically justified, easy to compute for triples and robust against overestimation of rare events. ",Neutral
In the supervised setting  a recent paper by  shows that  using a very simple feature augmentation method coupled with Support Vector Machines  he is able to effectively use both labeled target and source data to provide the best results in a number of NLP tasks Constraining learning by using document boundaries has been used quite effectively in unsupervised word sense disambiguation  ,Positive
SmadjaFrank1993The experimental results in  show a negative impact on the parsing accuracy from too long dependency relation k   P  A  P  E   3  1P  E   suggests that the units over which the kappa statistic is computed affects the outcome ,Neutral
In the thriving area of research on automatic analysis and processing of product reviews   little attention has been paid to the important task studied here assessing review helpfulness In such tasks  feature calculation is also very expensive in terms of time required  huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability p  f e  and reverse translation probability p  e f   ,Negative
For English  we use three stateoftheart taggers  the taggers of  and  in Step   and the SVM tagger  in Step 3 We used the average perceptron algorithm of  in our experiments  a variation that has been proven to be more effective than the standard algorithm shown in Figure 2 ,Positive
The notion of incrementally merging classes of lexical items is intuitively satisfying and is explored in detail in  Interand Intraannotator agreement We measured pairwise agreement among annotators usingthekappacoefficient  K  whichiswidelyused in computational linguistics for measuring agreement in category judgments  ,Positive
This increase of probabilities is defined as multiplicative change  N  as follows   N   P  E Tprime  \/ P  E T   2  The main innovation of the model in  is the possibility of adding at each step the best relation N   Ri  j  as well as N  I  Ri  j  that is Ri  j with all the relations by the existing taxonomy However  as also pointed out by   this observation does not hold uniformly over all possible cooccurrences of two words ,Positive
Och (2003) shows that setting those weights should take into account the evaluation metric by which the MT system will eventually be judged. ,Neutral
The local dependencies between sentiment labels on sentences is similar to the work of  where soft local consistency constraints were created between every sentence in adocument and inference wassolved using a mincut algorithm ,Neutral
The last two counts  CAUS and ANIM  were performed on a 29million word parsed corpus  gall Street Journal 1988  provided by Michael Collins   ,Neutral
Introduction We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms  often based on grammatical formalisms If we view MT as a machine learning problem  features and formalisms imply structural independence assumptions  which are in turn exploited by efficient inference algorithms  including decoders  ,Positive
However  the pb features yields no noticeable improvement unlike in prefect lexical choice scenario  this is similar to the findings in  ,Neutral
"In (Post and Gildea, 2008; Shen et al., 2008), target trees were employed to improve the scoring of translation theories. ",Neutral
From wordlevel alignments  such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages   or with the the wordlevel alignments alone without reference to external syntactic analysis   which is the scenario we address here ,Neutral
"Resnik and Diab (2000) present yet other measures of verb similarity, which could be used to arrive at a more data-driven definition of verb classes. ",Positive
1 word w 2 word bigram w1w2 3 singlecharacter word w 4 a word of length l with starting character c 5 a word of length l with ending character c 6 spaceseparated characters c1 and c2 7 character bigram c1c2 in any word 8 the first  last characters c1  c2 of any word 9 word w immediately before character c 10 character c immediately before word w 11 the starting characters c1 and c2 of two consecutive words 12 the ending characters c1 and c2 of two consecutive words 13 a word of length l with previous word w 14 a word of length l with next word w Table 1 Feature templates for the baseline segmentor 2 The Baseline System We built a twostage baseline system using the perceptron segmentation model from our previous work Zhang and Clark 2007 and the perceptron POS tagging model from Collins 2002,Neutral
"In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (zhang et al 2007a) and the impact of syntax on reordering is difficult to single out (Li et al., 2007).",Neutral
"This encoding was previously used for incremental sentence parsing by (Costa et al., 2001). With this method, there are many more choices of decision for the parser (195 decisions for our data) compared to the shift-reduce (32) and start-complete (82) methods. ",Positive
"According to Cremmins (1982), the last step in the human production of the sum-mary text is the “extracting” into “abstracting” step in which the extracted informa-tion will be mentally sorted into a preestablished format and will be “edited” usingcognitive techniques.",Neutral
2 Related Work Recently  several successful attempts have been made at using supervised machine learning for word alignment  Pr  cJ  aJ eI   p  J I   I    J Jproductdisplay j   p  cj eaj   8  32 Loglikelihood ratio The loglikelihood ratio statistic has been found to be accurate for modeling the associations between rare events  ,Positive
The first model  referred to as Maxent1 below  is a loglinear combination of a trigram language model with a maximum entropy translation component that is an analog of the IBM translation model 2  ,Neutral
"Collocation is generally defined as a group of words that occur together more often than by chance (McKeown and Radev, 2000).",Neutral
"In a previous comparison between GE and PDL (Mann and McCallum, 2008), GE outperformed PDL without the extra similarity features, whose construction may be problem-specific. ",Neutral
This can be the base of a principled method for detecting structural contradictions  6 Related Work Several works attempt to extend WordNet with additional lexical semantic information  ,Neutral
Johnson 1997 notes that this structure has a higher probability than the correct flat structure given counts taken from the treebank for a standard PCFGWe used a loglinear model with no Markov dependency between adjacent tags 3 and trained the parameters of the model with the perceptron algorithm  with averaging to control for overtraining  ,Neutral
Nowadays  most of the stateoftheart SMT systems are based on bilingual phrases  ,Positive
"The morphological FST is generated automatically from a large dictionary of French of about 90,000 entries and on-line corpora, such as Le Monde Newspapers (ECI, 1989 and 1990). ",Neutral
"In rule-based approaches, words are assigned a tag based on a set of rules and alexicon. These rules can either be hand-crafted (Garside et al., 1987; Klein & Simmons,1963; Green & Rubin, 1971), or learned, as in Hindle (1989) or the transformation-basederror-driven approach of Brill (1992).",Neutral
Instead  researchers routinely use automatic metrics like Bleu  as the sole evidence of improvement to translation quality ,Neutral
"We omit the proof here but point out that it is related to the unordered subtree matching problem which can be solved in linear time (Kilpelainen, 1992).",Neutral
His results may be improved if more sophisticated methods and larger corpora are used to establish similarity between words  Introduction Statistical parsing models have been shown to be successful in recovering labeled constituencies  and have also been shown to be adequate in recovering dependency relationships  ,Positive
Similarly  Structural Correspondence Learning  has proven to be successful for the two tasks examined  PoS tagging and Sentiment Classification A notable exception is the work of  ,Positive
Recently   have successfully constructed high quality and high coverage gazetteers from Wikipedia Introduction The maximum entropy model  has attained great popularity in the NLP field due to its power  robustness  and successful performance in various NLP tasks  The implementation of MEBA was strongly influenced by the notorious five IBM models described in  ,Positive
"In our experiments, we treated the 128 most frequent words in the corpus as function words, similar to Setiawan et al. (2007).",Positive
"Our method is similar to the work proposed by Hildebrand and Vogel (2008). However, except the language model and translation length, we only use intra-hypothesis n-gram agreement features as Hildebrand and Vogel did and use additional intra-hypothesis n-gram disagreement feature  as Li et al. (2009) did in their co-decoding Method. ",Positive
"In particular, because of problem 3, P(cld) would become an illegitimate value. In our experiments, as well as in Lewis' experiments (1992), P(cld ) ranges from 0 to more than 101°. ",Neutral
Levin  assumes that the syntactic realization of a verb s arguments is directly correlated with its meaning  cf ,Neutral
"They are useful indicators of overall importance (Pollock and Zamora 1975); they can also be relatively easily recognized with information extraction techniques (e.g., regular expressions).  ",Neutral
"We solve this optimization problem with a version of the branch-and-bound algorithm (Land and Doig, 1960). In general, this graph alignment problem is NP-hard (Klau, 2009) and usually solved approximately following a procedure similar to beam search. ",Positive
Headlexicalized stochastic grammars have recently become increasingly popular  Unsupervised algorit ~ m ~ such as  have reported good accuracy that rivals that of supervised algorithms is one of the most famous work that discussed learning polarity from corpus ,Positive
"The other term weighting scheme frequently used is TFIDF (Term Frequency Inverse Document Frequency) (Salton and Buckley, 1988). However, this technique needs a corpus for computing IDF score, causing the genre-dependent problem for generic text summarization task. ",Negative
"Other techniques exist for boosting thescore of longer phrases, such as adjusting the score of the phrase by a fixed factor thatdepends on the length of the phrase (Turney 1999).",Neutral
"Our work adds to a body of research learning deep models of language from evidence implicit in an agent’s interactions with its environment. It shares much of its motivation with co-training (Blum and Mitchell, 1998) in improving initial models by leveraging additional data that is easy to obtain. ",Neutral
Germann et al. (2004) identify two types of translation system error: model error and search Error,Neutral
"DeJean (1998), Hafer and Weiss (1974) follow a successor variety approach: the word is cut, if the number of distinct letters after a pre-specified sequence surpasses a threshold. ",Neutral
"The system was initially prototyped using the MUC-6 and MUC-7 data sets (Chinchor & Sundheim, 2003; Chinchor, 2001), using the standard partitioning of 30 texts for training and 20-30 texts for testing. Then, we developed and tested the system with the ACE 2003 Training Data corpus (Mitchell et al., 2003) ",Neutral
This method was shown to outperform the class based model proposed in  and can thus be expected to discover better clusters of words Although a rich literature covers bootstrapping methods applied to natural language problems  several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition ,Negative
In particular  previous work  has investigated the use of Markov random fields  MRFs  or loglinear models as probabilistic models with global features for parsing and other NLP tasks ,Neutral
This cost can often be substantial  as with the Penn Treebank  2 Previous work on Sentiment Analysis Some prior studies on sentiment analysis focused on the documentlevel classification of sentiment  where a document is assumed to have only a single sentiment  thus these studies are not applicable to our goal ,Negative
"Chang et al. (2007) present an algorithm for learning with constraints, but this method requires users to set weights by Hand. ",Neutral
"In the first set of experiments, we compare two settings of our UALIGN system with other aligners, GIZA++ (Union) (Och and Ney, 2003) and LEAF (with 2 iterations) (Fraser and Marcu, 2007). The GIZA++ aligner is based on IBM Model 4 (Brown et al., 1993).",Neutral
"In the next section we propose a novel method to learn word similarities, the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009).",Neutral
"In AI, the concept has appeared in several disciplines (from computer vision to robotics),using terminology such as similarity-based, example-based, memory-based, exemplar-based, case-based, analogical, lazy, nearest-neighbour, and instance-based (Stanfill andWaltz, 1986; Kolodner, 1993; Aha et al. 1991; Salzberg, 1990).",Neutral
"On the other hand, redundancy can be exploited to identify important and accurateinformation for applications such as summarization and question answering (Maniand Bloedorn 1997 Radev and McKeown 1998 Radev, Prager and Samn 2000 ClarkeCormack and Lynam 2001 Dumais et al. 2002 Chu-Carroll et al. 2003).",Neutral
"We continue a tradition of research that uses simple referential communication tasks to explore the organization and processing of human-c mputer and mediated human–human conversation, including recently (DeVault and Stone, 2007; Gergle et al., 2007; Healey and Mills, 2006; Schlangen and Fernández, 2007). ",Positive
Promising features might include those over source side reordering rules  or source context features  ,Positive
"This accuracy compares very favourably with results reported in (de Marcken, 1990; Weisehedel et al., 1993; Kempe, 1994) - for instance, to reach the recall of 99.3 %, the system by (Weischedel et al., 1993) has to leave as many as three readings per word in its output. ",Positive
Our approach to selectional preference is nearly identical to the one of Padó et al. (2007). We solve SAT analogies with a simplified version of the method of Turney (2006). ,Positive
Many methods for calculating the similarity have been proposed  ,Neutral
"In this tagger the dis ambiguation rules are applied in the same manner as the morphological rules in (Koskenniemi, 1983). Another relative is represented in (Roche and Schabes, 1994) which uses a single finitestate transducer to transform one tag into an Other.",Neutral
"There are good reasons for using such a hand-crafted, genre-specific verb lexicon instead of a general resource such as WordNet or Levin’s (1993) classes ",Neutral
Our experiments created translation modules for two evaluation corpora  written news stories from the Penn Treebank corpus  and spoken taskoriented dialogues from the TRAINS93 corpus  ,Neutral
"Raghavan and Allan (2007) also propose several methods for learning with labeled features, but in a previous comparison GE gave better results (Druck et al., 2008). Additionally, the generalization of these methods to structured output spaces is not straightforward. ",Negative
"The differences were jointly examined by the judges to see whether they were caused by inattention or by a genuine difference of opinion that could not be resolved by consulting the documentation that outlines the principles adopted for this grammatical representation (for the most part documented in (Karlsson et al., 1994)).  ",Neutral
In syntactic parse reranking supersenses have been used to build useful latent semantic features  In agreement with recent resuits on parsing with lexicalised probabilistic grammars   we find that statistics over lexical  as opposed to structural  features best correspond to human intuitivejudgments and to experimental findings ,Positive
"Please note that our approach is very different from other approaches to context dependent rule selection such as (Ittycheriah and Roukos, 2007) and (He et al., 2008).",Neutral
"Though the time complexity of the algorithm given by (Jiang and Ng, 2006) is also linear, it should assume all feature templates in the initial selected set ‘good’ enough and handles other feature template candidates in a strict incremental way. ",Neutral
"Categories that can be induced well (those characterized by local dependencies) could be in- put into procedures that learn phrase structure (e.g. (Brill and Marcus, 19925; Finch, 1993)).",Neutral
This study was motivated by the need to answer to the question of contentselection in text summarization (Sparck Jones 1993).,Neutral
"In order to push further this rule-extraction approach and according to our previous work (Dugast et al., 2007) (Dugast et al., 2008), the most promising would probably be the use of alternative meanings and a language model to decode the best translation in such a lattice. ",Positive
Some tasks can thrive on a nearly pure diet of unlabeled data  Machine Translation Experiments 4 Experimental Setting For our MT experiments  we used a reimplementation of Moses   a stateoftheart phrasebased system Motivation The success of Statistical Machine Translation  SMT  has sparked a successful line of investigation that treats paraphrase acquisition and generation essentially as a monolingual machine translation problem   Phrasebased ChinesetoEnglish MT The MT system used in this paper is Moses  a stateoftheart phrasebased system  eports a success rate of 96  disambiguating twelve words with two clear sense distinctions each one  ,Positive
"Compared to learning rule-based approaches such as the one by Brill (1992), a k-nn approach provides a uniform approach for all disambiguation tasks, more flexibility in the engineering of case representations, and a more elegant approach to handling of unknown words (see e.g. Cardie 1994). ",Negative
"The difference in performance as significant at p < 0.05, using stratified shuffling (Noreen, 1989). ",Neutral
In particular  previous work  has investigated the use of Markov random fields  MRFs  or loglinear models as probabilistic models with global features for parsing and other NLP tasks ,Neutral
 ie   ll  Lj   maz  zi  j  u   i  I where xi  j  u  E Qi and max  xi  j  u   is the highest score in the line of the matrix Qi which corresponds to the head word sense j n is the number of modifiers of the head word h at the current tree level  and k i Lj  j  l Lj where k is the number of senses of the head word h The reason why gj  I0  is calculated as a sum of the best scores  ll   rather than by using the traditional maximum likelihood estimate   Gah eta  ,Neutral
"A much simpler problem occurs in English, where for some wordsthe correct syntactic tag is necessary for pronunciation (Church 1988).",Neutral
 presented a thorough discussion on the Yarowsky algorithm ,Neutral
The tagger described in this paper is based on the standard Hidden Markov Model architecture  ,Neutral
"Semi-supervised learning has been suggested by many researchers as a solution to the annotation bottleneck (see (Chapelle et al., 2006; Zhu, 2005) for an overview), and has been applied successfully on a number of natural language processing tasks. ",Neutral
"As has been stressed at least since Chomsky’s early work (Chomsky, 1957), no matter how large a corpus is, if a phenomenon is productive there will always be new well-formed instances that are not in the corpus. ",Neutral
2 Detecting DiscourseNew Definite Descriptions 21 Vieira and Poesio Poesio and Vieira  carried out corpus studies indicating that in corpora like the Wall Street Journal portion of the Penn Treebank   around 52  of DDs are discoursenew   and another 15  or so are bridging references  for a total of about 6667  firstmention The distinction between lexical and relational similarity for word pair comparison is recognized by   hecallstheformer attributional similarity   though the methods he presents focus on relational similarity ,Neutral
It also differs from previous proposals on lexical acquisition using statistical measures such as  which either deny the prior existence of linguistic knowledge or use linguistic knowledge in ad hoc ways In pursuit of better translation  phrasebased models  havesignificantlyimprovedthe quality over classical wordbased models  ,Negative
"Existing treebanks of English ((Marcus et al., 1994), (Sampson, 1995), (Black et al., 1996)) contain conventional phrase structure trees augmented with annotations for discontinuous constituents. As this encoding strategy is not well-suited to a free word order language like German, we have focussed on a less surface-oriented level of description, most closely related to the LFG f-structure, and representationsused in dependency grammar.",Negative
"Thus, it is critical to understand which portion of a parse tree (i.e. structured feature space) is the most effective to represent a reordering instance. Motivated by the work of (Zhang et al., 2006), we here examine four cases that contain different sub-structures as shown in Fig. 1. ",Positive
Unsupervised methods have been developed for WSD  but despite modest success have not always been well understood statistically  In addition  uniform conditioning on mother grammatical function is more general than the casephenomena specific generation grammar transform of   in that it applies to each and every subpart of a recursive input fstructure driving generation  making available relevant generation history  context  to guide local generation decisions ,Negative
 introduced a transformationbased learning method which considered chunking as a kind of tagging problem ,Neutral
"This is very similar to Brill's use of contexts to induce transformation rules for his tagger (Brill, 1992; Brill, 1995), but instead of generating transformation rules from a training text, we gather statistics and apply them to parses in the text being disambiguated. ",Neutral
Furthermore  the BLEU score performance suggests that our model is not very powerful  but some interesting hints can be found in Table 3 when we compare our method with a 5gram language model to a stateoftheart system Moses  based on various evaluation metrics  including BLEU score  NIST score   METEOR   TER   WER and PER ,Positive
A representation which fits these requirements is adependency-based representation (Melcuk 1988).,Neutral
"The only other text-to-text generation approach able to produce new utterances isthat of Pang, Knight, and Marcu (2003).",Positive
"Tandem Learning (Raghavan and Allan, 2007) is an algorithm that combines feature and instance active learning for classification. The algorithm it- eratively queries the user first for instance labels, then for feature labels. ",Neutral
"We use the English Slot Grammar(ESG) parser developed at IBM (McCord, 1990) to analyze the syntactic structure of an input sentence and produce a sentence parse tree. The ESG parser not only annotates the syntactic category of a phrase, it also annotates the thematic role of a phrase . ",Positive
"Two paradigms are being pursued: extraction and abstraction (Hahn and Mani, 2000). ",Neutral
This hypothesized relationship between distributional similarity and semantic similarity has given rise to a large body of work on automatic thesaurus generation  ,Neutral
In addition  the semisupervised Morce performs  on single CPU and development data set  77 times faster than the combination and 23 times faster than  In a recent study by   nonlocal information is encoded using an independence model  and the inference is performed by Gibbs sampling  which enables us to use a stateoftheart factored model and carry out training efficiently  but inference still incurs a considerable computational cost  suggests use of an approximation summing over the training data  which does not sum over possible tags   h E f j  2 P    p  ti l hi  f j  hi  ti  i  1 However  we believe this passage is in error  such an estimate is ineffective in the iterative scaling algorithm ,Negative
"The work of (Yarowsky, 1994) is perhaps the most similar to our present work. However, his work used decision list to perform classification, in which only the single best disambiguating evidence that matched a target context is used. In contrast, we used exemplar-based learning, where the contributions of all features are summed up and taken into account in coming up with a classification. We also include verb-object syntactic relation as a feature, which is not used in (Yarowsky, 1994). ",Negative
The second uses Lin dependency similarity  a syntacticdependency based distributional word similarity resource described in  9 ,Neutral
"Juman (Matsumoto et al., 1994) was used in our experiments to generate the morpheme network. Juman is a rule-based Japanese tagging system which uses hand-coding cost values that represent the implausibility of morpheme connections, and word- and tag-occurences. ",Positive
In addition to adapting the idea of Head Word Chains   we also compared the input sentences argument structures against the treebank for certain syntactic categories ,Neutral
"We employ a slightly different clustering method here, the fullibmpredict method discussed in (Goodman, 2001). This method was shown to outperform the class based model proposed in (Brown et al., 1992) and can thus be expected to discover better clusters of words. ",Positive
Finally  we are investigating several avenues for using this system output for Machine Translation  MT  including   1  aiding word alignment for other MT system   and  2  aiding the creation various MT models involving analyzed text  eg   ,Neutral
"Furthermore, Fuhr (1989) pointed out that transformation, as in Eq. (6), is not monotonic of P(cld ). It follows then, that C T does not satisfy the probabilistic ranking principle ( P R P ) any More. ",Neutral
"Mann and McCallum (2007) apply Expectation Regularization to Named Entity Recognition and Part-Of-Speech tagging, achieving improved performance when compared to supervised methods, especially on small numbers of training data.",Neutral
4 Experiments The experiments described here were conducted using the Wall Street Journal Penn Treebank corpus  ,Neutral
Bootstrapping a PMTG from a lowerdimensional PMTG and a wordtoword translation model is similar in spirit to the way that regular grammars can help to estimate CFGs   and the way that simple translation models can help to bootstrap more sophisticated ones  ,Positive
For example  10 million words of the American National Corpus  will have manually corrected POS tags  a tenfold increase over the Penn Treebank   currently used for training POS taggers The process of phrase extraction is difficult to optimize in a nondiscriminative setting  many heuristics have been proposed   but it is not obvious which one should be chosen for a given language pair ,Negative
2 Related Work There has been a large and diverse body of research in opinion mining  with most research at the text   sentence  or word  level ,Neutral
"Semiring-weighted logic programming is a general framework to specify these algorithms (Pereira and Warren, 1983; Shieber et al., 1994; Goodman, 1999; Eisner et al., 2005; Lopez, 2009). Goodman (1999) describes many useful semirings (e.g., Viterbi, inside, and Viterbi-n-best). ",Neutral
For these first SMT systems  translationmodel probabilities at the sentence level were approximated from wordbased translation models that were trained by using bilingual corpora  ,Neutral
"Part-of-speechtagging is of interest for a number of applications, for example access to text data bases (Kupiec, 1993), robust parsing (Abney, 1991), and general parsing (deMarcken, 1990; Charniak et al., 1994). ",Neutral
ntroduction In recent years  statistical machine translation have experienced a quantum leap in quality thanks to automatic evaluation  and errorbased optimization  Introduction Automatic Metrics for machine translation  MT  evaluation have been receiving significant attention in the past two years  since IBM 's BLEU metric was proposed and made available  Introduction Treebankbased probabilistic parsing has been the subject of intensive research over the past few years  resulting in parsing models that achieve both broad coverage and high parsing accuracy  ,Positive
"As for the former (hereafter it is referred to synPth), we continue to use a dependency version of the pruning algorithm of (Xue and Palmer, 2004). The pruning algorithm is readdressed as the following. ",Positive
"We take BBN’s HierDec, a string-to-dependency decoder as described in (Shen et al., 2008), as our baseline for the following two reasons ",Positive
"The system was tested against 26,711 words of newspaper text from The Wall Street Journal, The Economist and Today, all taken from the 200-million word Bank of English corpus by the COBUILD team at the University of Birmingham, England (see also (J/irvinen, 1994)). ",Neutral
Nivre and McDonald (2008) presented an integrating method to provide additional information for graph-based and transition-based parsers. ,Neutral
Our baseline method for ambiguity resolution is the Collins parser as implemented by Bikel  We then built separate EnglishtoSpanish and SpanishtoEnglish directed word alignments using IBM model 4   combined them using the intersect  grow heuristic   and extracted phraselevel translation pairs of maximum length 7 using the alignment template approach  ,Neutral
This compression is achievedwhile maintaining random access using a procedure for sparse data tables followingthe method given by Tarjan and Yao (1979).,Positive
The technique employed by the learner is somewhat similar to that used in decisiontrees (Breiman et al. 1984; Quinlan 1986; Quinlan and Rivest 1989).,Neutral
"A similar argument applies to all other problems in (Robertson and Sparck Jones, 1976) that are caused by having insufficient training cases. ",Negative
Hanks and  proposed using pointwise mutual information to identify collocations in lexicography  however  the method may result in unacceptable collocations for lowcount pairs Unlike   Smadja  1993  goes beyond the  twoword  limitation and deals with  collocations of arbitrary length  ,Negative
It is often straightforward to obtain large amounts of unlabeled data  making semisupervised approaches appealing  previous work on semisupervised methods for dependency parsing includes  ,Positive
Table 2  Figures about clustering algorithms Algorithm  Sentences   Clusters SHAC 623 CHAC 217 QT 232 EM 416 In fact  table 2 shows that most of the clusters have less than 6 sentences which leads to question the results presented by  who only keep the clusters that contain more than 10 sentences ,Negative
"Prior work has used a number of heuristics to deal with these problems (Matusov, et. al., 2006, He et al 08). Some work has made such decisions in a more principled fashion by computing model-based scores (Matusov et al. 2008), but still special- purpose algorithms and heuristics are needed and a single alignment is fixed.",Positive
"Rindflesch et al. (1999) use the term ""macro-noun phrase"" to refer to structures that include reduced relative clauses (commonly introduced by prepositions or participles) as well as appositives.",Neutral
Statisticbased algorithms based on Belief Network  such as HiddenMarkovModel  HMM     Lexicalized HMM  and MaximalEntropy model  use the statistical information of a manually tagged corpus as background knowledge to tag new sentences One way of resolving query ambiguities is to use the statistics  such as mutual information   to measure associations of query terms  on the basis of existing corpora  ,Neutral
 has proposed a bootstrapping method for word sense disambiguation ,Neutral
As the tagger of  can not tag a word lattice  we can not back off to this tagging Pointwise mutual information  PMI  is commonly used for computing the association of two terms   which is defined as  nullnullnull null null  null null nullnullnull nullnullnullnull  nullnull nullnull null null null nullnullnullnullnull However  we argue that PMI is not a suitable measure for our purpose ,Negative
"Since the SIR system (Raphael, 1968), some have felt that automatic information management could best be addressed using semantic information. Subsequent research (Schank, 1975; Wilks, 1976) expanded this paradigm. More recently, a number of examples of knowledge-based applications show considerable promise. ",Neutral
"First, splitting and merging of sentences (Jing and McKeown, 2000), which seems related to content planning and aggregation. ",Neutral
"Jing (2000) proposes a novel algorithmfor sentence reduction that takes into account different sources of information to de-cide whether or not to remove a particular component from a sentence to be includedin a summary. The decision is made based on (1) the relation of the component toits context, (2) the probability of deleting such a component (estimated from a cor-pus of reduced sentences), and (3) linguistic knowledge about the essentiality of thecomponent in the syntactic structure. Sentence reduction is concerned only with theremoval of sentence components, so it cannot explain transformations observed in ourcorpus and in summarization in general, such as the reexpression of domain conceptsand verbs.",Negative
"The features used for the experiments reported here are inspired by previous work in text summarization on content selection (Kupiec et al., 1995), rhetorical classification (Teufel and Moens, 2002), and information ordering (Lapata, 2003). ",Positive
David McClosky  Eugene Charniak  and Mark Johnson Brown Laboratory for Linguistic Information Processing  BLLIP  Brown University Providence  RI 0292  dmcc ec mj   csbrownedu Abstract Selftraining has been shown capable of improving on stateoftheart parser performance  despite the conventional wisdom on the matter and several studies to the contrary  ,Positive
"Supposeone wants to encode the sample dictionary of Figure 9. The algorithm, as described byRevuz (1991), consists of first building a tree whose branches are labeled by letters andwhose leaves are labeled by a list of tags (such as nn vb), and then minimizing it intoa directed acyclic graph (DAG).",Neutral
"In particular, in our experiments we used the Large Grammatical Dictionary of Bulgarian (Paskaleva,2003), created at the Linguistic Modelling Department of the Bulgarian Academy of Sciences (CLPP-BAS) and comprising approximately 995,000 wordforms (about 65,000 lemmas), encoded in DELAF format (Silberztein,1993). ",Positive
Several teams had approaches that relied to varying degrees on an IBM model of statistical machine translation Brown et al  1993 with different improvements brought by different teams consisting of new submodels improvements in the HMM model model combination for optimal alignment etc Several teams used symmetrization metrics as introduced in Och and Ney 2003 union intersection refined most of the times applied on the alignments produced for the two directions sourcetarget and targetsource but also as a way to combine different word alignment systems,Negative
"Consider more complex context features, such as non-limited distance or barrier rules in the style of (Samuelsson et al., 1996). ",Positive
"However, in this paper we clarify a number of details that are omitted in major previous publications concerning tagging with Markov models. As two examples, (Rabiner, 1989) and (Charniak et al., 1993) give good overviews of the techniques and equations used for Markov models and part-of-speech tagging, but they are not very explicit in the details that are needed for their application. ",Negative
Following   we consider an anaphoric reference  NPi  correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition This algorithm adjusts the loglinear weights so that BLEU  is maximized over a given development set ,Neutral
2 Phrasebased SMT We use a phrasebased SMT system  Pharaoh    which is based on a loglinear formulation  ,Neutral
POS tag the text using the tagger of  ,Neutral
"Nguyen and Horiguchi (2003) describe an ex tension of the decision tree-based compression model (Knight and Marcu, 2002) which allows for word order changes. The key to their approach is that dropped constituents are temporarily stored on a deletion stack, from which they can later be renserted in the tree where required. Although this provides an unlimited freedom for rearranging constituents, it also complicates the task of learning the parsing steps, which might explain why their evaluation results show marginal improvements at best. ",Negative
Many methods have been proposed to measure the cooccurrence relation between two words such as 2   mutual information   ttest   and loglikelihood  It has been argued that the reliability of a coding schema can be assessed only on the basis of judgments made by naive coders  ,Neutral
"If we want to compare the performance of disambiguation models, we can employ the φ mesure (van Noord and Malouf, 2005; van Noord, 2007). Intuitively, it tells us how much of the disambiguation problem has been solved. ",Positive
"We then describe the word segmentation algorithm and the new word extraction method, with their derivation as an approximation of a generalization of the Forward-Backward algorithm (Baum, 1972).  ",Neutral
 compared two Bayesian inference algorithms  Variational Bayes and what we call here a pointwise collapsed Gibbs sampler  and found that Variational Bayes produced the best solution  and that the Gibbs sampler was extremely slow to converge and produced a worse solution than EM ,Neutral
"To evaluate our WSD program, named LEXAS we tested it on a common data set involving the noun ""interest"" used by Bruce and Wiebe (Bruce and Wiebe, 1994).LEXAS achieves a mean accuracy of 87.4% on this data set, which is higher than the accuracy of 78% reported in (Bruce and Wiebe, 1994). ",Negative
"We built a system for knowledge extraction and question answering on top of USP. It generated Stanford dependencies (de Marneffe et al., 2006) from the input text using the Stanford parser, and then fed these to USP-Learn 11 , which produced an MLN with learned weights and the MAP semantic parses of the input sentences.",Neutral
This averaging effect has been shown to reduce overfitting and produce much more stable results  Probably the most widely used association weight function is  pointwise  Mutual Information  MI          defined by         log    2 fPwP fwPfwMI  A known weakness of MI is its tendency to assign high weights for rare features    and Basque   which pose quite different and in the end less severe problems  there have been attempts at solving this problem for some of the highly inflectional European languages  such as     Slovenian       Czech  and   five Central and Eastern European languages   but so far no system has reached in the absolute terms a performance comparable to English tagging  such as    which stands around or above 97  ,Positive
 improves the F score from 882  to 897   while Charniak and Johnson  2005  improve from 903  to 94  ,Positive
"In some sense, this approach is similar to the notion of ""ambiguity classes"" explained in (Kupiec, 1992) and (Cutting et al., 1992) where words that belong to the same part-of-speech figure together. ",Neutral
"Good performance in many natural language processing tasks, such as part-of-speech tagging, shallow parsing and named entity recognition, has been shown to depend heavily on integrating many sources of information (Zhang et al., 2002; Jing et al., 2003; Ittycheriah et al., 2003). ",Neutral
While minimum error training  has by now become a standard tool for interpolating a small number of aggregate scores  it is not well suited for learning in highdimensional feature spaces ,Negative
"In this paper we develop the first unsupervised approach to semantic parsing, using Markov logic (Richardson and Domingos, 2006). ",Positive
2 Lexicalized parse trees The first successful work on syntactic disambiguation was based on lexicalized probabilistic contextfree grammar  LPCFG   Statistical Learning Model 32 Nave Bayes Learning Nave Bayes learning has been widely used in natural language processing with good results such as statistical syntactic parsing   hidden language understanding  ,Positive
"Beyond these domains, purely corpus-based methods play an increasingly important role in modeling constraints on composition of words, in particular verbal selectional preferences – finding out that, say, children are more likely to eat than apples, whereas the latter are more likely to be eaten (Erk, 2007; Padó et al., 2007). ",Neutral
"To identify themes, Simfinder extracts linguistically motivated features for eachsentence, including WordNet synsets (Miller et al. 1990) and syntactic dependencies,such as subject–verb and verb–object relations.",Neutral
"Our algorithmperforms local alignment, while the algorithm of Pang, Knight, and Marcu (2003)performs global alignment.",Positive
"Another interesting work, exploiting capitalisationand fixed/variable suffixes, is presented in Cucerzan and Yarowsky (2000).",Neutral
"Methods using the short context of a word in order to resolve ambiguity (usu-ally categorical ambiguity) are very common in English and other languages (DeRose1988; Church 1988; Karlsson 1990). A system using this approach was developed byLevinger and Ornan in order to serve as a component in their project of morphologicaldisambiguation in Hebrew (Levinger 1992). The main resource, used by this systemfor disambiguation, is a set of syntactic constraints that were defined manually bythe authors and followed two theoretical works that defined short context rules forHebrew (Pines 1975; Albeck 1992).",Positive
 have proposed a rulebased algorithm for sentence combination  but no results have been reported  provides anecdotal evidence that only incorrect alignments are eliminated by ITG constraints ,Negative
"Gildea and Jurafsky (2002) were the first to describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. This approach was soon followed by other researchers (Surdeanu et al., 2003; Pradhan et al., 2004; Xue and Palmer, 2004), focusing on improved sets of features, improved machine learning methods or both, and SRL became a shared task at the CoNLL 2004, 2005 and 2008 Conferences",Positive
"Again, we take advantage of the data fusion capabilities of a memory-based approach by combining these two sources of information in the case representation, and having the information gain feature relevance weighting technique figure out their relative relevance (see Schmid, 1994; Samuelsson, 1994 for similar solutions) ",Positive
Also related are the areas of word alignment for machine translation   induction of translation lexicons   and crosslanguage annotation projections to a second language  ,Neutral
"More recent works have also employed Luhn’s approach as a basis component for extracting relevant sentences (Buyukkokten et al.,2001; Lam- desina and Jones, 2001). This approach performs well despite of its simplicity. In our previous work (Jaruskulchai et al., 2003), we also applied this approach for summarizing and browsing Thai documents through PDAs. ",Positive
"Mani (2001) defines an abstract as “a summary at least some of whose material is not present in the input”. In a study of professional abstracting, Endres-Niggemeyer (2000) concluded that professional abstractors produce abstracts by “cut-and-paste” operations, and that standard sentence patterns are used in their production. ",Neutral
"Our manual analysis of paraphrased sen-tences (Barzilay 2003) revealed that such alignments most frequently occur in pairs ofnoun phrases (e.g., faculty member and professor) and pairs including verbs with parti-cles (e.g., stand up, rise).",Neutral
3 OverviewofExtractionWork 31 English As one mightexpect  the bulk of the collocation extractionwork concernsthe English language    amongmany others1 ,Neutral
"Our approach is theoretically elegant, like other work in this vein (Goodman, 1999; Lopez, 2009; Gimpel and Smith, 2009). We used it practically to enable a new form of minimum-risk training that improved Chinese-English MT by 1.0 BLEU point. ",Positive
"Forexample, all summary sentences may contain the full description of a named entity(e.g., President of Columbia University Lee Bollinger), while the use of shorter descriptionssuch as Bollinger or anaphoric expressions in some summary sentences would in-crease the summary’s readability (Schiffman, Nenkova, and McKeown 2002; Nenkovaand McKeown 2003).",Neutral
We perform word alignment using GIZA     symmetrize the alignments using the growdiagfinaland heuristic  and extract phrases up to length 3 ,Neutral
The fstructures are created automatically by annotating nodes in the gold standard WSJ trees with LFG functional equations and then passing these equations through a constraint solver  ,Neutral
"Chang et al. (2007) only obtain better results than 88.2% on cora when using 300 labeled examples (two hours of estimated annotation time), 5000 additional unlabeled examples, and extra test time inference constraints. ",Positive
For the extraction problem  there have been various methods proposed to date  which are quite adequate  whose training corpus for the noun drug was 9 times bigger than that of Karov and Edelman  reports 94  correct performance improved to impressive 939  when using the ` one sense per discourse ' constraint ,Positive
Although  there are various manualautomatic evaluation methods for these systems  eg  BLEU   these methods are basically incapable of dealing with an MTsystem and a wpMTsystem at the same time  as they have different output forms However  reordering models in traditional phrasebased systems are not sufficient to treat such complex cases when we translate long sentences  ,Negative
Indeed  researchers have shown that gigantic language models are key to stateoftheart performance   and the ability of phrasebased decoders to handle largesize  highorder language models with no consequence on asymptotic running time during decoding presents a compelling advantage over CKYdecoders  whosetimecomplexitygrowsprohibitively large with higherorder language models ,Positive
Lexicalization can increase parsing performance dramatically for English   and the lexicalized model proposed by Collins  997  has been successfully applied to Czech  and Chinese  ,Positive
"There are a number of efforts worldwide to manually annotate largecorpora with linguistic information, including parts of speech, phrase structure andpredicate-argument structure (e.g., the Penn Treebank and the British National Corpus(Marcus, Santorini, and Marcinkiewicz 1993; Leech, Garside, and Bryant 1994)).",Positive
"Recently, Brill (1992) described a rule-based tagger that performs as well as taggersbased upon probabilistic models and overcomes the limitations common in rule-basedapproaches to language processing",Neutral
We use the following features for our rules  sourceand targetconditioned neglog lexical weights as described in Koehn et al  2003b  neglog relative frequencies lefthandsideconditioned targetphraseconditioned sourcephraseconditioned  Counters no rule applications no target words  Flags IsPurelyLexical ie  contains only terminals IsPurelyAbstract ie  contains only nonterminals IsXRule ie  nonsyntactical span IsGlueRule 139  Penalties rareness penalty exp1  RuleFrequency unbalancedness penalty MeanTargetSourceRatio  no source words no target words 4 Parsing Our SynCFG rules are equivalent to a probabilistic contextfree grammar and decoding is therefore an application of chart parsing,Neutral
     Dave et al ,Neutral
We use GIZA    to train generative directed alignment models  HMM and IBM Model4  from training recordtext pairs ,Neutral
To facilitate comparisons with previous work   we used the training\/development\/test partition defined in the corpus and we also used the automaticallyassigned part of speech tags provided in the corpus0 Czech word clusters were derived from the raw text section of the PDT 0  which contains about 39 million words of newswire text We trained the parsers using the averaged perceptron   which represents a balance between strong performance and fast training times ,Positive
Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques  including TRP  and Gibbs sampling  ,Neutral
Clustering algorithms have been previously shown to work fairly well for the classification of words into syntactic and semantic classes   but determining the optimum number of classes for a hierarchical cluster tree is an ongoing difficult problem  particularly without prior knowledge of the item classification ,Negative
Most work on discriminative training for SMT has focussed on linear models  often with margin based algorithms   or rescaling a product of submodels  ,Neutral
"That is, an assumption of full statistical dependence (Yarowsky, 1994), rather than the more common full independence, is made  ",Neutral
"WordNet (Miller et al., 1990) is the largest lexical database to date. It provides lexical relations between words, including synonymy, antonymy, meronymy, entailment ",Neutral
"We have to compare our probabilistic model to other non probabilistic models like decision tree/rule based models, one of which has recently been reported to be promising (Apt4 et al., 1994). ",Positive
"To evaluate the performance of LEXAS, we conducted two tests, one on a common data set used in (Bruce and Wiebe, 1994), and another on a larger data set that we separately collected. ",Neutral
"The best known measures for evaluating text categorization models are recall and precision, calculated by the following equations (Lewis, 1992): ",Neutral
"Similar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial Conditions.",Neutral
We use MER  to tune the decoders parameters using a development data set The training set is extracted from TreeBank  section 1518  the development set  used in tuning parameters of the system  from section 20  and the test set from section 21 For nonlocal features  we adapt cube pruning from forest rescoring   since the situation here is analogous to machine translation decoding with integrated language models  we can view the scores of unit nonlocal features as the language model cost  computed onthefly when combining subconstituents ,Neutral
Support Vector Machines  SVMs   and Maximum Entropy  ME  method  are powerful learning methods that satisfy such requirements  and are applied successfully to other NLP tasks  ,Positive
Alternatively  order is modelled in terms of movement of automatically induced hierarchical structure of sentences  Parameters used to calculate P  D  are trained using MER training  on development data ,Neutral
"To estimate the parameters of the MEMM+predmodel we turn to the successful Maximum Entropy (Berger et al., 1996) parameter estimation Method.",Positive
In recent several years  the system combination methods based on confusion networks developed rapidly   which show stateoftheart performance in benchmarks Although bialignments are known to exhibit high precision   in the face of sparse annotations we use unidirectional alignments as a fallback  as has been proposed in the context of phrasebased machine translation  ,Positive
"The algorithm has been applied to part-of-speech tagging (Padr6, 1996), and to shallow parsing (Voutilainen and Padro. 1997).  ",Neutral
 evaluates both estimation techniques on the Bayesian bitag model  Goldwater and Griffiths  emphasize the advantage in the MCMC approach of integrating out the HMM parameters in a tritag model  yielding a tagging supported by many different parameter settings ,Neutral
"Assuming that the number of feature templates in a given set is n, the algorithm of (Ding and Chang, 2008) requires O(n 2 ) times of training/test routines, it cannot handle a set that consists of hundreds of templates. ",Negative
SVM has been shown to be useful for text classification tasks   and has previously given good performance in sentiment classification experiments  ,Positive
Stochastic models  have been widely used in POS tagging for simplicity and language independence of the models solved relational similarity problems using the Web as a corpus Albeit simple  the algorithm has proven to be very efficient and accurate for the task of parse selection  ,Positive
"Most empirical work in translation analyzes mod els and algorithms using BLEU (Papineni et al., 2002) and related metrics. Though such metrics are useful as sanity checks in iterative system development, they are less useful as analytial tools.",Negative
to the pairwise TER alignment described in  We obtain aligned parallel sentences and the phrase table after the training of Moses  which includes running GIZA     growdiagonalfinal symmetrization and phrase extraction  ,Neutral
We presented some theoretical arguments for not limiting extraction to minimal rules  validated them on concrete examples  and presented experiments showing that contextually richer rules provide a 363 BLEU point increase over the minimal rules of  ,Negative
"The myHAL model uses the same co-occurrence window, but, like HAL (Lund and Burgess, 1996), treats left and right co-occurrences as distinct features. ",Neutral
"The compatibility value for each constraint is the mutual information between the tag and the context (Cover and Thomas, 1991). ",Neutral
 Introduction IBM Model   is a wordalignment model that is widely used in working with parallel bilingual corpora ,Positive
Probabilistic generative models like IBM 15 Brown et al 1993 HMM Vogel et al 1996 ITG Wu 1997 and LEAF Fraser and Marcu 2007 define formulas for Pf  e or Pe f with okvoon ororok sprok atvoon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat okdrubel okvoon anok plok sprok atdrubel atvoon pippat rrat dat okvoon anok drok brok jok atvoon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok okyurp totat nnat quat oloat atyurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1 Word alignment exercise Knight 1997,Neutral
"The effect of recency on perplexity has also been observed elsewhere (see, for example, Rosenfeld (1995) and Whittaker (2001)).",Neutral
43 Relaxing Length Restrictions Increasing the maximum phrase length in standard phrasebased translation does not improve BLEU  ,Neutral
Given a weight vector w  the score wf  x  y  ranks possible labelings of x  and we denote by Yk  w  x  the set of k top scoring labelings for x We use the standard B  I  O encoding for named entities  ,Neutral
by diagand symmetrization  ,Neutral
"Nevertheless, since the seminal work of Hobbs et al. (1993), it has been possible to conceptualize pragmatic interpretation as a unified reasoning process that selects a representation of the speaker’s contribution that is most preferred according to a background model of how speakers tend to behave. ",Positive
"Corpora of spoken dialog are now widely available, and frequently come with annotations for tasks/games, dialog acts, named entities and elements of syntactic structure. These types of information provide rich clues for building dialog models (Grosz and Sidner, 1986).",Neutral
"Fortunately, using distributional characteristics of term contexts, it is feasible to induce part-of-speech categories directly from a corpus of sufficient size, as several papers have made clear (Brown et al., 1992; Schütze, 1993; Clark, 2000). ",Positive
Yarowsky has proposed an algorithm that requires as little user input as one seed word per sense to start the training process Thus  over the past few years  along with advances in the use of learning and statistical methods for acquisition of full parsers   significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  ,Positive
"This revision strategy was employed by the human reviser mentioned in section 2, and we consider this to be effective because our target document has a so-called inverse pyramid structure (Robin and McKeown 1996), in which the first sentence is elaborated by the following sentences. ",Neutral
 Introduction Stateoftheart Statistical Machine Translation  SMT  systems usually adopt a twopass search strategy  as shown in Figure  ,Positive
"We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see Baker et al. (2007) for an overview). SVM classifiers were trained 2 with the L IB L INEAR library (Fan et al., 2008) and learned to predict the frame name, role spans, and role labels. ",Positive
 gave a systematic examination of the efficacy of unigram  bigram and trigram features drawn from different representations surface text  constituency parse tree and dependency parse tree ,Neutral
If we consider these probabilities as a vector  the similarities of two English words can be obtained by computing the dot product of their corresponding vectors2 The formula is described below  similarity  ei  ej   Nsummationdisplay k  1 p  ei fk  p  ej fk   3  Paraphrasing methods based on monolingual parallel corpora such as  can also be used to compute the similarity ratio of two words  but they dont have as rich training resources as the bilingual methods do ,Negative
"In this section, we give a brief overview of generalized expectation criteria (GE) (Mann and McCallum, 2008; Druck et al., 2008) and explain how we can use GE to learn CRF parameters with estimates of feature expectations and unlabeled data. ",Neutral
"A very influential is the work of Brill (1997), who induces more linguistically motivated rules exploiting both a tagged corpus and a lexicon. He does not look at the affixes only, but also checks their POS class in a lexicon. Mikheev (1997) proposes a similar approach, but learns the rules from raw as opposed to tagged text. Daciuk (1999) speeds up the process by means of finite state transducers. ",Positive
"The above problems could be partially solved by introducing more resources into collocation extraction, such as chunker (Wermter and Hahn, 2004), parser (Lin, 1998; Seretan and We hrli, 2006) and WordNet (Pearce, 2001).",Positive
"To solve problems 1 and 2 of PRW, Kwok (1990) stresses the assumption that a document consists of terms. This theory is called the Component Theory (CT). ",Positive
The sources of information we use for implementing our system are a POS tag-ger (Foster 1991),Neutral
"For an introduction to the algorithms, see Cutting et al. (1992), or the lucid description by Sharman (1990). ",Neutral
"A translation model consists of two distinct elements: an unweighted ruleset, and a parameterization (Lopez, 2008a; 2009).",Neutral
In order to capture the dependency relationship between lexcial heads  breaks down the rules from head outwards  which prevents us from factorizing them in other ways Besides  our model  as being linguistically motivated  is also more expressive than the formally syntaxbased models of Chiang  and  ,Negative
"A remedy is to aggressively limit the feature space, e.g. to syntactic labels or a small fraction of the bi-lingual features available, as in (Chiang et al., 2008; Chiang et al., 2009), but that reduces the benefit of lexical features. ",Negative
While other systems  such as   have addressed these tasks to some degree  OPINE is the first to report results Thirdly   deploys the dependency language model to augment the lexical language model probability be1183 tween two head words but never seek a full dependency graph ,Negative
"The convenience of adding new rules in without worrying about where exactly it goes in terms of rule ordering (something that hampered our progress in our earlier work on disambiguating Turkish morphology (Oflazer and KuruSz, 1994; Oflazer and Tiir, 1996)), has also been a key positive point. ",Positive
Congress of the Italian Association for Artificial Intelligence  Palermo  1991 B Boguraev  Building a Lexicon  the Contribution of Computers  IBM Report  TJ Watson Research Center  1991 M Brent  Automatic Aquisition of Subcategorization frames from Untagged Texts  in  N Calzolari  R Bindi  Acquisition of Lexical Information from Corpus  in  K W   P Hanks  Word Association Norms  Mutual Information  and Lexicography  Computational Linguistics  vol ,Neutral
"Additionally, since phrase features can be any function of words and alignments, we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005; Ittycheriah and Roukos, 2007).",Neutral
"For English, we obtained results comparable with the results presented in (Merialdo, 1992) as well as in (Church, 1992). ",Neutral
"In the NLP context, this class of algorithms has been used previously in example-based machine translation, in which the goal is to find an optimal alignment betweenthe source and the target sentences (Meyers, Yangarber, and Grishman 1996).",Neutral
"To test if such a simple approach would be enough, we performed a text categorization experiment, using the Rainbow mplementation of a naı̈ve Bayes term requency times inverse document frequency (TF*IDF) method (McCallum 1997) and onsidering each sentence as a document. ",Neutral
Baseline We use the Moses MT system  as a baseline and closely follow the example training procedure given for the WMT07 and WMT08 shared tasks4 In particular  we perform word alignment in each direction using GIZA     apply the growdiagfinaland heuristic for symmetrization and use a maximum phrase length of 7 ,Neutral
"Pairs of similar syntactic nodes – either words or phrases – were aligned and labeled according to a set of five semantic similarity relations (Marsi and Krahmer, 2007). ",Neutral
We preferred the loglikelihood ratio to other statistical scores  such as the association ratio  or   2  since it adequately takes into account the frequency of the cooccurring words and is less sensitive to rare events and corpussize  The ubiquitous minimum error rate training  MERT  approach optimizes Viterbi predictions  but does not explicitly boost the aggregated posterior probability of desirable ngrams  ,Negative
"According to current tagger comparisons (van Halteren et al., 1998; Zavrel and Daelemans, 1999), and according to a comparsion of the results pre- sented here with those in (Ratnaparkhi, 1996), the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here. ",Neutral
"The choice is performed to maximize a scoring function using a set of features and a log-linear model (Matusov, et. al 2006, Rosti, et al. 2007a).",Neutral
Unfortunately  this is not the case for such widely used MT evaluation metrics as BLEU  and NIST   applied the parser of  developed for English  to Czech  and found thatthe performance wassubstantially lower when compared to the results for English ,Negative
In this paper we use the socalled Model 4 from  We would expect the opposite effect with handaligned data  Extensions to Hiero Several authors describe extensions to Hiero  to incorporate additional syntactic information   or to combine it with discriminative latent models  The other form of hybridization   a statistical MT model that is based on a deeper analysis of the syntactic 33 structure of a sentence   has also long been identified as a desirable objective in principle  consider   ,Neutral
Variables measured canbe the number of correct answers and the time to complete the task. Recent experi-ments (Jing et al. 1998) have shown how different parameters such as the length ofthe abstract can affect the outcome of the evaluation.,Neutral
The first stage parser is a bestfirst PCFG parser trained on sections 2 through 22  and 24 of the Penn WSJ treebank  ,Neutral
"Our summarization system relies on semantic predications provided by SemRep (Rindflesch and Fiszman, 2003), a program that draws on UMLS information to provide underspecified semantic interpretation in the biomedical domain (Srinivasan and Rindflesch, 2002; Rindflesch et al., 2000). ",Positive
Studies on the supervised task have shown that straightforward baselines  eg models based on source only  target only  or the union of the data  achieve a relatively high performance level and are surprisingly difficult to beat  ,Positive
6 The Experimental Results We used the Penn Treebank  to perform empirical experiments on this parsing model ,Neutral
Recently  it has gained renewed attention as empirical methods in parsing have emphasized the importance of relations between words  see  eg     which is what dependency grammars model explicitly  but contextfree phrasestructure grammars do not ,Neutral
One good example for this is full-text retrieval systems (Choueka 1980). Suchsystems must handle the morphological ambiguity problem.,Positive
"On the other hand, the literature emphasizes since the very beginning the relevance of world knowledge and inference (Charniak, 1973).",Neutral
"A recent study (Kan et al., 2001) uses topic composition from text headers, but other studies in the extraction paradigm (Goldstein et al., 1999), extraction coupled with rhetorical structural identification (Teufel and Moens, 2002), and syntactic abstraction paradigms use different meth- odologies (Barzilay et al., 1999; McKeown et al., 1999).  ",Neutral
"Our point of departure is the work of Lappin and Leass (1994, henceforth L&L) and Dagan et al. (1995). (See also Dagan and Itai (1990).  ",Positive
"The work on discourse parsing that is most similar to ours is that of Baldridge and Lascarides (2005). They used a probabilistic head- driven parsing method (described in (Collins, 2003)) to construct rhetorical structure trees for a spoken dialog corpus. However, their parser was not incremental; it used global features such as the number of turn changes. Also, it focused strictly in interpretation of input utterances; it could not predict actions by either dialog partner.",Negative
1 Introduction Over the past decade  researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation  ,Neutral
"In the following experiments we used this binary estimation method, but non binary estimates could be used as in (Fuhr, 1989). ",Neutral
"For more abstract matching, we would need syntacti- cally parsed data (Lin and Pantel, 2001). We ex- pect that this would also positively affect the cov- Erage.",Positive
2 Related Work This method is similar to blockorientation modeling  and maximum entropy based phrase reordering model   in which local orientations  leftright  of phrase pairs  blocks  are learned via MaxEnt classifiers ,Neutral
The remaining six entries were all fully automatic machine translation systems  in fact  they were all phrasebased statistical machine translation system that had been trained on the same parallel corpus and most used Bleubased minimum error rate training  to optimize the weights of their log linear models feature functions  ,Positive
"In the example, the patterns of co-occurrence suggest that objects of killing are rather similar to subjects of dying, hinting at the classic cause(subj,die(obj)) analysis of killing by Dowty (1977) and many others. ",Neutral
"Identifying our coreferential chunks is even harder than the conventional coreference resolution, and we made a simplifying assumption as in Nenkova (2008) with some additional conditions that were obtained through our preliminary experiments ",Positive
"While SCL has been successfully applied to PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007), its effectiveness for parsing was rather unexplored. ",Neutral
"The result is a ""specialized"" grammar; this has a larger number of rules, but a simpler structure, allowing it in practice to be parsed very much more quickly using an LR- based method (Samuelsson, 1994a). ",Positive
model reranking has also been established  both for synchronous binarization  and for targetonly binarization  ,Neutral
We also plan to apply selftraining of nbest tagger which successfully boosted the performance of one of the best existing English syntactic parser  In machine translation  the rankings from the automatic BLEU method  have been shown to correlate well with human evaluation  and it has been widely used since and has even been adapted for summarization  ,Positive
We ran the decoder with its default settings and then used Moses implementation of minimum error rate training  to tune the feature weights on the development set ,Neutral
WSD systems have been far more successful in distinguishing coarsegrained senses than finegrained ones   but does that approach neglect necessary meaning differences  Secondly  while most pronoun resolution evaluations simply exclude nonreferential pronouns  recent unsupervised approaches  must deal with all pronouns in unrestricted text  and therefore need robust modules to automatically handle nonreferential instances ,Negative
"Talbot and Osborne (2007) used a Bloom filter (Bloom, 1970) to encode a smoothed LM.",Neutral
"We also tried combining the tuggers, using first the rules and then the statistics (a similar approach was also used in (Tapanainen and Vouti- Lainen, 1994)). ",Positive
However  by exploiting the fact that the underlying scores assigned to competing hypotheses  w  e  h  f   vary linearly wrt changes in the weight vector  w   proposed a strategy for finding the global minimum along any given search direction ,Neutral
 for English  but not identical to strictly anaphoric ones5   since a nonanaphoric NP can corefer with a previous mention ,Neutral
In the concept extension part of our algorithm we adapt our concept acquisition framework  to suit diverse languages  including ones without explicit word segmentation ,Neutral
"Second, our method is an instance of a multisequence alignment, 15 in contrast to thepairwise alignment described in Meyers, Yangarber, and Grishman (1996).",Positive
"Sparck Jones (1999) argues that it is crucial for a summarization strategy to relate the large-scale document structure of texts to readers’ tasks in the real world (i.e., to the proposed use of the summaries). We feel that incorporating a robust analysis of discourse structure into a document summarizer is one step along this way. ",Negative
"In our experiments we use the ASSERT parser (Pradhan et al., 2004), an SVM based semantic role tagger which uses a full syntactic analysis to automatically identify all verb predicates in a sentence together with their semantic arguments, which are output as PropBank arguments (Palmer et al., 2005). ",Neutral
"We have tried other machine learning algorithms such as Decision Trees, Naive Bayes Classification, and Nearest Neighbor from the Weka toolkit (Witten and Frank, 1999), but the support vector machines gave us the best classification accuracy  ",Neutral
"The two systems we use are E N G C G (Karlsson et al., 1994) and the Xerox Tagger (Cutting et al., 1992). We discuss problems caused by the fact that these taggers use different tag sets, and present the results obtained by applying the combined taggers to a previously unseen sample of text. ",Neutral
Extracting semantic information from word cooccurrence statistics has been effective  particularly for sense disambiguation  High correlation is reported between the BLEU score and human evaluations for translations from Arabic  Chinese  French  and Spanish to English  ,Positive
Assuming that the parameters P  etk fsk  are known  the most likely alignment is computed by a simple dynamicprogramming algorithm1 Instead of using an ExpectationMaximization algorithm to estimate these parameters  as commonly done when performing word alignment   we directly compute these parameters by relying on the information contained within the chunks ,Neutral
This finding has been previously reported  among others  in  In order increase the likelihood that 909 only true paraphrases were considered as phraselevel alternations for an example  extracted sentences were clustered using completelink clustering using a technique proposed in  ,Neutral
"It is the most widely reported metric in MT research, and has been shown to correlate well with human judgment (Papineni et al., 2002; Coughlin, 2003).",Neutral
 reports results for different numbers of hidden states but it is unclear how to make this choice a priori  while Goldwater & Griffiths  leave this question as future work ,Neutral
"Before trying some completely different approach, we would like to improve the current simple approach by some other simple measures: adding a morphological analyzer (Hajji, 1994) as a frontend to the tagger (serving as a ""supplier"" of possible tags, instead of just taking all tags occurring in the training data for a given token), simplifying the tagset, adding more data. ",Positive
"Evalua-tion involving human judges revealed that Simfinder identifies similar sentences with49.3 precision at 52.9 recall (Hatzivassiloglou, Klavans, and Eskin 1999).",Neutral
"Kupiec, Pedersen, and Chen (1995) report sentence length as a useful feature for text extraction. ",Neutral
"Like in the DV model of Padó and Lapata (2007), only pairs connected by target links are preserved, but the links themselves are not part of the model. ",Neutral
"Several approaches provide similar output based on statistics (Church 1988, Zhai 1997, for example), a finite-state machine (AitMokhtar and Chanod 1997), or a hybrid approach combining statistics and linguistic rules (Voutilainen and Padro 1997).",Neutral
  or  ST    where no labeled target domain data is available  eg ,Neutral
The template we use here is similar to   but we have added extra context words before the X and after the Y Our morphological processing also differs from  ,Neutral
"Author summaries tend to be less systematic (Rowley 1982) and more “deep generated,” whereas sum- maries by professional abstractors follow an internalized building plan (Liddy 1991) and are often created through sentence extraction (Lancaster 1998).  ",Neutral
"This requirement speaks against the traditional sort of dependency trees, in which heads are represented as non-terminal nodes, cf. (Hudson, 1984). ",Neutral
In comparison we introduce 28 several metrics coefficients reported in Albrecht and Hwa  including smoothed BLEU   METEOR   HWCM   and the metric proposed in Albrecht and Hwa  using the full feature set This is one manifestation of what is commonly referred to as the data sparseness problem  and was discussed by  as a sideeffect of specificity ,Neutral
"Corpus-based information can be represented e.g. as neural networks (Eineborg and Gamback 1994; Schmid 1994), local rules (Brill 1992), or collocational matrices (Garside 1987). ",Neutral
We will show that some achieve significantly better results than the standard minimum error rate training of  Allomorphs  eg  deni and deny  are also automatically identified in   but the general problem of recognizing highly irregular forms is examined more extensively in  ,Negative
"In the biomedical domain, UMLS knowledge provides considerable support for text-based systems. (Burgun and Bodenreider (2001) compare the UMLS to WordNet.) ",Neutral
"Our translation system makes use of a hierarchical phrase-based translation model (Chiang, 2007), which we argue is a strong baseline for these language pairs.",Neutral
"The composite kernel Kcis a linear combination of the two individual kernels, where the coefficient α is set to its default value 0.3 as that in Moschitti (2004)’s implementation. ",Neutral
Also  the aspect of generalizing features across different products is closely related to fully supervised domain adaptation   and we plan to combine our approach with the idea from Daume III  2007  to gain insights into whether the composite backoff features exhibit different behavior in domaingeneral versus domainspecific feature subspaces ,Neutral
Maximum entropy models  are a class of exponential models which require no unwarranted independence assumptions and have proven to be very successful in general for integrating information from disparate and possibly overlapping sources ,Positive
"a database of interconnected concepts and properties (Rogers and McClelland, 2004), adapting the information stored there to the task at hand ",Neutral
Two popular techniques that incorporate the error criterion are Minimum Error Rate Training  MERT   and Minimum BayesRisk  MBR  decoding  Automated metrics such as BLEU   RED   Weighted Ngram model  WNM    syntactic relation \/ semantic vector model  have been shown to correlate closely with scoring or ranking by different human evaluation parameters ,Positive
Various clustering techniques have been proposed  which perform automatic word clustering optimizing a maximumlikelihood criterion with iterative clustering algorithms ,Neutral
"Phase 4 (saliency) is the final transforma tion phase and its operations are adapted from TOPIC’s (Hahn and Reimer, 1999) saliency operators. ",Positive
"The graph G is called the text relationship map of D (Salton et al., 1999). ",Neutral
"Grammatical functions are assigned using standard statistical part-of-speech tagging methods (cf. e.g. (Cutting et al., 1992) and (Feldweg, 1995)). ",Neutral
"Functional representation of phrases and clauses has been introduced to facilitate expressing syntactic generMisations. The representation is introduced in (Voutilainen and Tapanainen 1993; Voutilainen 1994); here, only the main characteristics are given ",Positive
"Pivots are features occurring frequently and behaving similarly in both domains (Blitzer et al., 2006). They are inspired by auxiliary problems from Ando and Zhang (2005).",Positive
This methodological process was established af-ter studying procedures for abstract writing (Cremmins 1982; Rowley 1982) and someinitial observations from our corpus.,Neutral
"In the news domain, sentence location is the single most important feature for sentence selection (Brandow, Mitze, and Rau 1995); in our domain, location information, although less dominant, can still give a useful indication.  ",Negative
Ad-ditional evaluations of SumUM using sentence acceptability criteria and content-basedmeasures of indicativeness have been presented in Saggion and Lapalme (2000b) andSaggion (2000).,Neutral
"The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and Ku~era, 1982). ",Neutral
"Our experimental finding, t h a t local collocations are the most predictive, agrees with past observa- tion that humans need a narrow window of only a few words to perform WSD (Choueka and Lusignan, 1985). ",Neutral
"We define a single, direct log-linear translation model (Papineni et al., 1997; Och and Ney, 2002) that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments.",Positive
We then piped the text through a maximum entropy sentence boundary detector  and performed text normalization using NSW tools  ,Neutral
"The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). ",Neutral
"The HMM training and tagging programs in our experiment [Wilkens and Kupiec, 1995] are based on bigrams, i.e. only the immediate context of a word is taken into account.",Neutral
 present a system called BABAR that uses contextual role knowledge to do coreference resolution ,Neutral
"The work of (Cardie, 1993) used a case-based approach that simultaneously learns part of speech, word sense, and concept activation knowledge, although the method is only tested on domain-specific texts with domain-specific word senses. ",Neutral
The experimental results show that our method outperforms the synchronous binarization method  with over 08 BLEU scores on both NIST 2005 and NIST 2008 ChinesetoEnglish evaluation data sets  produced a corpus of 4000 questions annotated with syntactic trees  and obtained an improvement in parsing accuracy for Bikels reimplementation of the Collins parser  by training a new parser model with a combination of newspaper and question data ,Negative
Robertson and Sparck Jones (1976) make use of the well-known logistic (or log-odds) transformation of the probability P(c]d). ,Positive
"The drawback is that the complexity in syntactic processing is coupled with semantic parsing and makes the latter even harder. For ex- ample, when applying their approach to a different domain with somewhat less rigid syntax, Zettlemoyer and Collins (2007) need to introduce new combinators and new forms of candidate lexical Entries. ",Negative
Sharp (1989) reports onexperiments carried out with abstractors in which it is shown that introductions andconclusions provide a basis for producing a coherent and informative abstract.,Neutral
This is the shared task baseline system for the 2006 NAACLHLT workshop on statistical machine translation  and consists of the Pharaoh decoder   SRILM   GIZA     mkcls   Carmel 1 and a phrase model training code ,Neutral
51 The Prague Dependency Tree Bank  PDT in the sequel   which has been inspired by the buildup of the Penn Treebank   is aimed at a complex annotation of  a part of  the Czech National Corpus  CNC in the sequel   the creation of which is under progress at the Department of Czech National Corpus at the Faculty of Philosophy  Charles University  the corpus currently comprises about 100 million tokens of word forms  ,Neutral
The chunker is trained on the answer side of the Training corpus in order to learn 2 and 3word collocations  defined using the likelihood ratio of  ,Neutral
"Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al.2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. ",Neutral
"In general, the stochastic tagging problem can be formulated as a search problem in the stochastic space of sequences of tags and words. In this formulation, the tagger searches for the best sequence that maximizes the probability (Nagata, 1994)  ",Neutral
Since there is no wellagreed to definition of what an utterance is  we instead focus on intonational phrases   which end with an acoustically signaled boundary lone ,Neutral
To overcome this problem  unsupervised learning methods using huge unlabeled data to boost the performance of rules learned by small labeled data have been proposed recently     ,Positive
Jing and McKeown (2000) and Jing (2000)propose a cut-and-paste strategy as a computational process of automatic abstractingand a sentence reduction strategy to produce concise sentences.,Neutral
Atthefinestlevel  thisinvolvesthealignment of words and phrases within two sentences that are known to be translations  ,Neutral
Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling  and dependency parsing  with a great deal of success It is important to realize that the output of all mentioned processing steps is noisy and contains plenty of mistakes  since the data has huge variability in terms of quality  style  genres  domains etc  and domain adaptation for the NLP tasks involved is still an open problem  ,Neutral
"As our immediate goal is to select important content from a text, we also need a second set of gold standards that are defined by relevance (as opposed to rhetorical status). Relevance is a difficult issue because it is situational to a unique occasion (Saracevic 1975 Sparck Jones 1990 Mizzaro 1997) ",Neutral
Running words 1864 14437 Vocabulary size 569 1081 Table 2  ChineseEnglish corpus statistics  using Phramer   a 3gram language model with KneserNey smoothing trained with SRILM  on the English side of the training data and Pharaoh  with default settings to decode ,Neutral
"In the case where additionally raw untagged text isavailable, the Maximum Likelihood training can be used to reestimate the parametersof H M M taggers (Merialdo 1994).",Neutral
4 Features We used a dependency structure as the context for words because it is the most widely used and one of the best performing contextual information in the past studies  ,Positive
"For example, Eisner (2002) uses finite-state operations such as composition, which do combine weights entirely within the expectation semiring before their result is passed to the forward-backward algorithm.",Neutral
"In order to test some of our assumptions regarding how the differences between general English language and the language of clinical notes may affect POS tagging, we have trained the HMM-based TnT tagger (Brandts, 2000) with default parameters at the tri-gram level both on Penn Treebank and the clinical notes data. ",Neutral
"Many researchers (Blum and Mitchell, 1998; K. Nigam and Mitchell, 2000; Corduneanu and Jaakkola, 2002) have attempted to improve performance with unlabeled data. In this paper, we also propose a framework to bootstrap with unlabeled data.",Positive
We wish to minimize this error function  so we select accordingly  argmin summationdisplay a E  a   a   argmax a p  a  f e     4  Maximizing performance for all of the weights at once is not computationally tractable  but  has described an efficient onedimensional search for a similar problem For French\/English translation we use a state of the art phrasebased MT system similar to  ,Positive
We conclude with some challenges that still remain in applying proactive learning for MT 2 Syntax Based Machine Translation In recent years  corpus based approaches to machine translation have become predominant  with Phrase Based Statistical Machine Translation  PBSMT   being the most actively progressing area  ,Positive
We can then use this newly identified set to   1  use  s method to find the orientation for the terms and employ the terms and their scores in a classifier  and  2  use  s method to find the orientation for the terms and add the new terms as additional seed terms for a second iteration As opposed to    we do not use the web as a resource to find associations  rather we apply the method directly to indomain data ,Neutral
"Barzilay and Elhadad (1997) proposed lexical chains as an intermediate step in the text summarization process. Attempts to determine the benefit of this proposal have been faced with a number of difficulties. First, previous methods for computing lexical chains have either been manual (Morris and Hirst 1991) or automated, but with exponential efficiency (Hirst and St.-Onge 1997; Barzilay and Elhadad 1997). Because of this, computing lexical chains for documents of any reasonable size has been impossible. ",Negative
"Elman (1990) trains a connectionist net to predict words, a process that generates internal representations that reflect grammatical category. Brill et al. (1990) try to infer grammatical category from bigram statistics.  inch and Chater (1992) and Finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in a corpus.",Neutral
"Evaluation in automatic summarization, especially for multidocument input, is daunting (Rad ev et al., 2003).  ",Neutral
Probabilistic translation models generally seek to find the translation string e that maximizes the probability Pra5 ea6fa7  given the source string f  ,Neutral
Edmundson and Wyllys (1961) find similarly low human agreement for research articles. More recent experimentsreporting more positive results all used news text (Jing et al. 1998; Zechner 1995).,Neutral
"Mathis and Rush (1985) indicate thatsome transformations in the source material are allowed, such as concatenation, trun-cation, phrase deletion, voice transformation, paraphrase, division, and word deletion.Rowley (1982) mentions the inclusion of the lead or topical sentence and the use ofactive voice and advocates conciseness.",Neutral
These scores are higher than those of several other parsers   but remain behind tim scores of Charniak  2000  who obtains 901  LP and 901  LR for sentences _  40 words In contrast to existing approaches   the context of the whole corpus rather than a single sentence is considered in this iterative  unsupervised procedure  yielding a more reliable alignment ,Negative
"Given the comparability of the accuracy of the rule-based part-of-speech (POS) tagger (Brill, 1992) with the accuracy of the stochastic tagger and given the fact that a rule-based POS tagger has never been used for a Slavic language we have tried to apply rule-based methods even for Czech. ",Neutral
We benchmark our results against a model  Hiero  which was directly trained to optimise BLEUNIST using the standard MERT algorithm  and the full set of translation and lexical weight features described for the Hiero model  ,Neutral
Having a single  canonical tree structure for each possible alignment can help when flattening binary trees  as it indicates arbitrary binarization decisions  ,Neutral
"Besides frequency, another way of looking at the predications is typicality (Kan et al., 2001), or distribution of predications across citations.  ",Neutral
Theproblem of anaphoric expressions in technical articles has been extensively addressedin research work carried out under the British Library Automatic Abstracting Project(BLAB) (Johnson et al. 1993; Paice et al. 1994).,Neutral
In an evaluation on the PENN treebank   the parser outperformed other unlexicalized PCFG parsers in terms of labeled bracketing fscore ,Neutral
"For our target-language syntactic features g syn , we  use features similar to lexicalized CFG events (collins, 1999), specifically following the dependency model of Klein and Manning (2004).",Positive
The candidates of unknown words can be generated by heuristic rules  or statistical word models which predict the probabilities for any strings to be unknown words  ,Neutral
Even with the current incomplete set of semantic templates  the hypertagger brings realizer performance roughly up to stateoftheart levels  as our overall test set BLEU score  slightly exceeds that of   though at a coverage of 96  insteadof98  An alternative method  makes decisions at the end but has a high computational requirement ,Negative
Presently  many systems        focus on online recognition of proper nouns  and have achieved inspiring results in newscorpus but will be deteriorated in special text  such as spoken corpus  novels ,Neutral
In training process  we use GIZA   4 toolkit for word alignment in both translation directions  and apply growdiagfinal method to refine it  The models in the comparative study by  did not include such features  and so  again for consistency of comparison  we experimentally verified that our maximum entropy model  a  consistently yielded higher scores than when the features were not used  and  b  consistently yielded higher scores than nave Bayes using the same features  in agreement with  ,Neutral
We have computed the BLEU score  accumulated up to 4grams    the NIST score  accumulated up to 5grams    the General Text Matching  GTM  Fmeasure  e  12    and the METEOR measure  ,Neutral
"The ENGTWOL lexicon is based on the two-level  model (Koskenniemi, 1983). ",Neutral
"Our specific approach is based on contribution tracking (DeVault, 2008), a framework which casts linguistic inference in situated, task-oriented dialogue in probabilistic terms. ",Neutral
"The structure of the model was inspired by a similar (although generative) model in (Thompson et al., 2006) where it was used for semantic frame classification.",Neutral
1 Introduction Statistical phrasebased systems  have consistently delivered stateoftheart performance in recent machine translation evaluations  yet these systems remain weak at handling word order changes With these linguistic annotations  we expect the LABTG to address two traditional issues of standard phrasebased SMT  in a more effective manner ,Negative
While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments   we are not aware of any studies that have shown that corpusbased evaluation metrics of NLG systems are correlated with human judgments  correlation studies have been made of individual components   but not of systems While EM has worked quite well for a few tasks  notably machine translations  starting with the IBM models 5   it has not had success in most others  such as partofspeech tagging   namedentity recognition  and contextfreegrammar induction  numerous attempts  too many to mention  ,Positive
"Takeuchi and Matsumoto (1995) proposed the bigram estimation method from an untagged Japanese corpus. Their algorithm divides a morpheme network into possible sequences that are then used for the normal Baum-Welch algorithm. This algorithm cannot take advantage of the scaling procedure, because it requires the synchronous calculation of all possible sequences in the morpheme network. ",Negative
Conjunctions are a major source of errors for English chunking as well  9  and we plan to address them in future work ,Neutral
Liddy (1991) produced a formal model of the informational or conceptual structure of abstracts of empirical research. ,Neutral
We use the same preprocessing steps as Turian and Melamed   during both training and testing  the parser is given text POStagged by the tagger of   with capitalization stripped and outermost punctuation removed ,Neutral
"To get an idea of how the sense assignments of our d a t a set compare with those provided by WoRDNET linguists in SEMCOR, the sense-tagged subset of Brown corpus prepared by Miller et al. (Miller et al., 1994), we compare a subset of the occurrences that overlap. ",Neutral
"Specifically, we test selectional preferences on the dataset constructed by Padó (2007), that collects average plausibility judgments (from 20 speakers) for nouns as either subjects or objects of verbs (211 noun-verb pairs). ",Positive
For symmetrization  we found that Och and Neys refined technique described in  produced the best AER for this data set under all experimental conditions Ramshaw and Marcus  successflflly applied Eric Brill 's transformationbased learning method to the chunking problem ,Positive
Since the word support model and triple context matching model have been proposed in our previous work  at the SIGHAN bakeoff 2005  and 2006   the major descriptions of this paper is on the WBT model ,Neutral
In the field of statistical analysis of natural language data  it is common to use measures of lexical association  such as the informationtheoretic measure of mutual information  to extract useful relationships between words  eg   ,Neutral
Abney  notes important problems with the soundness of the approach when a unificationbased grammar is actually determining the derivations  motivating the use of loglinear models  for parse ranking that Johnson and colleagues further developed  ,Neutral
"The ac-quisition methods range from supervised-inductive-learning-from-example algorithms (Quinlan, 1986; A h a et al., 1991) to genetic algorithm strategies(Losee, 1994), through the transformation-basederror-driven algorithm used in (Brill, 1995), Stillanother possibility are the hybrid models, which tryto join the advantages of both approaches (Vouti-lainen and Padr6, 1997).",Neutral
From this point of view  some of the measures used in the evaluation of Machine Translation systems  such as BLEU   have been imported into the summarization task ,Neutral
As  point out  WordNet does not encode antonymy across partofspeech  for example  legallyembargo  ,Neutral
"These probabilities are estimated on the training corpus parsed using the Stanford factored parser (Klein and Manning, 2003).",Neutral
It is clear that Appendix B contains far fewer true noncompositional phrases than Appendix A 7 Related Work There have been numerous previous research on extracting collocations from corpus  eg   and  ,Neutral
For unknown words  SCL gives a relative reduction in error of 195  over   even with 40000 sentences of source domain training data For comparison purposes  we revisit a fullygenerative Bayesian model for unsupervised coreference resolution recently introduced by   discuss its potential weaknesses and consequently propose three modifications to their model  Section 3  ,Negative
Rapp     but using cosine rather than cityblock distance to measure profile similarity ,Neutral
"The closest available system to USP in aims and capabilities is TextRunner (Banko et al., 2007), and we compare with it. TextRunner is the state-of-the-art system for open-domain informa- tion extraction; its goal is to extract knowledge from text without using supervised labels. ",Positive
"Previous research has addressed revision in single-document summaries [Jing & McKeown, 2000] [Mani et al, 1999] and has suggested that revising summaries can make them more informative and correct errors. We believe that a generateand-revise strategy might also be used in creating better multiple-document summaries, within the framework of current extractive summarization Systems. ",Positive
"As keyboard input is more efficient than mouse input (cf. (Lehmalm et al., 1995)) mnost effort has been put in developing an efficient keyboard interLace. ",Neutral
"this technique generally consists of using a small tagged corpus to train a system and having the system tag another subset of the corpus that gets disambiguated later. (Derouault and Merialdo, 1986) have used these techniques but the necessary human effort is still considerable. ",Neutral
It is often straightforward to obtain large amounts of unlabeled data  making semisupervised approaches appealing  previous work on semisupervised methods for dependency parsing includes  ,Positive
Our model improves the baseline provided by    i  accuracy is increased by creating a lexicalised PCFG grammar and enriching conditioning context with parent fstructure features  and  ii  coverage is increased by providing lexical smoothing and fuzzy matching techniques for rule smoothing ,Negative
"Computing the first term of the covariance in Equation 2 requires a marginal distribution over three labels, two of which will be consecutive, but the other of which could appear anywhere in the sequence. We can compute this marginal using the algorithm of Mann and McCallum (2008). ",Positive
"In this paper, we attempt to bring some clarity to the situation by taking a closer look at one of these existing methods. Specifically, we cast the popular technique of cube pruning (Chiang, 2007) in the well-understood terms of heuristic search (Pearl, 1984). ",Neutral
"The myPlain model implements a classic “flat” co-occurrence approach (Sahlgren, 2006) in which we keep track of verb-to-noun co-occurrence within a window that can include, maximally, one intervening noun, and noun-to-noun co-occurrence with no more than 2 intervening nouns. ",Neutral
"A case-based approach, similar to our memory-based approach, was also proposed by Cardie (1993a, 1994) for sentence analysis in limited domains (not only POS tagging but also semantic tagging and structural disambiguation). ",Neutral
"As the first step in the process, an existing program, MetaMap, (Aronson et al. 1994) attempts to map each simple noun phrase to a concept in the UMLS Metathesaurus.",Neutral
 Classification allows a word to align with a target word using the collective translation tendency of words in the same class ,Neutral
Successful discriminative parsers have used generative models to reduce training time and raise accuracy above generative baselines  ,Positive
"In total, we arrive at 13 features, including 8 boundary word features, 4 (kinds of) internal word features and 1 LM feature. The first 12 features have been proven useful (Xiong et al., 2006; Zhang et al., 2007a) to phrase reordering. ",Neutral
There are many research directions  eg  sentiment classification  classifying an opinion document as positive or negative    subjectivity classification  determining whether a sentence is subjective or objective  and its associated opinion    featuretopicbased sentiment analysis  assigning positive or negative sentiments to topics or product features   Hu and Liu 2004  Popescu and Etzioni  2005  Carenini et al  2005  Ku et al  2006  Kobayashi  Inui and Matsumoto  2007  Titov and  ,Neutral
"The learning corpus can consist of plain text, but the best results seem achievable with annotated corpora (Merialdo 1994; Elworthy 1994). ",Positive
"The system also uses a large scale, reusable lexicon we combined from multiple resources (Jing and McKeown, 1998). The resources that were combined include COMLEX syntactic dictionary (Macleod and Grishman, 1995), English Verb Classes and Alternations (Levin, 1993), the WordNet lexical database (Miller et al., 1990), the Brown Corpus tagged with WordNet senses (Miller et al., 1993). ",Neutral
"Since the official evaluation criterion for WMT09 is human sentence ranking, we chose to minimize a linear combination of two common evaluation metrics, BLEU and TER (Papineni et al., 2002; Snover et al., 2006), during system development and tuning ",Neutral
"For example, in variational decoding for machine translation (Li et al., 2009b), p is a distribution represented by a hypergraph, while q, represented by a finite state automaton, is an approximation to p. ",Neutral
Introduction Michael  parsing models have been quite influential in the field of natural language processing We do not completely rule out the possibility that some more sophisticated  ontologically promiscuous  firstorder analysis  perhaps along the lines of   might account for these kinds of monotonicity inferences It is promising to optimize the model parameters directly with respect to AER as suggested in statistical machine translation  ,Positive
"Instead, we propose a tractable strategy for reducing model uncertainty, motivated by traditional uncertainty sampling (Lewis and Gale, 1994). We assume that when a user responds to a query, the reduction in uncertainty will be equal to the Total Uncertainty (TU), the sum of the marginal entropies at the positions where the feature occurs. ",Neutral
For both experiments  we used dependency trees extracted from the Penn Treebank  using the head rules and dependency extractor from Yamada and Matsumoto  2003  ,Neutral
Before training the classifiers  we perform feature ablation by imposing a count cutoff of 10  and by limiting the number of features to the top 75K features in terms of log likelihood ratio  ,Neutral
"They place more emphasis on extending the lexicon rather than the annotations that come with it. In our earlier work (Fürstenau and Lapata, 2009) we acquire new training instances, by projecting annotations from existing FrameNet sentences to new unseen ones. ",Neutral
"These include systems for machine translation (Viegas et al., 1998), question answering, (Harabagiu et al., 2001; Clark et al., 2003), and information retrieval (Mihalcea and Moldovan, 2000). ",Neutral
We build a subset S C   incrementally by iterating to adjoin a feature f E   which maximizes loglikelihood of the model to S This algorithm is called the Basic Feature Selection  ,Neutral
The Penn Treebank  has until recently been the only such corpus  covering 45M words in a single genre of financial reporting ,Positive
  Pereira and Tishby   and Pereira  Tishby  and Lee  propose methods that derive classes from the distributional properties of the corpus itself  while other authors use external information sources to define classes  Resnik  uses the taxonomy of WordNet    uses the categories of Roget s Thesaurus  Slator  and Liddy and Paik  use the subject codes in the LDOCE  Luk  uses conceptual sets built from the LDOCE definitions ,Neutral
The basic LCS has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences    ,Neutral
The kappa value  was used to evaluate the agreement among the judges and to estimate how difficult the evaluation task was ,Neutral
As modern systems move toward integrating many features   resources such as this will become increasingly important in improving translation quality ,Neutral
IBM Model1  is a simplistic model which takes no account of the subtler aspects of language translation including the way word order tends to differ across languages A number of studies have investigated sentiment classification at document level  eg    and at sentence level  eg    however  the accuracy is still less than desirable ,Negative
"Thehigh level data trend acquires more sophisticated in-formation, such as context rules, constraints, or de-cision trees (Daelemans et al., 1996; M/~rquez andRodriguez, 1995; Samuelsson et al., 1996).",Neutral
 demonstrated that semisupervised WSD could be successful ,Positive
A more optimistic view can be found in   they argue that a near100  interjudge agreement is possible  provided the partofspeech annotation is done carefully by experts ,Neutral
"This approach, originally proposed by Knight andHatzivassiloglou (1995) and Langkilde and Knight (1998), is a standard method usedin statistical generation.",Positive
"For learning coreference decisions, we used a Maximum Entropy (Berger et al., 1996) model. ",Positive
"In the specific case of part-of-speech tagging, it is well-known (DeMarcken, 1990) that a large proportion of the incorrect tags can be eliminated safely i.e. with very low risk of eliminating correct tags. ",Positive
Our system improves over the latent namedentity tagging in   from 61  to 87  The generalized perceptron proposed by  is closely related to CRFs  but the best CRF training methods seem to have a slight edge over the generalized perceptron 2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT Phrasebased models  are good at learning local translations that are pairs of  consecutive  substrings  but often insufficient in modeling the reorderings of phrases themselves  especially between language pairs with very different wordorder ,Negative
"We could use partly disambiguated text (e.g. the output of parsers D1, D2 or D3~) and disambiguate the result using a knowledge-based syntactic parser (see experiments in (Vou- tilainen and Tapanainen, 1993)).  ",Neutral
"We intend to do so soon, and also to repeat the experiments on the French version of the CLE (Rayner, Carter and Bouillon, 1996). ",Positive
"The parameters (weights) θj can be estimated efficiently by maximizing the regularized conditional likelihood of a training corpus (Johnson et al., 1999; van Noord and Malouf, 2005)",Neutral
"The paper presents an application of Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for domain adaptation of a stochastic attribute-value grammar (SAVG). So far, SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007). ",Positive
22 Corpus occurrence In order to get a feel for the relative frequency of VPCs in the corpus targeted for extraction namely 0 5 10 15 20 25 30 35 40 0 10 20 30 40 50 60 70 VPC types  Corpus frequency Figure 1 Frequency distribution of VPCs in the WSJ Tagger correctextracted Prec Rec Ffl1 Brill 135135 1000 0177 0301 Penn 667800 0834 0565 0673 Table 1 POSbased extraction results the WSJ section of the Penn Treebank we took a random sample of 200 VPCs from the Alvey Natural Language Tools grammar Grover et al  1993 and did a manual corpus search for each,Neutral
"Lopez and Resnik (2006) also showed that feature engineering could be used to overcome deficiencies of poor alignment. To illustrate the usefulness of feature subspace in the SMT task, we start with the example shown in Table 1. In the example, the Chinese source sentence is translated with two settings of a hierarchical phrase-based system (Chiang, 2005).",Neutral
"In the approach of Zettle moyer and Collins (2005), the training data consists of sentences paired with their meanings in lambda form.",Neutral
Related Work The recent availability of large amounts of bilingual data has attracted interest in several areas  including sentence alignment   word alignment   alignment of groups of words   and statistical translation  ,Neutral
"A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality (Knight and Marcu, 2002). ",Neutral
"We trained a 5-gram language model from the provided English monolingual training data and the non-Europarl portions of the parallel training data using modified Kneser-Ney smoothing as implemented in the SRI language modeling toolkit (Kneser and Ney, 1995; Stolcke, 2002).",Positive
According to this model  when translating a stringf in the source language into the target language  a string e is chosen out of all target language strings e if it has the maximal probability given f   e  arg maxe LCB Pr  e f  RCB  arg maxe LCB Pr  f e  Pr  e  RCB where Pr  f e  is the translation model and Pr  e  is the target language model ,Neutral
"In a statistical translation model, trimming of the phrase table had been shown to be beneficial (Johnson et al., 2007). ",Neutral
"An alternative approach to semantic role labeling is the framework developed by Halliday (1994) and implemented by Mehay et al. (2005). PropBank has thus far received the most attention of the research community, and is used in our work ",Positive
"A word-pair classification is used to formulate semantic dependency parsing as in (Zhao and Kit, 2008). ",Neutral
"Since keyboard input is most efficient for assigning categories to words and phrases, cf. (Lehmann et al.,- 1996; Marcus et al., 1994), and structural manipulations are executed most efficiently using the mouse, both an elaborate keyboard and optical interface is Provided. ",Neutral
This concept of alignment has been also used for tasks like authomatic vocabulary derivation and corpus alignment  ,Neutral
Moreover  under this view  SMT becomes quite similar to sequential natural language annotation problems such as partofspeech tagging and shallow parsing  and the novel training algorithm presented in this paper is actually most similar to work on training algorithms presented for these task  eg the online training algorithm presented in  and the perceptron training algorithm presented in  Feature weights vector are trained discriminatively in concert with the language model weight to maximize the BLEU  automatic evaluation metric via Minimum Error Rate Training  MERT   ,Neutral
The wn   similarity package  to compute the Jiang & Conrath  J&C  distance  as in   propose using a statistical word alignment algorithm as a more robust way of aligning  monolingual  outputs into a confusion network for system com2  construct lattices over paraphrases using an iterative pairwise multiple sequence alignment  MSA  algorithm ,Neutral
"This massive interest in speed is bringing rapid progress to the field, but it comes with a certain amount of baggage. Each technique brings its own terminology (from the cubes of (Chiang, 2007) to the lazy lists of (Pust and Knight, 2009)) into the mix. Often, it is not entirely clear why they Work. ",Negative
 have implemented a dependency parser with good accuracy  it is almost as good at dependency parsing as Charniak   and very impressive speed  it is about ten times faster than  and four times faster than Charniak   Unlike minimum error rate training   our system is able to exploit large numbers of specific features in the same manner as static reranking systems  ,Negative
"Teufel and Moens (1998) report on a similar work, but this time onthe alignment of sentences from author-provided abstracts.",Neutral
For the multilingual dependency parsing track  which was the other track of the shared task  Nilsson et al achieved the best performance using an ensemble method  ,Neutral
"In the results reported in (Bruce and Wiebe, 1994), they used a test set of 600 randomly selected sentences from the 2369 sentences. Unfortunately, in the data set made available in the public domain, there is no indication of which sentences are used as test sentences. ",Negative
"This last result was in accordance with the previous acknowledgment (Callison-Burch et al., 2006) that systems of too differing structure could not be compared reliably with BLEU. ",Neutral
In this years shared task we evaluated a number of different automatic metrics  Bleu  Bleu remains the de facto standard in machine translation evaluation ,Neutral
The BLEU metric  and the closely related NIST metric  along with WER and PER 48 have been widely used by many machine translation researchers ,Positive
"The core of the approach is a novel decoder based on lattice parsing with quasisynchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic.",Positive
"Each electronic version of the abstract was processed using the freely available GATE text analysis software (Cunningham et al., 2002).",Neutral
 Introduction During the last four years  various implementations and extentions to phrasebased statistical models  have led to significant increases in machine translation accuracy  ,Positive
This kind of corpus has served as an extremely valuable resource for computational linguistics applications such as machine translation and question answering   and has also proved useful in theoretical linguistics research  Motivation Statistical partofspeech disambiguation can be efficiently done with ngram models  ,Positive
"We further describe an efficient approach to alleviate this problem by using an idea of phrase construction (Ohsawa et al., 1998). ",Positive
Nakov et al. (2003) use ending guessing rules to predict the morphological class of unknown German nouns. ,Neutral
"The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems (Soon et al., 2001; Ng & Cardie, 2002; Kehler et al., 2004, inter alia). Similarly, many researchers have explored techniques for robust, broad coverage semantic parsing in terms of semantic role labeling (Gildea & Jurafsky, 2002; Carreras & Màrquez, 2005, SRL Henceforth). ",Neutral
Connectionist models have been used successfully for lexical acquisition (Eineborgand Gamback 1993; Elenius 1990; Elenius and Carlson 1989; Nakamura et al. 1990).,Neutral
23 Classifier Training We chose maximum entropy  as our primary classifier  since it had been successfully applied by the highest performing systems in both the SemEval2007 preposition sense disambiguation task  and the general word sense disambiguation task  ,Positive
We utilise the automatic annotation algorithm of  to derive a version of PennII where each node in each tree is annotated with an LFG functional annotation  ie an attribute value structure equation  ,Neutral
How content-bearing a word is can also be measured with frequency counts (Salton and McGill 1983). ,Neutral
"A much more efficient approach (usually) is the traditional inside-outside algorithm (Baker, 1979). ",Positive
"Och (2003) provides evidence that Λ should be chosen by optimizing an objective function based on the evaluation metric of interest, rather than likelihood. Since the error surface is not smooth, and a grid search is too expensive, Och suggests an alternative, efficient, line optimization approach. ",Positive
The pervading method for estimating these probabilities is a simple heuristic based on the relative frequency of the phrase pair in the multiset of the phrase pairs extracted from the wordaligned corpus  ,Neutral
"The first-(Rabiner 1989) and second- (He 1988) order Viterbi algorithms have been presentedelsewhere. Recently, Tao (1992) described the Viterbi algorithm for generalized HMMs.",Neutral
51 Evaluation of Translation Translations are evaluated on two automatic metrics  Bleu  and PER  position independent errorrate  ,Neutral
Although the first three are particular cases where N  1 andor M  1  the distinction is relevant  because most wordbased translation models  eg IBM models   can typically not accommodate general MN alignments ,Negative
In order to improve sentencelevel evaluation performance  several metrics have been proposed  including ROUGEW  ROUGES  and METEOR  ,Neutral
"The articles were morphologically analyzed by Mecab (Kudo et al., 2003) and syntactically parsed by Cabocha (Kudo and Matsumoto, 2002). ",Neutral
"An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007), however, without any clear conclusions. ",Neutral
"These feature vectors and the associated parser actions are used to train maximum entropy models (Berger et al., 1996).",Neutral
Pennacchiotti et al. (2008) show that WordNet-based similarity measures outperform their simpler distributional alternatives. An interesting question is whether the incorporation of WordNet-based similarity would lead to similar improvements in our case. ,Neutral
"Endemic structural ambiguity,which can lead to such difficulties as trying to cope with the many thousands of possi-ble parses that a grammar can assign to a sentence, can be greatly reduced by addingempirically derived probabilities to grammar rules (Fujisaki et al. 1989; Sharman, Je-linek, and Mercer 1990; Black et al. 1993) and by computing statistical measures oflexical association (Hindle and Rooth 1993).",Positive
"Accounting for sparsity explicitly has achieved significant improvements in other areas such as in part of speech tagging (Goldwater and Griffiths, 2007).",Neutral
As shown in   using this representation  a linear classifier can not distinguish sentences sampled from a trigram and real sentences ,Neutral
In this paper  sentence pairs are extracted by a simple model that is based on the socalled IBM Model1  ,Neutral
The tagger used is thus one that does not need tagged and disambiguated material to be trained on  namely the XPOST originally constructed at Xerox Parc  ,Neutral
12Poon and Domingos  outperformed  As with similar work   the size of the corpus makes preprocessing such as lemmatization  POS tagging or partial parsing  too costly The size of the development set used to generate 1 and 2  compensates the tendency of the unsmoothed MERT algorithm to overfit  by providing a high ratio between number of variables and number of parameters to be estimated ,Negative
"Results are reported in case insensitive BLEU score in percentages (Papineni et. al., 2002).",Neutral
CPSTM  i   l This metric corresponds to the STM metric presented by  ,Neutral
"While P R W is the first attempt to formalize well known relevance weighting (Sparck Jones, 1972; Salton and McGill, 1983) by probability theory, there are several drawbacks in PRW. ",Negative
"Streaming algorithms have numerous applications in mainstream computer science (Muthukrishnan, 2003) but to date there has been very little awareness of this field within computational linguistics.",Neutral
"This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent ba.ckbone, cf. (McCawley, 1987), (Dowty, 1989), (Reape, 1993), (Kathol and Pollard, 1995). These approaches provide an adequate explanation for several issues problematic ibr phrase-structure g r a m m a r s (clause union, extraposition, diverse second-position phenomena). ",Positive
In comparison  we deployed the GIZA   MT modeling tool kit  which is an implementation of the IBM Models 1 to 4  ,Neutral
"The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al., 1993). They require a relatively large tagged training text. Transformation-based tagging as introduced by Brill (1993) also requires a hand- tagged text for training. . ",Negative
 Introduction Raw parallel data need to be preprocessed in the modern phrasebased SMT before they are aligned by alignment algorithms  one of which is the wellknown tool  GIZA     for training IBM models  4  ,Positive
"For example, (Briscoe and Carroll, 1993) train an LR parser based on a general grammar to be able to distinguish between likely and unlikely sequences of parsing actions; (Andry et al., 1994) automatically infer sortal constraints, that can be used to rule out otherwise grammatical constituents; and (Grishman et al., 1984) describes methods that reduce the size of a general grammar to include only rules actually useful for parsing the training corpus. ",Neutral
This is because their training data  the Penn Treebank   does not fully annotate NP structure Although this Wikipedia gazetteer is much smaller than the English version used by  that has over 2000000 entries  it is the largest gazetteer that can be freely used for Japanese NER ,Negative
We show translation results in terms of the automatic BLEU evaluation metric  on the MT03 ArabicEnglish DARPA evaluation test set consisting of a212a89a212a89a87 sentences with a98a89a212a161a213a89a214a89a215 Arabic words with a95 reference translations ,Neutral
"The first two of these scores are produced by Simfinder, and the salience score iscomputed using lexical chains (Morris and Hirst 1991; Barzilay and Elhadad 1997) asdescribed below.",Neutral
The creation of the Penn English Treebank   a syntactically interpreted corpus  played a crucial role in the advances in natural language parsing technology  for English ,Positive
To solve this problem  we adopt an idea one sense per collocation which was introduced in word sense disambiguation research  The results show that  as compared to BLEU  several recently proposed metrics such as Semanticrole overlap   ParaEvalrecall   and METEOR  achieve higher correlation Parsing models have been developed for different languages and stateoftheart results have been reported for  eg  English  ,Positive
To use the data from NANC  we use selftraining  ,Neutral
"The few studies on adapting disambiguation models (Hara et al., 2005; Plank and van Noord, 2008) have focused exclusively on the supervised scenario.",Positive
We measured associations using the log-likelihood measure (Dunning 1993) for each combination of target category and semantic class by converting each cell of the contingency into a 2×2 contingency table. ,Neutral
This was expected  as it has been observed before that very simple smoothing techniques can perform well on large data sets  such as web data  ,Neutral
"The inadequacy of the bag-of-words method to the fusion task demonstrates theneed for a more linguistically motivated approach. At the other extreme, previous ap-proaches (Radev and McKeown 1998) have demonstrated that this task is feasible whena detailed semantic representation of the input sentences is available. However, theseapproaches operate in a limited domain (e.g terrorist events), where information ex-traction systems can be used to interpret the source text.",Negative
"More powerful compression models may draw on existing NLG methods for text revision (Inui et al., 1992) to accommodate full Paraphrasing. ",Neutral
"We have recently completed a prototype implementation of this approach (in C) for English (Brown Corpus) and have obtained quite similar results (Tiir, Of- lazer, and Oz-kan, 1997). ",Positive
Given this  the mutual information ratio  is expressed by Formula 1 ,Neutral
"In many NLP applications, there exist rich relations among objects, and recent work in statistical relational learning (Getoor and Taskar, 2007)and structured prediction (Bakir et al., 2007) has shown that leveraging these can greatly improve Accuracy. ",Positive
"While the literature suggests that Baum-Welch training can degrade performance on the tagging task (Elworthy, 1994; Merialdo, 1994), we have found in early experiments that agreement between a tagger trained in this way and the tagger from the XTag Project consistently increases with each iteration of Baum-Welch, eventually reaching a plateau, but not Decreasing. ",Negative
"(Chang et al., 1995) proposed an automatic dictionary construction method for Chinese from a large unsegmented corpus (311591 sentences) with the help of a small segmented seed corpus (1000 Sentences). ",Positive
This has been now an active research area for a couple of decades  ,Neutral
Finally  it should be noted that in the current implementation  we have not applied any of the possible optimizations that appear in the literature  to speed up normalization of the probability distribution q These improvements take advantage of a models structure to simplify the evaluation of the denominator in  1  ,Neutral
Jacquemin (1997) focuses on the morphological processes.,Neutral
"The expectation semiring (Eisner, 2002), originally proposed for finite-state machines, is one such “training” semiring, and can be used to compute feature expectations for the Estep of the EM algorithm, or gradients of the like-lihood function for gradient descent.  ",Positive
The piecewise linearity observation made in  is no longer applicable since we can not move the log operation into the expected value ,Neutral
1 Introduction In global linear models  GLMs  for structured prediction   eg     the optimal label y for an input x is y  arg max yY  x  w f  x  y   1  where Y  x  is the set of possible labels for the input x  f  x  y  Rd is a feature vector that represents the pair  x  y   and w is a parameter vector ,Neutral
Document level sentiment classification is mostly applied to reviews  where systems assign a positive or negative sentiment for a whole review document  ,Neutral
Study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction  ,Positive
"We used a 5-gram language model with modified Kneser-Ney smoothing, trained on the bitext’s English using SRILM (Stolcke, 2002). ",Positive
There are many possible methods for combining unlabeled and labeled data   but we simply concatenate unlabeled data with labeled data to see the effectiveness of the selected reliable parses ,Neutral
"Notice that the term coreferential is used in an extended way as it is usually used to describe the phenomena in noun group pairs (Mitkov, 2002).   ",Neutral
The straightforward way is to first generate the best BTG tree for each sentence pair using the way of   then annotate each BTG node with linguistic elements by projecting sourceside syntax tree to BTG tree  and finally extract rules from these annotated BTG trees ,Neutral
Furthermore  early work on classbased language models was inconclusive  ,Neutral
"We planned a linguistic evaluation like DUC2005 (Hoa Trang, 2005). ",Neutral
Global information is known to be useful in other NLP tasks  especially in the named entity recognition task  and several studies successfully used global features  ,Positive
"Clearly, adding more features improves (statistically significant) the case with only five features. We plan to incorporate more informative features described by Chiang et al. (2009). ",Positive
"The literature on relational similarity, on the other hand, has focused on pairs of words, devising various methods to compare how similar the contexts in which target pairs appear are to the contexts of other pairs that instantiate a relation of interest (Turney, 2006; Pantel and Pennacchiotti, 2006). ",Neutral
a timeconsuming process  Other statistical machine translation systems such as  and  also produce a tree a15 given a sentence a16 Their models are based on mechanisms that generate two languages at the same time  so an English tree a15 is obtained as a subproduct of parsing a16 However  their use of the LM is not mathematically motivated  since their models do not decompose into Pa4a5a2a9a8a3a10a6 and a12a14a4a5a3a7a6 unlike the noisy channel model ,Negative
"Thus, corpus-based approaches may have serious difficulties in capturing these relations (Havasi et al., 2007), but there are reasons to believe that they could still be useful: Eslick (2006) uses the assertions of ConceptNet as seeds to parse Web search results and augment ConceptNet by new candidate relations.",Positive
"Thus, we can compute the source dependency LM score in the same way we compute the target side score, using a procedure described in (Shen et al., 2008). ",Neutral
"We empirically compared our tagger with Eric Brill's implementation of his tagger,and with our implementation of a trigram tagger adapted from the work of Church(1988) that we previously implemented for another purpose.",Positive
"Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance.",Positive
Section 5 presents an error analysis for  lexicalized model  which shows that the headhead dependencies used in this model fail to cope well with the flat structures in Negra Even the creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level   a problem also noted by  and  ,Negative
6 Experiments We evaluated the translation quality of the system using the BLEU metric  ,Neutral
Methods like McDonalds  including the wellknown Maximal Marginal Relevance  MMR  algorithm   are subject to another problem  Summarylevel redundancy is not always well modeled by pairwise sentencelevel redundancy ,Negative
"In the previous version of thesystem (Barzilay, McKeown, and Elhadad 1999), we performed linearization of afusion dependency structure using the language generator FUF/SURGE (Elhadadand Robin 1996).",Neutral
Rowley (1982)proposes the following typology of different types of document condensations,Neutral
But in fact  the issue of editing in text summarization has usually been neglected  notable exceptions being the works by  and Mani  Gates  and Bloedorn  999  We also use Cube Pruning algorithm  to speed up the translation process ,Positive
1 Motivation A major component in phrasebased statistical Machine translation  PBSMT   is the table of conditional probabilities of phrase translation pairs ,Neutral
"One line of research focuses on the use of the knowledge contained in a machine-readable dictionary to perform WSD, such as (Wilks et al., 1990; Luk, 1995). In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including (Bruce and Wiebe, 1994; Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1994; Yarowsky, 1993; Yarowsky, 1992). ",Positive
"A number of part-of-speech taggers are readilyavailable and widely used, all trained and retrainable on text corpora (Church 1988;Cutting et al. 1992; Brill 1992; Weischedel et al. 1993).",Positive
The disambiguation algorithms also require that the semantic relatedness measures WordNet   Similarity  be installed ,Neutral
"The application of the word segmenter is described elsewhere (Nagata, 1996).",Neutral
To support distributed computation   we further split the Ngram data into shards by hash values of the first bigram ,Neutral
"The MT system we used is Joshua (Li et al., 2009), a software package that comes complete with a grammar extraction module and a MERT module, in addition to the decoder itself. ",Positive
Most stateoftheart SMT systems treat grammatical elements in exactly the same way as content words  and rely on generalpurpose phrasal translations and target language models to generate these elements  For instance   shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks ,Positive
"This model can be seen as an extension of the standard Maximum Entropy Markov Model (MEMM, see (Ratnaparkhi, 1996)) with an extra dependency on the predicate label, we will hence-forth refer to this model as MEMM+pred. ",Neutral
We adopt a similar approach to the one used in Turney (2008) and consider each question as a separate binary classification problem with one positive training instance and 5 unknown pairs. ,Positive
"The alignment method described in Section 3 falls into a class of tree comparisonalgorithms extensively studied in theoretical computer science (Sankoff 1975; Findenand Gordon 1985; Amir and Keselman 1994; Farach, Przytycka, and Thorup 1995)and widely applied in many areas of computer science, primarily computational bi-ology (Gusfield 1997).",Neutral
"While feature functions exploit statistics extracted from monolingual or word-aligned texts from the training data, the scaling factors λ of the log-linear model arestimated on the development data by applying a minimum error training procedure (Och, 2004). ",Neutral
"A serious challenge in unsupervised learning is the identifiability problem (i.e., the optimal parameters are not unique) (Liang and Klein, 2008). This problem is particularly severe for log-linear models with hard constraints, which are common in MLNs. ",Neutral
We assign tags of partofspeech  POS  to the words with MXPOST that adopts the Penn Treebank tag set  Such coarsegrained inventories can be produced manually from scratch  or by automatically relating  or clustering  existing word senses  a contextual word cw that occurs in the paragraphs of bc  a loglikelihood ratio  G2  test is employed   which checks if the distribution of cw in bc is similar to the distribution of cw in rc  p  cw bc   p  cw rc   null hypothesis  ,Neutral
"We held out 300 sentences for minimum error rate training (MERT) (Och, 2003) and optimised the parameters of the feature functions of the decoder for each experimental run.",Neutral
The creation of the Penn English Treebank   a syntactically interpreted corpus  played a crucial role in the advances in natural language parsing technology  for English Bleu is fast and easy to run  and it can be used as a target function in parameter optimization training procedures that are commonly used in stateoftheart statistical MT systems  ,Positive
"There has now been considerable work on discourse parsing using statistical bottom-up parsing (Soricut and Marcu, 2003), hierarchical agglomerative clustering (Sporleder and Lascarides, 2004), parsing from lexicalized tree-adjoining grammars (Cristea, 2000), and rule-based approaches that use rhetorical relations and discourse cues (Forbes et al., 2003; Polanyi et al., 2004; LeThanh et al., 2004). With the exception of Cristea (2000), most of this research has been limited to non-incremental parsing of textual monologues where, in contrast to incremental dialog parsing, predicting a system action is not relevant.",Positive
The parser is coupled with an online averaged perceptron  as the learning method ,Neutral
The optimum tag tio is estimated using the probabilities of the forward-backwardalgorithm (Rabiner 1989),Neutral
"Syntactic similarity may be operationalized in many ways, for example by taking account a hierarchy of grammatical relations (Keenan and Comrie, 1977). ",Neutral
"In general, phrases are extracted with maximum length in the source and target defined by the parameters J max and I max . All such phrase-pairs are efficiently computed by an 2 algorithm with complexity O(lI max J max ) (Cet- tolo et al., 2005). ",Positive
"The parser is basically based on the MSTParser 8 using all the features presented by (McDonald et al., 2006) with projective parsing. Moreover, we exploit three types of additional features to improve the parser. ",Positive
Table 4 shows the linguistic features of the resulting model compared to the models of Carroll and Rooth     and Charniak  2000  ,Neutral
First  we compared our system output to human reference translations using Bleu   a widelyaccepted objective metric for evaluation of machine translations ,Positive
1 Introduction Word Sense Disambiguation  WSD  competitions have focused on general domain texts  as attested in the last Senseval and Semeval competitions  ,Neutral
After this conversion  we had 1000 positive and 1000 negative examples for each domain  the same balanced composition as the polarity dataset  ,Neutral
32 Maximum Entropy ME models implement the intuition that the best model will be the one that is consistent with the set of constrains imposed by the evidence  but otherwise is as uniform as possible  ,Neutral
Typical examples of linguistically sophisticated annotation include tagging words with their syntactic category  although this has not been found to be effective for R   lemma of the word  eg ` corpus ' for ` corpora '   phrasal information  eg identifying noun groups and phrases    and subjectpredicate identification  ,Positive
"[Halliday & Hasan, 1976] offer a clear definition for text cohesion",Neutral
Several studies have demonstrated that for instance Statistical Machine Translation  SMT  benefits from incorporating a dedicated WSD module  In our experiments  we have used Averaged Perceptron  and Perceptron with margin  to improve performance In 2004  Conroy  tested Maximal Marginal Relevance  as well as QR decomposition ,Positive
"In all of our experiments, the same normalization method and classification algorithm is used with the default parameters: First, a TF-IDF feature weighting is applied to the cooccurrence matrix (Salton and Buckley, 1988). ",Neutral
We measured associations using the loglikelihood measure  for each combination of target category and semantic class by converting each cell of the contingency into a 22 contingency table ,Neutral
"Citations may vary in many dimensions; for example, they can be central or perfunctory, positive or negative (i.e., critical); apart from scientific reasons, there is also a host of social reasons for citing (“politeness, tradition, piety” [Ziman 1969]).  ",Neutral
To generate phrase pairs from a parallel corpus  we use the ` diagand  phrase induction algorithm described in   with symmetrized word alignments generated using IBM model 2  ,Neutral
"Several taggers based on rules, stochastic models, neural networks, and hybridsystems have already been presented for Part-of-speech (POS) tagging. Rule-basedtaggers (Brill 1992; Elenius 1990; Jacobs and Zernik 1988; Karlsson 1990; Karlsson etal. 1991; Voutilainen, Heikkila, and Antitila 1992; Voutilainen and Tapanainen 1993)use POS-dependent constraints defined by experienced linguists.",Neutral
1 A cept is defined as the set of target words connected to a source word  ,Neutral
"The reason for the small optimal n-best list size could be that the low-rank hypotheses might introduce more noises into the combined translation candidate pool for sentence-level combination (Hasan et al., 2007; Hildebrand and Vogel, 2008) ",Neutral
In particular  knowing a little about the structure of a language can help in developing annotated corpora and tools  since a little knowledge can go a long way in inducing accurate structure and annotations  ,Neutral
"The literature on word space models (Sahlgren, 2006) has focused on taxonomic similarity (synonyms, antonyms, co-hyponyms. . . ) and general association (e.g., finding topically related words), exploiting the idea that taxonomically or associated words will tend to occur in similar contexts, and thus share a vector of cooccurring words. ",Neutral
For the Brown corpus  we based our division on  ,Neutral
"Actually, our dependency structure alignment is almost the same as that of Filippova and Strube (2008), and our lead sentence plays the role of a basis tree in the Barzilay and McKeown approach (2005). ",Neutral
We took 232 causative/inchoative verbs and 170 non- alternating transitive verbs from Levin (1993).  ,Neutral
"Next, it looks promising to try to estimate the dictionary word frequencies using a search engine instead of text corpus, as proposed by Lapata and Keller (2004) ",Positive
"Hence, either the best translation hypothesis is directly extracted from the word graph and output, or an N-best list of translations is computed (Tran et al., 1996). ",Neutral
"Therefore, whenever we have ac cess to a large amount of labeled data from some “source” (out-of-domain), but we would like a model that performs well on some new “target” domain (Gildea, 2001; Daumé III, 2007), we face the problem of domain adaptation. ",Neutral
In an experiment on 6800 sentences of ChineseEnglish newswire text with segmentlevel human evaluation from the Linguistic Data Consortium?s LDC Multiple Translation project we compare the LFGbased evaluation method with other popular metrics like BLEU NIST General Text Matcher GTM Turian et al  2003 Translation Error Rate TER Snover et al  2006 and METEOR Banerjee and Lavie 2005 and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment,Positive
"With respect to computational morphology, witness for instance the success of the Two-Level paradigm introduced by Koskenniemi (1983) ",Positive
"A well-known remedy for this problem is to use (r + 0.5)/(R + 1) as the estimate of P(T = lie ) (Robertson and Sparck Jones, 1976). While various smoothing methods (Church and Gale, 1991; Jelinek, 1990) are also applicable to these situations and would be expected to work better, we used the simple ""add one"" remedy in the following experiments.",Positive
"Term weighting for target documents would also be necessary for sophisticated information retrieval (Fuhr, 1989; Kwok, 1990). ",Neutral
We trained a trigram model with Good–Turing smoothingover 60 megabytes of news articles collected by Newsblaster using the second versionCMU–Cambridge Statistical Language Modeling toolkit (Clarkson and Rosenfeld 1997).,Neutral
It can be applied to complicated models such IBM Model4  By 17 0 10 20 30 40 50 60 70 80 90 100 10000 100000 1e06 1e07 Test Set Items with Translations  Training Corpus Size num words unigrams bigrams trigrams 4grams Figure 1 Percent of unique unigrams bigrams trigrams and 4grams from the Europarl Spanish test sentences for which translations were learned in increasingly large training corpora increasing the size of the basic unit of translation phrasebased machine translation does away with many of the problems associated with the original wordbased formulation of statistical machine translation Brown et al  1993,Negative
"There is a growing amount of work on automatic extraction of paraphrases from text corpora (Lin and Pantel, 2001; Barzilay and Lee, 2003; Ibrahim et al., 2003; Dolan et al., 2004). ",Positive
"However, Voutilainen and Jarvinen (1995) empirically show that an interjudge agreement virtually of 1()0% is possible, at least with the EngCG tag set if not with the original Brown Corpus tag set. ",Positive
This latter point is a critical difference that contrasts to the major weakness of the work of  which uses a topN list of translations to select the maximum BLEU sentence as a target for training  so called local update  ,Negative
Gaussier (1999) induces derivational morphology from a lexicon by means of p-similarity based splitting based splitting.,Neutral
Chen et al. (2008) used features derived from short dependency pairs based on large-scale auto-parsed data to enhance dependency parsing.,Neutral
Annotated reference corpora  such as the Brown Corpus   the Penn Treebank   and the BNC   have helped both the development of English computational linguistics tools and English corpus linguistics Introduction Syntactically annotated corpora like the Penn Treebank   the NeGra corpus  or the statistically dismnbiguated parses in  provide a wealth of intbrmation  which can only be exploited with an adequate query language ,Positive
"Experimental tests (M&rquez and Rodriguez, 1995) have shown that the pruning process reduces tree sizes at about 50% and improves their accuracy in a 2-5%. ",Neutral
"we list the first twenty transformations learned from training on thePenn Treebank Wall Street Journal Corpus (Marcus, Santorini, and Marcinkiewicz1993).",Neutral
"Much work in computational linguistics and related fields relies on measuring similarity among words/concepts in terms of their patterns of co-occurrence with other words/concepts (Sahlgren, 2006). ",Neutral
"The IWSLT and WMT workshops also have a manual evaluation component, as does the NIST Evaluation, in the form of adequacy and fluency (LDC, 2005). ",Neutral
"In (Deschacht and Moens, 2009) we peform a number of experiments, comparing different corpora (news texts from Reuters and from Associated Press, and articles from Wikipedia) and n-gram sizes (3-gram and 4-gram). ",Neutral
prime 1 1 1 05 05 1 05 05 01 01 01 00001 00001 01 00001 00001 Further  we ran each setting of each estimator at least 10 times  from randomly jittered initial starting points  for at least 1000 iterations  as  showed that some estimators require many iterations to converge ,Neutral
For each differently tokenized corpus  we computed word alignments by a HMM translation model  and by a word alignment refinement heuristic of growdiagfinal  ,Neutral
12 Related Work Recently  discriminative methods for alignment have rivaled the quality of IBM Model 4 alignments  ,Neutral
"[Filatova & Hovy, 2001] addressed the issue of resolving temporal references in news stories. Although events in articles are not always presented in chronological order, readers must be able to reconstruct the timeline of events in order to comprehend the story. They endeavored to develop a module that could automatically assign a time stamp to each clause in a document. ",Positive
"This has been quite useful for our work on tagging English (Tfir, Oflazer, and 0z-kan, 1997) where such rules with negative weights were used to fine tune the behavior of the tagger in various prob- lematic cases. ",Positive
"We have used the basic source channel model (de-scribed e.g. in (Merialdo, 1992)).",Neutral
 binarize grammars into CNF normal form  while  allow only GriebachNormal form grammars ,Neutral
"Because of this, it is generally accepted that some kind of postprocessing should be performed to improve the final result, by shortening, fusing, or otherwise revising the material (Grefenstette 1998; Mani, Gates, and Bloedorn 1999; Jing and McKeown 2000; Barzilay et al. 2000; Knight and Marcu 2000). ",Neutral
"This allows us to get a possibly noisy, but more abstract representation of the underlying data. The set of features used in Alpino is further described in van Noord and Malouf (2005).",Positive
 has described an efficient exact onedimensional accuracy maximization technique for a similar search problem in machine translation While these are based on a relatively few number of items and while we have not performed any tests to determine whether the differences in ? are statistically significant the results 7The CzechEnglish conditions were excluded since there were so few systems 46 are nevertheless interesting since three metrics have higher correlation than Bleu ??Semantic role overlap Gimenez and M`arquez 2007 which makes its debut in the proceedings of this workshop ??ParaEval measuring recall Zhou et al  2006 which has a model of allowable variation in translation that uses automatically generated paraphrases CallisonBurch 2007 ??Meteor Banerjee and Lavie 2005 which also allows variation by introducing synonyms and by flexibly matches words using stemming,Positive
"The preprocessor module also performs a number of additional functions such as grouping of lexicalizcd and non-lexicalized collocations, compound verbs, etc., (Ofiazer and Kurubz, 1994; Oflazer and Tiir, 1996). ",Neutral
1 Introduction The most widely applied training procedure for statistical machine translation IBM model 4  unsupervised training followed by postprocessing with symmetrization heuristics  yields low quality word alignments This is in contrast to purely statistical systems   which are difficult to inspect and modify ,Negative
"This should not be too surprising, as it is widely believed that sense tagging using the full set of refined senses found in a large dictionary like WORDNET involve making subtle human judgments (Wilks et al., 1990; Bruce and Wiebe, 1994), such that there are many genuine cases where two humans will not agree fully on the best sense assignments.",Neutral
"Koo et al. (2008) present an algorithm for dependency parsing that uses clusters of semantically related words, which were learned in an unsupervised manner. ",Neutral
Unlike wellknown bootstrapping approaches   EM and CE have the possible advantage of maintaining posteriors over hidden labels  or structure  throughout learning  bootstrapping either chooses  for each example  a single label  or remains completely agnostic ,Negative
As resolving direct anaphoric descriptions  the ones where anaphor and antecedent have the same head noun  is a much simpler problem with high performance rates as shown in previous results   these heuristics should be applied first in a system that resolves definite descriptions ,Neutral
21 The Evaluator The evaluator is a function ptt s which assigns to each targettext unit t an estimate of its probability given a source text s and the tokens t which precede t in the current translation of s 1 Our approach to modeling this distribution is based to a large extent on that of the IBM group Brown et al  1993 but it differs in one significant aspect whereas the IBM model involves a noisy channel decomposition we use a linear combination of separate predictions from a language model ptlt  and a translation model ptls ,Neutral
"Thus, even when spoken language interfaces use probabilistic inference for dialogue management (Williams and Young, 2007), new techniques may be needed to mine their experience for correct interpretations.",Neutral
"This should be regarded neither as a surprise nor a criticism, considering cube pruning’s origins in hierarchical phrase-based MT models (Chiang, 2007), which have only a small number of distinct Nonterminals.",Neutral
"For Chinese, (Sproat et al., 1994) used the word unigram model in their word segmenter based on weighted finite-state transducer.  ",Neutral
But without the global normalization  the maximumlikelihood criterion motivated by the maximum entropy principle  is no longer a feasible option as an optimization criterion   word class   measures polarity using only adjectives  however in our approach we consider the noun  the verb  the adverb and the adjective content words While most parsing methods are currently supervised or semisupervised   they depend on handannotated data which are difficult to come by and which exist only for a few languages ,Negative
We use a standard maximum entropy classifier  implemented as part of MALLET  8412 only PTB  baseline  8358 1st  8342 2nd  8338 3rd  8308 third row lists the three highest scores of the domain adaptation track of the CoNLL 2007 shared task ,Neutral
"Here, a CST-enhancement procedure [Zhang et al, 2002] may take place, ensuring that interdependent sentences appear together in a summary. ",Neutral
After maximum BLEU tuning  on a heldout tuning set  we evaluate translation quality on a heldout test set ,Neutral
"Kneser and Ney (1993) present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus. Biber (1993) applies factor analysis to collocations of two target words (""certain"" and ""right"") with their immediate neighbors. ",Neutral
Similar to work in image retrieval   we cast the problem in terms of Machine Translation  given a paired corpus of words and a set of video event representations to which they refer  we make the IBM Model 1 assumption and use the expectationmaximization method to estimate the parameters      m j ajm jvideowordpl Cvideowordp 1    1     1  This paired corpus is created from a corpus of raw video by first abstracting each video into the feature streams described above ,Neutral
Sentences containing many “content-bearing” words have been hypothesized to be good candidates for text extraction. Baxendale (1958) extracted all words except those on the stop list from the title and the headlines and determined for each sentence whether or not it contained these words.  ,Neutral
"However, the use of regular relations and finite state transducers (Kaplan and Kay, 1994) provide a very efficient implementation Method. ",Positive
Even the creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level  1 Introduction In recent years  various phrase translation approaches  have been shown to outperform wordtoword translation models  ,Negative
"We analyzed a set of articles and identified six major operations that can be used for editing the extracted sentences, including removing extraneous phrases from an extracted sentence, combining a reduced sentence with other sentences, syntactic transformation, substituting phrases in an extracted sentence with their paraphrases, substituting phrases with more general or specific descriptions, and reordering the extracted sentences (Jing and McKeown, 1999; Jing and McK-Eown, 2000). ",Neutral
"Generally speaking, a joint system is slower than a pipeline system in training. (Xue and Palmer, 2004) found out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. ",Neutral
Jing and McKeown (1999; 2000) found that human summarization can be traced back to six cut-and-paste operations of a text and proposed a revision method consisting of sentence reduction and combination modules with a sentence extraction part. Mani and colleagues (1999) proposed a summarization system based on “draft and revision” together with sentence extraction. The revision part is achieved with the sentence aggregation and smoothing modules. ,Positive
However  work in that direction has so far addressed only parse reranking  ,Negative
"The emerging technology of information extraction (Appelt and Israel 1997, Hearst 1999) provides a means of gaining access to this information.",Neutral
"The four sets of outputs from the parser were then translated into Swedish by the SLT transfer and generation mechanism (Agn et al., 1994). ",Neutral
