{
 "cells": [
  {
   "cell_type": "code",
   "id": "c91b01d95cc760c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T06:02:16.042601Z",
     "start_time": "2024-09-16T06:02:13.249121Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from modelscope.msdatasets import MsDataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T06:02:42.838034Z",
     "start_time": "2024-09-16T06:02:17.456920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = 'Meta-Llama-3-8B-Instruct'\n",
    "model_dir = f'./pretrain_models/{model_name}'\n",
    "device = 'cuda:0'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir,\n",
    "                                             torch_dtype='auto',\n",
    "                                             device_map=device)"
   ],
   "id": "8fab8c6d65d0007c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dabc152ea49b477196bd0a78d613e833"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def test():\n",
    "    test_data = MsDataset.load('imdb', split='test')\n",
    "    real_labels = [data['label'] for data in test_data]\n",
    "\n",
    "    texts = [data['text'] for data in test_data]\n",
    "\n",
    "    print(\"Total texts: \", len(texts))\n",
    "    print(\"Positive labels: \", real_labels.count(1))\n",
    "\n",
    "    pred_labels = []\n",
    "    exception_responses = []\n",
    "\n",
    "    # prompt source: https://github.com/aielte-research/LlamBERT/blob/main/LLM/model_inputs/IMDB/promt_eng_0-shot_prompts.json\n",
    "    for text in tqdm(texts):\n",
    "        prompt = text\n",
    "\n",
    "        messages = [{\n",
    "            'role':\n",
    "            'system',\n",
    "            'content':\n",
    "            'You are an expert in scientific citation sentiment analysis who can judge the attitude of a citation text. Please answer with \\'positive\\' , \\'neutral\\' or \\'negative\\' only!\\n'\n",
    "        }]\n",
    "        prompt = f'Decide if the following scientific citation text expresses a positive, neutral, or negative sentiment towards the research or findings: \\n {prompt} \\nIf the citation text is positive, indicating approval, agreement, or support for the research, please answer \\'positive\\'. If the citation text is neutral, meaning it does not express a clear opinion or sentiment, please answer \\'neutral\\'. If the citation text is negative, indicating criticism, disagreement, or rejection of the research, please answer \\'negative\\'. Make your decision based on the overall tone and content of the citation. If the sentiment is unclear, default to \\'neutral\\'.'\n",
    "        \n",
    "        messages.append({'role': 'user', 'content': prompt})\n",
    "\n",
    "        text = tokenizer.apply_chat_template(messages,\n",
    "                                             tokenize=False,\n",
    "                                             add_generation_prompt=True)\n",
    "\n",
    "        model_input = tokenizer([text], return_tensors='pt').to(device)\n",
    "        attention_mask = torch.ones(model_input.input_ids.shape,\n",
    "                                    dtype=torch.long,\n",
    "                                    device=device)\n",
    "        generated_ids = model.generate(\n",
    "            model_input.input_ids,\n",
    "            max_new_tokens=512,\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(\n",
    "                model_input.input_ids, generated_ids)\n",
    "        ]\n",
    "\n",
    "        response = tokenizer.batch_decode(generated_ids,\n",
    "                                          skip_special_tokens=True)[0]\n",
    "        response = response.lower()  # avoid Negative != negative\n",
    "        print(response)\n",
    "        if response not in ['positive', 'negative', 'neutral']:\n",
    "            exception_responses.append(response)\n",
    "            response = 'neutral'\n",
    "        if response == 'negative':\n",
    "            pred_labels.append(2)\n",
    "        elif response == 'positive':\n",
    "            pred_labels.append(1)\n",
    "        else:\n",
    "            pred_labels.append(0)\n",
    "\n",
    "        print(f'{response} \\n')\n",
    "\n",
    "    acc = accuracy_score(real_labels, pred_labels)\n",
    "    f1 = f1_score(real_labels, pred_labels)\n",
    "    precision = precision_score(real_labels, pred_labels)\n",
    "    recall = recall_score(real_labels, pred_labels)\n",
    "    mcc = matthews_corrcoef(real_labels, pred_labels)\n",
    "\n",
    "    df = pd.read_csv('results.csv')\n",
    "\n",
    "    df = df._append(\n",
    "        {\n",
    "            'model_name': 'llama3-8b',\n",
    "            'seed': 0,\n",
    "            'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'mcc': mcc\n",
    "        },\n",
    "        ignore_index=True)\n",
    "    df.to_csv('results.csv', index=False)\n",
    "\n",
    "    print(\"exception_responses: \", exception_responses)\n",
    "\n",
    "\n",
    "def label_unsupervised():\n",
    "    unsupervised_data = MsDataset.load('imdb', split='unsupervised')\n",
    "    texts = [data['text'] for data in unsupervised_data]\n",
    "\n",
    "    pred_labels = []\n",
    "\n",
    "    exception_responses = []\n",
    "\n",
    "    # prompt source:\n",
    "    for text in tqdm(texts):\n",
    "        print(f'Enter a prompt to generate a response:')\n",
    "        prompt = text\n",
    "\n",
    "        messages = [{\n",
    "            'role':\n",
    "            'system',\n",
    "            'content':\n",
    "            'You are an expert in sentiment analysis who can judge the attitude of a text. Please answer with \\'positive\\' or \\'negative\\' only!\\n'\n",
    "        }]\n",
    "        prompt = f'Decide if the following movie review is positive or negative: \\n {prompt} \\nIf the movie review is positive please answer \\'positive\\', if the movie review is negative please answer \\'negative\\'. Make your decision based on the whole text. If the overall sentiment is not clear, base your decision on whether or not the reviewer recommends the movie for watching. If the sentiment is still not clear, say \\'negative\\'.'\n",
    "\n",
    "        messages.append({'role': 'user', 'content': prompt})\n",
    "\n",
    "        text = tokenizer.apply_chat_template(messages,\n",
    "                                             tokenize=False,\n",
    "                                             add_generation_prompt=True)\n",
    "\n",
    "        model_input = tokenizer([text], return_tensors='pt').to(device)\n",
    "        attention_mask = torch.ones(model_input.input_ids.shape,\n",
    "                                    dtype=torch.long,\n",
    "                                    device=device)\n",
    "        generated_ids = model.generate(\n",
    "            model_input.input_ids,\n",
    "            max_new_tokens=512,\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(\n",
    "                model_input.input_ids, generated_ids)\n",
    "        ]\n",
    "\n",
    "        response = tokenizer.batch_decode(generated_ids,\n",
    "                                          skip_special_tokens=True)[0]\n",
    "        response = response.lower()  # avoid Negative != negative\n",
    "        print(response)\n",
    "        if response not in ['positive', 'negative']:\n",
    "            exception_responses.append(response)\n",
    "            response = 'negative'\n",
    "        if response == 'positive':\n",
    "            pred_labels.append(1)\n",
    "        else:\n",
    "            pred_labels.append(0)\n",
    "\n",
    "        print(f'{response} \\n')\n",
    "\n",
    "    json.dump(pred_labels,\n",
    "              open('output/unsupervised_labels.json', 'w'),\n",
    "              indent=2)\n",
    "    print(\"exception_responses: \", exception_responses)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()\n",
    "    label_unsupervised()"
   ],
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
