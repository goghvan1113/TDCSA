{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T02:33:24.779280Z",
     "start_time": "2024-09-10T02:33:24.765259Z"
    }
   },
   "source": [
    "import os\n",
    "# 模型相关参数\n",
    "num_labels = 3\n",
    "TEST_DATASET_SIZE = 0.4\n",
    "id2label={0:\"Neutral\", 1:\"Positive\", 2:\"Negative\"}\n",
    "label2id={\"Neutral\":0, \"Positive\":1, \"Negative\":2}\n",
    "MODEL_NAME = f'distilbert-base-uncased'\n",
    "DATA_PATH = f'./data/citation_sentiment_corpus.csv'\n",
    "DATA_PATH_NEW = f'./data/citation_sentiment_corpus_new.csv'\n",
    "SST2_FINETUNED_MODEL_PATH = f'./model/distilbert-base-uncased-finetuned-sst-2-english'\n",
    "BASE_MODEL_PATH = f'./model/distilbert-base-uncased'\n",
    "\n",
    "\n",
    "# 训练相关参数\n",
    "USE_DATALOADER = False\n",
    "USE_BASE_MODEL = False\n",
    "batch_size = 32\n",
    "loss_type='ce_loss' # 自定义参数 focal_loss dsc_loss ce_loss\n",
    "weight_decay = 0.01\n",
    "lr = 5e-6\n",
    "num_epochs = 5\n",
    "warmup_steps = 100\n",
    "label_smoothing_factor = 0.0\n"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T02:33:24.856024Z",
     "start_time": "2024-09-10T02:33:24.790406Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "rawdata_df = pd.read_csv(DATA_PATH)\n",
    "rawdata_df['Sentiment'] = rawdata_df['Sentiment'].map({'o': 0, 'p': 1, 'n': 2})\n",
    "rawdata_df['Sentence_Length'] = rawdata_df['Citation_Text'].str.len()\n",
    "rawdata_df = rawdata_df[rawdata_df['Sentence_Length'] <= 1024]\n",
    "print(rawdata_df.shape)\n",
    "rawdata_df.to_csv(DATA_PATH_NEW, index=False)\n",
    "rawdata_df = pd.read_csv(DATA_PATH_NEW)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8699, 5)\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T02:33:24.918347Z",
     "start_time": "2024-09-10T02:33:24.904328Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T02:33:30.802190Z",
     "start_time": "2024-09-10T02:33:24.966281Z"
    }
   },
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# 分割数据集：训练集80%，验证集10%，测试集10%\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    rawdata_df['Citation_Text'].tolist(), rawdata_df['Sentiment'].tolist(), test_size=TEST_DATASET_SIZE, stratify=rawdata_df['Sentiment'], random_state=42)\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    重构数据集类，使其能够返回字典格式的数据，有标签\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=num_labels, id2label=id2label, label2id=label2id).to(device)\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "# 将数据转换为 HuggingFace 的 Dataset 格式\n",
    "train_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})\n",
    "val_dataset = Dataset.from_dict({'text': val_texts, 'label': val_labels})\n",
    "test_dataset = Dataset.from_dict({'text': test_texts, 'label': test_labels})\n",
    "emotion_dataset = DatasetDict({'train': train_dataset, 'val': val_dataset, 'test': test_dataset})\n",
    "\n",
    "tokenized_datasets = emotion_dataset.map(lambda f: tokenizer(f['text'], padding=True, truncation=True, max_length=512),\n",
    "                                  batch_size=None, batched=True) # WordPiece分词器\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "train_dataset = tokenized_datasets['train']\n",
    "val_dataset = tokenized_datasets['val']\n",
    "test_dataset = tokenized_datasets['test']\n",
    "\n",
    "\n",
    "print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
    "print(f\"Test Dataset Size: {len(test_dataset)}\")\n",
    "print(f\"Val Dataset Size: {len(val_dataset)}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5219 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "edddc53e4b20424abf4a2284a128848f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1740 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c658c6c226b4218aa48d09faad0738c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1740 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f36d901a8b334f5197c2e9df615fefca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Size: 5219\n",
      "Test Dataset Size: 1740\n",
      "Val Dataset Size: 1740\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T02:33:31.110661Z",
     "start_time": "2024-09-10T02:33:30.894491Z"
    }
   },
   "source": [
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # 输出结果目录\n",
    "    num_train_epochs=num_epochs,\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=weight_decay,\n",
    "    warmup_steps=warmup_steps,\n",
    "    logging_strategy='steps',\n",
    "    logging_dir='./logs',            # 日志目录\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    disable_tqdm=False,\n",
    "    save_strategy=\"steps\",\n",
    "    fp16= torch.cuda.is_available(), # faster and use less memory\n",
    "    metric_for_best_model='F1',\n",
    "    # load_best_model_at_end=True,\n",
    "    label_smoothing_factor=label_smoothing_factor\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T02:33:31.173646Z",
     "start_time": "2024-09-10T02:33:31.159051Z"
    }
   },
   "source": [
    "# 使用Focal loss作为损失函数解决样本不均衡问题\n",
    "def py_sigmoid_focal_loss(pred, target, gamma=2.0, alpha=0.9, reduction='mean'):\n",
    "    pred_sigmoid = pred.sigmoid()\n",
    "    target = torch.nn.functional.one_hot(target, num_classes=pred.shape[1]).type_as(pred)  # 转换为one-hot编码\n",
    "    pt = (1 - pred_sigmoid) * target + pred_sigmoid * (1 - target)\n",
    "    focal_weight = (alpha * target + (1 - alpha) * (1 - target)) * pt.pow(gamma)\n",
    "    loss = F.binary_cross_entropy_with_logits(pred, target, reduction='none') * focal_weight\n",
    "    return loss.mean() if reduction == 'mean' else loss.sum()\n",
    "\n",
    "class MultiFocalLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Focal_Loss= -1*alpha*((1-pt)**gamma)*log(pt)\n",
    "    Args:\n",
    "        num_class: number of classes\n",
    "        alpha: class balance factor shape=[num_class, ]\n",
    "        gamma: hyper-parameter\n",
    "        reduction: reduction type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_class, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(MultiFocalLoss, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.smooth = 1e-4\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        if alpha is None:\n",
    "            self.alpha = torch.ones(num_class, ) - 0.5\n",
    "        elif isinstance(alpha, (int, float)):\n",
    "            self.alpha = torch.as_tensor([alpha] * num_class)\n",
    "        elif isinstance(alpha, (list, np.ndarray)):\n",
    "            self.alpha = torch.as_tensor(alpha)\n",
    "        if self.alpha.shape[0] != num_class:\n",
    "            raise RuntimeError('the length not equal to number of class')\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        # assert isinstance(self.alpha,torch.Tensor)\\\n",
    "        alpha = self.alpha.to(logit.device)\n",
    "        prob = F.softmax(logit, dim=1)\n",
    "\n",
    "        if prob.dim() > 2:\n",
    "            # used for 3d-conv:  N,C,d1,d2 -> N,C,m (m=d1*d2*...)\n",
    "            N, C = logit.shape[:2]\n",
    "            prob = prob.view(N, C, -1)\n",
    "            prob = prob.transpose(1, 2).contiguous()  # [N,C,d1*d2..] -> [N,d1*d2..,C]\n",
    "            prob = prob.view(-1, prob.size(-1))  # [N,d1*d2..,C]-> [N*d1*d2..,C]\n",
    "\n",
    "        ori_shp = target.shape\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        prob = prob.gather(1, target).view(-1) + self.smooth  # avoid nan\n",
    "        logpt = torch.log(prob)\n",
    "        # alpha_class = alpha.gather(0, target.squeeze(-1))\n",
    "        alpha_weight = alpha[target.squeeze().long()]\n",
    "        loss = -alpha_weight * torch.pow(torch.sub(1.0, prob), self.gamma) * logpt\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'none':\n",
    "            loss = loss.view(ori_shp)\n",
    "\n",
    "        return loss"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T02:33:31.236296Z",
     "start_time": "2024-09-10T02:33:31.222259Z"
    }
   },
   "source": [
    "# DSELoss\n",
    "class MultiDSCLoss(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Creates a criterion that optimizes a multi-class Self-adjusting Dice Loss\n",
    "    (\"Dice Loss for Data-imbalanced NLP Tasks\" paper)\n",
    "\n",
    "    Args:\n",
    "        alpha (float): a factor to push down the weight of easy examples\n",
    "        gamma (float): a factor added to both the nominator and the denominator for smoothing purposes\n",
    "        reduction (string): Specifies the reduction to apply to the output:\n",
    "            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
    "            ``'mean'``: the sum of the output will be divided by the number of\n",
    "            elements in the output, ``'sum'``: the output will be summed.\n",
    "\n",
    "    Shape:\n",
    "        - logits: `(N, C)` where `N` is the batch size and `C` is the number of classes.\n",
    "        - targets: `(N)` where each value is in [0, C - 1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1.0, smooth=1.0, reduction=\"mean\"):\n",
    "        super(MultiDSCLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.smooth = smooth\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        probs = torch.gather(probs, dim=1, index=targets.unsqueeze(1))\n",
    "\n",
    "        probs_with_factor = ((1 - probs) ** self.alpha) * probs\n",
    "        loss = 1 - (2 * probs_with_factor + self.smooth) / (probs_with_factor + 1 + self.smooth)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        elif self.reduction == \"none\" or self.reduction is None:\n",
    "            return loss\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Reduction `{self.reduction}` is not supported.\")"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T02:33:31.298364Z",
     "start_time": "2024-09-10T02:33:31.284351Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    " \n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, loss_type='focal_loss', *args, **kwargs):\n",
    "        super(CustomTrainer, self).__init__(*args, **kwargs)\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        if self.loss_type == 'focal_loss':\n",
    "            loss_fct = MultiFocalLoss(num_class=3, alpha=0.8, gamma=2.0, reduction='none')\n",
    "        elif self.loss_type == 'dsc_loss':\n",
    "            loss_fct = MultiDSCLoss(alpha=1.0, smooth=1.0, reduction='none')\n",
    "        elif self.loss_type == 'ce_loss':\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T02:33:33.229070Z",
     "start_time": "2024-09-10T02:33:31.345454Z"
    }
   },
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    " \n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    }\n",
    "\n",
    "# 创建Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    loss_type=loss_type # 自定义参数 focal_loss dsc_loss\n",
    ")\n",
    "\n",
    "results = trainer.train() "
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[41], line 26\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# 创建Trainer\u001B[39;00m\n\u001B[0;32m     16\u001B[0m trainer \u001B[38;5;241m=\u001B[39m CustomTrainer(\n\u001B[0;32m     17\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     18\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     23\u001B[0m     loss_type\u001B[38;5;241m=\u001B[39mloss_type \u001B[38;5;66;03m# 自定义参数 focal_loss dsc_loss\u001B[39;00m\n\u001B[0;32m     24\u001B[0m )\n\u001B[1;32m---> 26\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:1885\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1883\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   1884\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1885\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1886\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1887\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1888\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1889\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1890\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:2216\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2213\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[0;32m   2215\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[1;32m-> 2216\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2218\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2219\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   2220\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m   2221\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   2222\u001B[0m ):\n\u001B[0;32m   2223\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   2224\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:3250\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m   3248\u001B[0m         scaled_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m   3249\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 3250\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3252\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mdetach() \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\accelerate\\accelerator.py:2147\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[1;34m(self, loss, **kwargs)\u001B[0m\n\u001B[0;32m   2145\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m   2146\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 2147\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaler\u001B[38;5;241m.\u001B[39mscale(loss)\u001B[38;5;241m.\u001B[39mbackward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2148\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m learning_rate \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhas_lomo_optimizer:\n\u001B[0;32m   2149\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlomo_backward(loss, learning_rate)\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:193\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    189\u001B[0m inputs \u001B[38;5;241m=\u001B[39m (inputs,) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inputs, torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;28;01melse\u001B[39;00m \\\n\u001B[0;32m    190\u001B[0m     \u001B[38;5;28mtuple\u001B[39m(inputs) \u001B[38;5;28;01mif\u001B[39;00m inputs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m()\n\u001B[0;32m    192\u001B[0m grad_tensors_ \u001B[38;5;241m=\u001B[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001B[38;5;28mlen\u001B[39m(tensors))\n\u001B[1;32m--> 193\u001B[0m grad_tensors_ \u001B[38;5;241m=\u001B[39m \u001B[43m_make_grads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_grads_batched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retain_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:88\u001B[0m, in \u001B[0;36m_make_grads\u001B[1;34m(outputs, grads, is_grads_batched)\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m out\u001B[38;5;241m.\u001B[39mrequires_grad:\n\u001B[0;32m     87\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m out\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m---> 88\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad can be implicitly created only for scalar outputs\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     89\u001B[0m     new_grads\u001B[38;5;241m.\u001B[39mappend(torch\u001B[38;5;241m.\u001B[39mones_like(out, memory_format\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mpreserve_format))\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mRuntimeError\u001B[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T02:33:33.242620600Z",
     "start_time": "2024-07-22T09:12:13.567464Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66f0b6516e944458ba4fbbd7da635e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "preds_output = trainer.predict(test_dataset)\n",
    "probs = torch.softmax(torch.tensor(preds_output.predictions), dim=-1).numpy()\n",
    "predicted_labels = np.argmax(preds_output.predictions, axis=1)\n",
    "plot_confusion_matrix(test_labels, predicted_labels, list(label2id.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 误差分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T02:33:33.243620300Z",
     "start_time": "2024-07-22T09:12:16.581604Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "    # Place all input tensors on the same device as the model\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        if loss_type == 'dsc_loss':\n",
    "            loss_fct = MultiDSCLoss(alpha=1.0, smooth=1.0, reduction='none')\n",
    "        elif loss_type == 'focal_loss':\n",
    "            loss_fct = MultiFocalLoss(num_class=3, alpha=0.8, gamma=2.0, reduction='none')\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        loss = loss_fct(output.logits, batch[\"label\"].to(device))\n",
    "    \n",
    "    # Place outputs on CPU for compatibility with other dataset columns   \n",
    "    return {\"loss\": loss.cpu().numpy(), \n",
    "            \"predicted_label\": pred_label.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T02:33:33.244124Z",
     "start_time": "2024-07-22T09:12:16.596707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ae7c25408d4227a949942cd01dc79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert our dataset back to PyTorch tensors\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "# Compute loss values\n",
    "tokenized_datasets[\"val\"] = tokenized_datasets[\"val\"].map(forward_pass_with_label, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T02:33:33.244206600Z",
     "start_time": "2024-07-22T09:12:20.653008Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(\"pandas\")\n",
    "cols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\n",
    "df_test = tokenized_datasets[\"val\"][:][cols]\n",
    "df_test[\"label\"] = df_test[\"label\"].map(id2label)\n",
    "df_test[\"predicted_label\"] = (df_test[\"predicted_label\"].map(id2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T02:33:33.245127700Z",
     "start_time": "2024-07-22T09:12:20.667941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>polarity turney verb measures noun consider cl...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>3.081703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>bootstrapping em structure yarowsky each eithe...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>3.045681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>tillmann reference liang instead directly zhan...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>3.019126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>account ibm phrase alignment under standard do...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>3.012346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>cannot tag tagging ratnaparkhi tagger word 199...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>3.010471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>uses note similar 2007 method to our which tha...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2.974247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>bootstrapping yarowsky disambiguation sense pa...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>averaged perceptron estimated collins training...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2.952198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mt shen dependency provides over models langua...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2.919110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>wiebe labeled yarowsky small resnik supervised...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2.893411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text     label  \\\n",
       "239  polarity turney verb measures noun consider cl...  Negative   \n",
       "674  bootstrapping em structure yarowsky each eithe...  Negative   \n",
       "502  tillmann reference liang instead directly zhan...  Negative   \n",
       "6    account ibm phrase alignment under standard do...  Negative   \n",
       "822  cannot tag tagging ratnaparkhi tagger word 199...  Negative   \n",
       "101  uses note similar 2007 method to our which tha...  Negative   \n",
       "933  bootstrapping yarowsky disambiguation sense pa...  Negative   \n",
       "873  averaged perceptron estimated collins training...  Negative   \n",
       "3    mt shen dependency provides over models langua...  Negative   \n",
       "601  wiebe labeled yarowsky small resnik supervised...  Negative   \n",
       "\n",
       "    predicted_label      loss  \n",
       "239         Neutral  3.081703  \n",
       "674         Neutral  3.045681  \n",
       "502         Neutral  3.019126  \n",
       "6           Neutral  3.012346  \n",
       "822         Neutral  3.010471  \n",
       "101         Neutral  2.974247  \n",
       "933         Neutral  2.959000  \n",
       "873         Neutral  2.952198  \n",
       "3           Neutral  2.919110  \n",
       "601         Neutral  2.893411  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.sort_values(\"loss\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T02:33:33.246128100Z",
     "start_time": "2024-07-22T09:12:20.683048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>ramshaw marcus scheme annotated tagging 1995 i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.171856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>sources evaluate implementation collins parse ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.179551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>ramshaw marcus labeling entity labeled where c...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.179588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>patterns form collins parsed then verb these t...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.183005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>hoang mapping value koehn target side sequence...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.183141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>marcus sections treebank penn grammars learn 1...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.184061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>12 provided yarowsky each list words 1995 show...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.184273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>agreement carletta inter annotations follow an...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.184496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>dunning ratio pair log pairs compute associati...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.184609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>melamed strings parsing grammars inference fre...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.184730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    label  \\\n",
       "732   ramshaw marcus scheme annotated tagging 1995 i...  Neutral   \n",
       "368   sources evaluate implementation collins parse ...  Neutral   \n",
       "706   ramshaw marcus labeling entity labeled where c...  Neutral   \n",
       "584   patterns form collins parsed then verb these t...  Neutral   \n",
       "292   hoang mapping value koehn target side sequence...  Neutral   \n",
       "870   marcus sections treebank penn grammars learn 1...  Neutral   \n",
       "571   12 provided yarowsky each list words 1995 show...  Neutral   \n",
       "1028  agreement carletta inter annotations follow an...  Neutral   \n",
       "657   dunning ratio pair log pairs compute associati...  Neutral   \n",
       "273   melamed strings parsing grammars inference fre...  Neutral   \n",
       "\n",
       "     predicted_label      loss  \n",
       "732          Neutral  0.171856  \n",
       "368          Neutral  0.179551  \n",
       "706          Neutral  0.179588  \n",
       "584          Neutral  0.183005  \n",
       "292          Neutral  0.183141  \n",
       "870          Neutral  0.184061  \n",
       "571          Neutral  0.184273  \n",
       "1028         Neutral  0.184496  \n",
       "657          Neutral  0.184609  \n",
       "273          Neutral  0.184730  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.sort_values(\"loss\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "best_model_dir = './model/distilbert-base-uncased-finetuned-citation'\n",
    "trainer.save_model(best_model_dir)\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained(best_model_dir).to(device)\n",
    "\n",
    "\n",
    "# def predict_sentiment_score(model, dataset, batched=True, batch_size=32, weights=[-0.5, 1, -1], shuffle=False):\n",
    "#     '''\n",
    "#     预测句子的情感\n",
    "#     '''\n",
    "#     results = []\n",
    "#     sentiment_scores = []\n",
    "#     model.eval()  # 将模型设置为评估模式\n",
    "\n",
    "#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "#     if batched:\n",
    "#         with torch.no_grad():  # 不计算梯度，以加速和节省内存\n",
    "#             for batch in dataloader:\n",
    "#                 # 将批次数据移动到cuda\n",
    "#                 batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#                 outputs = model(**batch)\n",
    "#                 logits = outputs.logits\n",
    "#                 probabilities = F.softmax(logits, dim=-1)\n",
    "                \n",
    "#                 logits_list = logits.tolist()\n",
    "#                 probabilities_list = probabilities.tolist()\n",
    "#                 results.extend(zip(logits_list, probabilities_list))\n",
    "#     else:\n",
    "#         with torch.no_grad():\n",
    "#             inputs = tokenizer(dataset, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "#             outputs = model(**inputs)\n",
    "#             logits = outputs.logits\n",
    "#             probabilities = F.softmax(logits, dim=-1)\n",
    "#             logits_list = logits.tolist()\n",
    "#             probabilities_list = probabilities.tolist()\n",
    "#             results.extend(zip(logits_list, probabilities_list))\n",
    "\n",
    "#     for _, softmax_probs in results:\n",
    "#         softmax_probs_array = np.array(softmax_probs)\n",
    "#         weights_array = np.array(weights)\n",
    "#         score = np.sum(softmax_probs_array * weights_array)\n",
    "#         score = max(min(score, 1), -1)\n",
    "#         sentiment_scores.append(score)\n",
    "#     return sentiment_scores\n",
    "\n",
    "# weights=[-0.1, 1, -1]\n",
    "# all_dataset = ConcatDataset([train_dataset, test_dataset, val_dataset])\n",
    "# sentiment_scores = predict_sentiment_score(best_model, all_dataset, weights=weights, batched=True, batch_size=1, shuffle=True) # 句子维度不一样就把batch_size设为1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
