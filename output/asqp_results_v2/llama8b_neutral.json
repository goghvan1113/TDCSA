[
    {
        "text": "In this paper, we give an overview of NLPWin, a multi-application natural language analysis and generation system under development at Microsoft Research  , incorporating analysis systems for 7 languages  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NLPWin",
                "multi-application natural language analysis and generation system",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "analysis systems",
                "for 7 languages",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "After each step the annotations were compared using the ~ statistic as reliability measure for all classification tasks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotations",
                "compared using the ~ statistic",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "reliability measure",
                "for all classification tasks",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In many applications, it is natural and convenient to construct class-based language models, that is models based on classes of words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "class-based language models",
                "natural and convenient",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "models based on classes of words",
                "based on classes of words",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The latter problem of developing methods that can work with incomplete supervisory information is addressed in a subsequent effort  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "can work with incomplete supervisory information",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "effort",
                "subsequent",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "c2009 Association for Computational Linguistics Semi-supervised Training for the Averaged Perceptron POS Tagger Drahomra johanka Spoustova Jan Hajic Jan Raab Miroslav Spousta Institute of Formal and Applied Linguistics Faculty of Mathematics and Physics, Charles University Prague, Czech Republic {johanka,hajic,raab,spousta}@ ufal.mff.cuni.cz Abstract This paper describes POS tagging experiments with semi-supervised training as an extension to the   averaged perceptron algorithm, first introduced for this task by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semi-supervised training",
                "as an extension to the averaged perceptron algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "averaged perceptron algorithm",
                "first introduced for this task",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "ang and Lee   applied two different classifiers to perform sentiment annotation in two sequential steps: the first classifier separated subjective   texts from objective   ones and then they used the second classifier to classify the subjective texts into positive and negative",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifiers",
                "two different",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "classifier",
                "separated subjective texts from objective ones",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The k-best list is also frequently used in discriminative learning to approximate the whole set of candidates which is usually exponentially large  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "k-best list",
                "frequently used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "candidates",
                "exponentially large",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "he fact that information consisting of nothing more than bigrams can capture syntactic information about English has already been noted by  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bigrams",
                "capture syntactic information about English",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "information",
                "has already been noted",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "he mutual information of a pair of words is defined in terms of their co-occurrence frequency and respective occurrence frequencies  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "co-occurrence frequency",
                "is defined",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "respective occurrence frequencies",
                "are used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A third of this is syntactically parsed as part of the Penn Treebank   and has dialog act annotation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "has dialog act annotation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "part of the Penn Treebank",
                "is syntactically parsed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The minimum error training   was used on the development data for parameter estimation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training",
                "minimum error",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parameter estimation",
                "on development data",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5.4 IBM-3 Word Alignment Models Since the true distribution over alignments is not known, we used the IBM-3 statistical translation model   to approximate . This model is specified through four components: Fertility probabilities for words; Fertility probabilities for NULL; Word Translation probabilities; and Distortion probabilities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM-3 Word Alignment Models",
                "is specified through four components",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM-3 statistical translation model",
                "approximate the true distribution",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For evaluation we use a state-of-the-art baseline system     which works with a log-linear interpolation of feature functions optimized by MERT  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "state-of-the-art baseline system",
                "works with log-linear interpolation of feature functions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "MERT",
                "optimized",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In general the training set is the parsed Wall Street Journal  , with few exceptions, and the size of the training samples is around 10-20,000 test cases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training set",
                "parsed Wall Street Journal",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "size of the training samples",
                "around 10-20,000 test cases",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this approach we extend the denition overlap by considering the distributional similarity   rather than identify of the words in the two denitions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "denition",
                "distributional similarity",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "words",
                "identify",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "It is for all three reasons, i.e. translation, induction from alignment structures and induction of alignment structures, important that the synchronous grammars are expressive enough to induce all the alignment structures found in hand-aligned gold standard parallel corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "  Apply some statistical tests such as the Binomial Hypothesis Test   and loglikelihood ratio score   to SCCs to filter out false SCCs on the basis of their reliability and likelihood.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SCCs",
                "reliability and likelihood",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "statistical tests",
                "such as the Binomial Hypothesis Test and loglikelihood ratio score",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Except where noted, each system was trained on 27 million words of newswire data, aligned with GIZA++   and symmetrized with the grow-diag-final-and heuristic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "aligned with",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "grow-diag-final-and heuristic",
                "symmetrized with",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We evaluate its performance on the standard Penn English Treebank   dependency parsing task, i.e., train on sections 02-21 and test on section 23 with automatically assigned POS tags   using a tagger similar to Collins  , and using the headrules of Yamada and Matsumoto   for conversion into dependency trees.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn English Treebank",
                "dependency parsing task",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "headrules of Yamada and Matsumoto",
                "for conversion into dependency trees",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The reason may be that shorter dependencies are often modifier of nouns such as determiners or adjectives or pronouns modifying their direct neighbors, while longer dependencies typically represent modifiers of the root or the main verb in a sentence .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependencies",
                "modifier of nouns",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dependencies",
                "represent modifiers",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For MCE learning, we selected the reference compression that maximize the BLEU score    ) from the set of reference compressions and used it as correct data for training.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reference compression",
                "maximize the BLEU score",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "reference compressions",
                "set of reference compressions",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Furthermore, Bikel   provides evidence that lexical information   only makes a small contribution to the performance of parsing models such as Collinss  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing models",
                "such as Collinss",
                "PERFORMANCE",
                "neutral",
                0.85
            ],
            [
                "lexical information",
                "only makes a small contribution",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ur technique of generating negative examples is similar to the approach of Okanohara and Tsujii  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "similar to",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Okanohara and Tsujii",
                "their approach",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Different methods have been proposed to reduce error propagation between pipelined tasks, both in general   and for specific problems such as language modeling and utterance classification   and labeling and chunking  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "pipelined tasks",
                "reducing error propagation",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "All reviews were automatically preprocessed to remove both explicit rating indicators and objective sentences; the motivation for the latter step is that it has previously aided positive vs. negative classi cation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rating indicators",
                "preprocessed to remove",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "latter step",
                "has previously aided positive vs. negative classification",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The software also required GIZA++ word alignment tool .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ word alignment tool",
                "required",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "GIZA++ word alignment tool",
                "GIZA++",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Previous work has shown that data collected through the Mechanical Turk service is reliable and comparable in quality with trusted sources  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data",
                "reliable",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "data",
                "comparable in quality with trusted sources",
                "PERFORMANCE",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "This can be done by smoothing the observed frequencies 7   or by class-based methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "observed frequencies",
                "smoothing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "class-based methods",
                "methods",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our approach is closely related to previous CoTraining methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CoTraining methods",
                "closely related",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "CoTraining methods",
                "previous",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A further development has been first introduced by   who recasts the problem of adding latent annotations as an unsupervised learning problem: given an observed PCFG induced from the treebank, the latent grammar is generated by combining every non terminal of the observed grammar to a predefined set H of latent symbols.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "latent grammar",
                "generated by combining every non terminal of the observed grammar to a predefined set H of latent symbols",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "latent symbols",
                "predefined set H",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Since the creation of BLEU   and NIST  , the subject of automatic evaluation metrics for MT has been given quite a lot of attention.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU and NIST",
                "given quite a lot of attention",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "subject of automatic evaluation metrics for MT",
                "has been given",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Semi-supervised conditional random fields   based on a minimum entropy regularizer   have been proposed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "minimum entropy regularizer",
                "have been proposed",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "semi-supervised conditional random fields",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Even for relatively general texts, such as the Wall Street Journal   or terrorism articles  , Roark and Charniak   reported that 3 of every 5 terms generated by their semantic lexicon learner were not present in WordNet.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic lexicon learner",
                "were not present in WordNet",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "Roark and Charniak",
                "reported",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Word Error Rate  , which penalizes the edit distance against reference translations   BLEU: the geometric mean of n-gram precision for the translation results found in reference translations   Translation Accuracy  : subjective evaluation ranks ranging from A to D  , judged blindly by a native speaker   In contrast to WER, higher BLEU and ACC scores indicate better translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Word Error Rate",
                "penalizes the edit distance against reference translations",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU",
                "geometric mean of n-gram precision for the translation results",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "After this conversion, we had 1000 positive and 1000 negative examples for each domain, the same balanced composition as the polarity dataset  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "polarity dataset",
                "same balanced composition",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "examples",
                "positive and negative",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Smadja, Frank   \"\"Retrieving collocations from text\"\", Computational Linguistics 19 :143-177.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Retrieving collocations from text",
                "opinion term",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Computational Linguistics",
                "neutral description",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, the sentence I went to California last May would be marked for base NPs as: I went to California last May I 0 0 I B I indicating that the NPs are I, California and last May. This approach has been studied in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NPs",
                "base NPs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence",
                "would be marked",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We have used a state-of-the-art Chinese handwriting recognizer   developed by ATC, CCL, ITRI, Taiwan as the basis of our experiments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Chinese handwriting recognizer",
                "state-of-the-art",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "basis",
                "basis of our experiments",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In particular, we use the name/instance lists described by   and available on Fleischmans web page to generate features between names and nominals  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "name/instance lists",
                "described by Fleischman and available on Fleischman's web page",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "features",
                "between names and nominals",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Moreover, as Collins   mentions, some of the benefits of Model 2 are already captured by inclusion of the distance measure.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distance measure",
                "are already captured",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Model 2",
                "are already captured",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Recently,   introduced an approach for incorporating a dependency-based language model into SMT.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency-based language model",
                "into SMT",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "approach",
                "recently introduced",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Applications of word clustering include language modeling  , text classification  , thesaurus construction   and so on.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word clustering",
                "include language modeling, text classification, thesaurus construction",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "word clustering",
                "and so on",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To 848 make feature ranking computationally tractable in   and   a simplified process proposed: at the feature ranking stage when adding a new feature to the model, all previously computed parameters are kept fixed and, thus, we have to fit only one new constraint imposed by the candidate feature.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature ranking",
                "simplified process",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "candidate feature",
                "imposed by",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4.2 Interpreting reliability results It has been argued elsewhere   that since the amount of agreement one would expect by chance depends on the number and relative frequencies of the categories under test, reliability for category classifications should be measured using the kappa coefficient.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa coefficient",
                "should be measured",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "amount of agreement",
                "depends on number and relative frequencies of categories under test",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Thus, an orthogonal line of research can involve inducing classes for words which are more general than single categories, i.e., something akin to ambiguity classes  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classes",
                "more general than single categories",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ambiguity classes",
                "akin to",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "We compared our system Lynx against a freely available phrase-based decoder Pharaoh  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Lynx",
                "against a freely available phrase-based decoder",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Pharaoh",
                "freely available phrase-based decoder",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Similar to  , each word in the confusion network is associated with a word posterior probability.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word",
                "posterior probability",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "confusion network",
                "associated with",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Alignment spaces can emerge from generative stories  , from syntactic notions  , or they can be imposed to create competition between links  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment spaces",
                "emerge from generative stories",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "links",
                "create competition between",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Experiments We have conducted a series of lexical acquisition experiments with the above algorithm on largescale English corpora, e.g., the Brown corpus \\ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "on largescale English corpora",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Brown corpus",
                "e.g.",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In our approach, we take into account both the relative positions of the nearby context words as well as the mutual information   associated with the occurrence of a particular context word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "nearby context words",
                "relative positions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "context word",
                "mutual information",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "dditional evidence for this distinction is given in Pustejovsky and Anick   and Briscoe et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pustejovsky and Anick",
                "given in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Briscoe et al",
                "given in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "CFGs extracted from such structures were then annotated with hidden variables encoding the constraints described in the previous section and trained until convergence by means of the Inside-Outside algorithm defined in   and applied in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Inside-Outside algorithm",
                "defined in  and applied in ",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "hidden variables",
                "encoding constraints described in the previous section",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman   and Collins  ; 336 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "heuristics",
                "are used to automatically annotate",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "HPSG category",
                "is assigned in accordance with its CFG category",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Since Soon   started the trend of using the machine learning approach by using a binary classifier in a pairwise manner for solving co-reference resolution problem, many machine learning-based systems have been built, using both supervised and, unsupervised learning methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning approach",
                "using a binary classifier in a pairwise manner",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "machine learning-based systems",
                "using both supervised and unsupervised learning methods",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Barzilay & Lee   employ Multiple Sequence Alignment   to align strings extracted from closely related news articles.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Multiple Sequence Alignment",
                "employed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "closely related news articles",
                "extracted from",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Since we approach decoding as xR transduction, the process is identical to that of constituencybased algorithms  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "xR transduction",
                "identical to that of constituency-based algorithms",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "constituency-based algorithms",
                "that of",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Discriminative parsing has been investigated before, such as in Johnson  , Clark and Curran  , Henderson  , Koo and Collins  , Turian et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Discriminative parsing",
                "has been investigated before",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Johnson, Clark and Curran, Henderson, Koo and Collins, Turian et al.",
                "previous research",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The data sets used are the standard data sets for this problem   taken from the Wall Street Journal corpus in the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data sets",
                "standard data sets",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Wall Street Journal corpus",
                "standard",
                "APPLICABILITY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "This corpus-based information typically concerns sequences of 1-3 tags or words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sequences of 1-3 tags or words",
                "concerns",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "1-3 tags or words",
                "typically",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Not unlike   we use confidence of our classifier on unannotated data to enrich itself; that is, by adding confidently-classified instances to the memory.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifier",
                "confidently-classified instances",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "memory",
                "enrich itself",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "3.1 Binarizable segmentations   Following  , every sequence of phrase alignments can be viewed 1For example, if the cut-off on phrase pairs is ten words, all sentence pairs smaller than ten words in the training data will be included as phrase pairs as well.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase pairs",
                "ten words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence pairs",
                "smaller than ten words",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "WSD that use information gathered from raw corpora      .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSD",
                "use information gathered from raw corpora",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "corpora",
                "raw",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These dependencies differ from those used by Liu and Gildea  , in that they are extracted according to the rules of the LFG grammar and they are labelled with a type of grammatical relation that connects the head and the modifier, such as subject, determiner, etc. The presence of grammatical relation labels adds another layer of important linguistic information into the comparison and allows us to account for partial matches, for example when a lexical item finds itself in a correct relation but with an incorrect partner.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Prior to running the parsers, we trained the POS tagger described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger",
                "described in ",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tagger",
                "trained",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Like baseNP chunking , content chunk parsing is also a kind of shallow parsing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseNP chunking",
                "kind of shallow parsing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "content chunk parsing",
                "kind of shallow parsing",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction There has been a great deal of recent interest in the unsupervised discovery of syntactic structure from text, both parts-of-speech   and deeper grammatical structure like constituency and dependency trees  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parts-of-speech",
                "unsupervised discovery",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "syntactic structure",
                "from text",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Hence our classifier evaluation omits those two word positions, leading to n2 classifications for a string of length n. Table 1 shows statistics from sections 2-21 of the Penn WSJ Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifier evaluation",
                "omits those two word positions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn WSJ Treebank",
                "statistics from sections 2-21",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For instance, for Maximum Entropy, I picked   for the basic theory,   for an application  , and   for more advanced topics such as optimization and smoothing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy",
                "basic theory",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Maximum Entropy",
                "more advanced topics",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Note that the results of MB-D here cannot be directly compared with those in  , mainly because the data used are different.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data",
                "are different",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "results",
                "cannot be directly compared",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Endemic structural ambiguity, which can lead to such difficulties as trying to cope with the many thousands of possible parses that a grammar can assign to a sentence, can be greatly reduced by adding empirically derived probabilities to grammar rules   and by computing statistical measures of lexical association  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "In experiments with the system of   we have found that in practice a large number of complete translations are completely monotonic  , suggesting that the system has difficulty learning exactly what points in the translation should allow reordering.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "has difficulty learning",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "translations",
                "are completely monotonic",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The approach is related, but not identical, to distributional similarity   and  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "related, but not identical",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "distributional similarity",
                "and ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2.4 Maximum Entropy Classifier Maximum Entropy Models   seek to maximise the conditional probability of classes, given certain observations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy Classifier",
                "seek to maximise the conditional probability of classes, given certain observations",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Maximum Entropy Models",
                "seek to maximise",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For the log-linear model training, we take minimum-error-rate training method as described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-linear model",
                "minimum-error-rate training method",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "minimum-error-rate training method",
                "as described in",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation  , and parameterized by mixtures of a robust nonlexical syntax/alignment model with a lexical-semantics-drivenlog-linear model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilistic quasi-synchronous grammar",
                "inspired by one proposed for machine translation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "model",
                "parameterized by mixtures of a robust nonlexical syntax/alignment model with a lexical-semantics-driven log-linear model",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Wu   modeled the reordering process with binary branching trees, where each production could be either in the same or in reverse order going from source to target language.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reordering process",
                "binary branching trees",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "production",
                "either in the same or in reverse order",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In our experiments, we used the full parse output from Collins parser  , in which every non-terminal node is already annotated with head information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins parser",
                "full parse output",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "non-terminal node",
                "already annotated with head information",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "imum error rate training     to maximize BLEU score  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "maximize",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "error rate training",
                "to minimize",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Using techniques described in Church and Hindle  , Church and Hanks  , and Hindle and Rooth  , below are some examples of the most frequent V-O pairs from the AP corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques",
                "described in Church and Hindle, Church and Hanks, and Hindle and Rooth",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "V-O pairs",
                "from the AP corpus",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Thus, it may not suffer from the issues of non-isomorphic structure alignment and non-syntactic phrase usage heavily  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "structure alignment",
                "heavily",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "issues",
                "non-isomorphic structure alignment and non-syntactic phrase usage",
                "LIMITATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Since the use of cluster of machines is not always practical,   showed a randomized data structure called Bloom filter, that can be used to construct space efficient language models 513 for SMT.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "cluster of machines",
                "not always practical",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "Bloom filter",
                "space efficient",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "where they are expected to be maximally discriminative  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "discriminative",
                "expected to be maximally",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "they",
                "where they are",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Goldwater and Griffiths   evaluated against the reduced tag set of 17 tags developed by Smith and Eisner  , while Johnson   evaluated against the full Penn Treebank tag set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reduced tag set of 17 tags",
                "developed by Smith and Eisner",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "full Penn Treebank tag set",
                "",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The agreement on identifying the boundaries of units, using the  statistic discussed in  , was  =.9  ; the agreement on features   was as follows: UTYPE: =.76; VERBED: =.9; FINITE: =.81.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistic",
                "discussed in ",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "agreement",
                "UTYPE: =.76",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This strategy is commonly used in multi-document summarization  , where the combination step eliminates the redundancy across selected excerpts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "combination step",
                "eliminates redundancy",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "excerpts",
                "selected",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As an overall decoding performance measure, we used the BLEU metric  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metric",
                "overall decoding performance measure",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Four alternatives are proposed in these special issues:   Brent  ,   Briscoe and Carroll  ,   Hindle and Rooth  , and   Weischedel et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alternatives",
                "Four",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "proposed",
                "by Brent, Briscoe and Carroll, Hindle and Rooth, and Weischedel et al.",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The method described by Kazama and Torisawa   is to rst extract the rst   noun phrase after the rst is, was, are, or were in the rst sentence of a Wikipedia article.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kazama and Torisawa's method",
                "extract the rst noun phrase",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "rst sentence of a Wikipedia article",
                "is",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These parser output trees can by produced by a second parser in a co-training scenario  , or by the same parser with a reranking component in a type of selftraining scenario .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser output trees",
                "can be produced",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parser",
                "with a reranking component",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, our system is the unsupervised learning with small POS-tagged corpus,and we do not restrict the word's sense set within either binary senses  or dictionary's homograph level .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS-tagged corpus",
                "small",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word's sense set",
                "within binary senses or dictionary's homograph level",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We build sentencespecific zero-cutoff stupid-backoff   5-gram language models, estimated using 4.7B words of English newswire text, and apply them to rescore either 10000-best lists generated by HCP or word lattices generated by HiFST.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "5-gram language models",
                "estimated using 4.7B words of English newswire text",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "10000-best lists",
                "generated by HCP or HiFST",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 Formulation Following Klein and Manning  , we use weighted directed hypergraphs   as an abstraction of the probabilistic parsing problem.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Formulation",
                "weighted directed hypergraphs",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probabilistic parsing problem",
                "abstraction",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For phrase-based translation model training, we used the GIZA++ toolkit  , and 1.0M bilingual sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ toolkit",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "1.0M bilingual sentences",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "everal sentiment information retrieval models were proposed in the framework of probabilistic language models by Eguchi and Lavrenko  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilistic language models",
                "framework of",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "sentiment information retrieval models",
                "were proposed",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Since the introduction of BLEU   the basic n-gram precision idea has been augmented in a number of ways.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "has been augmented in a number of ways",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "basic n-gram precision idea",
                "has been augmented",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Examples of this are bilexical grammars--such as Eisner and Satta  , Charniak  , Collins  --where the lexical heads of each constituent are annotated on both the rightand left-hand sides of the context-free rules, under the constraint that every constituent inherits the lexical head from exactly one of its children, and the lexical head of a POS is its terminal item.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bilexical grammars",
                "Eisner and Satta",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "lexical heads",
                "inherited from exactly one of its children",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The learning algorithm used for each stage of the classification task is a regularized variant of the structured Perceptron  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "learning algorithm",
                "regularized variant of the structured Perceptron",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "regularized",
                "METHODOLOGY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "BLEU   is a precision metric that assesses the quality of a translation in terms of the proportion of its word n-grams   that it shares with several reference translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "proportion of its word n-grams",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "translation quality",
                "assesses the quality",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "6.1.1 Nugget-Based Pyramid Evaluation For our first approach we used a nugget-based evaluation methodology  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation methodology",
                "nugget-based",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "evaluation",
                "based on nuggets",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "These heuristics are extensions of those developed for phrase-based models  , and involve symmetrising two directional word alignments followed by a projection step which uses the alignments to find a mapping between source words and nodes in the target parse trees  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "heuristics",
                "extensions of those developed for phrase-based models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "projection step",
                "uses the alignments to find a mapping",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Word alignment was carried out by running Giza++ implementation of IBM Model 4 initialized with 5 iterations of Model 1, 5 of the HMM aligner, and 3 iterations of Model 4   in both directions and then symmetrizing using the grow-diag-final-and heuristic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Giza++ implementation",
                "IBM Model 4",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "aligner",
                "HMM",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This increase of probabilities is defined as multiplicative change   as follows:   = P /P    The main innovation of the model in   is the possibility of adding at each step the best relation N = {Ri,j}as well as N = I  that is Ri,j with all the relations by the existing taxonomy.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "possibility of adding at each step the best relation N = {Ri,j} as well as N = I",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "taxonomy",
                "existing taxonomy",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Therefore, to make the phrase-based SMT system robust against data sparseness for the ranking task, we also make use of the IBM Model 4   in both directions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 4",
                "in both directions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase-based SMT system",
                "robust against data sparseness",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Feature comparison measures: to convert two feature sets into a scalar value, several measures have been proposed, such as cosine, Lins measure  , Kullback-Leibler   divergence and its variants.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measure",
                "have been proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Kullback-Leibler divergence and its variants",
                "variations of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The performance of cross-language information retrieval with a uniform T is likely to be limited in the same way as the performance of conventional information retrieval without term-frequency information, i.e., where the system knows which terms occur in which documents, but not how often  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "uniform T",
                "likely to be limited",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "conventional information retrieval without term-frequency information",
                "same way",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The lexical acquisition phase uses the GIZA++ word-alignment tool, an implementation   of IBM Model 5   to construct an alignment of MRs with NL strings.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ word-alignment tool",
                "implementation of IBM Model 5",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "alignment of MRs with NL strings",
                "construct",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For non-local features, we adapt cube pruning from forest rescoring  , since the situation here is analogous to machine translation decoding with integrated language models: we can view the scores of unit nonlocal features as the language model cost, computed on-the-fly when combining sub-constituents.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "non-local features",
                "cube pruning from forest rescoring",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "language model cost",
                "computed on-the-fly when combining sub-constituents",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he samplers that Goldwater and Griffiths   and Johnson   describe are pointwise collapsed Gibbs samplers",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "samplers",
                "are pointwise collapsed Gibbs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "samplers",
                "describe",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  and Smith and Smith   show how to employ the matrix-tree theorem",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "matrix-tree theorem",
                "employ",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Smith and Smith",
                "show how",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Their experiments were performed using a decoder based on IBM Model 4 using the translation techniques developed at IBM  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "based on IBM Model 4",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translation techniques",
                "developed at IBM",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Similarity-based smoothing   is an intuitively appealing approach to this problem where probabilities of unseen co-occurrences are estimated from probabilities of seen co-occurrences of distributionally similar events.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "intuitively appealing",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "probabilities of seen co-occurrences of distributionally similar events",
                "are estimated from",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In order to prove this induction step, we use the concept of order annotations  , which are strings that lexicalise the precedence relation between the nodes of a dependency tree.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "order annotations",
                "lexicalise the precedence relation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dependency tree",
                "nodes",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3A hypergraph is analogous to a parse forest  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hypergraph",
                "is analogous to a parse forest",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Words appearing in similax grammatical contexts are assumed to be similar, and therefore classified into the same class  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "are assumed to be similar",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "similarity classification",
                "classified into the same class",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For English, we have used sections 03-06 of the WSJ portion of the Penn Treebank   distributed by the Linguistic Data Consortium  , which have frequently been used to evaluate sentence boundary detection systems before; compare Section 7.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "distributed by the Linguistic Data Consortium",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "WSJ portion",
                "have frequently been used to evaluate",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "It was later applied by   as a way to determine if a sequence of N words   came from an independently distributed sample.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "N words",
                "came from an independently distributed sample",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "sequence",
                "way to determine",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Because their joint distributions have such closed-form expressions, the parameters can be estimated directly from the training data without the need for an iterative fitting procedure  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "estimated directly from the training data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "iterative fitting procedure",
                "need",
                "LIMITATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "We calculated the translation quality using Bleus modified n-gram precision metric   for n-grams of up to length four.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-gram precision metric",
                "modified Bleu",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "n-grams of up to length four",
                "up to",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We were already using a generative statistical model for part-of-speech tagging  , and more recently, had begun using a generative statistical model for name finding  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generative statistical model",
                "for part-of-speech tagging",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "generative statistical model",
                "for name finding",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this paper, we employed the Chinese word segmentation tool   that achieved about 0.93-0.96 recall/precision rates in the SIGHAN-3 word segmentation task  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Chinese word segmentation tool",
                "achieved 0.93-0.96 recall/precision rates",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "SIGHAN-3 word segmentation task",
                "in",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The reordered sentence is then re-tokenized to be consistent with the baseline system, which uses a different tokenization scheme that is more friendly to the MT system.3 We use BLEU scores as the performance measure in our evaluation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tokenization scheme",
                "more friendly",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "BLEU scores",
                "performance measure",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Furthermore, the underlying decoding strategies are too time consuming for our application We therefore use a translation model based on the simple linear interpolation given in equation 2 which combines predictions of two translation models -Ms and M~ -both based on IBM-like model 2 .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoding strategies",
                "too time consuming",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "translation model",
                "simple linear interpolation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Section 4 compares our results to Itindle's ones  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Itindle's results",
                "ones",
                "PERFORMANCE",
                "neutral",
                0.5
            ],
            [
                "results",
                "to",
                "PERFORMANCE",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "4.1 Training and Translation Setup Our decoder is a phrase-based multi-stack implementation of the log-linear model similar to Pharaoh  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "phrase-based multi-stack implementation of the log-linear model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "decoder",
                "similar to Pharaoh",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "The accuracy of the derived model depends heavily on the initial bias, but with a good choice results are comparable to those of method three  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "initial bias",
                "heavily on",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "results",
                "are comparable to those of method three",
                "PERFORMANCE",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "Ultinmtely, however, it seems that a more complex ai)t)roach incorporating back-off and smoothing is necessary ill order to achieve the parsing accuracy achieved by Charniak   and Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "incorporating back-off and smoothing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parsing accuracy",
                "achieved by Charniak and Collins",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "For this paper, we use an exact inference   CYK parser, using a simple probabilistic context-free grammar   induced from the Penn WSJ Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CYK parser",
                "exact inference",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn WSJ Treebank",
                "induced from",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There are many different similarity measures, which variously use taxonomic lexical hierarchies or lexical-semantic networks, large text corpora, word definitions in machine-readable dictionaries or other semantic formalisms, or a combination of these  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "similarity measures",
                "variously use",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpora",
                "large text",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "PairClass generates probability estimates, whereas Turney   uses a cosine measure of similarity.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PairClass",
                "probability estimates",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "Turney",
                "cosine measure of similarity",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction  , phrasal translation  , target word selection  , domain word translation  , sense disambiguation  , and even recently for query translation in cross-language IR as well  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Co-occurrence information",
                "has been used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase extraction",
                "phrasal translation",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For English, self-training contributes 0.83% absolute improvement to the PCFG-LA parser, which is comparable to the improvement obtained from using semi-supervised training with the twostage parser in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PCFG-LA parser",
                "0.83% absolute improvement",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "twostage parser",
                "comparable to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1.2 Statistical modeling for translation Earlier work in statistical machine translation   is based on the noisy-channel formulation where T = arg max T p  = argmax T p p    where the target language model p  is further decomposed as p  / productdisplay i p  where k is the order of the language model and the translation model p  has been modeled by a sequence of five models with increasing complexity  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical modeling",
                "based on noisy-channel formulation",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "translation model",
                "modeled by sequence of five models with increasing complexity",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Finally we trained model weights by maximizing BLEU   and set decoder optimization parameters   on a development test set of 200 held-out sentences each with a single reference translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model weights",
                "maximizing BLEU",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "decoder optimization parameters",
                "set on a development test set",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The highest BLEU score   was chosen as the optimization criterion.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "optimization criterion",
                "highest BLEU score",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "consistency among raters who may have different levels of fluency in the source language, raters are not shown the original French or Spanish sentence  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "raters",
                "different levels of fluency in the source language",
                "APPLICABILITY",
                "neutral",
                0.75
            ],
            [
                "original French or Spanish sentence",
                "not shown",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We want to avoid training a metric that as5Or, in a less adversarial setting, a system may be performing minimum error-rate training   signs a higher than deserving score to a sentence that just happens to have many n-gram matches against the target-language reference corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metric",
                "signs a higher than deserving score",
                "METHODOLOGY",
                "negative",
                0.7
            ],
            [
                "system",
                "performing minimum error-rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "here has been a large interest in recognizing non-overlapping noun phrases   and follow-up papers) but relatively little has been written about identifying phrases of other syntactic categories",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "noun phrases",
                "non-overlapping",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "syntactic categories",
                "other",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Practically, the grammar relaxation is done via the introduction of non-standard CCG rules  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CCG rules",
                "non-standard",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "grammar relaxation",
                "done via introduction",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Accurate automatic analysis of these aspects of language will augment existing research in the fields of sentiment   andsubjectivityanalysis(Wiebeetal.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "fields of sentiment and subjectivity analysis",
                "augment existing research",
                "APPLICABILITY",
                "positive",
                0.85
            ],
            [
                "analysis of these aspects of language",
                "will be accurate",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A superset of the parallel data was word aligned by GIZA union   and EMD  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA union",
                "was word aligned",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "EMD",
                "was word aligned",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There are many POS taggers developed using different techniques for many major languages such as transformation-based error-driven learning  , decision trees  , Markov model  , maximum entropy methods   etc for English.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS taggers",
                "different techniques",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "techniques",
                "for many major languages",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "This is in sharp contrast to the smoothed fixed-word statistics in most lexicalized parsing models derived from sparse data  , Collins  , Charniak  , etc.).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicalized parsing models",
                "smoothed fixed-word statistics",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sparse data",
                "derived from",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Figure 1  shows several orders of the sentence which violate this constraint.1 Previous studies have shown that if both the source and target dependency trees represent linguistic constituency, the alignment between subtrees in the two languages is very complex  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency trees",
                "represent linguistic constituency",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "alignment",
                "very complex",
                "PERFORMANCE",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Many of these tasks have been addressed in other fields, for example, hypothesis verification in the field of machine translation  , sense disambiguation in speech synthesis  , and relation tagging in information retrieval  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "field of machine translation",
                "hypothesis verification",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "relation tagging in information retrieval",
                "relation tagging",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ine 4 and 5 are similar to the phrase extraction algorithm by Och  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase extraction algorithm",
                "similar to",
                "METHODOLOGY",
                "positive",
                0.7
            ],
            [
                "phrase extraction algorithm",
                "by Och",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The first is a baseline of sorts, our own version of the \"\"chunking as tagging\"\" approach introduced by Ramshaw and Marcus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunking as tagging approach",
                "introduced by Ramshaw and Marcus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "baseline",
                "our own version",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "These rules can be handcrafted grammar rules, such as those of  , created semi-automatically   or, alternatively, extracted fully automatically from treebanks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar rules",
                "handcrafted or semi-automatically created",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "treebanks",
                "extracted fully automatically",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Some authors have already designed similar matching techniques, such as the ones described in   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques",
                "described in  and  ",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "matching techniques",
                "similar",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "his feature is implemented by using the IBM-1 lexical parameters  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM-1 lexical parameters",
                "implemented by using",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IBM-1 lexical parameters",
                "lexical parameters",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Recap of BLEU, ROUGE-W and METEOR The most commonly used automatic evaluation metrics, BLEU   and NIST  , are based on the assumption that The closer a machine translation is to a promt1: Life is like one nice chocolate in box ref: Life is just like a box of tasty chocolate ref: Life is just like a box of tasty chocolate mt2: Life is of one nice chocolate in box Figure 1: Alignment Example for ROUGE-W fessional human translation, the better it is  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "based on the assumption",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "machine translation",
                "the better it is",
                "PERFORMANCE",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "It is also related to loglinear models for machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "loglinear models",
                "for machine translation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "machine translation",
                "related to",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Four teams had approaches that relied   on an IBM model of statistical machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM model",
                "of statistical machine translation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approaches",
                "relied on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "So, we pre-tagged the input to the Bikel parser using the MXPOST tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bikel parser",
                "pre-tagged by MXPOST tagger",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "MXPOST tagger",
                "used for pre-tagging",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, only recently has work been done on the automatic computation of such relationships from text, quantifying similarity between words and clustering them  ,  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "automatic computation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "quantifying similarity",
                "clustering",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The triplet lexicon model presented in this work can also be interpreted as an extension of the standard IBM model 1   with an additional trigger.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM model 1",
                "standard",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "triplet lexicon model",
                "extension",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Algorithm 1 SCL   1: Select m pivot features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "m pivot features",
                "Select",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "m pivot features",
                "Select",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Neither   with 67% nor   with 59% noun attachment were anywhere close to this figure.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "noun attachment",
                "were anywhere close to this figure",
                "PERFORMANCE",
                "negative",
                0.85
            ],
            [
                "67%",
                "neither",
                "PERFORMANCE",
                "negative",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Empty categories   are used in the annotation of the PENN treebank   in order to represent syntactic phenomena like constituent movement  , discontinuous constituents, and missing elements  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PENN treebank",
                "annotation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "syntactic phenomena",
                "constituent movement, discontinuous constituents, missing elements",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We also present the results of \\  in Table 4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Table 4",
                "results",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "results",
                "presented",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "96 Research on DA classification initially focused on two-party conversational speech   and, more recently, has extended to multi-party audio recordings like the ICSI corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Research on DA classification",
                "initially focused on",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ICSI corpus",
                "more recently, has extended to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In particular, Abney defines a function K that is an upper bound on the negative log-likelihood, and shows his bootstrapping algorithms locally minimize K. We now present a generalization of Abneys K function and relate it to another semi-supervised learning technique, entropy regularization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "K",
                "an upper bound on the negative log-likelihood",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "bootstrapping algorithms",
                "locally minimize K",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use the finite-state parses of FaSTU$   for recognizing these entities, but the method extends to any basic phrasal parser 4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "FaSTU$",
                "finite-state parses",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "method",
                "extends to any basic phrasal parser",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Second, instead of disambiguating phrase senses as in  , we model word selection independently of the phrases used in the MT models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase senses",
                "as in  ",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "word selection",
                "independently of the phrases used in the MT models",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 Results The model summaries were compared against 24 summaries generated automatically using SUMMA by calculating ROUGE-1 to ROUGE4, ROUGE-L and ROUGE-W-1.2 recall metrics  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "summaries",
                "generated automatically using SUMMA",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "recall metrics",
                "ROUGE-1 to ROUGE4, ROUGE-L and ROUGE-W-1.2",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We are encoding the knowledge as axioms in what is for the most part  first-order logic, described in Hobbs  , although quantification over predicates is sometimes convenient.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "axioms",
                "first-order logic",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "quantification",
                "convenient",
                "METHODOLOGY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "Previous work has shown that high-quality unlexicalized PCFGs can be learned from a treebank, either by manual annotation   or automatic state splitting  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PCFGs",
                "high-quality",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "treebank",
                "can be learned",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The difference between MWA and bilingual word alignment   is that the MWA method works on monolingual parallel corpus instead of bilingual corpus used by bilingual word alignment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MWA",
                "works on monolingual parallel corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "bilingual word alignment",
                "uses bilingual corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Tag sets for English are derived from the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "is derived from",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Tag sets",
                "are derived",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use Entropy Regularization     to leverage unlabeled instances.7 We weight the ER term by choosing the best8 weight in {103,102,101,1,10} multiplied by #labeled#unlabeled for each data set and query selection method.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ER term",
                "choosing the best weight",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "weight",
                "multiplied by #labeled#unlabeled",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Dirichlet priors can be used to bias HMMs toward more skewed distributions  , which is especially useful in the weakly supervised setting consideredhere.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Dirichlet priors",
                "bias HMMs toward more skewed distributions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "weakly supervised setting",
                "considered here",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities   to putting non-parametric priors over derivations   to learning the set of states in a grammar  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bayesian modeling",
                "probabilistic grammars",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "grammar probabilities",
                "priors over",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "We determined appropriate training parameters and network size based on intermediate validation 1We used a publicly available tagger   to provide the tags.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger",
                "publicly available",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "network size",
                "based on intermediate validation",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Previous uses of this model include language modeling , machine translation , prepositional phrase attachment , and word morphology .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "language modeling, machine translation, prepositional phrase attachment, and word morphology",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "model",
                "previous uses",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "More recently, there have been many proposals to introduce syntactic knowledge into SMT models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT models",
                "introduce syntactic knowledge",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "proposals",
                "many",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Smadja   also detailed techniques for collocation extraction and developed a program called XTRACT, which is capable of computing flexible collocations based on elaborated statistical calculation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "XTRACT",
                "program",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "collocations",
                "capable of computing flexible",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Some methods use sentence alignment and additional statistics to find candidate translations of terms  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "use sentence alignment and additional statistics",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translations of terms",
                "candidate",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Introduction The Inversion Transduction Grammar   of Wu   is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Inversion Transduction Grammar",
                "syntactically motivated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word-level alignments",
                "of pairs of translationally equivalent sentences",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Set Test Set ENGLISH-WSJ Sections Section 22 Section 23   2-21 ENGLISH-BROWN see 10% of 10% of the   ENGLISH-WSJ the data6 the data6 FRENCH7 Sentences Sentences Sentences   1-18,609 18,610-19,609 19,609-20,610 GERMAN Sentences Sentences Sentences   1-18,602 18,603-19,602 19,603-20,602 Table 1: Corpora and standard experimental setups.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ENGLISH-WSJ",
                "standard experimental setup",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Corpora",
                "standard experimental setups",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Parsing sentences using statistical information gathered from a treebank was first examined a decade ago in   and is by now a fairly well-studied problem  ,  ,  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "problem",
                "fairly well-studied",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "sentences",
                "parsed using statistical information",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "urney   and Wiebe   focused on learning adjectives and adjectival phrases and Wiebe et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "urney and Wiebe",
                "focused on learning adjectives and adjectival phrases",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Wiebe et al",
                "focused on learning adjectives and adjectival phrases",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We further assume that the degree of difficulty of a phrase is directly correlated with the quality of the translation produced by the MT system, which can be approximated using an automatic evaluation metric, such as BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT system",
                "produced by",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU",
                "automatic evaluation metric",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "similar approach has been advocated for the interpretation of discourse relations by Marcu and Echihabi  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "has been advocated for",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "interpretation of discourse relations",
                "by Marcu and Echihabi",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This is in line with earlier work on consistent estimation for similar models  , and agrees with the most up-to-date work that employs Bayesian priors over the estimates  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "similar models",
                "consistent estimation",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "earlier work",
                "agrees with",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "DTM2, introduced in  , expresses the phrase-based translation task in a unified log-linear probabilistic framework consisting of three components:   a prior conditional distribution P0 ,   a number of feature functions i  that capture the translation and language model effects, and   the weights of the features i that are estimated under MaxEnt  , as in  : P  = P0 Z expsummationdisplay i ii    Here J is the skip reordering factor for the phrase pair captured by i  and represents the jump from the previous source word, and Z is the per source sentence normalization term.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "DTM2",
                "unified log-linear probabilistic framework",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "weights of the features",
                "estimated under MaxEnt",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.3 Collinss Head-Lexicalized Model In contrast to Carroll and Rooths   approach, the model proposed by Collins   does not compute rule probabilities directly.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collinss Head-Lexicalized Model",
                "does not compute rule probabilities directly",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Carroll and Rooths approach",
                "contrast to",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Furthermore, recent studies revealed that word clustering is useful for semi-supervised learning in NLP  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word clustering",
                "is useful",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "semi-supervised learning",
                "in NLP",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The IBM models   benefit from a one-tomany constraint, where each target word has ex105 the tax causes unrest l' impt cause le malaise Figure 1: A cohesion constraint violation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM models",
                "benefit from a one-to-many constraint",
                "METHODOLOGY",
                "positive",
                0.7
            ],
            [
                "cohesion constraint violation",
                "A cohesion constraint violation",
                "LIMITATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Some works focused on learning rules from comparable corpora, containing comparable documents such as different news articles from the same date on the same topic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "comparable documents",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "documents",
                "different news articles from the same date on the same topic",
                "INNOVATION",
                "neutral",
                0.75
            ]
        ]
    },
    {
        "text": "An important aspect of web search is to be able to narrow down search results by distinguishing among people with the same name leading to multiple efforts focusing on web person name disambiguation in the literature  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "people with the same name",
                "leading to multiple efforts",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "web person name disambiguation",
                "focusing on",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.4 Perspectives for automatic paraphrase extraction There is a growing amount of work on automatic extraction of paraphrases from text corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "growing amount of work",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "work",
                "on automatic extraction of paraphrases",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Section 7 considers recent efforts to induce effective procedures for automated sense labelling of discourse relations that are not lexically marked  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "procedures",
                "effective",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "discourse relations",
                "not lexically marked",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Finally, recent efforts have also looked at transfer learning mechanisms for sentiment analysis, e.g., see  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transfer learning mechanisms",
                "for sentiment analysis",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "sentiment analysis",
                "e.g.",
                "PERFORMANCE",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "The de-facto answer came during the 1990s from the research community on Statistical Machine Translation, who made use of statistical tools based on a noisy channel model originally developed for speech recognition  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Statistical Machine Translation",
                "research community",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical tools",
                "based on a noisy channel model",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Previous work in statistical synchronous grammars has been limited to forms of synchronous context-free grammar  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical synchronous grammars",
                "has been limited",
                "INNOVATION",
                "negative",
                0.6
            ],
            [
                "synchronous context-free grammar",
                "forms of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  and   discuss different ways of generalizing the tree-level crosslinguistic correspondence relation, so it is not confined to single tree nodes, thereby avoiding a continuity assumption.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "crosslinguistic correspondence relation",
                "not confined to single tree nodes",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "continuity assumption",
                "avoiding",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "3.1 Phrase-Based Models According to the translation model presented in  , given a source sentence f, the best target translation can be obtained using the following model best e 288 )     Where the translation model can be decomposed into ) ,|  | |  Where )|( i i ef is the phrase translation probability.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase translation probability",
                "best target translation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation model",
                "can be decomposed into",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "thesaurus acquisition",
                "based on various measures of similarity",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "various measures of similarity",
                "topically related words",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Because treebank annotation for individual formalisms is prohibitively expensive, there have been a number of efforts to extract TAGs, LFGs, and, more recently, HPSGs, from the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treebank annotation",
                "prohibitively expensive",
                "LIMITATION",
                "negative",
                0.75
            ],
            [
                "Penn Treebank",
                " ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Training Set  : There are many labeled English corpora available on the Web and we used the corpus constructed for multi-domain sentiment classification   9 , because the corpus was large-scale and it was within similar domains as the test set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "large-scale",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "corpus",
                "within similar domains as the test set",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "OUGE-LCS calculated the longest common 2 Details of our official DUC 2004 headline generation system can be found in Doran et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "OUGE-LCS",
                "calculated the longest common 2",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Details of our official DUC 2004 headline generation system",
                "can be found in Doran et al",
                "LIMITATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Hyponymy relations were extracted from definition sentences  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "definition sentences",
                "were extracted",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Hyponymy relations",
                "were extracted",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "here are some existing corpus linguistic researches on automatic extraction of collocations from electronic text  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus linguistic researches",
                "existing",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "automatic extraction of collocations from electronic text",
                "from",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To test whether a better set of initial parameter estimates can improve Model 1 alignment accuracy, we use a heuristic model based on the loglikelihood-ratio   statistic recommended by Dunning  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 1",
                "alignment accuracy",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "loglikelihood-ratio statistic",
                "recommended by Dunning",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction and Previous Research It is by now commonplace knowledge that accurate syntactic parsing is not possible given only a context-free grammar with standard Penn Treebank   labels  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "standard Penn Treebank labels",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "accurate syntactic parsing",
                "not possible",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "On the other hand, statistical MT employing IBM models   translates an input sentence by the combination of word transfer and word re-ordering.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM models",
                "translates an input sentence by the combination of word transfer and word re-ordering",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "statistical MT",
                "employing",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Since their appearance, BLEU   and NIST   have been the standard tools used for evaluating the quality of machine translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "standard tools",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "NIST",
                "standard tools",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Generally, WSD methods use the context of a word for its sense disambiguation, and the context information can come from either annotated/unannotated text or other knowledge resources, such as WordNet  , SemCor  , Open Mind Word Expert  , eXtended WordNet  , Wikipedia  , parallel corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSD methods",
                "use the context of a word for its sense disambiguation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "context information",
                "comes from annotated/unannotated text or other knowledge resources",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We argue that linguistic knowledge could not only improve results   but is essential when extracting collocations from certain languages: this knowledge provides other applications   with a ne-grained description of how the extracted collocations are to be used in context.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linguistic knowledge",
                "improve results",
                "PERFORMANCE",
                "positive",
                0.7
            ],
            [
                "linguistic knowledge",
                "essential",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "We view L2P as a tagging task that can be performed with a discriminative learning method, such as the Perceptron HMM  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "L2P",
                "tagging task",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Perceptron HMM",
                "discriminative learning method",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.2 Perceptron algorithm Our discriminative n-gram model training approach uses the perceptron algorithm, as presented in  , which follows the general approach presented in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron algorithm",
                "as presented in",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "general approach",
                "presented in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This decomposition applies both to discriminative linear models and to generative models such as HMMs and CRFs, in which case the linear sum corresponds to log likelihood assigned to the input/output pair by the model   for the classi cation case and   for the structured case).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linear models",
                "applies to",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "models",
                "log likelihood assigned",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Following this idea, there have been introduced a parameter estimation approach for non-generative approaches that can effectively incorporate unlabeled data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameter estimation approach",
                "can effectively incorporate unlabeled data",
                "METHODOLOGY",
                "positive",
                0.85
            ],
            [
                "non-generative approaches",
                "can effectively incorporate unlabeled data",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The superiority of discriminative models has been shown on many tasks when the discriminative and generative models use exactly the same model structure  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "discriminative models",
                "has been shown on many tasks",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "model structure",
                "use exactly the same",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Since Czech is a language with relatively high degree of word-order freedom, and its sentences contain certain syntactic phenomena, such as discontinuous constituents  , which cannot be straightforwardly handled using the annotation scheme of Penn Treebank  , based on phrase-structure trees, we decided to adopt for the PCEDT the dependency-based annotation scheme of the Prague Dependency Treebank  PDT  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "straightforwardly handled",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Prague Dependency Treebank",
                "dependency-based annotation scheme",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For Czech, we created a prototype of the first step of this process -the part-of-speech   tagger -using Rank Xerox tools  ,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Rank Xerox tools",
                "using Rank Xerox tools",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "part-of-speech tagger",
                "the first step of this process",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Since Chinese text is not orthographically separated into words, the standard methodology is to first preproce~ input texts through a segmentation module  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methodology",
                "standard methodology",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "segmentation module",
                "preprocess input texts",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Second, phrase translation pairs are extracted from the word alignment corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase translation pairs",
                "extracted from word alignment corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word alignment corpus",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The maximum entropy models used here are similar in form to those in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy models",
                "similar in form to those in ",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "maximum entropy models",
                "used here",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In examining the combination of the two types of parsing, McDonald and Nivre   utilized similar approaches to our empirical analysis.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "similar",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "empirical analysis",
                "our",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "5.1.2 Learning Translation Model According to the standard statistical translation model  , we can find the optimal model M by maximizing the probability of generating queries from documents or M = argmax M NY i=1 P  524 qw dw P  journal kdd 0.0176 journal conference 0.0123 journal journal 0.0176 journal sigkdd 0.0088 journal discovery 0.0211 journal mining 0.0017 journal acm 0.0088 music music 0.0375 music purchase 0.0090 music mp3 0.0090 music listen 0.0180 music mp3.com 0.0450 music free 0.0008 Table 1: Sample user profile To find the optimal word translation probabilities P , we can use the EM algorithm.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "standard statistical translation model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "EM algorithm",
                "can use",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Among them,   have proposed a way to exploit bilingual dictionnaries at training time.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dictionnaries",
                "exploit bilingual at training time",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "proposed",
                "a way",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For Hw6, students compared their POS tagging results with the ones reported in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "students",
                "compared their POS tagging results",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "results",
                "reported in",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For phrase-based translation model training, we used the GIZA++ toolkit  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ toolkit",
                "used for phrase-based translation model training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase-based translation model training",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We report that our parsing framework achieved high accuracy   in dependency analysis of Japanese with a combination of an underspecified HPSG-based Japanese grammar, SLUNG   and the maximum entropy method  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing framework",
                "achieved high accuracy",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "maximum entropy method",
                "combined with SLUNG and underspecified HPSG-based Japanese grammar",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Five chunk tag sets, IOB1, IOB2, IOE1, IOE2   and SE  , are commonly used.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunk tag sets",
                "are commonly used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IOB1, IOB2, IOE1, IOE2, SE",
                "are commonly used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We also compare ASIA on twelve additional benchmarks to the extended Wordnet 2.1 produced by Snow et al  , and show that for these twelve sets, ASIA produces more than five times as many set instances with much higher precision  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ASIA",
                "produces more than five times as many set instances with much higher precision",
                "PERFORMANCE",
                "positive",
                0.9
            ],
            [
                "Wordnet 2.1",
                "produced by Snow et al",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.2.1 BLEU Evaluation The BLEU score   was defined to measure overlap between a hypothesized translation and a set of human references.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "was defined to measure overlap between a hypothesized translation and a set of human references",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "BLEU score",
                "to measure overlap",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The second approach   takes triples   and  , like those in Table 10, as training data for acquiring semantic knowledge and performs PP-attachment disambiguation on quadruples.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "triples",
                "as training data",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "quadruples",
                "performs PP-attachment disambiguation",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A single translation is then selected by finding the candidate that yields the best overall score   or by cotraining  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "candidate",
                "yields the best overall score",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "cotraining",
                "cotraining",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1PMI is subject to overestimation for low frequency items  , thus we require a minimum frequency of occurrence for the expressions under study.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "1PMI",
                "subject to overestimation for low frequency items",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "minimum frequency of occurrence",
                "required",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Such a similarity is calculated by using the WordNet::Similarity tool  , and, concretely, the Wu-Palmer measure, as defined in Equation1  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet::Similarity tool",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Wu-Palmer measure",
                "is defined",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Specifically, the following information can be either automatically identified or manually annotated:  Syntactic structures automatically identified from a parser  ;  Semantic roles of entities in the question  ;  Discourse roles either manually annotated or identified by rules that map directly from semantic roles to discourse roles.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "automatically identified",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "semantic roles",
                "manually annotated or identified by rules",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Their weights are calculated by deleted interpolation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights",
                "calculated by deleted interpolation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "deleted interpolation",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We then parse both sides of the corpus with syntactic parsers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic parsers",
                "parse both sides of the corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpus",
                "both sides",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.3 Unknown word features Most of the models presented here use a set of unknown word features basically inherited from  , which include using character n-gram prefixes and suffixes  , and detectors for a few other prominent features of words, such as capitalization, hyphens, and numbers.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unknown word features",
                "basically inherited",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "character n-gram prefixes and suffixes",
                "and detectors for a few other prominent features of words",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This resembles the re-ranking approach  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "re-ranking approach",
                "resembles",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "re-ranking approach",
                "re-ranking approach",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It extracts all consistent phrase pairs from word-aligned bitext  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bitext",
                "consistent phrase pairs",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "phrase pairs",
                "extracts",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "1 Introduction Sentiment classification is a special task of text categorization that aims to classify documents according to their opinion of, or sentiment toward a given subject    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentiment classification",
                "special task of text categorization",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "sentiment classification",
                "classifies documents according to their opinion of, or sentiment toward a given subject",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Instances of this work include information extraction, ontology induction and resource acquisition  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "information extraction",
                "include",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ontology induction",
                "include",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Model Overall Unknown Word Accuracy Accuracy Baseline, 96.72% 84.5% J Ratnaparkhi 96.63% 85.56%   Table 3 Baseline model performance This table also shows the results reported in Ratnaparkhi  for COnvenience.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Baseline model performance",
                "reported in Ratnaparkhi",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "COnvenience",
                "reported in Ratnaparkhi",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.1.4 Model Features Our MST models are based on the features described in  ; specifically, we use features based on a dependency nodes form, lemma, coarse and fine part-of-speech tag, and morphologicalstring attributes.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MST models",
                "based on the features described in",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "features",
                "based on dependency nodes form, lemma, coarse and fine part-of-speech tag, and morphological string attributes",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "However, based on annotation differences in the datasets   and a bug in their system  , their results are inconclusive.1 Thus, the effectiveness of SCL is rather unexplored for parsing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SCL",
                "is rather unexplored",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "results",
                "are inconclusive",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This set of 800 sentences was used for Minimum Error Rate Training   to tune the weights of our system with respect to BLEU score.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "tune the weights",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "BLEU score",
                "used to evaluate",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "So far, pivot features on the word level were used  , e.g. Does the bigram not buy occur in this document?  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pivot features",
                "on the word level",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "bigram",
                "not buy occur",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Be-Comp Following the general idea in  , we identify the ISA pattern in the definition sentence by extracting nominal complements of the verb be, taking 451 No.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ISA pattern",
                "extracting nominal complements of the verb be",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "definition sentence",
                "taking 451 No.",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For the first two tasks, all heuristics of the Pharaoh-Toolkit   as well as the refined heuristic   to combine both IBM4-alignments were tested and the best ones are shown in the tables.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pharaoh-Toolkit",
                "tested",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IBM4-alignments",
                "combined",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "When an S alignment exists, there will always also exist a P alignment such that P a65 S. The English sentences were parsed using a state-of-the-art statistical parser   trained on the University of Pennsylvania Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "state-of-the-art",
                "METHODOLOGY",
                "positive",
                0.7
            ],
            [
                "University of Pennsylvania Treebank",
                "statistical",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he weights of these models are determined using the max-BLEU method described in Och  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "weights are determined using the max-BLEU method",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "max-BLEU method",
                "described in Och",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Since these morphological generalizations are based on the initial categorization provided by the algorithm of  , we hope that they will foster speedy convergence of HNN training.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "initial categorization provided",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "HNN training",
                "speedy convergence",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "For instance, there is a substantial body of papers on the extraction of \"\"frequently co-occurring words\"\" from corpora using statistical methods  ,  ,   to list only a few).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "statistical methods",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "frequently co-occurring words",
                "to list only a few",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "Bilingual lexicographers can work with bilingual concordancing software that can point them to instances of any link type induced from a bitext and display these instances sorted by their contexts  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bilingual concordancing software",
                "display instances sorted by their contexts",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "bilingual lexicographers",
                "work with",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  have build a chunker by applying transformation-based learning to sections of the Penn Treebank.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunker",
                "applying transformation-based learning",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank",
                "sections of",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "By analyzing rhetorical discourse structure of aim, background, solution, etc. or citation context, we can obtain appropriate abstracts and the most influential contents from scientific articles  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rhetorical discourse structure",
                "of aim, background, solution, etc. or citation context",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "scientific articles",
                "most influential contents",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Past work   has examined the use of monolingual parallel corpora for paraphrase extraction.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "monolingual parallel corpora",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "work",
                "examined the use of",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We split the treebank into training  , development   and test   as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treebank",
                "into training, development, and test",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "split",
                "as in ",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "In   a standard phrase-based model is augmented with more than a million features whose weights are trained discriminatively by a variant of the perceptron algorithm.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "more than a million",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "perceptron algorithm",
                "trained discriminatively",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "NP chunks in the shared task data are BaseNPs, which are non-recursive NPs, a definition first proposed by Ramshaw and Marcus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BaseNPs",
                "non-recursive NPs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "definition",
                "first proposed by Ramshaw and Marcus",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Similarlyto , we define the strength of a pattern p in a category y as the precision of p in the set of documents labeled with category y, estimated using Laplace smoothing: strength  = count  + epsilon1count  + kepsilon1   where count  is the number of documents labeled y containing pattern p, count  is the overall number of labeled documents containing p, and k is the number of domains.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pattern p",
                "estimated using Laplace smoothing",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "precision of p",
                "in the set of documents labeled with category y",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The algorithms were trained and tested using version 3 of the Penn Treebank, using the training, development, and test split described in Collins   and also employed by Toutanova et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training, development, and test split",
                "described in Collins and also employed by Toutanova et al.",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Word alignment, which can be defined as an object for indicating the corresponding words in a parallel text, was first introduced as an intermediate result of statistical translation models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Pharaoh also includes lexical weighting parameters that are derived from the alignments used to induce its phrase pairs  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexical weighting parameters",
                "derived from the alignments used to induce its phrase pairs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "alignments",
                "used to induce",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A problem mentioned in   is that the algorithm that computes the compressed representation might need to retain the entire database in memory; in their paper, they design strategies to work around this problem.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "might need to retain the entire database in memory",
                "LIMITATION",
                "neutral",
                0.95
            ],
            [
                "strategies",
                "design strategies to work around this problem",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this case, one is often required to find the translation  in the hypergraph that are most similar to the desired translations, with similarity computed via some automatic metric such as BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation",
                "most similar to the desired translations",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "similarity computed via",
                "automatic metric such as BLEU",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4.5 Hindles Measure Hindle   proposed an MI-based measure, which he used to show that nouns could be reliably clustered based on their verb co-occurrences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hindle",
                "proposed",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "Hindle's measure",
                "could be reliably clustered",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he research presented in this paper is similar in motivation to Resnik's   work on selectional restrictions",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Resnik's work",
                "on selectional restrictions",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "research",
                "is similar",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Many researchers  ;  ) have suggested that the informationtheoretic notion of mutual information score   directly captures the idea of context.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "informationtheoretic notion",
                "mutual information score",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "idea of context",
                "captures",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Approaches include word substitution systems  , phrase substitution systems  , and synchronous context-free grammar systems  , all of which train on string pairs and seek to establish connections between source and target strings.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word substitution systems",
                "train on string pairs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase substitution systems",
                "seek to establish connections",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Figure 1 exhibits this scenario with a typical IE system such as SRI's FASTUS system  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "FASTUS system",
                "typical IE system",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "FASTUS system",
                "such as",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The learning algorithm, which is illustrated in Collins  , proceeds as follows.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "learning algorithm",
                "illustrated in Collins",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "proceeds",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We took part the Multilingual Track of all ten languages provided by the CoNLL-2007 shared task organizers .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Multilingual Track",
                "provided by the CoNLL-2007 shared task organizers",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "CoNLL-2007 shared task organizers",
                "shared task",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Sometimes, the notion of collocation is defined in terms of syntax   or in terms of semantics    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocation",
                "defined in terms of syntax or semantics",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntax",
                "or",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However, they make different types of errors, which can be seen as a reflection of their theoretical differences  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "theoretical differences",
                "reflection",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "errors",
                "different types",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Using GIZA++ model 4 alignments and Pharaoh  , we achieved a BLEU score of 0.3035.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ model 4 alignments and Pharaoh",
                "achieved",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU score 0.3035",
                "score",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There are more sophisticated surface generation packages, such as FUF/SURGE  , KPML  , MUMBLE  , and RealPro  , which produce natural language text from an abstract semantic representation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "FUF/SURGE",
                "sophisticated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "RealPro",
                "produce natural language text",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The resulting intercoder reliability, measured with the Kappa statistic , is considered excellent  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa statistic",
                "is considered excellent",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "intercoder reliability",
                "measured with Kappa statistic",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This model is similar in spirit to IBM model 1  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "similar in spirit to IBM model 1",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "2 Statistical Word Alignment Statistical translation models   only allow word to word and multi-word to word alignments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Statistical Word Alignment",
                "only allow word to word and multi-word to word alignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Statistical translation models",
                "only allow",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To quickly   evaluate this phenomenon, we trained the statistical IBM wordalignment model 4  ,1 using the GIZA++ software   for the following language pairs: ChineseEnglish, Italian English, and DutchEnglish, using the IWSLT-2006 corpus   for the first two language pairs, and the Europarl corpus   for the last one.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM wordalignment model",
                "statistical",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IWSLT-2006 corpus",
                "for the first two language pairs",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A quick search in the Penn Treebank   shows that about 17% of all sentences contain parentheticals or other sentence fragments, interjections, or unbracketable constituents.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "contains 17% of all sentences",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "sentences",
                "contain parentheticals or other sentence fragments, interjections, or unbracketable constituents",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For English there are many POS taggers, employing machine learning techniques like transformation-based error-driven learning  , decision trees  , markov model  , maximum entropy methods   etc. There are also taggers which are hybrid using both stochastic and rule-based approaches, such as CLAWS  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning techniques",
                "like transformation-based error-driven learning, decision trees, markov model, maximum entropy methods",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "taggers",
                "hybrid using both stochastic and rule-based approaches",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To overcome these limitations, many syntaxbased SMT models have been proposed  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT models",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "many",
                "neutral",
                "LIMITATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction A recent development in data-driven parsing is the use of discriminative training methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data-driven parsing",
                "discriminative training methods",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "recent development",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model   with many features for parse trees  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parse trees",
                "many features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parse tree",
                "discriminating correct parse tree",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We use MXPOST tagger   for POS tagging, Charniak parser   for extracting syntactic relations, SVMlight1 for SVM classifier and David Bleis version of LDA2 for LDA training and inference.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MXPOST tagger",
                "POS tagging",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Charniak parser",
                "extracting syntactic relations",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Also in the Penn Treebank  ,  ) a limited set of relations is placed over the constituencybased annotation in order to make explicit the   roles that the constituents play.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "constituency-based annotation",
                "make explicit the roles",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "limited set of relations",
                "placed over",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In this respect, it resembles bilingual bracketing  , but our model has more lexical items in the blocks with many-to-many word alignment freedom in both inner and outer parts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "blocks",
                "many-to-many word alignment freedom",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "model",
                "more lexical items",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The leader of the pack is the MXPOST tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MXPOST tagger",
                "leader of the pack",
                "METHODOLOGY",
                "positive",
                0.75
            ],
            [
                "MXPOST tagger",
                "the",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "??word proximity: For the web searches, Turney   uses the NEAR operator and considers only those documents that contain the adjectives within a specific proximity.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NEAR operator",
                "uses the NEAR operator",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "specific proximity",
                "considers only those documents that contain within a specific proximity",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Furthermore, statistical generation systems   could use  as a means of directly optimizing information ordering, much in the same way MT systems optimize model parameters using BLEU as a measure of translation quality  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical generation systems",
                "optimizing information ordering",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "BLEU",
                "measure of translation quality",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2.2 Brown clustering algorithm In order to provide word clusters for our experiments, we used the Brown clustering algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brown clustering algorithm",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word clusters",
                "for experiments",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "his model is very similar to the markovized rule models in Collins  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "very similar",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "markovized rule models in Collins",
                "similar",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For our experiments, we used the binary-only distribution of the tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger",
                "binary-only distribution",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "tagger",
                "binary-only",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The concept of baseNP has undergone a number of revisions   but has previously always been tied to extraction from a more completely annotated treebank, whose annotations are subject to other pressures than just initial material up to the head . To our knowledge, our gures for inter-annotator agreement on the baseNP task itself 169   are the rst to be reported.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseNP",
                "always been tied to extraction from a more completely annotated treebank",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "gures for inter-annotator agreement",
                "the rst to be reported",
                "PERFORMANCE",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "So far, research in automatic opinion recognition has primarily addressed learning subjective language  , identifying opinionated documents   and sentences  , and discriminating between positive and negative language  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "primarily addressed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "subjective language",
                "identifying opinionated documents and sentences",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This is similar to Model 3 of  , but without null-generated elements or re-ordering.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 3",
                "without null-generated elements or re-ordering",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Model 3",
                "similar to",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Typically, a phrase-based SMT system includes a feature that scores phrase pairs using lexical weights   which are computed for two directions: source to target and target to source.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature",
                "scores phrase pairs using lexical weights",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase pairs",
                "computed for two directions",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Many research groups use a decoder based on a log-linear approach incorporating phrases as main paradigm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "based on a log-linear approach",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrases",
                "as main paradigm",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "It also shows that DOP's frontier lexicalization is a viable alternative to constituent lexicalization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "DOP's frontier lexicalization",
                "a viable alternative",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "constituent lexicalization",
                "alternative",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "   , and extend it to structured shape descriptions of visual data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "structured shape descriptions",
                "of visual data",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "extend",
                "structured shape descriptions",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For ROUGE-S and ROUGE-SU, we use three variations following  : the maximum skip distances are 4, 9 and infinity 7.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum skip distances",
                "4, 9 and infinity",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "maximum skip distances",
                "are",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For the simple bag-of-word bilingual LSA as describedinSection2.2.1,afterSVDonthesparsematrix using the toolkit SVDPACK  , all source and target words are projected into a lowdimensional   LSA-space.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LSA-space",
                "lowdimensional",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "SVDPACK",
                "toolkit",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A number of researchers have explored learning words and phrases with prior positive or negative polarity    ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words and phrases",
                "prior positive or negative polarity",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "learning",
                "explored",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4 The Corpus We used two corpora for our analysis: hospital discharge summaries from 1991 to 1997 from the Columbia-Presbyterian Medical Center, and the January 1996 part of the Wall Street Journal corpus from the Penn TreeBank \\ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "hospital discharge summaries from 1991 to 1997 from the Columbia-Presbyterian Medical Center",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpora",
                "Wall Street Journal corpus from the Penn TreeBank",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In line with the reports in   we do observe the performance improvement against the baseline   for all the domains.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "performance",
                "improvement",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "domains",
                "all the",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Models of this kind assume that an input word is generated by only one output word  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Models of this kind",
                "assume that an input word is generated by only one output word",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "one output word",
                "is generated",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "  always predicts a flat NP for such configurations).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "predicts",
                "a flat NP",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "configurations",
                "such",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2.2 Maximum Entropy Our next approach is the Maximum Entropy   classification approach.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy",
                "classification approach",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "classification approach",
                "next approach",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our method does not suppose a uniform distribution over all possible phrase segmentationsas   since each phrase tree has a probability.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase segmentations",
                "does not suppose a uniform distribution",
                "METHODOLOGY",
                "negative",
                0.75
            ],
            [
                "phrase tree",
                "has a probability",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Taken together with cube pruning  , k-best tree extraction  , and cube growing  , these results provide evidence that lazy techniques may penetrate deeper yet into MT decoding and other NLP search problems.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lazy techniques",
                "penetrate deeper",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "MT decoding",
                "other NLP search problems",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "5 Comparison with other approaches In some sense, this approach is similar to the notion of \"\"ambiguity classes\"\" explained in   and   where words that belong to the same part-of-speech figure together.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "similar to the notion of ambiguity classes",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "ambiguity classes",
                "explained in and where words that belong to the same part-of-speech figure together",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Statistical Phrase-based Translation  : Here phrase-based means subsequence-based, as there is no guarantee that the phrases learned by the model will have any relation to what we would think of as syntactic phrases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based translation",
                "subsequence-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrases learned",
                "have no relation to syntactic phrases",
                "INNOVATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "As two examples,   and   give good overviews of the techniques and equations used for Markov models and part-ofspeech tagging, but they are not very explicit in the details that are needed for their application.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques and equations",
                "used for Markov models and part-ofspeech tagging",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "details",
                "needed for their application",
                "LIMITATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Researchers extracted opinions from words, sentences, and documents, and both rule-based and statistical models are investigated  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "rule-based and statistical",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "models",
                "are investigated",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However, with the algorithms proposed in  , it is possible to develop a general-purpose decoder that can be used by all the parsing-based systems.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "proposed in  ",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "decoder",
                "can be used by all the parsing-based systems",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Automatic subjectivity analysis would also be useful to perform flame recognition  , e-mail classification  , intellectual attribution in text  , recognition of speaker role in radio broadcasts  , review mining  , review classification  , style in generation  , and clustering documents by ideological point of view  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "subjectivity analysis",
                "Automatic",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "review classification",
                "mining",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.2 ROUGE Version 1.5.5 of the ROUGE scoring algorithm   is also used for evaluating results.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE scoring algorithm",
                "is used for evaluating results",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Applications have included the categorization of documents by topic  , language  , genre  , author  , sentiment  , and desirability  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "desirability",
                "desirability",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "sentiment",
                "sentiment",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Previous work used all possible pre xes and suf xes ranging in length from 1 to k characters, with k = 4  , and k = 10  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "k",
                "ranging in length from 1 to k characters",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "pre xes and suf xes",
                "used all possible",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " ; we also introduce an approach related to the conditional log-linear models of Ratnaparkhi, Roukos, and Ward  , Papineni, Roukos, and Ward  , Johnson et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ratnaparkhi, Roukos, and Ward",
                "conditional log-linear models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Papineni, Roukos, and Ward",
                "conditional log-linear models",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Previous research in automatic acquisition focuscs primarily on the use of statistical techniques, such as bilingual alignment  , or extraction of syntactic constructions from online dictionaries and corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical techniques",
                "primarily",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "online dictionaries and corpora",
                "extraction of syntactic constructions",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Typical examples of linguistically sophisticated annotation include tagging words with their syntactic category  , lemma of the word  , phrasal information  ), and subject-predicate identification  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic category",
                "tagging words with",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrasal information",
                "phrasal information",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Results for chunking Penn Treebank data were previously presented by several authors  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank data",
                "previously presented",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "authors",
                "several",
                "LIMITATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In recent work, Koehn and Hoang   proposed a general framework for including morphological features in a phrase-based SMT system by factoring the representation of words into a vector of morphological features and allowing a phrase-based MT system to work on any of the factored representations, which is implemented in the Moses system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Moses system",
                "implemented",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase-based SMT system",
                "allowing to work on any factored representations",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It is easy to see that the main difference between the PA algorithms and the Perceptron algorithm     as well as the MIRA algorithm   is in line 9.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PA algorithms",
                "main difference",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Perceptron algorithm",
                "main difference",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "18 More recently, Bean and Riloff   have proposed methods for automatically extracting from a corpus heads that correlate well with discourse novelty.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "correlate well with discourse novelty",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpus",
                "well with discourse novelty",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "2.1 Training the model As with  , we train the language model on the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "on the Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "training",
                "As with ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "OS tag the text using Ratnaparkhi  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ratnaparkhi",
                "tag the text",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "OS",
                "tag the text",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3Huang and Chiang   describes the cube growing algorithm in further detail, including the precise form of the successor function for derivations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "cube growing algorithm",
                "describes in further detail",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "successor function for derivations",
                "precise form",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We show that link 1For a complete discussion of alignment symmetrization heuristics, including union, intersection, and refined, refer to  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment symmetrization heuristics",
                "including union, intersection, and refined",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "heuristics",
                "including union, intersection, and refined",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We trained IBM Translation Model 4   both on our corpus alone and on the augmented corpus, using the EGYPT toolkit  , and then translated a number of texts using different translation models and different transfer methods, namely glossing   and Model 4 decoding  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Translation Model 4",
                "trained on our corpus alone and on the augmented corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "transfer methods",
                "glossing and Model 4 decoding",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "7 Automated Sense Labelling of Discourse Connectives The focus here is on automated sense labelling of discourse connectives   438   151   SUMMARIES 2118 275 0.130 166   99   10   LETTERS 739 200 0.271 126   56   18   NEWS 40095 9336 0.233 5514   3015   807   Figure 4: Distribution of Explicit Intra-Sentential Connectives.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Discourse Connectives",
                "automated sense labelling",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "Explicit Intra-Sentential Connectives",
                "Distribution",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Fortunately, Wu   provides a method to have an ITG respect a known partial structure.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG",
                "respect a known partial structure",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "method",
                "to have an ITG respect a known partial structure",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For the MER training  , we modify Koehns MER trainer   to train our system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Koehns MER trainer",
                "modify",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "our system",
                "train",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, in machine translation, BLEU score   is developed to assess the quality of machine translated sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "assess the quality of machine translated sentences",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Daume allows an extra degree of freedom among the features of his domains, implicitly creating a two-level feature hierarchy with one branch for general features, and another for domain specific ones, but does not extend his hierarchy further  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Daume",
                "extra degree of freedom",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "domain specific ones",
                "creating a two-level feature hierarchy",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "hese are most directly presented in Ostler and Atkins  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ostler and Atkins",
                "are most directly presented",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Ostler and Atkins",
                "are",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In that table, TBL stands for Brill's transformation-based error-driven tagget  , ME stands for a tagger based on the maimum entropy modelling  , SPATTER stands for a statistical parser based on decision trees  , IGTREE stands for the memory-based tagger by Daelemans et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TBL",
                "Brill's transformation-based error-driven tagger",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ME",
                "based on the maximum entropy modelling",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "SPATTER",
                "based on decision trees",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our work builds upon Turneys work on semantic orientation   and synonym learning  , in which he used a PMI-IR algorithm to measure the similarity of words and phrases based on Web queries.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Turneys work",
                "semantic orientation and synonym learning",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "PMI-IR algorithm",
                "measure the similarity of words and phrases",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "On the one hand using 1 human reference with uniform results is essential for our methodology, since it means that there is no more trouble with Recall    a systems ability to avoid under-generation of N-grams can now be reliably measured.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methodology",
                "essential for our",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "systems ability",
                "reliably measured",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "9.1 Training Methodology Given a training set, we first run a variant of IBM alignment model 1   for 100 iterations, and then initialize Model I with the learned parameter values.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM alignment model 1",
                "variant of IBM alignment model 1",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Model I",
                "initialized with learned parameter values",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use the IBM Model 1   and the Hidden Markov Model  ) to estimate the alignment model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 1",
                "to estimate the alignment model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Hidden Markov Model",
                "to estimate the alignment model",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Because it is not feasible here to have humans judge the quality of many sets of translated data, we rely on an array of well known automatic evaluation measures to estimate translation quality :  BLEU   is the geometric mean of the n-gram precisions in the output with respect to a set of reference translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation measures",
                "array of well known",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU",
                "geometric mean of n-gram precisions",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Text-to-text generation is an emerging area of research in NLP  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "emerging",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "area",
                "NLP",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Consequently, here we employ multiple references to evaluate MT systems like BLEU   and NIST  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT systems",
                "BLEU and NIST",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "references",
                "multiple",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We ran each estimator with the eight different combinations of values for the hyperparameters  and prime listed below, which include the optimal values for the hyperparameters found by Johnson  , and report results for the best combination for each estimator below 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hyperparameters",
                "eight different combinations of values",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "results",
                "best combination for each estimator",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The model is composed of three parts  : a set of candidate SAPTs GEN, which is the top n SAPTs of a sentence from SCISSOR; a function  that maps a sentence Inputs: A set of training examples  , i = 1n, where xi is a sentence, and yi is a candidate SAPT that has the highest similarity score with the gold-standard SAPT Initialization: Set W = 0 Algorithm: For t = 1T,i = 1n Calculate yi = argmaxyGEN     W If   then W = W +     Output: The parameter vector W Figure 2: The perceptron training algorithm.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GEN",
                "set of candidate SAPTs",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "W",
                "parameter vector",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Some other researchers also work on detecting negative cases, i.e. contradiction, instead of entailment  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "researchers",
                "work on detecting negative cases",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "contradiction",
                "instead of entailment",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5.2 Assigning complex ambiguity tags In the tagging literature  ) an ambiguity class is often composed of the set of every possible tag for a word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ambiguity class",
                "set of every possible tag for a word",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tagging literature",
                "often composed of",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman   and Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "corrected and extended",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "approaches of Magerman and Collins",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "is relevant to finite-state phrase-based models that use no parse trees  , tree-tostring models that rely on one parse tree  , and tree-to-tree models that rely on two parse trees  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "use no parse trees",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "models",
                "rely on one parse tree",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We have also used TPTs to encode n-gram count databases such as the Google 1T web n-gram database  , but are not able to provide detailed results within the space limitations of this paper.4 5.1 Perplexity computation with 5-gram language models We compared the performance of TPT-encoded language models against three other language model implementations: the SRI language modeling toolkit  , IRSTLM  , and the language model implementation currently used in the Portage SMT system  , which uses a pointer-based implementation but is able to perform fast LM filtering at load time.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TPTs",
                "encode n-gram count databases",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "language model implementations",
                "uses pointer-based implementation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There is potential of developing Sense Definition Model to identify and represent semantic and stylistic differentiation reflected in the MRD glosses pointed out in DiMarco, Hirst and Stede  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Sense Definition Model",
                "developing to identify and represent semantic and stylistic differentiation",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The simple model 1   for the translation of a SL sentence d = dldt in a TL sentence e = el em assumes that every TL word is generated independently as a mixture of the SL words: m l P ,,~ H ~ t    j=l i=O In the equation above t  stands for the probability that ej is generated by di.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "simple model",
                "assumes every TL word is generated independently",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probability",
                "stands for the probability that ej is generated by di",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 The METEOR Metric 2.1 Weaknesses in BLEU Addressed in METEOR The main principle behind IBMs BLEU metric   is the measurement of the 66 overlap in unigrams   and higher order n-grams of words, between a translation being evaluated and a set of one or more reference translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metric",
                "principle behind",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "METEOR",
                "addressed weaknesses in BLEU",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Ourmodelisthusa form of quasi-synchronous grammar    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "form of quasi-synchronous grammar",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "model",
                "is a form of quasi-synchronous grammar",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The reliability of the annotations was checked using the kappa statistic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa statistic",
                "checked using",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "annotations",
                "reliability of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this paper we will compare and evaluate several aspects of these techniques, focusing on Minimum Error Rate   training   and Minimum Bayes Risk   decision rules, within a novel training environment that isolates the impact of each component of these methods.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Minimum Error Rate training",
                "novel training environment",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Minimum Bayes Risk decision rules",
                "isolates the impact of each component",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, bilingual lexicographers can use bitexts to discover new cross-language lexicalization patterns  ; students of foreign languages can use one half of a bitext to practice their reading skills, referring to the other half for translation when they get stuck  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bitexts",
                "discover new cross-language lexicalization patterns",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "students",
                "practice their reading skills",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For the efficiency of minimum-error-rate training  , we built our development set   using sentences not exceeding 50 characters from the NIST MT-02 evaluation test data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "development set",
                "using sentences not exceeding 50 characters",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "NIST MT-02 evaluation test data",
                "evaluation test data",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "able 8 compares the F1 results of our baseline model with Nakagawa and Uchimoto   and Zhang and Clark   on CTB 3.0",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseline model",
                "compares the F1 results",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Nakagawa and Uchimoto and Zhang and Clark",
                "results",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Word features are introduced primarily to help with unknown words, as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "help with unknown words",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "unknown words",
                "as in  ",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Algorithms for the more difficult task of word alignment were proposed in   and were applied for parameter estimation in the IBM statistical machine translation system  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "proposed in and applied for parameter estimation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM statistical machine translation system",
                "used in",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Most related to our approach, Wu   used inversion transduction grammarsa synchronous context-free formalism  for this task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "inversion transduction grammars",
                "synchronous context-free formalism",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Wu's approach",
                "used for this task",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "From the above discussion, we can see that traditional tree sequence-based method uses single tree as translation input while the forestbased model uses single sub-tree as the basic translation unit that can only learn tree-to-string   rules.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "traditional tree sequence-based method",
                "uses single tree as translation input",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "forest-based model",
                "single sub-tree as the basic translation unit",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We adopted IOB   labeling  , where the rst word of an entity of class C is labeled B-C, the words in the entity are labeled I-C, and other words are labeled O.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IOB labeling",
                "adopted",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "words in the entity",
                "labeled I-C",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "My guess is that the features used in e.g., the Collins   or Charniak   parsers are probably close to optimal for English Penn Treebank parsing  , but that other features might improve parsing of other languages or even other English genres.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "probably close to optimal",
                "METHODOLOGY",
                "neutral",
                0.7
            ],
            [
                "features",
                "might improve",
                "METHODOLOGY",
                "neutral",
                0.75
            ]
        ]
    },
    {
        "text": "The association relationship between two words can be indicated by their mutual information, which can be further used to discover phrases \\ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information",
                "can be used to discover phrases",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "mutual information",
                "can be indicated by their association relationship",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The agreement on identifying the boundaries of units, using the kappa statistic discussed in Carletta  , was  = .9  ; the agreement on features   was as follows: utype:  = .76; verbed:  = .9; nite:  = .81.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa statistic",
                "discussed in Carletta",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "agreement on identifying boundaries",
                " =.9",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Following the framework of global linear models in  , we cast this task as learning a mapping F from input verses x  X to a text-reuse hypothesis y  Y  {epsilon1}.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "global linear models",
                "in framework of",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "text-reuse hypothesis",
                "y Y epsilon1",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5.3 Related works and discussion Our two-step model essentially belongs to the same category as the works of   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "works",
                "same category",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "two-step model",
                "essentially belongs",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The statistical classifier used in the experiments reported in this paper is a maximum entropy classifier  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifier",
                "maximum entropy classifier",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "experiments",
                "reported in this paper",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Even though there are some studies that compare the results from statistically computed association measures with word association norms from psycholinguistic experiments   there has not been any research on the usage of a digital, network-based dictionary reflecting the organisation of the mental lexicon to our knowledge.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word association norms",
                "from psycholinguistic experiments",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "statistically computed association measures",
                "with word association norms",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The Bloomier filter LM   has a precomputed matching of keys shared between a constant number of cells in the filter array.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "filter array",
                "precomputed matching of keys shared between a constant number of cells",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Bloomier filter LM",
                "has",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "NULL) Compared with the B-Chunk and I-Chunk used in Ramshaw and Marcus , structural relations 99 and 90 correspond to B-Chunk which represents the first word of the chunk, and structural relations 00 and 09 correspond to I-Chunk which represnts each other in the chunk while 90 also means the beginning of the sentence and 09 means the end of the sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "B-Chunk",
                "represents the first word of the chunk",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "I-Chunk",
                "represents each other in the chunk",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Another way to look the algorithm is from the self-training perspective  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "self-training perspective",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "self-training",
                "perspective",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "I have made a preliminary analysis of the inventory of syntactic categories used in the tagging for labelling trees in the 18 Penn Treebank  , comparing them to the categories used in CGEL.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "used in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "categories used in CGEL",
                "compared to",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3 2.4 Intonation Annotations For our intonation annotation, we have annotated the intonational phrase boundaries, using the ToBI   definition  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "For instance, the most relaxed IBM Model-1, which assumes that any source word can be generated by any target word equally regardless of distance, can be improved by demanding a Markov process of alignments as in HMM-based models  , or implementing a distribution of number of target words linked to a source word as in IBM fertility-based models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model-1",
                "assuming any source word can be generated by any target word equally regardless of distance",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "HMM-based models",
                "Markov process of alignments",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We perform named entity tagging using the Stanford four-class named entity tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stanford four-class named entity tagger",
                "named entity tagger",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "ntroduction The automated analysis of large corpora has many useful applications  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "many useful applications",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "analysis",
                "of large corpora",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This research has focused mostly on the development of statistical parsers trained on large annotated corpora, in particular the Penn Treebank WSJ corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "large annotated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank WSJ corpus",
                "particular",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "erceptron Learning a discriminative structure prediction model with a perceptron update was first proposed by Collins  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron Learning",
                "was first proposed by Collins",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "perceptron update",
                "was proposed by Collins",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction: Defining SCMs The work presented here was done in the context of phrase-based MT  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based MT",
                "context of phrase-based MT",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "work",
                "done",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Its previous applications   demonstrated that cooccurrence statistics on a target word is often sufficient for its automatical classification into one of numerous classes such as synsets of WordNet.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "cooccurrence statistics",
                "is often sufficient",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "WordNet",
                "synsets of",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5 Related Work Evidence from the surrounding context has been used previously to determine if the current sentence should be subjective/objective  ) and adjacency pair information has been used to predict congressional votes  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Evidence from the surrounding context",
                "has been used previously",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "adjacency pair information",
                "has been used to predict",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "publications",
                "proposing new measures, proposing improvements and extensions",
                "INNOVATION",
                "neutral",
                0.75
            ],
            [
                "evaluation measures",
                "automatic evaluation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction In the first SMT systems  , word alignment was introduced as a hidden variable of the translation model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT systems",
                "introduced as a hidden variable",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation model",
                "word alignment",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Training discriminative parsers is notoriously slow, especially if it requires generating examples by repeatedly parsing the treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing",
                "notoriously slow",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "generating examples",
                "requires",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In shift-reduce parsing, further mistakes are often caused by previous ones, so only the first mistake in each sentence   is easily identifiable;7 this is also the argument for early update in applying perceptron learning to these incremental parsing algorithms    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mistakes",
                "easily identifiable",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "early update",
                "argument for",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "In order to minimize the number of decision errors at the sentence level, we have to choose the sequence of target words eI1 according to the equation  : eI1 = argmax eI1 n Pr  o = argmax eI1 n Pr Pr  o : Here, the posterior probability Pr  is decomposed into the language model probability Pr  and the string translation probability Pr .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "target words",
                "according to the equation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "posterior probability",
                "decomposed into the language model probability and the string translation probability",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To generate the n-best lists, a phrase based SMT   was used.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase based SMT",
                "was used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "n-best lists",
                "generate",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However, as pointed out in  , there is no reason to believe that the resulting parameters are optimal with respect to translation quality measured with the Bleu score.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "optimal with respect to translation quality",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "Bleu score",
                "measured",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The model scaling factors M1 are trained on a development corpus according to the final recognition quality measured by the word error rate   .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model scaling factors M1",
                "trained on development corpus",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word error rate",
                "measured by",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A word link extension algorithm similar to the one presented in this paper is given in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "similar to the one presented in this paper",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word link extension",
                "given in  ",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "An exception is the use of similarity for alleviating the sparse data problem in language modeling  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sparse data problem",
                "alleviating",
                "LIMITATION",
                "neutral",
                0.8
            ],
            [
                "similarity",
                "use of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 Discussion As stated above, we aim to build an unsupervised generative model for named entity clustering, since such a model could be integrated with unsupervised coreference models like Haghighi and Klein   for joint inference.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generative model",
                "unsupervised",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "coreference models",
                "unsupervised",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Since the advent of manually tagged corpora such as the Brown Corpus and the Penn Treebank  , Marcus ), the efficacy of machine learning for training a tagger has been demonstrated using a wide array of techniques, including: Markov models, decision trees, connectionist machines, transformations, nearest-neighbor algorithms, and maximum entropy  , Black , Schmid , Brill ,Daelemans ,Ratnaparkhi ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques",
                "array of techniques",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "maximum entropy",
                "using maximum entropy",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This method of co-training has been previously applied to a variety of natural language tasks, such as word sense disambiguation  , lexicon construction for information extraction  , and named entity classification  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method of co-training",
                "applied to a variety of natural language tasks",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "named entity classification",
                "previously applied",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Several frameworks for finding translation equivalents or translation units in machine translation, such as \\  and other example-based MT approaches, might be used to select the preferred mapping.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "frameworks",
                "such as and other example-based MT approaches",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "preferred mapping",
                "select",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "3.1 Word Sequence Classification Similar to English text chunking  , the word sequence classification model aims to classify each word via encoding its context features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word sequence classification model",
                "classifies each word via encoding its context features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word sequence classification",
                "similar to English text chunking",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "e draw on and extend the work of Marcu and Echihabi  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Marcu and Echihabi",
                "work",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "extend the work",
                "METHODOLOGY",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Table 1 reports values for the Kappa   coefficient of agreement   for Forward and Backward Functions .6 The columns in the tables read as follows: if utterance Ui has tag X, do coders agree on the subtag?",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa coefficient of agreement",
                "for Forward and Backward Functions",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "columns in the tables",
                "read as follows",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Examples are the Penn Treebank   for American English annotated at the University of Pennsylvania, the French treebank   developed in Paris, the TIGER Corpus   for German annotated at the Universities of Saarbrcurrency1ucken and This research was funded by a German Science Foundation grant  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "annotated at the University of Pennsylvania",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "German Science Foundation grant",
                "funded",
                "LIMITATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The training algorithm we used is the improved iterative scaling   described in  3.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "improved iterative scaling",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "described in 3",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilistic evaluation models",
                "different feature types",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "different models",
                "use different feature types",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Firstly, there is also H  A  declined H  H  the dollar A  H  C  H  H  Figure 2: A tree with constituents marked the top-down method, which is a version of the algorithm described by Hockenmaier et al  , but used for translating into simple   CG rather than the Steedmans Combinatory Categorial Grammar    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "top-down method",
                "is a version of the algorithm described by Hockenmaier et al",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "described by Hockenmaier et al",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In a next step, chunk information was added by a rule-based language-independent chunker   that contains distituency rules, which implies that chunk boundaries are added between two PoS codes that cannot occur in the same constituent.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunker",
                "contains distituency rules",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "chunk boundaries",
                "added between two PoS codes that cannot occur in the same constituent",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The last two counts   were performed on a 29-million word parsed corpus  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "29-million word parsed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parsed corpus",
                "was performed on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  also uses wide context, but incorporates the one-senseper-discourse and one-sense-per-collocation constraints, using an unsupervised learning technique.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "one-senseper-discourse and one-sense-per-collocation constraints",
                "uses",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "unsupervised learning technique",
                "uses",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "arcu and Echihabi   demonstrated that word pairs extracted from the respective text spans are a good signal of the discourse relation between arguments",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word pairs",
                "good signal",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "text spans",
                "are a good signal",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Its still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MSA",
                "pre-clustered to have the same constituent ordering",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "input",
                "is pre-clustered",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Recent research on statistical machine translation   has lead to the development of phrasebased systems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based systems",
                "development",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "phrase-based systems",
                "phrase-based systems",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We have processed the Susanne corpus   and Penn treebank   to provide tables of word and subtree alignments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "Susanne corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "treebank",
                "Penn treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, Animal would be mapped to Aa, G.M. would again be mapped to A.A The tagger was applied and trained in the same way as described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger",
                "applied and trained in the same way",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tagger",
                "applied and trained in the same way as described in ",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In   a set of transformational rules is used for modifying the classification of words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "set of transformational rules",
                "used for modifying the classification of words",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "transformational rules",
                "modifying the classification of words",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.2 Global Linear Models We follow the framework of Collins  , recently applied to language modeling in Roark et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins",
                "recently applied to language modeling",
                "INNOVATION",
                "positive",
                0.7
            ],
            [
                "framework of Collins",
                "applied to language modeling",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning  , manually-de ned coreference patterns to mine speci c kinds of data  , or accepted the noise inherent in unsupervised schemes  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "coarser features",
                "learned from smaller sets",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "unsupervised schemes",
                "inherent noise",
                "LIMITATION",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "Typically, a small set of seed polar phrases are prepared, and new polar phrases are detected based on the strength of co-occurrence with the seeds  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "seed polar phrases",
                "prepared",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "strength of co-occurrence",
                "with the seeds",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The feature weights i in the log-linear model are determined using a minimum error rate training method, typically Powells method  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "determined using minimum error rate training method",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Powells method",
                "typically used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The GIZA++ aligner is based on IBM Model 4  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ aligner",
                "is based on IBM Model 4",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IBM Model 4",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Brown et al. proposed a class-based n-gram model, which generalizes the n-gram model, to predict a word from previous words in a text  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-gram model",
                "generalizes the n-gram model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word",
                "predict from previous words in a text",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, Wu   used an English-Chinese bilingual parser based on stochastic transduction grammars to identify terms, including multiword expressions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "based on stochastic transduction grammars",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "terms, including multiword expressions",
                "identified",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The concept of these alignments is similar to the ones introduced by  , but we will use another type of dependence in the probability distributions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignments",
                "similar to the ones introduced by",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "probability distributions",
                "another type of dependence",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To prune away those pairs, we used the log-likelihood-ratio algorithm   to compute the degree of association between the verb and the noun in each pair.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-likelihood-ratio algorithm",
                "compute the degree of association",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "verb and the noun",
                "in each pair",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For English, we used the Penn Treebank version 3.0   and extracted dependency relations by applying the head-finding rules of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "version 3.0",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "head-finding rules",
                "of ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In fact, we still have a question as to whether SS-CRF-MER is really scalable in practical time for such a large amount of unlabeled data as used in our experiments, which is about 680 times larger than that of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SS-CRF-MER",
                "scalable in practical time",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "large amount of unlabeled data",
                "680 times larger",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "The LFG annotation algorithm of   was used to produce the f-structures for development, test and training sets.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LFG annotation algorithm",
                "was used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "f-structures",
                "for development, test and training sets",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction and Motivation Parse selection constitutes an important part of many parsing systems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing systems",
                "important part",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "parse selection",
                "constitutes",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "6 Conclusions In this paper, we showed that it is sometimes possible indeed, preferableto eliminate the initial bit of supervision in bootstrapping algorithms such as the Yarowsky   algorithm for word sense disambiguation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "bootstrapping algorithms",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "eliminate the initial bit of supervision",
                "preferable",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "ascaded models for fine-to-coarse sentiment analysis were studied by Pang and Lee  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "ascaded models for fine-to-coarse sentiment analysis",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentiment analysis",
                "were studied",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "It has been shown by Shapiro and Stephens   and Wu (1997, Sec.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Shapiro and Stephens",
                "1997",
                "INNOVATION",
                "neutral",
                0.5
            ],
            [
                "Wu",
                "1997",
                "INNOVATION",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "Under the maximum entropy framework  , evidence from different features can be combined with no assumptions of feature independence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "can be combined with no assumptions of feature independence",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "maximum entropy framework",
                "no assumptions of feature independence",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": ".2.1 Inside/Outside Encoding The Inside/Outside scheme of encoding chunking states of base noun phrases was studied in Ibmlshaw and Marcus  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Inside/Outside scheme",
                "of encoding chunking states of base noun phrases",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Ibmlshaw and Marcus",
                "studied",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "hese include the bootstrapping approach  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bootstrapping approach",
                "bootstrapping approach",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "approach",
                "include",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "While error-driven training techniques are commonly used to improve the performance of phrasebased translation systems  , this paper presents a novel block sequence translation approach to SMT that is similar to sequential natural language annotation problems 727 such as part-of-speech tagging or shallow parsing, both in modeling and parameter training.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based translation systems",
                "commonly used",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "novel block sequence translation approach",
                "similar to sequential natural language annotation problems",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This alignment representation is a generalization of the baseline alignments described in   and allows for many-to-many alignments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment representation",
                "generalization of the baseline alignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "many-to-many alignments",
                "allows for",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Running words 1,864 14,437 Vocabulary size 569 1,081 Table 2: ChineseEnglish corpus statistics   using Phramer  , a 3-gram language model with Kneser-Ney smoothing trained with SRILM   on the English side of the training data and Pharaoh   with default settings to decode.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Phramer",
                "a 3-gram language model with Kneser-Ney smoothing",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Pharaoh",
                "with default settings",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There are other types of variations for phrases; for example, insertion, deletion or substitution of words, and permutation of words such as view point and point of view are such variations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "variations",
                "such as insertion, deletion or substitution of words, and permutation of words",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "view point and point of view",
                "are such variations",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Distributional approaches, on the other hand, rely on text corpora, and model relatedness by comparing the contexts in which two words occur, assuming that related words occur in similar context  , Lin  , Mohammad and Hirst  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "text corpora",
                "rely on",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "words",
                "occur in similar context",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For each, we give case-insensitive scores on version 0.6 of METEOR   with all modules enabled, version 1.04 of IBMstyle BLEU  , and version 5 of TER  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "METEOR",
                "version 0.6 with all modules enabled",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU",
                "version 1.04",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ntroduction Translation of two languages with highly different morphological structures as exemplified by Arabic and English poses a challenge to successful implementation of statistical machine translation models  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "languages",
                "highly different morphological structures",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical machine translation models",
                "successful implementation",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "To achieve step  , we first apply a set of headfinding rules which are similar to those described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "headfinding rules",
                "similar to those described in",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "set of headfinding rules",
                "are applied",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Cutting et al. 1992), local rules   and neural networks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "local rules",
                "local rules",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "neural networks",
                "neural networks",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4 Testing the Four Hypotheses The question of why self-training helps in some cases   but not others   has inspired various theories.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "theories",
                "various",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "self-training",
                "helps in some cases",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Background 2.1 Previous Work 2.1.1 Research on Phrase-Based SMT The original work on statistical machine translation was carried out by researchers at IBM  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "researchers at IBM",
                "carried out original work",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "statistical machine translation",
                "original work",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There are several works that try to learn paraphrase pairs from parallel or comparable corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "parallel or comparable",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "works",
                "try to learn paraphrase pairs",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "irst as the configuration space we can use only the reference nodes   from the lattice which makes it similar to the method of Berger et al. 1996 described in section 2.1",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "configuration space",
                "use only the reference nodes",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "method",
                "similar to the method of Berger et al. 1996",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "To summarize, we can describe our system as follows: it is based on  s implementation of  , which has been fed at each iteration by a different dataset consisting of the supervised and unsupervised part: precisely, by a concatenation of the manually tagged training data   and a chunk of automatically tagged unsupervised data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dataset",
                "manually tagged training data and a chunk of automatically tagged unsupervised data",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "system",
                "based on an implementation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We used Pharoah   as a baseline system for comparison; the s-phrases used in our system include all phrases, with the same scores, as those used by Pharoah, allowing a direct comparison.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pharoah",
                "as a baseline system",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "s-phrases",
                "used in our system",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We use the log-likelihood ratio for determining significance as in  , but other measures are possible as well.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-likelihood ratio",
                "as in ",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "other measures",
                "are possible as well",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To counteract this, we introduce two brevity penalty measures   inspired by BLEU   which we incorporate into the loss function, using a product, loss = 1PrecBP: BP1 = exp )   BP2 = exp ) where r is the reference length and c is the candidate length.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "brevity penalty measures",
                "inspired by BLEU",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "loss function",
                "using a product",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The most similar work to ours is  , in which two most common synsets from WordNet for all words in an NP and their hypernyms are extracted as features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "synsets",
                "from WordNet",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "hypernyms",
                "extracted as features",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Note that unlike the constructions in   and   no errors are possible for ngrams stored in the model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ngrams stored in the model",
                "no errors are possible",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "constructions",
                "no errors possible",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To avoid this problem we use the concept of class proposed for a word n-gram model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-gram model",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "concept of class",
                "used to avoid problem",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "GIZA++  , an implementation of the IBM   and HMM  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "implementation of the IBM and HMM",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IBM and HMM",
                "implementation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Furthermore, training corpora for information extraction are typically annotated with domain-specific tags, in contrast to general-purpose annotations such as part-of-speech tags or noun-phrase bracketing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "domain-specific tags",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "annotations",
                "general-purpose annotations",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he algorithm we implemented is inspired by the work of Yarowsky   on word sense disambiguation",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "inspired by the work of Yarowsky",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "work of Yarowsky",
                "on word sense disambiguation",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "These models can be tuned using minimum error rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "tuned using minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Salience Feature Pronoun Name Nominal TOP 0.75 0.17 0.08 HIGH 0.55 0.28 0.17 MID 0.39 0.40 0.21 LOW 0.20 0.45 0.35 NONE 0.00 0.88 0.12 Table 2: Posterior distribution of mention type given salience  ) 3.3 Modifications to the H&K Model Next, we discuss the potential weaknesses of H&Ks model and propose three modifications to it.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "H&K Model",
                "potential weaknesses",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "H&K Model",
                "propose three modifications to it",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This statistical technique of labeling predicate argument operates on the output of the probabilistic parser reported in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "predicate argument",
                "labeling",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probabilistic parser",
                "reported in",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "It is possible to use unsupervised learning to train stochastic taggers without the need for a manually annotated corpus by using the Baum-Welch algorithm \\ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Baum-Welch algorithm",
                "without the need for a manually annotated corpus",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "unsupervised learning",
                "train stochastic taggers",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The learning algorithm for level-0 dependency is similar to the guided learning algorithm for labelling as described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "learning algorithm",
                "similar to the guided learning algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm for labelling",
                "described in",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Themodeling approachhere describedis discriminative, and is based on maximum entropy   models, firstly applied to natural language problems in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "modeling approach",
                "discriminative",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "maximum entropy models",
                "firstly applied to natural language problems",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The phrase translation table is learnt in the following manner: The parallel corpus is word-aligned bidirectionally, and using various heuristics   for details) phrase correspondences are established.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase translation table",
                "learnt",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase correspondences",
                "established",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Figures 1 and 2 present best results in the learning experiments for the complete set of patterns used in the collocation approach, over two of our evaluation corpora.11 Type Positions Tags/Words Features Accuracy Precision Recall GIS 1 W 1254 0.97 0.96 0.98 IIS 1 T 136 0.95 0.96 0.94 NB 1 T 136 0.88 0.97 0.84 9 see Rish, 2001, Ratnaparkhi, 1997 and Berger et al, 1996 for a formal description of these algorithms.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "complete set of patterns",
                "best results",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "evaluation corpora",
                "two of",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In addition, their system does not classify non-anaphoric pronouns, A third paper that has significantly influenced our work is that of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "does not classify non-anaphoric pronouns",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "paper",
                "has significantly influenced our work",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "e then apply Brills rule-based tagger   and BaseNP noun phrase chunker   to extract noun phrases from these sentences",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brill's rule-based tagger",
                "rule-based",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "noun phrases",
                "extract",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Minimum Error Rate training   over BLEU was used to optimize the weights for each of these models over the development test data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Minimum Error Rate training",
                "optimize the weights",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "weights",
                "for each of these models",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": ".1 Candidate NPs Noun phrases were extracted using Ramshaw and Marcus's base NP chunker  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ramshaw and Marcus's base NP chunker",
                "was used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Candidate NPs",
                "were extracted",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "P   P L     Statistical approaches to language modeling have been used in much NLP research, such as machine translation   and speech recognition  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "to language modeling",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "NLP research",
                "such as machine translation and speech recognition",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Several papers have looked at higher-order representations, but have not examined the equivalence of syn/para distributions when formalized as Markov chains  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "representations",
                "higher-order",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "distributions",
                "syn/para",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Klein and Manning   argue for CL on grounds of accuracy, but see also Johnson  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CL",
                "accuracy",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Johnson",
                "see also",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The candidates of unknown words can be generated by heuristic rules  or statistical word models which predict the probabilities for any strings to be unknown words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "candidates",
                "generated by heuristic rules or statistical word models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "probabilities",
                "predict for any strings to be unknown words",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "2.2 The Translation Model We adapted Model 1   to our purposes.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 1",
                "adapted to our purposes",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Model 1",
                "to our purposes",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Additionally, automatic evaluation of content coverage using ROUGE   was explored in 2004.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE",
                "was explored",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "evaluation",
                "of content coverage",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The Collins parser   does use dynamic programming in its search.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "dynamic programming in its search",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "dynamic programming",
                "use",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "They recover additional latent variables so-called nuisance variablesthat are not of interest to the user.1 For example, though machine translation   seeks to output a string, typical MT systems   1These nuisance variables may be annotated in training data, but it is more common for them to be latent even there, i.e., there is no supervision as to their correct values.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "nuisance variables",
                "are not of interest to the user",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "MT systems",
                "typical",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": ".1 Collocation Features The collocation features were inspired by the one-sense-per-collocation heuristic proposed by Yarowsky  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocation features",
                "one-sense-per-collocation heuristic",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Yarowsky",
                "proposed",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "A variety of methods are used to account for the re-ordering stage: word-based  , templatebased  , and syntax-based  , to name just a few.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "word-based, template-based, and syntax-based",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "to name just a few",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The significance of G 2 based on the exact conditional distribution does not rely on an asymptotic approximation and is accurate for sparse and skewed data samples   4.2 Information criteria The family of model evaluation criteria known as information criteria have the following expression: IC,~ = G 2 ~ x dof   where G ~ and dof are defined above.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "G 2",
                "based on the exact conditional distribution",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "information criteria",
                "have the following expression",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "He uses a specic reliability statistic, , for his measurements, but Carletta   implicitly assumes kappa-like metrics are similar enough in practice for the rule of thumb to apply to them as well.A detailed discussion on the differences and similarities of these, and other, measures is provided by Krippendorff  ; in this article we will use Cohens    to investigate the value of the 0.8 reliability cut-off for computational linguistics.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reliability statistic",
                "specic",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "kappa-like metrics",
                "similar enough",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In this work we use the following contextual information: a3 Target context: As in   we consider a window of 3 words to the left and to the right of the target word considered.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "target context",
                "window of 3 words to the left and to the right",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "target word",
                "considered",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he feasibility of such post-parse deepening   is demonstrated by Cahill et al  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "post-parse deepening",
                "is demonstrated",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "feasibility",
                "is demonstrated",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "So fitr, we have implemented the following,: sentence ~dignment btLsed-on word correspondence information, word correspondence estimation by cooccnl'rence-ffequency-based methods in GMe mid Church   and Kay and R6scheisen  , structured Imttehlng of parallel sentences  , and case Dame acquisition of Japanese verbs  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentence ~dignment btLsed-on word correspondence information",
                "implemented",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "cooccnl'rence-ffequency-based methods",
                "in GMe mid Church and Kay and R6scheisen",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts  , semantic lexicons  , concept lists  , and word similarity lists  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "mining knowledge from text and the Web",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "semantic lexicons",
                "concept lists",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "At each training-set size, a new copy of the network is trained under each of the following conditions:   using SULU,   using SULU but supplying only the labeled training examples to synthesize,   standard network training,   using a re-implementation of an algorithm proposed by Yarowsky  , and   using standard network training but with all training examples labeled to establish an upper bound.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SULU",
                "using SULU",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "standard network training",
                "standard network training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": " , sometimes augmented by an HMM-based model or Och and Neys Model 6  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMM-based model",
                "augmented",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Och and Neys Model 6",
                "sometimes used",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "e adopt a similar approach to the one used in Turney   and consider each question as a separate binary classification problem with one positive training instance and 5 unknown pairs",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "used in Turney",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "question",
                "as a separate binary classification problem",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A related example would be a version of synchronous CFG that allows only one pair of linked nonterminals and any number of unlinked nonterminals, which could be bitextparsed in O  time, whereas inversion transduction grammar   takes O .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "nonterminals",
                "any number of unlinked",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "transduction grammar",
                "takes O",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Concluding Remarks Formalisms for finite-state and context-free transduction have a long history  , and such formalisms have been applied to the machine translation problem, both in the finite-state case   and the context-free case  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "formalisms",
                "have a long history",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "formalisms",
                "have been applied to the machine translation problem",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We evaluate the string chosen by the log-linear model against the original treebank string in terms of exact match and BLEU score   DEF ATTR ADJ definite descriptions with adjectival modifier DEF GENARG definite descriptions with a genitive argument DEF PPADJ definite descriptions with a PP adjunct DEF RELARG definite descriptions including a relative clause DEF APP definite descriptions including a title or job description as well as a proper name   Names PROPER combinations of position/title and proper name   BARE PROPER bare proper names Demonstrative descriptions SIMPLE DEMON simple demonstrative descriptions MOD DEMON adjectivally modified demonstrative descriptions Pronouns PERS PRON personal pronouns EXPL PRON expletive pronoun REFL PRON reflexive pronoun DEMON PRON demonstrative pronouns   GENERIC PRON generic pronoun   DA PRON da-pronouns   LOC ADV location-referring pronouns TEMP ADV,YEAR Dates and times Indefinites SIMPLE INDEF simple indefinites NEG INDEF negative indefinites INDEF ATTR indefinites with adjectival modifiers INDEF CONTRAST indefinites with contrastive modifiers   INDEF PPADJ indefinites with PP adjuncts INDEF REL indefinites with relative clause adjunct INDEF GEN indefinites with genitive adjuncts INDEF NUM measure/number phrases INDEF QUANT quantified indefinites Table 5: An inventory of interesting syntactic characteristics in IS phrases Label 1   Label 2   B/A Total D-GIVEN-PRONOUN INDEF-REL 0 19 PERS PRON 39 INDEF ATTR 23 DA PRON 25 SIMPLE INDEF 17 DEMON PRON 19 GENERIC PRON 11 D-GIVEN-PRONOUN D-GIVEN-CATAPHOR 0.1 11 PERS PRON 39 SIMPLE DEF 13 DA PRON 25 DA PRON 10 DEMON PRON 19 GENERIC PRON 11 D-GIVEN-REFLEXIVE NEW 0.11 31 REFL PRON 54 SIMPLE INDEF 113 INDEF ATTR 53 INDEF NUM 32 INDEF PPADJ 26 INDEF GEN 25  Table 6: IS asymmetric pairs augmented with syntactic characteristics 822 2002).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "DEF ATTR ADJ definite descriptions with adjectival modifier",
                "DEF GENARG definite descriptions with a genitive argument",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "INDEF QUANT quantified indefinites",
                "INDEF QUANT quantified indefinites",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  have proposed an algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "applies a discriminative step",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Expectation-Maximization algorithm",
                "used in IBM models",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We distinguish two main approaches to domain adaptation that have been addressed in the literature  : supervised and semi-supervised.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "main",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "supervised and semi-supervised",
                "main approaches",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "the Wall Street Journal   sections of the Penn Treebank   as training set, tests on BROWN Sections typically result in a 6-8% drop in labeled attachment scores, although the average sentence length is much shorter in BROWN than that in WSJ.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BROWN",
                "shorter sentence length",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "WSJ",
                "average sentence length",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The idea of word class   gives a general solution to this problem.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word class",
                "gives a general solution",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "idea",
                "general solution",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " , we introduced the Movie Review Polarity Dataset Enriched with Annotator Rationales.8 It is based on the dataset of Pang and Lee  ,9 which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Movie Review Polarity Dataset Enriched with Annotator Rationales",
                "is introduced",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "dataset of Pang and Lee",
                "consists of 1000 positive and 1000 negative movie reviews",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Since then this idea has been applied to several tasks, including word sense disambiguation   and named-entity recognition  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "idea",
                "applied to several tasks",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "tasks",
                "including word sense disambiguation and named-entity recognition",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One important application of bitext maps is the construction of translation lexicons   and, as discussed, translation lexicons are an important information source for bitext mapping.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bitext maps",
                "construction of translation lexicons",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "translation lexicons",
                "important information source",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "  develop a bottom-up decoder for BTG   that uses only phrase pairs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "uses only phrase pairs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BTG",
                "bottom-up",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction Word associations   have a wide range of applications including: Speech Recognition, Optical Character Recognition and Information Retrieval    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Word associations",
                "have a wide range of applications",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "applications",
                "including Speech Recognition, Optical Character Recognition and Information Retrieval",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As with other randomised models we construct queries with the appropriate sanity checks to lower the error rate efficiently  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "queries",
                "with the appropriate sanity checks",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "error rate",
                "is lowered efficiently",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "he collocations have been calculated according to the method described in Church and Hanks   by moving a window on the texts",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "described in Church and Hanks",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "window on the texts",
                "moving a window",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In particular, mutual information   and other statistical methods such as   and frequency-based methods such as   exclude infrequent phrases because they tend to introduce too much noise.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information",
                "exclude infrequent phrases",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "frequency-based methods",
                "introduce too much noise",
                "METHODOLOGY",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "The table in Figure 9 shows a comparison of different systems for which tagging accuracies have been reported previously for the 17-tagset case  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "systems",
                "different",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tagging accuracies",
                "reported previously",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As mentioned in Section 2.2, there are words which have two or more candidate POS tags in the PTB corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PTB corpus",
                "has two or more candidate POS tags",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "words",
                "in the PTB corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The word alignments were created with Giza++   applied to a parallel corpus containing the complete Europarl training data, plus sets of 4,051 sentence pairs created by pairing the test sentences with the reference translations, and the test sentences paired with each of the system translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Giza++",
                "applied",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "system translations",
                "each of the",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "9  report that, for translation reranking, such local updates   outperform bold updates  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "local updates",
                "outperform",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "bold updates",
                "do not outperform",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "We annotated with the BIO tagging scheme used in syntactic chunkers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BIO tagging scheme",
                "used in syntactic chunkers",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntactic chunkers",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To this purpose, different authors   propose the use of the so-called log-linear models, where the decision rule is given by the expression y = argmax y Msummationdisplay m=1 mhm    where hm  is a score function representing an important feature for the translation of x into y, M is the number of models   and m are the weights of the log-linear combination.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-linear models",
                "use of log-linear models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "log-linear combination",
                "important feature for translation",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Alignment performance is measured by the Alignment Error Rate     AER  = 12|B B|/  where B is a set reference word links, and B are the word links generated automatically.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Alignment Error Rate",
                "measured by",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "word links generated automatically",
                "generated automatically",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "5.1 The baseline System used for comparison was Pharaoh  , which uses a beam search algorithm for decoding.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "System",
                "uses beam search algorithm for decoding",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Pharaoh",
                "uses beam search algorithm for decoding",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To test the reliability of group segmentation within GDM-IS, we calculate the kappa coefficient   8   to measure pairwise agreement between the subject and the expert.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa coefficient",
                "measure pairwise agreement",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "expert",
                "subject",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The search is based on the property that when computing sim , words that have high mutual information values 5The nominator in our metric resembles the similarity metric in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sim",
                "have high mutual information values",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "metric",
                "resembles the similarity metric",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "To determine the tree head-word we used a set of rules similar to that described by    and also used by  , which we modified in the following way:  The head of a prepositional phrase   was substituted by a function the name of which corresponds to the preposition, and its sole argument corresponds to the head of the noun phrase NP.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "set of rules",
                "similar to that described by and also used by",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "function",
                "the name of which corresponds to the preposition",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.3 BLEU Score The BLEU score   measures the agreement between a hypothesiseI1 generated by the MT system and a reference translation eI1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "measures the agreement",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "MT system",
                "generated by",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The use of such relations   for various purposes has received growing attention in recent research  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "relations",
                "received growing attention",
                "INNOVATION",
                "positive",
                0.7
            ],
            [
                "research",
                "has received growing attention",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The lexicalized parsing experiments were run using Dan Bikels probabilistic parsing engine   which in addition to replicating the models described by Collins   also provides a convenient interface to develop corresponding parsing models for other languages.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilistic parsing engine",
                "provides a convenient interface",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "models described by Collins",
                "replicating",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "  defined two local search operations for their 1-to-N alignment models 3, 4 and 5.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "local search operations",
                "for 1-to-N alignment models 3, 4 and 5",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "alignment models 3, 4 and 5",
                "1-to-N",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number  , or contextual role-knowledge  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unsupervised methods",
                "useful information",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "role-knowledge",
                "contextual",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "They used the Bleu evaluation metric  , but capped the n-gram precision at 4-grams.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bleu evaluation metric",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "n-gram precision",
                "capped at 4-grams",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Restricting phrases to syntactic constituents has been shown to harm performance  , so we tighten our definition of a violation to disregard cases where the only point of overlap is obscured by our phrasal resolution.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrasal resolution",
                "obscured by",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "performance",
                "harm",
                "PERFORMANCE",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "Both systems are built around from the maximum-entropy technique  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum-entropy technique",
                "built around",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "systems",
                "both",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Decoding used beam search with the cube pruning algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "beam search",
                "with the cube pruning algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "cube pruning algorithm",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 Data The data consists of sections of the Wall Street Journal part of the Penn TreeBank  , with information on predicate-argument structures extracted from the PropBank corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data",
                "sections of the Wall Street Journal part of the Penn TreeBank",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "PropBank corpus",
                "extracted from",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.5 Adding Context to the Model Next, we added of a stochastic POS tagger   to provide a model of context.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "stochastic POS tagger",
                "provide a model of context",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "model",
                "adding context",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Proceedings of the 40th Annual Meeting of the Association for  , a number of other algorithms have been developed.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "have been developed",
                "INNOVATION",
                "neutral",
                0.5
            ],
            [
                "other",
                "a number of",
                "METHODOLOGY",
                "neutral",
                0.6
            ]
        ]
    },
    {
        "text": "This text was part-of-speech tagged using the Xerox HMM tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Xerox HMM tagger",
                "part-of-speech tagged",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "5.2 Evaluation Criteria For the automatic evaluation, we used the criteria from the IWSLT evaluation campaign  , namely word error rate  , positionindependent word error rate  , and the BLEU and NIST scores  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation criteria",
                "from the IWSLT evaluation campaign",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU and NIST scores",
                "scores",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There have been many studies on POS guessing of unknown words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS guessing",
                "of unknown words",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "studies",
                "many",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Brown,   uses the same bigrams and by means of a greedy algorithm forms the hierarchical clusters of words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bigrams",
                "forms hierarchical clusters",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "greedy",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In addition, since word senses are often associated with domains  , word senses can be consequently distinguished by way of determining the domain of each description.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word senses",
                "can be distinguished by way of determining the domain of each description",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "domains",
                "associated with",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In all of the cited approaches, the Penn Wall Street Journal Treebank   is used, the availability of whichobviates the standard eort required for treebank traininghandannotating large corpora of specic domains of specic languages with specic parse types.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Wall Street Journal Treebank",
                "availability of which obviates the standard effort required",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "treebank training",
                "handannotating large corpora",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There are many possible methods for combining unlabeled and labeled data  , but we simply concatenate unlabeled data with labeled data to see the effectiveness of the selected reliable parses.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods for combining unlabeled and labeled data",
                "concatenate unlabeled data with labeled data",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "selected reliable parses",
                "see the effectiveness of",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For instance, one might be interested in frequencies of co-occurences of a word with other words and phrases    , or one might be interested in inducing wordclasses from the text by collecting frequencies of the left and right context words for a word in focus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word",
                "co-occurences",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "frequencies",
                "collecting frequencies of left and right context words",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "They are part of an effort to better integrate a linguistic, rule-based system and the statistical correcting layer also illustrated in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linguistic, rule-based system",
                "part of an effort to better integrate",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "statistical correcting layer",
                "also illustrated in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models  , but also for syntax-based models  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "excellent source",
                "INNOVATION",
                "positive",
                0.85
            ],
            [
                "models",
                "phrase-based and syntax-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "F-Measure with an appropriate setting of  will be useful during the development process of new alignment models, or as a maximization criterion for discriminative training of alignment models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "F-Measure",
                "will be useful",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "alignment models",
                "as a maximization criterion",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4 Experiments The Penn Treebank   is used as the testing corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "used as the testing corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "It is explored extensively in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "",
                "extensively",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                ",",
                "",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Parameters were tuned with minimum error-rate training   on the NIST evaluation set of 2006   for both C-E and A-E.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NIST evaluation set",
                "minimum error-rate training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "C-E and A-E",
                "tuned",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A second pass aligns the sentences in a way similar1 to the algorithm described by Gale and Church  , but where the search space is constrained to be close to the one delimited by the word alignment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "search space",
                "constrained to be close to the one",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "similar to the algorithm described by Gale and Church",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For  , the morphemes and labels for our task are:   kita NEG tINC inE1S chabe VT -j SC laj PREP inA1S yol S -j SC iin PRON We also consider POS-tagging for Danish, Dutch, English, and Swedish; the English is from sections 00-05   and 19-21   of the Penn Treebank  , and the other languages are from the CoNLL-X dependency parsing shared task  .1 We split the original training data into training and development sets.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "from sections 00-05 and 19-21",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "CoNLL-X dependency parsing shared task",
                "from the",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Then, it models the correlations between the pivot features and all other features by training linear pivot predictors to predict occurrences of each pivot in the unlabeled data from both domains  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pivot features",
                "predict occurrences of each pivot",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "linear pivot predictors",
                "predicting occurrences",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Using this heuristic, BABAR identifies existential definite NPs in the training corpus using our previous learning algorithm   and resolves all occurrences of the same existential NP with each another.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BABAR",
                "previous learning algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "existential NP",
                "resolves all occurrences",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Distributional clustering and dimensionality reduction techniques are typically applied when linguistically meaningful classes are desired  ; probabilistic models have been used to find classes that can improve smoothing and reduce perplexity  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques",
                "applied when linguistically meaningful classes are desired",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "models",
                "can improve smoothing and reduce perplexity",
                "PERFORMANCE",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "The NIST MT03 test set is used for development, particularly for optimizing the interpolation weights using Minimum Error Rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NIST MT03 test set",
                "used for development",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "Minimum Error Rate training",
                "used for optimizing interpolation weights",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Techniques for weakening the independence assumptions made by the IBM models 1 and 2 have been proposed in recent work  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM models",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "independence assumptions",
                "weakening",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The f-structures are created automatically by annotating nodes in the gold standard WSJ trees with LFG functional equations and then passing these equations through a constraint solver  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LFG functional equations",
                "used for annotating nodes",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "constraint solver",
                "passing equations through",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The translation component is an analog of the IBM model 2  , with parameters that are optimized for use with the trigram.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation component",
                "analog of the IBM model 2",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parameters",
                "optimized for use with the trigram",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Recently, confusion-network-based system combination algorithms have been developed to combine outputs of multiple machine translation   systems to form a consensus output  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine translation systems",
                "outputs",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "system combination algorithms",
                "confusion-network-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Answer Extraction: We select the top 5 ranked sentences and return them as Collins, 1997, can be used to capture the binary dependencies between the head of each phrase.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins, 1997",
                "can be used to capture binary dependencies",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ranked sentences",
                "top 5",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Default parameters were used for all experiments except for the numberofiterationsforGIZA++ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "numberofiterationsforGIZA++",
                "default parameters",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "numberofiterationsforGIZA++",
                "except for",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It is an implementation of Models 1-4 of Brown et al. \\ , where each of these models produces a Viterbi alignment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Models 1-4 of Brown et al.",
                "produces a Viterbi alignment",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Brown et al.",
                "implementation of",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "1 Yarowsky   proposes a method for word sense   disambiguation that is based on a bootstrapping technique, which we refer to here as Monolingual Bootstrapping  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "based on a bootstrapping technique",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Monolingual Bootstrapping",
                "proposed by Yarowsky",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Previous literature on GB parsing /Wehrli, 1984; Sharp, 1985; Kashket, 1986; Kuhns, 1986; Abney, 1986/has not addressed the issue of implementation of the Binding theory) The present paper intends in part to fill this gap.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "previous literature",
                "has not addressed",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Binding theory",
                "implementation",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Recently, Yarowsky   combined a MIlD and a corpus in a bootstrapping process.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MIlD",
                "combined with corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpus",
                "combined with MIlD",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Congress of the Italian Association for Artificial Intelligence, Palermo, 1991 B. Boguraev, Building a Lexicon: the Contribution of Computers, IBM Report, T.J. Watson Research Center, 1991 M. Brent, Automatic Aquisition of Subcategorization frames from Untagged Texts, in   N. Calzolari, R. Bindi, Acquisition of Lexical Information from Corpus, in   K. W. Church, P. Hanks, Word Association Norms, Mutual Information, and Lexicography, Computational Linguistics, vol.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Boguraev, Building a Lexicon",
                "the Contribution of Computers",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Automatic Aquisition of Subcategorization frames",
                "from Untagged Texts",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our model uses an exemplar memory that consists of 133566 verb-role-noun triples extracted from the Wall Street Journal and Brown parts of the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "verb-role-noun triples",
                "133566 extracted from the Wall Street Journal and Brown parts of the Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank",
                "parts of",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction Recent work in machine translation has evolved from the traditional word   and phrase based   models to include hierarchical phrase models   and bilingual synchronous grammars  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine translation",
                "traditional word and phrase based models",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "phrase models and bilingual synchronous grammars",
                "include",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For more detail, explanations and experiments see  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "explanations",
                "see",
                "APPLICABILITY",
                "neutral",
                1.0
            ],
            [
                "experiments",
                "see",
                "INNOVATION",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "We demonstrate that allowing different values for these hyperparameters significantly improves performance over both a strong baseline and   within both a conditional random field sequence model for named entity recognition and a discriminatively trained dependency parser.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hyperparameters",
                "significantly improves performance",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "baseline",
                "strong",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Also related are the areas of word alignment for machine translation  , induction of translation lexicons  , and cross-language annotation projections to a second language  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word alignment",
                "for machine translation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "cross-language annotation projections",
                "to a second language",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There has been some previous work on accuracy-driven training techniques for SMT, such as MERT   and the Simplex Armijo Downhill method  , which tune the parameters in a linear combination of various phrase scores according to a held-out tuning set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MERT",
                "accuracy-driven training techniques",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Simplex Armijo Downhill method",
                "tune the parameters",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bloom Filter LM",
                "in Joshua",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Bloom Filter LM",
                "following Talbot and Osborne",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "esults This algorithm was applied to a fragment of the Canadian Hansards that has been used in a number of other studies: Church   and Simard et al  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Church et al",
                "has been used in a number of other studies",
                "APPLICABILITY",
                "positive",
                0.7
            ],
            [
                "Hansards fragment",
                "has been used in a number of other studies",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  managed to extract LFG subcategorisation frames and paths linking long distance dependencies reentrancies from f-structures generated automatically for the PennII treebank trees and used them in an long distance dependency resolution algorithm to parse new text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LFG subcategorisation frames and paths",
                "linking long distance dependencies reentrancies",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "PennII treebank trees",
                "generated automatically",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We then used Cohens Kappa to determine the level of agreement  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Cohens Kappa",
                "determine the level of agreement",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The noun phrases in this data set are the same as in the Treebank and therefore the baseNPs in this data set are slightly different from the ones in the   data sets.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "noun phrases",
                "the same",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "baseNPs",
                "slightly different",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To group the letters into classes, we employ a hierarchical clustering algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "hierarchical clustering",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "employ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "757 hbps strong tendency to overestimate the probability of rare bi-phrases; it is computed as in equation  , except that bi-phrase probabilities are computed based on individual word translation probabilities, somewhat as in IBM model 1  : Pr  = 1|s||t| productdisplay tt summationdisplay ss Pr   The target language feature function htl: this is based on a N-gram language model of the target language.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bi-phrase probabilities",
                "computed based on individual word translation probabilities",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "N-gram language model",
                "target language",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We measure translation performance by the BLEU score   with one reference for each hypothesis.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "with one reference for each hypothesis",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "translation performance",
                "measured by BLEU score",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Indeed, our methods were inspired by past work on variational decoding for DOP   and for latent-variable parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "inspired by past work on variational decoding for DOP and for latent-variable parsing",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "work on variational decoding",
                "past work",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However, the learning curve for Negra   indicates that the performance of the Collins   model is stable, even for small training sets.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins model",
                "stable",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "small training sets",
                "even for",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "We will provide a more detailed and systematic comparison between MAXIMUM ENTROPY MODELING   and MEMORY BASED LEARNING   for morpho-syntactic disambiguation and we investigate whether earlier observed differences in tagging accuracy can be attributed to algorithm bias, information source issues or both.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MAXIMUM ENTROPY MODELING",
                "and MEMORY BASED LEARNING",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm bias",
                "information source issues",
                "LIMITATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Yarowsky   has proposed automatically augmenting a small set of experimenter-supplied seed collocations   into a much larger set of training materials.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "experimenter-supplied seed collocations",
                "a small set",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "augmenting",
                "into a much larger set",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Previous publications on Meteor   have described the details underlying the metric and have extensively compared its performance with Bleu and several other MT evaluation metrics.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Meteor",
                "underlying the metric",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "performance",
                "compared with Bleu and several other MT evaluation metrics",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "im and Hovy   integrated verb information from FrameNet and incorporated it into semantic role labeling",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "verb information",
                "integrated",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "semantic role labeling",
                "incorporated",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Machine translation systems based on probabilistic translation models   are generally trained using sentence-aligned parallel corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilistic translation models",
                "are generally trained",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentence-aligned parallel corpora",
                "are used",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Much research has been done to improve tagging accuracy using several different models and methods, including: hidden Markov models    ,  ; rule-based systems  ,  ; memory-based systems  ; maximum-entropy systems  ; path voting constraint systems  ; linear separator systems  ; and majority voting systems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "several different",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "memory-based systems",
                "none",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2.4 Formalization of   As mentioned earlier, our model is equivalent to that presented in  , and can be viewed as a formal version of his model.2 In his presentation, the adapation is done through feature augmentation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "equivalent to that presented in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "adaptation",
                "done through feature augmentation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ilingual Bracketing   is one of the bilingual shallow parsing approaches studied for Chinese-English word alignment",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bilingual shallow parsing approaches",
                "studied for Chinese-English word alignment",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Chinese-English word alignment",
                "one of",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Turney   showed that it is possible to use only a few of those semantically oriented words   to label other phrases co-occuring with them as positive or negative.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantically oriented words",
                "to label other phrases co-occuring with them as positive or negative",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Turney",
                "showed that it is possible",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For example, Liu and Gildea   developed the Sub-Tree Metric   over constituent parse trees and the Head-Word Chain Metric   over dependency parse trees.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Sub-Tree Metric",
                "developed over constituent parse trees",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Head-Word Chain Metric",
                "developed over dependency parse trees",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In both perceptron and CRF training, we average the parameters over training iterations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "average the parameters",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "training iterations",
                "over training iterations",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.2 Automatic metrics Similarly to the Pyramid method, ROUGE   and Basic Elements   require multiple topics and model summaries to produce optimal results.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE",
                "require multiple topics and model summaries to produce optimal results",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Basic Elements",
                "require multiple topics and model summaries to produce optimal results",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The recent work of   and   were also sources of inspiration.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "sources of inspiration",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "work",
                "inspiration",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "ll of the convergence and generalization results in Collins   depend on notions of separability rather than the size of GEN. Two questions come to mind",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "notions of separability",
                "rather than the size of GEN",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "GEN",
                "size of",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "First, we noted how frequently WordNet   gets used compared to other resources, such as FrameNet   or the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet",
                "gets used",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "FrameNet",
                "or the Penn Treebank",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Neural networks have been used in NLP in the past, e.g. for machine translation   and constituent parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Neural networks",
                "used in NLP",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "machine translation and constituent parsing",
                "e.g. for",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The MBT   180 Tagger Type Standard Trigram   MBT   Rule-based   Maximum-Entropy   Full Second-Order HMM SNOW   Voting Constraints   Full Second-Order HMM Known Unknown Overall Open/Closed Lexicon?",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MBT 180 Tagger Type Standard Trigram",
                "Rule-based Maximum-Entropy Full Second-Order HMM SNOW",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Voting Constraints",
                "Full Second-Order HMM Known Unknown Overall Open/Closed Lexicon",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We do not completely rule out the possibility that some more sophisticated, ontologically promiscuous, first-order analysis  ) might account for these kinds of monotonicity inferences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "analysis",
                "sophisticated",
                "METHODOLOGY",
                "positive",
                0.7
            ],
            [
                "analysis",
                "ontologically promiscuous",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "This allows us to compute the conditional probability as follows  : P  = ~i~ '    Z~  Z~  = ~I~I~ '    ff i The maximum entropy estimation technique guarantees that for every feature gi, the expected value of gi according to the M.E. model will equal the empirical expectation of gi in the training corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature gi",
                "will equal the empirical expectation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "maximum entropy estimation technique",
                "guarantees",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This paper continues a line of research on online discriminative training  , extending that of Watanabe et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "online discriminative training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "line of research",
                "extends that of Watanabe et al.",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Part-of-speech features Based on the lexical categories produced by GATE  , each token xi is classified into one of a set of coarse part-of-speech tags: noun, verb, adverb, wh-word, determiner, punctuation, etc. We do the same for neighboring words in a   window in order to assist noun phrase segmentation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GATE",
                "lexical categories",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "part-of-speech tags",
                "coarse",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In addition to collocation translation, there is also some related work in acquiring phrase or term translations from parallel corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parallel corpus",
                "related work",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase or term translations",
                "acquiring",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ur method uses assumptions similar to Berger et al. 1996 but is naturally suitable for distributed parallel computations",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "naturally suitable for distributed parallel computations",
                "APPLICABILITY",
                "positive",
                0.85
            ],
            [
                "assumptions",
                "similar to Berger et al. 1996",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "These tasks include collocation discovery  , smoothing and model estimation   and text classi cation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocation discovery",
                "discovery",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "model estimation",
                "estimation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "    to train this probability model).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probability model",
                "to train",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "probability model",
                "this probability model",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "We also cannot use prior graph construction methods for the document level  ) at the word sense level.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prior graph construction methods",
                "cannot use",
                "METHODOLOGY",
                "negative",
                0.75
            ],
            [
                "document level",
                "at the word sense level",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In fact, it has been shown that the agreement of subjects annotating bridging   or discourse   relations can be too low for tentative conclusion to be drawn  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "subjects annotating bridging or discourse relations",
                "agreement can be too low",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "tentative conclusion",
                "to be drawn",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For this we aligned 170,863 pairs of Arabic/English newswire sentences from LDC, trained a state-of-the-art syntax-based statistical machine translation system   on these sentences and alignments, and measured BLEU scores   on a separate set of 1298 newswire test sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical machine translation system",
                "state-of-the-art",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU scores",
                "measured on a separate set of newswire test sentences",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As multiple derivations are used for finding optimal translations, we extend the minimum error rate training   algorithm   to tune feature weights with respect to BLEU score for max-translation decoding  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "tune feature weights with respect to BLEU score",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation decoding",
                "max-translation decoding",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Evaluation metrics such as BLEU   have a built-in preference for shorter translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "built-in preference for shorter translations",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "translations",
                "shorter",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "in   defined the similarity between two concepts as the information that is in common to both concepts and the information contained in each individual concept",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "concepts",
                "in common to both concepts",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "concepts",
                "each individual concept",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Yarowsky   proposed such a method for word sense disambiguation, which we refer to as monolingual bootstrapping.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Yarowsky",
                "proposed such a method",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "monolingual bootstrapping",
                "refers to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the general language UPenn annotation efforts for the WSJ sections of the Penn Treebank  , sentences are annotated with POS tags, parse trees, as well as discourse annotation from the Penn Discourse Treebank  , while verbs and verb arguments are annotated with Propbank rolesets  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "WSJ sections",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Propbank rolesets",
                "verb arguments annotated with",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "According to the document, it is the output of Ratnaparkhis tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "e apply the log likelihood principle   to compute this score",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log likelihood principle",
                "to compute this score",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "score",
                "is computed",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Most current approaches emphasize within-sentence dependencies such as the distortion in  , the dependency of alignment in HMM  , and syntax mappings in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distortion",
                "in HMM",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "syntax mappings",
                "in ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Besides relative frequencies, lexical weights   are widely used to estimate how well the words in f translate the words in e. To do this, one needs first to estimate a lexical translation probability distribution w  by relative frequency from the same word alignments in the training corpus: w  = count summationtext e count    Note that a special source NULL token is added to each source sentence and aligned to each unaligned target word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexical weights",
                "widely used",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "lexical translation probability distribution",
                "estimated by relative frequency",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he effectiveness of these features for recognition of discourse relations has been previously shown by Marcu and Echihabi  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "previously shown",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "Marcu and Echihabi",
                "previously shown",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The intercoder reliability is a constant concern of everyone working with corpora to test linguistic hypotheses  , and the more so when one is coding for semanto-pragmatic interpretations, as in the case of the analysis of connectives.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "to test linguistic hypotheses",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "semanto-pragmatic interpretations",
                "as in the case of the analysis of connectives",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A null Assuming that one SMS word is mapped exactly to one English word in the channel model under an alignment, we need to consider only two types of probabilities: the alignment probabilities denoted by Pm and the lexicon mapping probabilities denoted by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "channel model",
                "under an alignment",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignment probabilities",
                "Pm",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our baseline method for ambiguity resolution is the Collins parser as implemented by Bikel  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins parser",
                "as implemented",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "baseline method",
                "for ambiguity resolution",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Because our system uses a synchronous CFG, it could be thought of as an example of syntax-based statistical machine translation  , joining a line of research   that has been fruitful but has not previously produced systems that can compete with phrase-based systems in large-scale translation tasks such as the evaluations held by NIST.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "synchronous CFG",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "phrase-based systems",
                "compete in large-scale translation tasks",
                "PERFORMANCE",
                "negative",
                0.75
            ]
        ]
    },
    {
        "text": "3 Analysis Results 3.1 Kappa Statistic Kappa coefficient   is commonly used as a standard to reflect inter-annotator agreement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa coefficient",
                "is commonly used as a standard",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "standard",
                "to reflect inter-annotator agreement",
                "APPLICABILITY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "We run the decoder with its default settings and then use Koehn's implementation of minimum error rate training   to tune the feature weights on the development set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "with its default settings",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "feature weights",
                "tune on the development set",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Improvements are obtained  , showing that a reranker is necessary for successful self-training in such a high-resource scenario.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reranker",
                "necessary",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "high-resource scenario",
                "high-resource",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Before training the classifiers, we perform feature ablation by imposing a count cutoff of 10, and by limiting the number of features to the top 75K features in terms of log likelihood ratio  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature ablation",
                "imposing a count cutoff of 10 and limiting the number of features to the top 75K features in terms of log likelihood ratio",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "log likelihood ratio",
                "in terms of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "c2009 Association for Computational Linguistics Automatic Treebank-Based Acquisition of Arabic LFG Dependency Structures Lamia Tounsi Mohammed Attia NCLT, School of Computing, Dublin City University, Ireland {lamia.tounsi, mattia, josef}@computing.dcu.ie Josef van Genabith Abstract A number of papers have reported on methods for the automatic acquisition of large-scale, probabilistic LFG-based grammatical resources from treebanks for English  ,  , German  , Chinese  ,  , Spanish  ,   and French  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Treebank-Based Acquisition",
                "automatic acquisition",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "LFG-based grammatical resources",
                "probabilistic",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Our approach permits an alternative to minimum error-rate training  ; it is discriminativebuthandleslatentstructureandregularization in more principled ways.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training",
                "minimum error-rate",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "approach",
                "handles latent structure and regularization in more principled ways",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The phrases in the translations were located using standard phrase extraction techniques  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrases",
                "standard phrase extraction techniques",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translations",
                "located using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Still, a confidence range for BLEU can be estimated by bootstrapping  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "can be estimated by bootstrapping",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "confidence range",
                "can be estimated",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "With the exception of Fraser and Marcu  , these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Fraser and Marcu",
                "do not discard",
                "LIMITATION",
                "neutral",
                0.85
            ],
            [
                "generative models",
                "integrate IBM model predictions as features",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": " ) and view the POS tags and word identities as two separate sources of information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tags",
                "two separate sources of information",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word identities",
                "two separate sources of information",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In some recent grammar induction and MT work   it has been shown that even a small amount of knowledge about a language, in the form of grammar fragments, treelets or prototypes, can go a long way in helping with the induction of a grammar from raw text or with alignment of parallel corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar fragments",
                "can go a long way",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "small amount of knowledge",
                "helping with induction of a grammar",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "It has been shown that the methods can be ported to other languages and treebanks  , including Cast3LB  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "can be ported to other languages and treebanks",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Cast3LB",
                "including",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "IBM constraints  , lexical word reordering model  , and inversion transduction grammar   constraints   belong to this type of approach.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM constraints",
                "belong to this type of approach",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "lexical word reordering model",
                "belong to this type of approach",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  proposed a new algorithm for parameter estimation as an alternate to CRF.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "alternate to CRF",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parameter estimation",
                "new",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The prior probability P0 is the prior distribution for the phrase probability which is estimated using the phrase normalized counts commonly used in conventional Phrasebased SMT systems, e.g.,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase normalized counts",
                "commonly used in conventional Phrasebased SMT systems",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "prior distribution",
                "estimated using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Models that support non-monotonic decoding generally include a distortion cost, such as|aibi11|where ai is the starting position of the foreign phrasefi andbi1 is the ending position of phrase fi1  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distortion cost",
                "a distortion cost",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "non-monotonic decoding",
                "supports",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "On the base of the chunk scheme proposed by Abney   and the BIO tagging system proposed in Ramshaw and Marcus , many machine learning techniques are used to deal with the problem.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunk scheme",
                "proposed by Abney",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "machine learning techniques",
                "are used",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "it is relatively easy to train a translation system to translate from English to Chinese, except that weneed to train aChinese language model from the Chinese monolingual data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation system",
                "train a translation system",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Chinese language model",
                "train from Chinese monolingual data",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Amazon Reviews: The dataset contains product reviews taken from Amazon.com from 4 product types: Kitchen, Books, DVDs, and Electronics  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "product reviews",
                "taken from Amazon.com",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "product types",
                "Kitchen, Books, DVDs, and Electronics",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.1 A Note on State-Splits Recent studies   suggest that category-splits help in enhancing the performance of treebank grammars, and a previous study on MH   outlines specific POS-tags splits that improve MH parsing accuracy.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "category-splits",
                "help in enhancing performance",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "POS-tags splits",
                "improve parsing accuracy",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For example, if we make a mean-field assumption, with respect to hidden structure and weights, the variationalalgorithmforapproximatelyinferringthe distribution over  and trees y resembles the traditional EM algorithm very closely  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "variational algorithm",
                "resembles the traditional EM algorithm very closely",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "algorithm",
                "approximately inferring the distribution over and trees y",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "GIZA++ consists of a set of statistical translation models of different complexity, namely the IBM ones  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "set of statistical translation models of different complexity",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "IBM ones",
                "different complexity",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "A few unsupervised metrics have been applied to automatic paraphrase identification and extraction  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unsupervised metrics",
                "applied to automatic paraphrase identification and extraction",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "automatic paraphrase identification and extraction",
                "unsupervised metrics have been applied to",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The feature templates in Ratnaparkhi   that were left out were the ones that look at the previous word, the word two positions before the current, and the word two positions after the current.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature templates",
                "look at the previous word, the word two positions before the current, and the word two positions after the current",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "feature templates",
                "were left out",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Translation results are given in terms of the automaticBLEUevaluation metric   as well as the TER metric  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEUevaluation metric",
                "automatic",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "TER metric",
                "",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Following Collins and Roark   we also use the early-update strategy, where an update happens whenever the goldstandard action-sequence falls off the beam, with the rest of the sequence neglected.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "early-update strategy",
                "happens whenever goldstandard action-sequence falls off the beam",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "rest of the sequence",
                "neglected",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "It has been shown repeatedly--e.g. , Briscoe and Carroll  , Charniak  , Collins  , Inui et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Briscoe and Carroll",
                "repeatedly shown",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Charniak",
                "repeatedly shown",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc. Semantic orientation classification is a task of determining positive or negative sentiment of words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "subjectivity",
                "helps separate opinions from fact",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "sentiment of words",
                "positive or negative",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "These transtbr rules are pairs of corresponding rooted substructures, where a substructure   is a connected set of arcs and nodes.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transtbr rules",
                "pairs of corresponding rooted substructures",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "substructure",
                "connected set of arcs and nodes",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "220  ; they can overlap.5 Additionally, since phrase features can be any function of words and alignments, we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase features",
                "consider phrase pairs with gaps",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase pairs",
                "permit features that consider",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "109 machine translation evaluation  ,paraphraserecognition  , and automatic grading  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine translation evaluation",
                "evaluation",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "automatic grading",
                "grading",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As a result, the problem of opinion mining has seen increasing attention over the last three years from   and many others.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "problem of opinion mining",
                "has seen increasing attention",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "many others",
                "from",
                "APPLICABILITY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "arowsky   used this method for word sense disambiguation",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "used for word sense disambiguation",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "method",
                "used by arowsky",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The Decision List   algorithm is described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Decision List algorithm",
                "described",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Decision List algorithm",
                "is described in",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A more recent bootstrapping approach is described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bootstrapping approach",
                "described",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approach",
                "more recent",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "In the concept extension part of our algorithm we adapt our concept acquisition framework   to suit diverse languages, including ones without explicit word segmentation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "adapt to suit diverse languages",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "concept acquisition framework",
                "to suit diverse languages",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "proposed a similarity-based model in which each word is generalized, not to its own specific class, but to a set of words which are most similar to it  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word",
                "most similar to it",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "model",
                "proposed",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Because of its central role in building machine translation systems and because of the complexity of the task, sub-sentential alignment of parallel corpora continues to be an active area of research  , and this implies a continuing demand for manually created or human-verified gold standard alignments for development and evaluation purposes.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parallel corpora",
                "sub-sentential alignment",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "gold standard alignments",
                "for development and evaluation purposes",
                "APPLICABILITY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "We compared our system to Pharaoh, a leading phrasal SMT decoder  , and our treelet system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pharaoh",
                "leading",
                "INNOVATION",
                "positive",
                0.7
            ],
            [
                "Pharaoh",
                "phrasal SMT decoder",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Such transformations are typically denoted as paraphrases in the literature, where a wealth of methods for their automatic acquisition were proposed  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "paraphrases",
                "denoted as",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In the following experiments, the NIST BLEU score is used as the evaluation metric  , which is reported as a percentage in the following sections.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NIST BLEU score",
                "used as the evaluation metric",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "evaluation metric",
                "reported as a percentage",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This paper proposes a method for building a bilingual lexicon through a pivot language by using phrase-based statistical machine translation    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "phrase-based statistical machine translation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "bilingual lexicon",
                "building",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2.3 Experiment The training set for these experiments was sections 01-21 of the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "sections 01-21",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "training set",
                "was",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This kind of synchronizer stands in contrast to more ad-hoc approaches  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "synchronizer",
                "ad-hoc approaches",
                "METHODOLOGY",
                "negative",
                0.7
            ],
            [
                "synchronizer",
                "stands in contrast",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Using our WSD model to constrain the translation candidates given to the decoder hurts translation quality, as measured by the automated BLEU metric  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSD model",
                "hurts translation quality",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "BLEU metric",
                "as measured by",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 Monolingual comparable corpus: Similar to the methods in  , we construct a corpus of comparable documents from a large corpus D of news articles.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "comparable documents",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "methods",
                "similar to",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Recently, some kinds of learning techniques have been applied to cumulatively acquire exemplars form large corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "learning techniques",
                "applied to cumulatively acquire exemplars",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "corpora",
                "large",
                "APPLICABILITY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Unlike Choueka  , Church and Hanks   identify as collocations both interrupted and uninterrupted sequences of words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Choueka",
                "identify as collocations",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Church and Hanks",
                "identify as collocations",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Previous attempts have used, for instance, the similarities between case frames  , anchor words  , and a web-based method .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "case frames",
                "similarities between",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "web-based method",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman   and Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "of Magerman and Collins",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "node",
                "as either a head, complement, or adjunct",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.2 Automatic evaluation metric Since the official evaluation criterion for WMT09 is human sentence ranking, we chose to minimize a linear combination of two common evaluation metrics, BLEU and TER  , during system development and tuning: TERBLEU 2 Although we are not aware of any work demonstrating that this combination of metrics correlates better than either individually in sentence ranking, Yaser Al-Onaizan   reports that it correlates well with the human evaluation metric HTER.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation metric",
                "minimize a linear combination of two common evaluation metrics",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "correlates well with the human evaluation metric HTER",
                "reports that",
                "PERFORMANCE",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "For instance, the resulting word graph can be used in the prediction engine of a CAT system  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word graph",
                "can be used in the prediction engine of a CAT system",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "prediction engine",
                "of a CAT system",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.2 Implementation of GIZA++ GIZA++ is an implementation of ML estimators for several statistical alignment models, including IBM Model 1 through 5  , HMM   and Model 6  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "implementation of ML estimators for several statistical alignment models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM Model 1 through 5, HMM, Model 6",
                "including",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Each linked fragment pair consists of a source-language side and a target-language side, similar to  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linked fragment pair",
                "similar to",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "source-language side",
                "target-language side",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Second, the significance of the K-S distance in case of the null hypothesis   can be calculated  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "K-S distance",
                "can be calculated",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "null hypothesis",
                "can be calculated",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ang and Lee   proposed to eliminate objective sentences before the sentiment classification of documents",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "proposed",
                "eliminate objective sentences",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentiment classification of documents",
                "before",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "We then train word alignment models   using 6 Model-1 iterations and 6 HMM iterations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model-1 iterations",
                "6 iterations",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "HMM iterations",
                "6 iterations",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Finally, the fourth and fifth feature functions corresponded to two lexicon models based on IBM Model 1 lexical parameters p   .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature functions",
                "corresponded to two lexicon models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM Model 1 lexical parameters",
                "based on",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "e used Collins   statistical parser trained on examples from the Penn Treebank to generate parses of the same format for the sentences in our data",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins statistical parser",
                "trained on examples from the Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parses of the same format",
                "for the sentences in our data",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, since we are interested in the word counts that correlate to w, we adopt the concept of the translation model proposed by Brown et al  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "proposed by Brown et al",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word counts",
                "correlate to w",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "These 30 questions are determined by growing a classification tree on the word vocabulary as described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Dependency models have recently gained considerable interest in many NLP applications, including machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency models",
                "gained considerable interest",
                "INNOVATION",
                "positive",
                0.75
            ],
            [
                "NLP applications",
                "including machine translation",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "More recently, other approaches have investigated the use of machine learning to nd patterns in documents  and the utility of parameterized modules so as to deal with dierent genres or corpora .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning",
                "use of machine learning",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parameterized modules",
                "utility of parameterized modules",
                "METHODOLOGY",
                "neutral",
                0.75
            ]
        ]
    },
    {
        "text": "is the previous BIO tag, S is the target sentence, and fj and lj are feature functions and parameters of a log-linear model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BIO tag",
                "previous",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "log-linear model",
                "fj and lj are feature functions and parameters",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Although bi-alignments are known to exhibit high precision  , in the face of sparse annotations we use unidirectional alignments as a fallback, as has been proposed in the context of phrase-based machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bi-alignments",
                "exhibit high precision",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "unidirectional alignments",
                "fallback",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We implemented these models within an maximum entropy framework  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "framework",
                "maximum entropy",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "models",
                "implemented within",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "6 The Experiments To investigate the e ects of lookahead on our family of deterministic parsers, we ran empirical experiments on the standard the Penn Treebank   datasets.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank datasets",
                "standard",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "parsers",
                "deterministic",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The computation mechanism of GP and LP bears a resemblance to the EM algorithm , which iteratively computes maximum likelihood estimates from incomplete data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mechanism",
                "bears a resemblance to the EM algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "iteratively computes maximum likelihood estimates",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We only describe these models briefly since full details are presented elsewhere .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "briefly described",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "details",
                "presented elsewhere",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For transfer-learning baseline, we implement traditional SCL model    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SCL model",
                "traditional",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "SCL model",
                "traditional",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.2 Learning Algorithm For learning coreference decisions, we used a Maximum Entropy   model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy model",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Maximum Entropy",
                "model",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The group of collocations and compounds should be delimited using statistical approaches, such as Xtract   or LocalMax  , so that only the most relevantthose of higher frequency are included in the database.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical approaches",
                "Xtract or LocalMax",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "collocations and compounds",
                "most relevant",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "ROUGE   is a set of recall-based criteria that is mainly used for evaluating summarization tasks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Both left-corner strategy   and head-corner strategy   were employed in incremental parsing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "left-corner strategy",
                "and head-corner strategy",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "incremental parsing",
                "was employed",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The results are comparable to other results reported using the Inside/Outside method   (see Table 7.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "results",
                "comparable to other results",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "results",
                "reported using Inside/Outside method",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In the domain adaptation track, participants were provided with English training data from the Wall Street Journal portion of the Penn Treebank   converted to dependencies   to train parsers to be evaluated on material in the biological   and chemical   domains  , and optionally on text from the CHILDES database  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "converted to dependencies",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "material in the biological and chemical domains",
                "to be evaluated on",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "6 Comparison With Previous Work The two parsers which have previously reported the best accuracies on the Penn Treebank Wall St. Journal are the bigram parser described in   and the SPATTER parser described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsers",
                "reported best accuracies",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "bigram parser",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Second, we follow Snow et al.s work   on taxonomy induction in incorporating transitive closure constraints in our probability calculations, as explained below.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probability calculations",
                "in incorporating transitive closure constraints",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "work on taxonomy induction",
                "as explained below",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We observe that AER is loosely correlated to BLEU   though the relation is weak, as observed earlier by Fraser and Marcu  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "AER",
                "loosely correlated",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "relation",
                "weak",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "According to Carletta  , K measures pairwise agreement among a set of coders making category judgments, correcting for expected chance agreement as follows: KP  -P  1 -P  where P  is the proportion of times that the coders agree and P  is the proportion of times that they would be expected to agree by chance.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "P",
                "proportion of times that the coders agree",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "P",
                "proportion of times that they would be expected to agree by chance",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 System Description 2.1 Data Representation In this paper, we change the representation of the original data as follows: Bracketed representation of roles is converted into IOB2 representation   Word tokens are collapsed into base phrase   tokens.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bracketed representation",
                "converted into IOB2 representation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word tokens",
                "collapsed into base phrase tokens",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The automatic metrics that were evaluated in this years shared task were the following:  Bleu  Bleu remains the de facto standard in machine translation evaluation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bleu",
                "de facto standard",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "Bleu",
                "remains",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Recent work by Koehn and Hoang   pro514 poses factored translation models that combine feature functions to handle syntactic, morphological, and other linguistic information in a log-linear model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature functions",
                "combine",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "factored translation models",
                "log-linear model",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "While it was initially believed that lexicalization of PCFG parsers   is crucial for obtaining good parsing results, Gildea   demonstrated that the lexicalized Model-1 parser of Collins   does not benefit from bilexical information when tested on a new text domain, and only marginally benefits from such information when tested on the same text domain as the training corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model-1 parser of Collins",
                "does not benefit from bilexical information",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Model-1 parser of Collins",
                "marginally benefits from bilexical information",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The Brill tagger comes with an English default version also trained on general-purpose language corpora like the PENN TREEBANK  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "The Brill tagger",
                "comes with an English default version",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "general-purpose language corpora",
                "like the PENN TREEBANK",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  present a probabilistic model for pronoun resolution trained on a small subset of the Penn Treebank Wall Street Journal corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "probabilistic",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "subset of the Penn Treebank Wall Street Journal corpus",
                "small",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "he reader is referred to Schmid   and Collins   for details",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Schmid and Collins",
                "details",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The senses are: 1 material from cellulose 2 report 3 publication 4 medium for writing 5 scientific 6 publishing firm 7 physical object inventory is suitable for which application, other than cross-lingual applications where the inventory can be determined from parallel data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "senses",
                "material from cellulose",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "physical object inventory",
                "suitable for",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This approach, however, does not have a theoretical guarantee on optimality unless certain nontrivial conditions are satisfied  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "does not have a theoretical guarantee on optimality",
                "INNOVATION",
                "negative",
                0.7
            ],
            [
                "conditions",
                "are satisfied",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Beside simple cooccurrence counts within sliding windows, other SoA measures include functions based on TF/IDF  , mutual information    , conditional probabilities  , chi-square test, and the loglikelihood ratio  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SoA measures",
                "based on TF/IDF, mutual information, conditional probabilities, chi-square test, and the loglikelihood ratio",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "functions",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We propose a corpus-based method   which generates Noun Classifier Associations   to overcome the problems in classifier assignment and semantic construction of noun phrase.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "generates Noun Classifier Associations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "problems in classifier assignment and semantic construction of noun phrase",
                "overcome",
                "LIMITATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The subsequent construction of translation table was done in exactly the same way as explained 4 in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "construction",
                "done in exactly the same way as explained 4",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "table",
                "same way as explained 4",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The MLFs use reification to achieve flat expressions, very much in the line of Davidson  , Hobbs  , and Copestake et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MLFs",
                "achieve flat expressions",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "Davidson, Hobbs, and Copestake et al.",
                "in the line of",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "While previous researchers have used agglomerative nesting clustering  , Futrelle and Gauch  ), comparisons with our work are difficult to draw, due to their use of the 1,000 commonest words from their respective corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "use of 1,000 commonest words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "comparisons",
                "difficult to draw",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "It would be necessary to apply either semiautomatic or automatic methods such as those in   to extend FrameNet coverage for final application to machine translation tasks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "semiautomatic or automatic methods",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "those in FrameNet",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2.3 ITG Constraints The Inversion Transduction Grammar    , a derivative of the Syntax Directed Transduction Grammars  , constrains the possible permutations of the input string by defining rewrite rules that indicate permutations of the string.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Inversion Transduction Grammar",
                "defines rewrite rules",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "rewrite rules",
                "indicate permutations of the string",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We measured inter-annotator agreement with the Kappa statistic   using the 1,391 items that two annotators scored in common.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa statistic",
                "using",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "1,391 items",
                "in common",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Afterwards, we select and remove a subset of highly informative sentences from U, and add those sentences together with their human-provided translations to L. This process is continued iteratively until a certain level of translation quality is met    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentences",
                "informative",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "translation quality",
                "certain level of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As Carletta   notes, many tasks in computational linguistics are simply more difficult than the content analysis classifications addressed by Krippendorff, and according to Fleiss  , kappa values between .4 and .75 indicate fair to good agreement anyhow.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tasks in computational linguistics",
                "more difficult",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "kappa values",
                "indicate fair to good agreement",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Parse each sentence using a Treebank-trained parser  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Treebank-trained parser",
                "using a Treebank-trained parser",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "parser",
                "parse each sentence",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One other work that investigates the use of a limited lexicon is  , which develops a prototype-drive approach to propagate the categorical property using distributional similarity features; using only three exemplars of each tag, they achieve a tagging accuracy of 80.5% using a somewhat larger dataset but also the full Penn tagset, which is much larger.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prototype-drive approach",
                "propagate the categorical property using distributional similarity features",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tagging accuracy",
                "80.5%",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Motivation Most of the noisy-channel-based models used in statistical machine translation     are conditional probability models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "noisy-channel-based models",
                "are conditional probability models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "most",
                "statistical machine translation",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For our baseline, we have selected the method based on binomial loglikelihood ratio test   described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "based on binomial loglikelihood ratio test",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "method",
                "described in  ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, the approach raises two major challenges: 7In practice, MERT training   will be used to train relative weights for the different model components.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MERT training",
                "will be used to train relative weights",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "different model components",
                "have relative weights",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The method thereby retains the full set of lexical entries of phrase-based systems  ).1  The model allows a straightforward integration of lexicalized syntactic language modelsfor example the models of  in addition to a surface language model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicalized syntactic language models",
                "allows straightforward integration",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "surface language model",
                "in addition to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.1 Variational Bayes Beal   and Johnson   describe variational Bayes for hidden Markov model in detail, which can be directly applied to our bilingual model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "variational Bayes",
                "describe in detail",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "bilingual model",
                "can be directly applied",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The Stanford parser is representative of a large number of PTB parsers, exemplified by Collins   and Charniak  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stanford parser",
                "representative",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "PTB parsers",
                "exemplified by Collins and Charniak",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In the usual case considered by Dunning   and discussed by Manning and Sch utze  , the right-hand side of the equation is larger than the left-hand side.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "right-hand side of the equation",
                "larger than",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "left-hand side of the equation",
                "smaller than",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We can stipulate the time line to be linearly ordered   nor in approaches using branching futures  ), and we can stipulate it to be dense  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "time line",
                "linearly ordered",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "time line",
                "dense",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The future score is based on the source-language words that are still to be translatedthis can be directly inferred from the items bit-stringthis is similar to the use of future scores in Pharoah  , and in fact we use Pharoahs future scores in our model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "future scores",
                "similar to Pharoah's future scores",
                "PERFORMANCE",
                "positive",
                0.75
            ],
            [
                "Pharoah's future scores",
                "use in our model",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The IBM model 1   is used to find an initial estimate of the translation probabilities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM model 1",
                "initial estimate of translation probabilities",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation probabilities",
                "find",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The table also shows Cohen's to, an agreement measure that corrects for chance agreement  ; the most important t value in the table is the value of 0.7 for the two human judges, which can be interpreted as sufficiently high to indicate that the task is reasonably well defined.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Cohen's to",
                "corrects for chance agreement",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "task",
                "reasonably well defined",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "  reports a success rate of 96% disambiguating twelve words with two clear sense distinctions each one.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "success rate",
                "96%",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "twelve words with two clear sense distinctions each one",
                "clear sense distinctions",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1.2 Recent work A few publications, so far, deal with POS-tagging of Northern Sotho; most prominently, de Schryver and de Pauw   have presented the MaxTag method, a tagger based on Maximum Entropy 38 Learning   as implemented in the machine learning package Maxent  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MaxTag method",
                "based on Maximum Entropy Learning",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Maxent",
                "machine learning package",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "  makes a similar point, noting that for reviews, \\the whole is not necessarily the sum of the parts\"\").",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parts",
                "the whole is not necessarily the sum of",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "reviews",
                "not necessarily the sum of the parts",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For classi cation, we use a maximum entropy model  , from the logistic regression package in Weka  , with all default parameter settings.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy model",
                "from the logistic regression package in Weka",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "default parameter settings",
                "with all default",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1153 While much research   has explored how to reconcile pairwise decisions to form coherent clusters, we simply take the transitive closure of our pairwise decision   and Bengston and Roth  ) which can and does cause system errors.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pairwise decisions",
                "form coherent clusters",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "transitive closure",
                "can and does cause system errors",
                "METHODOLOGY",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "Many existing systems for statistical machine translation   implement models presented by Brown, Della Pietra, Della Pietra, and Mercer  : The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "presented by Brown, Della Pietra, Della Pietra, and Mercer",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignments",
                "assign target word positions to each source word position",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "On the other hand, the thesaurus-based method of Yarowsky   may suffer from loss of information   as well as data sparseness   are based on the WordNet taxonomy while classes of Brown et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "thesaurus-based method",
                "may suffer from loss of information",
                "LIMITATION",
                "neutral",
                0.8
            ],
            [
                "WordNet taxonomy",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This approach builds a subjectivity-annotated corpus for the target language through projection, and then trains a statistical classifier on the resulting corpus  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "subjectivity-annotated corpus",
                "through projection",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "statistical classifier",
                "trained on the resulting corpus",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A limitation of Church's method, and therefore also of Dagan, Church, and Gale's method, is that orthographic cognates exist only among languages with similar alphabets  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "orthographic cognates exist only among languages with similar alphabets",
                "LIMITATION",
                "neutral",
                0.9
            ],
            [
                "method",
                "Church's method",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In comparison, most corpus-based algorithms employ substantially larger corpora  , 2.5 million words  , 6 million words  , 13 million words  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "larger",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "corpora",
                "substantially",
                "METHODOLOGY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "The model scaling factors are optimized on the development corpus with respect to mWER similar to  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model scaling factors",
                "are optimized",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "model scaling factors",
                "with respect to mWER",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Graphically speaking, parsing amounts to identifying rectangular crosslinguistic constituents  by assembling smaller rectangles that will together cover the full string spans in both dimensions  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rectangular crosslinguistic constituents",
                "assembling smaller rectangles",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parsing",
                "identifying",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 Bi-Stream HMMs for Transliteration Standard IBM translation models   can be used to obtain letter-to-letter translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMMs",
                "IBM translation models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translations",
                "letter-to-letter",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For comparison, Haghighi and Klein   report an unsupervised baseline of 41.3%, and a best result of 80.5% from using hand-labeled prototypes and distributional similarity.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseline",
                "41.3%",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "hand-labeled prototypes and distributional similarity",
                "best result",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "5 Related Work Although there have been many studies on collocation extraction and mining using only statistical approaches  , there has been much less work on collocation acquisition which takes into account the linguistic properties typically associated with collocations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical approaches",
                "only",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "linguistic properties",
                "typically associated with collocations",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "aume III and Marcu   propose a model that encodes how likely it is that different sized spans of text are skipped to reach words and phrases to recycle",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "encodes how likely it is that different sized spans of text are skipped to reach words and phrases to recycle",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "spans of text",
                "different sized",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Many corpus based methods have been proposed to deal with the sense disambiguation problem when given de nition for each possible sense of a target word or a tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation  , and semi-supervised sense disambiguation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "supervised sense disambiguation",
                "and semi-supervised sense disambiguation",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In contrast, approaches to WSD attempt to take advantage of many different sources of information  ); it seems possible to obtain benefit from sources ranging from local collocational clues   to membership in semantically or topically related word classes   to consistency of word usages within a discourse  ; and disambignation seems highly lexically sensitive, in effect requiring specialized disamhignators for each polysemous word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sources of information",
                "many different sources of information",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "disambignation",
                "highly lexically sensitive",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This is the same separation of arguments and adjuncts as that employed by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "arguments and adjuncts",
                "employed by  ",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "separation",
                "same as that",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The diversity function rewards summaries that cover many important aspects and plays the redundancy reducing role that is common in most extractive summarization frameworks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "diversity function",
                "rewards summaries that cover many important aspects",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "redundancy reducing role",
                "common in most extractive summarization frameworks",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "5.3 Comparison with SS-CRF-MER When we consider semi-supervised SOL methods, SS-CRF-MER   is the most competitive with HySOL, since both methods are defined based on CRFs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SS-CRF-MER",
                "is the most competitive",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "SS-CRF-MER",
                "defined based on CRFs",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The results of these studies have important applications in lexicography, to detect lexicosyntactic regularities  , such as, for example~ support verbs   prepositional verbs   idioms, semantic relations   and fixed expressions  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicosyntactic regularities",
                "detect",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "semantic relations",
                "and",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Word-based features are used as well, e.g. feature a75 a11a39a99a78a99a18a11 captures word-to-word translation de4On our test set,   reports a BLEU score of a100a63a101a63a102a43a103 and   reports a BLEU score of a104a89a103a63a102 a105 . pendencies similar to the use of Model a98 probabilities in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Word-based features",
                "captures word-to-word translation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU score",
                "reports",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We have chosen to work with a corpus with parse information, the Wall Street Journal WSJ part of the Penn Treebank II corpus  , and to extract chunk information from the parse trees in this corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "Wall Street Journal WSJ part of the Penn Treebank II corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parse trees",
                "extract chunk information from",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Sentiment classification at the document level investigates ways to classify each evaluative document   as positive or negative  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Sentiment classification",
                "evaluative document",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "document level",
                "positive or negative",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This is concordant with the usage in the maximum entropy literature  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "literature",
                "usage in the maximum entropy literature",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "usage",
                "concordant with",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However there has recently been much work drawing connections between the two methods  ; in this section we review this work.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "drawing connections",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "work",
                "review",
                "INNOVATION",
                "neutral",
                0.75
            ]
        ]
    },
    {
        "text": "However, another approach is to train a separate out-of-domain parser, and use this to generate additional features on the supervised and unsupervised in-domain data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "generate additional features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "in-domain data",
                "unsupervised",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Standard SMT alignment models   are used to align letter-pairs within named entity pairs for transliteration.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Standard SMT alignment models",
                "are used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "named entity pairs",
                "for transliteration",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 Related work Cutting introduced grouping of words into equiva.lence classes based on the set of possible tags to reduce the number of the parameters   . Schmid used tile equivaleuce classes for smoothing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "reduce the number of the",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "equivaleuce classes",
                "for smoothing",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It has been implemented in the TACITUS System   and has been applied to several varieties of text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TACITUS System",
                "has been implemented in",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "varieties of text",
                "has been applied to",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 Model As an extension to commonly used lexical word pair probabilities p  as introduced in  , we define our model to operate on word triplets.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Bilingual configurations that condition on tprime,wprime   are incorporated into the generative process as in Smith and Eisner  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bilingual configurations",
                "are incorporated into the generative process",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Smith and Eisner",
                "as in",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Seen from Table 2, our result about SCL is in accord with that in   on the whole.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SCL",
                "in accord",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "result",
                "in accord with that in on the whole",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We can confirm that changing the dimensionality parameter h has rather little effect  , which is in line with previous findings  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dimensionality parameter h",
                "rather little effect",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "findings",
                "in line with previous",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We adopted the stop condition suggested in   the maximization of the likelihood on a cross-validation set of samples which is unseen at the parameter estimation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameter estimation",
                "unseen at the parameter estimation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "stop condition",
                "suggested in the maximization of the likelihood",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Once the set of features functions are selected, algorithm such as improved iterative scaling   or sequential conditional generalized iterative scaling   can be used to find the optimal parameter values of fkg and fig.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "improved iterative scaling or sequential conditional generalized iterative scaling",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parameter values",
                "optimal",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Note that the need to consider segmentation and alignment at the same time is also mentioned in  , and related issues are reported in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "segmentation and alignment",
                "need to consider",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "related issues",
                "reported in",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We use the GIZA toolkit  , a suffix-array architecture  , the SRILM toolkit  , and minimum error rate training   to obtain wordalignments, a translation model, language models, and the optimal weights for combining these models, respectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA toolkit",
                "suffix-array architecture",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "SRILM toolkit",
                "minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4.2 Experiments on SRL dataset We used two different corpora: PropBank along with Penn Treebank 2   and FrameNet.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "PropBank along with Penn Treebank 2 and FrameNet",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "dataset",
                "SRL",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The Maximum Entropy model   is a conditional model that assigns a probability to every possible parse  for a given sentence s. The model consists of a set of m feature functions fj  that describe properties of parses, together with their associated weights j. The denominator is a normalization term where Y   is the set of parses with yield s: p  = exp )summationtext yY   exp ))   The parameters   j can be estimated efficiently by maximizing the regularized conditional likelihood of a training corpus  :  = argmax  logL   summationtextm j=1  2j 22   where L  is the likelihood of the training data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy model",
                "conditional model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parameters",
                "estimated efficiently",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  BLEU-4   is used as the evaluation metric.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU-4",
                "is used as the evaluation metric",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "evaluation metric",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ne example of the 450 latter problem is the following: in   the nature of a syntactic link between two associated words is detected a posteriori",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic link",
                "is detected a posteriori",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "nature",
                "of a syntactic link between two associated words",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It ewduato.s the pairwise agreement mnong a set; of coders making category.iudgment, correcting tbr expected chance agreement  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "coders",
                "making category judgments",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "expected chance agreement",
                "correcting for",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "he algorithm employs the OpenNLP MaxEnt implementation of the maximum entropy classification algorithm   to develop word sense recognition signatures for each lemma which predicts the most likely sense for the lemma according to the context in which the lemma occurs",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "OpenNLP MaxEnt implementation",
                "maximum entropy classification algorithm",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word sense recognition signatures",
                "predicts the most likely sense for the lemma",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Many studies focus on rare words  ; butterflies are more interesting than moths.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rare words",
                "are more interesting",
                "INNOVATION",
                "positive",
                0.75
            ],
            [
                "moths",
                "are less interesting",
                "INNOVATION",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "Specifically, we will consider a system which was developed for the ACE   task 3 and includes the following stages: name structure parsing, coreference, semantic relation extraction and event extraction  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "developed for the ACE task",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "event extraction",
                "is included",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We analyzed a set of articles and identified six major operations that can be used for editing the extracted sentences, including removing extraneous phrases from an extracted sentence, combining a reduced sentence with other sentences, syntactic transformation, substituting phrases in an extracted sentence with their paraphrases, substituting phrases with more general or specific descriptions, and reordering the extracted sentences  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "operations",
                "including removing extraneous phrases from an extracted sentence",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "operations",
                "substituting phrases with more general or specific descriptions",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "REALM uses an HMM trained on a large corpus to help determine whether the arguments of a candidate relation are of the appropriate type  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMM",
                "trained on a large corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "arguments of a candidate relation",
                "are of the appropriate type",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We might find better suited metrics, such as METEOR  , which is oriented towards word selection8.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "METEOR",
                "oriented towards word selection",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "metrics",
                "better suited",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "4 Related work Algorithms for retrieving collocations has been described    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "has been described",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithms for retrieving collocations",
                "has been described",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our results are similar to those for conventional phrase-based models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "results",
                "similar to those for conventional phrase-based models",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "After parsing the corpus  , we artificially introduced verb form errors into these sentences, and observed the resulting disturbances to the parse trees.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "artificially introduced verb form errors",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parse trees",
                "resulting disturbances",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The four models we compare are a maximum a posteriori   method and three discriminative training methods, namely the boosting algorithm  , the average perceptron   and the minimum sample risk method  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "maximum a posteriori method and three discriminative training methods",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "boosting algorithm",
                "discriminative training methods",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": ".1 Linear Models for NLP We follow the framework outlined in Collins  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Linear Models",
                "for NLP",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "framework",
                "outlined in Collins",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In Turneys work, the co-occurrence is considered as the appearance in the same window  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "co-occurrence",
                "appearance in the same window",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "window",
                "same",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "3 Domain Adaptation Following  , we present an application of structural correspondence learning   to non-projective dependency parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Domain Adaptation",
                "Following",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "structural correspondence learning",
                "to non-projective dependency parsing",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For nonprojective parsing, the analogy to the inside algorithm is the O  matrix-tree algorithm, which is dominated asymptotically by a matrix determinant  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "O matrix-tree algorithm",
                "is dominated asymptotically",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "matrix determinant",
                "asymptotically",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Even if the idea of using Wikipedia links for disambiguation is not novel  , it is applied for the first time to FrameNet lexical units, considering a frame as a sense definition.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "idea of using Wikipedia links for disambiguation",
                "is not novel",
                "INNOVATION",
                "negative",
                0.7
            ],
            [
                "application to FrameNet lexical units",
                "is applied for the first time",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Many strategies have been proposed to integrate morphology information in SMT, including factored translation models  , adding a translation dictionary containing inflected forms to the training data  , entirely replacing surface forms by representations built on lemmas and POS tags  , morphemes learned in an unsupervised manner  , and using Porter stems and even 4-letter prefixes for word alignment  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "factored translation models",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Porter stems and even 4-letter prefixes",
                "for word alignment",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We will do this by examining how humans perform on summary extraction and evaluating the reliability of their performance, using the kappa statistic, a metric standardly used in the behavioral sciences  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa statistic",
                "standardly used in the behavioral sciences",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "reliability of their performance",
                "evaluating the reliability",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For the MER training  , Koehns MER trainer   is modified for our system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Koehns MER trainer",
                "modified for our system",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "MER trainer",
                "modified for our system",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Kanayama and Nasukawa used both intraand inter-sentential co-occurrence to learn polarity of words and phrases  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "co-occurrence",
                "to learn polarity of words and phrases",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Kanayama and Nasukawa",
                "used both intraand inter-sentential",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A more optimistic view can be found in  ; they argue that a near-100% interjudge agreement is possible, provided the part-of-speech annotation is done carefully by experts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "part-of-speech annotation",
                "done carefully by experts",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "near-100% interjudge agreement",
                "possible",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Examples have been class-based D2-gram models  , smoothing techniques for structural disambiguation   and word sense disambiguation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "D2-gram models",
                "class-based",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "smoothing techniques",
                "for structural disambiguation and word sense disambiguation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Marcu and Echihabi   use a pattern-based approach in mining instances of RSRs such as Contrast and Elaboration from large, unannotated corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pattern-based approach",
                "in mining instances of RSRs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "large, unannotated corpora",
                "such as Contrast and Elaboration",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The output by each approach will be evaluated using benchmark data sets of Bakeoff-32  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "benchmark data sets",
                "Bakeoff-32",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Six features from   were used as baseline features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "baseline features",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "features",
                "used as baseline",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 GM Representation of IBM MT Models In this section we present a GM representation for IBM model 3   in fig.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM model 3",
                "GM representation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IBM MT Models",
                "GM representation",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Both taggers used the Penn Treebank tagset and were trained on the Wall Street Journal corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "taggers",
                "Penn Treebank tagset",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpus",
                "Wall Street Journal corpus",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence  , or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bitext space",
                "more general representations of bitext correspondence",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "matching predicate",
                "extrinsic to the model",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The natural next step in sentence alignment is to account for word ordering in the translation model, e.g., the models described in   could be used.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "described in",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "translation model",
                "account for word ordering",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "translation including the joint probability phrasebased model   and a variant on the alignment template approach  , and contrast them to the performance of the word-based IBM Model 4  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrasebased model",
                "phrasebased model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IBM Model 4",
                "word-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Despite the above differences, since the theorems of convergence and their proof   are only dependent on the feature vectors, and not on the source of the feature definitions, the perceptron algorithm is applicable to the training of our CWS model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron algorithm",
                "is applicable",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "feature vectors",
                "are only dependent",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In such tasks, feature calculation is also very expensive in terms of time required; huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability p  and reverse translation probability p   .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature calculation",
                "very expensive",
                "METHODOLOGY",
                "negative",
                0.7
            ],
            [
                "huge sets of extracted rules",
                "must be sorted in two directions",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In addition to raw inter-tagger agreement, the kappa statistic, which removes from the agreement rate the amount of agreement that is expected by chance , was also determined.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "3The usefulness of position varies significantly in different genres  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "position",
                "varies significantly",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "usefulness",
                "varies significantly",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Although the training algorithm can handle realvalued features as used in   the current paper intentionally excludes them.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training algorithm",
                "handle realvalued features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "excludes them",
                "METHODOLOGY",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "Since there is no practical way of determining the classification a0 which maximizes this quantity for a given corpus,   use a greedy algorithm which proceeds from the initial classification, performing the merge which results in the least loss in mutual information at each stage.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classification",
                "which maximizes this quantity",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "results in the least loss in mutual information at each stage",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Recall that the log likelihood of our model is:  d parenleftBigg Lorig  i  2 2 2d parenrightBigg  i  2 2 2 We now introduce a new variable d = d , and plug it into the equation for log likelihood:  d parenleftBigg Lorig  i  2 2 2d parenrightBigg  i  2 2 2 The result is the model of  , where the d are the domain-specific feature weights, and d are the domain-independent feature weights.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "domain-specific feature weights",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "model",
                "domain-independent feature weights",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "While early machine learning approaches for the task relied on local, discriminative classifiers  , more recent approaches use joint and/or global models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "relied on local, discriminative classifiers",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "approaches",
                "use joint and/or global models",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This is contrastive to the one dimensional models used by Collinss perceptronbased sequence method   which our algorithms are based upon, and by the linear-chain CRFs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collinss perceptronbased sequence method",
                "used by",
                "METHODOLOGY",
                "neutral",
                0.5
            ],
            [
                "one dimensional models",
                "used by",
                "METHODOLOGY",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "It is also possible to train statistical models using unlabeled data with the expectation maximization algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical models",
                "using unlabeled data with the expectation maximization algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "expectation maximization",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Another WSD approach incorporating context-dependent phrasal translation lexicons is given in   and has been evaluated on several translation tasks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSD approach",
                "incorporating context-dependent phrasal translation lexicons",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation tasks",
                "has been evaluated on",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Hobbs, Jerry   \"\"Ontological Promiscuity\"\", Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics, Chicago, Illinois, pp.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hobbs",
                "Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Annual Meeting",
                "23rd",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3 Experiments We tested our methods experimentally on the English Penn Treebank   and on the Czech Prague Dependency Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "experimentally tested on English Penn Treebank and Czech Prague Dependency Treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "experimentally tested",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Thus, we are lead to an 'ontologically promiscuous' semantics  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantics",
                "'ontologically promiscuous'",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "semantics",
                "lead to",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "As for parser, we train three off-shelf maximum-entropy parsers   using the Arabic, Chinese and English Penn treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum-entropy parsers",
                "off-shelf",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The earliest work in this direction are those of  ,  ,  ,  ,   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "earliest",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "work",
                "in this direction",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Furthermore, it is not possible to apply the powerful \"\"one sense per discourse\"\" property   because there is no discourse in dictionaries.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "discourse",
                "there is no discourse in dictionaries",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "property",
                "one sense per discourse",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The other 5 have been suggested for Dutch by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "5",
                "suggested",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Dutch",
                "by ",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Block Orientation Bigrams This section describes a phrase-based model for SMT similar to the models presented in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based model",
                "similar to the models presented",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "models presented",
                "in  ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.2 Conversion to Dependencies 3.2.1 Syntactic Dependencies There exists no large-scale dependency treebank for English, and we thus had to construct a dependency-annotated corpus automatically from the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency treebank",
                "no large-scale dependency treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank",
                "dependency-annotated corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  Och   provides evidence that  should be chosen by optimizing an objective function basd on the evaluation metric of interest, rather than likelihood.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "objective function",
                "based on the evaluation metric of interest",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "likelihood",
                "should not be chosen",
                "PERFORMANCE",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "The features we used are as follows:  word posterior probability  ;  3, 4-gram target language model;  word length penalty;  Null word length penalty; Also, we use MERT   to tune the weights of confusion network.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word posterior probability",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word length penalty",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Besides being used in SMT, it is also used in translation lexicon building  , transfer rule learning  , example-based machine translation  , etc. In previous alignment methods, some researches modeled the alignments as hidden parameters in a statistical translation model   or directly modeled them given the sentence pairs  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment methods",
                "modeled the alignments as hidden parameters",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "translation lexicon building",
                "etc.",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  In this example, we can see that after compression the lead sentence reads 156 more like a headline.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "The first of these nonstructural problems with Model 1, as standardly trained, is that rare words in the source language tend to act as garbage collectors  , aligning to too many words in the target language.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rare words in the source language",
                "act as garbage collectors",
                "METHODOLOGY",
                "neutral",
                0.7
            ],
            [
                "rare words in the source language",
                "aligning to too many words in the target language",
                "METHODOLOGY",
                "neutral",
                0.75
            ]
        ]
    },
    {
        "text": "However, due to the computational issues with the voted perceptron, the averaged perceptron algorithm   is used instead.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "voted perceptron",
                "computational issues",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "averaged perceptron algorithm",
                "used instead",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Unlabeled dependencies can be readily obtained by processing constituent trees, such as those in the Penn Treebank  , with a set of rules to determine the lexical heads of constituents.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "such as those in the Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "set of rules",
                "to determine the lexical heads of constituents",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Smadja employsthez-scoreinconjunction with several heuristics andextractspredicativecollocations, 1E.g.,(Frantziet al. ,2000;Pearce,2001;Goldmanet al. , 2001;ZaiuInkpenandHirst,2002;Dias,2003;Seretanetal.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "z-score",
                "employs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "heuristics",
                "and extract predicative collocations",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The algorithm to acquire the lexicon, implemented in the ARIOSTQLEX system, has been extensively described in \\ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ARIOSTQLEX system",
                "implemented",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "has been extensively described",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 Related Work There has not been much previous work on graphical models for full parsing, although recently several latent variable models for parsing have been proposed  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "not much previous",
                "PERFORMANCE",
                "neutral",
                0.5
            ],
            [
                "latent variable models",
                "proposed",
                "INNOVATION",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "First, for each verb occurrence subjects and objects were extracted from a parsed corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "verb occurrence",
                "were extracted",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parsed corpus",
                "parsed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Recently, some work has been done on corpusbased paraphrase extraction  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "  To reduce the inference time, following  , we collapsed the 45 different POS labels contained in the original data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS labels",
                "contained in the original data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "inference time",
                "reduce the",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In order increase the likelihood that 909 only true paraphrases were considered as phraselevel alternations for an example, extracted sentences were clustered using complete-link clustering using a technique proposed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "extracted sentences",
                "clustered using complete-link clustering",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "complete-link clustering",
                "proposed in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Most previous work on paraphrase has focused on high quality rather than coverage  , but generating artificial references for MT parameter tuning in our setting has two unique properties compared to other paraphrase applications.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "previous work",
                "focused on high quality rather than coverage",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "paraphrase applications",
                "unique properties",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "A similar approach was taken in   where an unknown word was guessed given the probabilities for an unknown word to be of a particular POS, its capitalisation feature and its ending.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unknown word",
                "guess given probabilities",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probabilities",
                "for an unknown word to be of a particular POS",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our next steps will be to take a closer look at the following work: clustering of similar words  , topic signatures   and Kilgariffs sketch engine  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kilgariffs sketch engine",
                "Kilgariffs sketch engine",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "topic signatures",
                "topic signatures",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "xternal information such as the discourse or domain dependency of each word sense   is expected to lead to system improvement",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "expected to lead to system improvement",
                "INNOVATION",
                "positive",
                0.85
            ],
            [
                "domain dependency",
                "is expected to",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In fact, many studies that try to exploit Wikipedia as a knowledge source have recently emerged  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "studies",
                "recently emerged",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Wikipedia",
                "as a knowledge source",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A remedy is to aggressively limit the feature space, e.g. to syntactic labels or a small fraction of the bi-lingual features available, as in  , but that reduces the benefit of lexical features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature space",
                "aggressively limit",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "lexical features",
                "reduces the benefit",
                "PERFORMANCE",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "Given sentence-aligned bi-lingual training data, we first use GIZA++   to generate word level alignment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "generate word level alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "GIZA++",
                "generate",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A large database of human judgments might also be useful as an objective function for minimum error rate training   or in other system development tasks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "database",
                "useful",
                "METHODOLOGY",
                "positive",
                0.75
            ],
            [
                "system development tasks",
                "other",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Finally, methods in the literature more focused on a specific disambiguation task include statistical methods for the attachment of hyponyms under the most likely hypernym in the WordNet taxonomy  , structural approaches based on semantic clusters and distance metrics  , supervised machine learning methods for the disambiguation of meronymy relations  , etc. 6 Conclusions In this paper we presented a novel approach to disambiguate the glosses of computational lexicons and machine-readable dictionaries, with the aim of alleviating the knowledge acquisition bottleneck.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "statistical methods for the attachment of hyponyms",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "approaches",
                "semantic clusters and distance metrics",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "unning   reports that we should not rely on the assumption of a normal distribution when performing statistical text analysis and suggests that parametric analysis based on the binomial or multinomial distributions is a better alternative for smaller texts",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical text analysis",
                "should not rely on the assumption of a normal distribution",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "parametric analysis",
                "is a better alternative",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The literature on relational similarity, on the other hand, has focused on pairs of words, devising various methods to compare how similar the contexts in which target pairs appear are to the contexts of other pairs that instantiate a relation of interest  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "devising various methods",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "relational similarity",
                "has focused on pairs of words",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.3 Features Similar to the default features in Pharaoh  , we used following features to estimate the weight of our grammar rules.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "default features in Pharaoh",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "grammar rules",
                "weight",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Pereira et al. , Curran and Moens   and Lin   use syntactic features in the vector definition.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic features",
                "in the vector definition",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "features",
                "in the vector definition",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "iezler and Maxwell   describe a method for learning a probabilistic model that maps LFG parse structures in German into LFG parse structures in English",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "probabilistic model that maps LFG parse structures",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "LFG parse structures",
                "in German into LFG parse structures in English",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ords in test data that have not been seen in training are deterministically assigned the POS tag that is assigned by the tagger described in Ratnaparkhi  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ratnaparkhi",
                "tagger described in",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "POS tag",
                "assigned by",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5.3   Snow   has extended the WordNet 2.1 by adding thousands of entries   at a relatively high precision.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet 2.1",
                "at a relatively high precision",
                "METHODOLOGY",
                "positive",
                0.75
            ],
            [
                "thousands of entries",
                "adding",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.2 Further practical issues of SCL In practice, there are more free parameters and model choices   besides the ones discussed above.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "more free parameters",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "model choices",
                "besides the ones discussed above",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "OpenCCG   and XLE  , or created semi-automatically  , or fully automatically extracted from annotated corpora, like the HPSG  , LFG   and CCG   resources derived from the Penn-II Treebank    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "OpenCCG",
                "created semi-automatically or fully automatically",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "HPSG",
                "derived from Penn-II Treebank",
                "INNOVATION",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "The self-training protocol is the same as in  : we parse the entire unlabeled corpus in one iteration.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "self-training protocol",
                "same as in : we parse the entire unlabeled corpus in one iteration",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpus",
                "entire unlabeled",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The words we want to aggregate for text analysis are not rigorous synonyms, but the role is the same, so we have to consider the syntactic relation based on the assumptions that words with the same role tend to modify or be modified by similar words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "same role",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "words",
                "similar words",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Inversion transduction grammars Inversion transduction grammars     are a notational variant of binary syntax-directed translation schemas   and are usually presented with a normal form: A   A. The universal recognition problem of ITGs can be solved in time O  by a CYKstyle parsing algorithm with two charts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITGs",
                "can be solved in time O",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "CYKstyle parsing algorithm",
                "with two charts",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Similar techniques are used in   for socalled direct translation models instead of those proposed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques",
                "used in direct translation models",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "proposed",
                "in ",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This second source of evidence is sometimes referred to as distributional similarity  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distributional similarity",
                "is referred to",
                "APPLICABILITY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The program takes the output of char_align  , a robust alternative to sentence-based alignment programs, and applies word-level constraints using a version of Brown el al.'s Model 2  , modified and extended to deal with robustness issues.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "char_align",
                "robust alternative",
                "METHODOLOGY",
                "positive",
                0.7
            ],
            [
                "Brown el al.'s Model 2",
                "modified and extended",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Then the words are tagged as inside a phrase  , outside a phrase   or beginning of a phrase    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "tagged as inside a phrase, outside a phrase, or beginning of a phrase",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase",
                "inside, outside, or beginning",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We accordingly introduce approaches which attempt to include semantic information into the coreference models from a variety of knowledge sources, e.g. WordNet  , Wikipedia   and automatically harvested patterns  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "knowledge sources",
                "variety of knowledge sources",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "coreference models",
                "include semantic information",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Most of the annotation approaches tackling these issues, however, are aimed at performing classifications at either the document level  , or the sentence or word level  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotation approaches",
                "performing classifications at document level or sentence or word level",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "annotation approaches",
                "tackling these issues",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "As noted in Dolan  , it is possible to run a sense-clustering algorithm on several MRDs to build an integrated lexical database with more complete coverage of word senses.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MRDs",
                "several",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "lexical database",
                "more complete coverage",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "Becausesuchapproachesdirectly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models   or the Hidden Markov Model    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "theoretically preferable",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "heuristics",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We used a bottom-up, CKY-style decoder that works with binary xRs rules obtained via a synchronous binarization procedure  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "CKY-style",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "binarization procedure",
                "synchronous",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "We use the standard NIST MTEval data sets for the years 2003, 2004 and 2005  .6 We report results in terms of case-insensitive 4gram BLEU   scores.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NIST MTEval data sets",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU scores",
                "case-insensitive 4gram",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The K&M model creates a packed parse forest of all possible compressions that are grammatical with respect to the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "K&M model",
                "packed parse forest of all possible compressions",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank",
                "Penn Treebank",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Moreover, the parameters of the model must be estimated using averaged perceptron training  , which can be unstable.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "must be estimated using averaged perceptron training",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "model",
                "can be unstable",
                "PERFORMANCE",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "Koehn and Hoang   propose factored translation models that combine feature functions to handle syntactic, morphological, and other linguistic information in a log-linear model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature functions",
                "combine",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "log-linear model",
                "handle syntactic, morphological, and other linguistic information",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In several papers  , selection criteria for single word trigger pairs were studied.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "selection criteria",
                "were studied",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "single word trigger pairs",
                "were studied",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Distributional measures of distance, such as those proposed by Lin  , quantify how similar the two sets of contexts of a target word pair are.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Distributional measures",
                "quantify how similar",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "target word pair",
                "contexts of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Where Pantel and Lin use Lins   measure, we use Wu and Palmers   measure.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Lins measure",
                "Pantel and Lin use",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Wu and Palmer's measure",
                "we use",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he preci781 start Palestinian suicide bomberblew himself up in SLOT1 on SLOT2 killing SLOT3 other people and injuring wounding SLOT4 end detroit the *e* a s *e* building buildingin detroit flattened ground levelled to blasted leveled *e* was reduced razed leveled to down rubble into ashes *e* to *e*     Figure 1: Examples of paraphrase patterns extracted by Barzilay and Lee   and Pang et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SLOT1",
                "on SLOT2",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "SLOT3",
                "other people",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "7 Related Work There has been a recent interest in training methods that enable the use of first-order features  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "training methods that enable the use of first-order features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "first-order features",
                "use of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "13Huang and Chiang   give an informal example, but do not elaborate on it.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "example",
                "informal",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "Huang and Chiang",
                "do not elaborate on it",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In addition, explicitly using the left context symbols allows easy use of smoothing techniques, such as deleted interpolation  , clustering techniques  , and model refinement techniques   to estimate the probabilities more reliably by changing the window sizes of the context and weighting the various estimates dynamically.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "smoothing techniques",
                "easy use",
                "METHODOLOGY",
                "positive",
                0.75
            ],
            [
                "probabilities",
                "estimating more reliably",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Collins  s parser and its reimplementation and extension by Bikel   have by now been applied to a variety of languages: English  , Czech  , German  , Spanish  , French  , Chinese   and, according to Dan Bikels web page, Arabic.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "applied to a variety of languages",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "reimplementation and extension",
                "have been",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2.1 The averaged perceptron The averaged perceptron algorithm   was proposed as a way of reducing overfitting on the training data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron algorithm",
                "proposed as a way of reducing overfitting",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "perceptron algorithm",
                "reducing overfitting",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "MET   iterative parameter estimation under IBM BLEU is performed on the development set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameter estimation",
                "under IBM BLEU",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "development set",
                "performed on",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the recent years, there have been a number of papers considering this or similar problems:  ,  ,  ,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "papers",
                "considering this or similar problems",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "problems",
                "similar",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction The probabilistic relation between verbs and their arguments plays an important role in modern statistical parsers and supertaggers  , and in psychological theories of language processing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "verbs",
                "plays an important role",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "psychological theories of language processing",
                "plays an important role",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Bikel and Chiang   in fact contains two parsers: one is a lexicalized probabilistic contextfree grammar   similar to  ; the other is based on statistical TAG  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsers",
                "lexicalized probabilistic context-free grammar",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parsers",
                "based on statistical TAG",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Let w be a target word and Nw = fn1,n2nkg be the ordered set of the top scoring k neighbours of w from the thesaurus with associated distributional similarity scores fdss ,dss ,dss g using  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "w",
                "ordered set of top scoring k neighbours",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "thesaurus",
                "associated distributional similarity scores",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Obviously, these productions are not in the normal form of an ITG, but with the method described in  , they can be normalized.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "productions",
                "are not in the normal form of an ITG",
                "PERFORMANCE",
                "neutral",
                0.85
            ],
            [
                "method",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The features are the same as those in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "same as those in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "features",
                "same as those in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4.1 Model 1 Score We used IBM Model 1   as one of the feature functions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 1",
                "as one of the feature functions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM Model 1",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, the distancebased reordering model   allows a decoder to translate in non-monotonous order, under the constraint that the distance between two phrases translated consecutively does not exceed a limit known as distortion limit.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distancebased reordering model",
                "allows a decoder to translate in non-monotonous order",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "distortion limit",
                "does not exceed",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Tuning is done for each experimental condition using Ochs Minimum Error Training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "experimental condition",
                "using Ochs Minimum Error Training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Ochs Minimum Error Training",
                "used for tuning",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "where mk is one mention in entity e, and the basic model building block PL  is an exponential or maximum entropy model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PL",
                "exponential or maximum entropy model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "model building block",
                "basic",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Following Hatzivassiloglou and McKeown   and Turney  , we decided to observe how often the words from the headline co-occur with each one of the six emotions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "We used GIZA++   to align approximately 751,000 sentences from the German-English portion of the Europarl corpus  , in both the German-to-English and English-to-German directions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "align approximately 751,000 sentences",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Europarl corpus",
                "German-English and English-German directions",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In recent years, many researchers build alignment links with bilingual corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bilingual corpora",
                "build alignment links with",
                "INNOVATION",
                "neutral",
                0.5
            ],
            [
                "recent years",
                "many researchers",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The PropBank superimposes an annotation of semantic predicate-argument structures on top of the Penn Treebank    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PropBank",
                "annotation of semantic predicate-argument structures",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank",
                "annotation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Above the phrase level, these models typically have a simple distortion model that reorders phrases independently of their content  , or not at all  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "reorders phrases independently of their content",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "models",
                "not at all",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The first-sense heuristic can be thought of as striving for maximal specificity at the risk of precluding some admissible senses  , 7Allowing for multiple fine-grained senses to be judged as appropriate in a given context goes back at least to Sussna  ; discussed more recently by, e.g., Navigli  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "heuristic",
                "striving for maximal specificity",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "multiple fine-grained senses",
                "judged as appropriate",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "e used the same 58 feature types as Ratnaparkhi  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature types",
                "same as Ratnaparkhi",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "feature types",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Using alignment for grammar and lexicon induction has been an active area of research, both in monolingual settings   and in machine translation     | interestingly, statistical MT techniques have been used to derive lexico-semantic mappings in the \\reverse\"\" direction of language understanding rather than generation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "active area",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical MT techniques",
                "used to derive lexico-semantic mappings",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Reported work includes improved model variants   and applications such as web data extraction  , scientific citation extraction  , word alignment  , and discourselevel chunking  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model variants",
                "improved",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word alignment",
                "and discourse-level chunking",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": ".4 Related work and issues for future research Smadja   and van der Eijk   describe term translation methods that use bilingual texts that were aligned at the sentence level",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "term translation methods",
                "use bilingual texts aligned at sentence level",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "bilingual texts",
                "aligned at sentence level",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We use the Europarl corpus  , and the statistical word alignment was performed with the GIZA++ toolkit  .1 For the current experiments we assume no preexisting parser for any of the languages, contrary to the information projection scenario.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Europarl corpus",
                "statistical word alignment",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "GIZA++ toolkit",
                "statistical word alignment",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "On the other hand,   proposed an algorithm, borrowed to the field of dynamic programming and based on the output of their previous work, to find the best alignment, subject to certain constraints, between words in parallel sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "based on the output of their previous work",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "constraints",
                "subject to certain",
                "LIMITATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "4.1.3 Letter Lexical Transliteration Similar to IBM Model-1  , we use a bag-of-letter generative model within a block to approximate the lexical transliteration equivalence: P = j+lproductdisplay jprime=j i+ksummationdisplay iprime=i P P ,   where P  similarequal 1/  is approximated by a bagof-word unigram.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bag-of-letter generative model",
                "within a block",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "P",
                "is approximated by a bagof-word unigram",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Discriminative training with hidden variables has been handled in this probabilistic framework  , but we choose Equation 3 for efficiency.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hidden variables",
                "has been handled",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Equation 3",
                "choose for efficiency",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "To address this drawback, we proposed a new method3 to compute a more reliable and smoothed score in the undefined case, based on the IBM model 1  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "compute a more reliable and smoothed score",
                "METHODOLOGY",
                "positive",
                0.7
            ],
            [
                "IBM model",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As mentioned earlier, both of these methods are based on Collinss averaged-perceptron algorithm for sequence labeling  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "averaged-perceptron",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "methods",
                "based on Collinss averaged-perceptron algorithm",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Feature selection methods have been proposed in the maximum-entropy literature by several authors  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Feature selection methods",
                "have been proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "maximum-entropy literature",
                "by several authors",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Named entities also pose another problem with the Haghighi and Klein   coreference model; since it models only the heads of NPs, it will fail to resolve some references to named entities:  , while erroneously merging others:  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "coreference model",
                "models only the heads of NPs",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Haghighi and Klein coreference model",
                "will fail to resolve some references",
                "PERFORMANCE",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "In addition to CFG-oriented approaches, a number of richer treebank-based grammar acquisition and parsing methods based on HPSG  , CCG  , LFG   and Dependency Grammar   incorporate non-local dependencies into their deep syntactic or semantic representations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treebank-based grammar acquisition and parsing methods",
                "in incorporate non-local dependencies",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "HPSG, CCG, LFG, and Dependency Grammar",
                "into their deep syntactic or semantic representations",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Each element of the resulting vector was replaced with its log-likelihood value   which can be considered as an estimate of how surprising or distinctive a co-occurrence pair is  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-likelihood value",
                "estimate of how surprising or distinctive",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "co-occurrence pair",
                "can be considered",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our MT system was evaluated using the n-gram based Bleu   and NIST machine translation evaluation software.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine translation evaluation software",
                "Bleu and NIST",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "n-gram based",
                "machine translation evaluation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Following Wu  , the prevailing opinion in the research community has been that more complex patterns of word alignment in real bitexts are mostly attributable to alignment errors.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "patterns of word alignment",
                "mostly attributable to alignment errors",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "research community",
                "prevailing opinion",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "n the following section we show how this drawback can be overcome using statistical alignments  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "drawback",
                "can be overcome",
                "LIMITATION",
                "neutral",
                0.95
            ],
            [
                "statistical alignments",
                "using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Similarly to classical NLP tasks such as base noun phrase chunking  , text chunking   or named entity recognition  , we formulate the mention detection problem as a classification problem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mention detection problem",
                "formulate as a classification problem",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "token",
                "starts a specific mention, is inside a specific mention, or is outside any mentions",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "BLEU For all translation tasks, we report caseinsensitive NIST BLEU scores   using 4 references per sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "NIST BLEU scores",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "references per sentence",
                "4",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he weights of the models are computed automatically using a variant of the Maximum Bleu training procedure proposed by Och  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "computed automatically",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Maximum Bleu training procedure",
                "proposed by Och",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "As a final note, following Collins  , we used the averaged parameters from the training algorithm in decoding test examples in our experiments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training algorithm",
                "averaged parameters",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "test examples",
                "in decoding",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "303 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language While it is common in studies of collocations to omit low-frequency words and expressions from analysis, because they give rise to invalid or unrealistic statistical measures  , we are able to identify higher-precision collocations by including placeholders for unique words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wiebe, Wilson, Bruce, Bell, and Martin",
                "omitting low-frequency words and expressions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "unique words",
                "give rise to invalid or unrealistic statistical measures",
                "LIMITATION",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "Practical Model 4 systems therefore make substantial search approximations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "search approximations",
                "substantial",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "search approximations",
                "make",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The original training set   consisted of a few dozen examples, in comparison to thousands of examples needed in other corpus-based methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training set",
                "a few dozen examples",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "thousands of examples",
                "needed in other corpus-based methods",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "2 Linguistic and Context Features 2.1 Non-terminal Labels In the original string-to-dependency model  , a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "non-terminal Labels",
                "original string-to-dependency model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translation rule",
                "well-formed dependency structure",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These joint counts are estimated using the phrase induction algorithm described in  , with symmetrized word alignments generated using IBM model 2  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase induction algorithm",
                "described in ",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "symmetrized word alignments",
                "generated using IBM model 2",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Following Ponzetto and Strube  , we consider an anaphoric reference, NPi, correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NPi",
                "are in the same coreference chain",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "coreference chain",
                "in the resulting partition",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Along similar lines,   combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generative model of word alignment",
                "small set of hand aligned sentences",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "log-linear discriminative model",
                "trained on a small set of hand aligned sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Given the training pairs, any sequence predictor can be used, for example a Conditional Random Field     or a structured perceptron  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sequence predictor",
                "Conditional Random Field or a structured perceptron",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sequence predictor",
                "any sequence predictor can be used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We are currently investigating more challenging problems like multiple category classification using the Reuters-21578 data set   and subjective sentiment classification  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data set",
                "Reuters-21578",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "subjective sentiment classification",
                "subjective",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, the Penn Treebank   provides a large corpus of syntactically annotated examples mostly from the Wall Street Journal.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "large corpus of syntactically annotated examples",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Wall Street Journal",
                "mostly from",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this respect it resembles Wus 264 bilingual bracketer  , but ours uses a different extraction method that allows more than one lexical item in a rule, in keeping with the phrasebased philosophy.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wus 264 bilingual bracketer",
                "resembles",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "extraction method",
                "allows more than one lexical item in a rule",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We use ROUGE   to assess summary quality using common n-gram counts and longest common subsequence   measures.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE",
                "assess summary quality",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "common n-gram counts and longest common subsequence measures",
                "measures",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Vilain and Day   identify   name phrases such as company names, locations, etc. Ramshaw and Marcus   detect noun phrases, by classifying each word as being inside a phrase, outside or on the boundary between phrases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "name phrases",
                "such as company names, locations, etc.",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "noun phrases",
                "by classifying each word as being inside a phrase, outside or on the boundary between phrases",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Statistical language modeling has been widely used in natural language processing applications such as Automatic Speech Recognition  , Statistical Machine Translation     and Information Retrieval    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "language modeling",
                "widely used",
                "METHODOLOGY",
                "positive",
                0.85
            ],
            [
                "language modeling",
                "in natural language processing applications",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Alignment models to structure the translation model are introduced in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Alignment models",
                "are introduced",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation model",
                "structure",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters of the model",
                "estimated using simple likelihood based techniques",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "various smoothing and back-off estimation tricks",
                "helped cope with sparse data problems",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "In the latter case, we use an unsupervised attachment disambiguation method, based on the log-likelihood ratio  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unsupervised attachment disambiguation method",
                "based on log-likelihood ratio",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "log-likelihood ratio",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction The dominance of traditional phrase-based statistical machine translation   models   has recently been challenged by the development and improvement of a number of new models that explicity take into account the syntax of the sentences being translated.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based statistical machine translation models",
                "has recently been challenged",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "new models",
                "take into account the syntax of the sentences being translated",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We perform word alignment using GIZA++  , symmetrize the alignments using the grow-diag-final-and heuristic, and extract phrases up to length 3.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "word alignment",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "grow-diag-final-and heuristic",
                "symmetrize alignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Pattern-based IE approaches employ seed data to learn useful patterns to pinpoint required fields values  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "seed data",
                "learn useful patterns",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "required fields values",
                "pinpoint",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Och   observed, however, that the piecewiseconstant property could be exploited to characterize the function exhaustively along any line in parameter space, and hence to minimize it globally along that line.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "function",
                "could be exploited",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "property",
                "could be exploited",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Previous research has shown that RST trees can play a crucial role in building natural language generation systems   and text summarization systems  ; can be used to increase the naturalness of machine translation outputs  ; and can be used to build essayscoring systems that provide students with discourse-based feedback  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "RST trees",
                "play a crucial role",
                "INNOVATION",
                "positive",
                0.85
            ],
            [
                "machine translation outputs",
                "increase the naturalness",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "First, manyto-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++  , in both directions and by combining the results based on a heuristic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "one-to-many word alignment model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "results",
                "based on a heuristic",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The probabilities are ordered according to, at least my, intuition with pronoun being the most likely  , followed by proper nouns  , followed by common nouns  , a fact also noted by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pronouns",
                "most likely",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "fact",
                "also noted by",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Applying the projection WTx   would give us m new features, however, for both computational and statistical reasons   a low-dimensional approximation of the original feature space is computed by applying Singular Value Decomposition   on W  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WTx",
                "low-dimensional approximation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Singular Value Decomposition",
                "applied on W",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Bilingual bracketing methods were used to produce a word alignment in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bilingual bracketing methods",
                "used to produce a word alignment",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word alignment",
                "produced",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A variety of methods have been applied, ranging from simple frequency  , modified frequency measures such as c-values   and standard statistical significance tests such as the t-test, the chi-squared test, and loglikelihood  , and information-based methods, e.g. pointwise mutual information  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "simple frequency",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "methods",
                "information-based methods",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4This was a straightforward task; two annotators annotated independently, with very high agreementkappa score of over 0.95  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotators",
                "annotated independently",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "agreement",
                "kappa score of over 0.95",
                "PERFORMANCE",
                "positive",
                0.95
            ]
        ]
    },
    {
        "text": "The class-based kappa statistic of   cannot be applied here, as the classes vary depending on the number of ambiguities per entry in the lexicon.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "class-based kappa statistic",
                "cannot be applied",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "classes",
                "vary depending on the number of ambiguities per entry",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In supervised domain adaptation  , besides the labeled source data, we have access to a comparably small, but labeled amount of target data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data",
                "labeled",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "amount of target data",
                "comparably small",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction The task of sentence compression   can be defined as summarizing a single sentence by removing information from it  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentence compression",
                "removing information",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentence summarization",
                "by removing",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One interesting approach to extending the current system is to introduce a statistical translation model   to filter out irrelevant translation candidates and to extract the most appropriate subpart from a long English sequence as the translation by locally aligning the Japanese and English sequences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical translation model",
                "filter out irrelevant translation candidates",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "local alignment",
                "aligning the Japanese and English sequences",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In  , an undirected graphical model for constituent parse reranking uses dependency relations to define the edges.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "undirected graphical model",
                "uses dependency relations to define the edges",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dependency relations",
                "define the edges",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions   or a grammar fixed a priori (Blunsom et al., 1f and e are the input and output sentences respectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sampler",
                "reasons over the infinite space of possible translation units",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "grammar",
                "fixed a priori",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "aume III   proposed a simple feature augmentation method to achieve domain adaptation",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature augmentation method",
                "simple",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "domain adaptation",
                "achieve",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Dunning   argues for the use of G 2 rather than X 2, based on an analysis of the sampling distributions of G 2 and X 2, and results obtained when using the statistics to acquire highly associated bigrams.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "G 2",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "G 2",
                "based on analysis of sampling distributions",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For scoring MT outputs, the proposed RSCM uses a score based on a translation model called IBM4     and a score based on a language model for the translation target language  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM4",
                "translation model",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "language model",
                "for the translation target language",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": " , it is much higher than the 2.6% unknown word rate in the test set for Ratnaparkhis   English POS tagging experiments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "This approach to minimally supervised classifier construction has been widely studied  , especially in cases in which the features of interest are orthogonal in some sense  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifier construction",
                "widely studied",
                "INNOVATION",
                "positive",
                0.75
            ],
            [
                "features of interest",
                "are orthogonal",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It is promising to optimize the model parameters directly with respect to AER as suggested in statistical machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model parameters",
                "directly with respect to AER",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "AER",
                "promising",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "2 WordNet-based semantic relatedness measures 2.1 Basic measures Two similarity/distance measures from the Perl package WordNet-Similarity written by   are used.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet-Similarity",
                "written by",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "similarity/distance measures",
                "from the Perl package",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Statistic-based algorithms based on Belief Network  such as Hidden-MarkovModel   , Lexicalized HMM  and Maximal-Entropy model  use the statistical information of a manually tagged corpus as background knowledge to tag new sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Belief Network",
                "statistic-based algorithms",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "statistical information",
                "as background knowledge",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Parameters  used to calculate P  are trained using MER training   on development data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "trained using MER training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "development data",
                "used to calculate P",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 The Learning Architecture The synchronous derivations described above are modelled with an Incremental Sigmoid Belief Network    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Incremental Sigmoid Belief Network",
                "modelled with",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "synchronous derivations",
                "described above",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction Hyponymy relations can play a crucial role in various NLP systems, and there have been many attempts to develop automatic methods to acquire hyponymy relations from text corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NLP systems",
                "crucial role",
                "APPLICABILITY",
                "neutral",
                0.85
            ],
            [
                "automatic methods",
                "acquire hyponymy relations",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We use the discriminative perceptron learning algorithm   to train the values of vectorw.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "discriminative perceptron learning",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "values",
                "of vectorw",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, minimum entropy regularization  , aims to maximize the conditional likelihood of labeled data while minimizing the conditional entropy of unlabeled data: summationdisplay i logp |x ) 122bardblbardbl2H    This approach generally would result in sharper models which can be data-sensitive in practice.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "minimum entropy regularization",
                "maximize the conditional likelihood",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "models",
                "sharper",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "he fact that the error rate more than doubles when the seeds in Yarowsky's   experiments are reduced from a sense's best collocations to just one word per sense suggests that the error rate would increase further if no seeds were provided",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "seeds in Yarowsky's experiments",
                "reduced from a sense's best collocations to just one word per sense",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "error rate",
                "would increase further if no seeds were provided",
                "PERFORMANCE",
                "negative",
                0.75
            ]
        ]
    },
    {
        "text": "7 Independently, Cutting et aL   quote a performance of 800 words per second for their part-of-speech tagger based on hidden Markov models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "part-of-speech tagger",
                "based on hidden Markov models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "performance",
                "800 words per second",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This problem will be solved by incorporating other resources such as thesaurus or a dictionary,orcombiningourmethodwithothermethods using external wider contexts  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "resources",
                "thesaurus or a dictionary",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "combining with other methods using external wider contexts",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Each model can represent an important feature for the translation, such as phrase-based, language, or lexical models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "important feature",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase-based, language, or lexical models",
                "represent",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Turney   noted that the unigram unpredictable might have a positive sentiment in a movie review  , but could be negative in the review of an automobile  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unigram unpredictable",
                "positive sentiment",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "unigram unpredictable",
                "negative sentiment",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In our own work on document compression models  , both of which extend the sentence compression model of Knight and Marcu  , we assume that sentences and documents can be summarized exclusively through deletion of contiguous text segments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "document compression models",
                "extend the sentence compression model of Knight and Marcu",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentences and documents",
                "can be summarized exclusively through deletion of contiguous text segments",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ll our MT systems were trained using a variant of the alignment template model described in  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment template model",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "MT systems",
                "trained using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 Datasets and Evaluation We train our models with verb instances extracted from three parsed corpora:   the Wall Street Journal section of the Penn Treebank  , which was parsed by human annotators  ,   the Brown Laboratory for Linguistic Information Processing corpus of Wall Street Journal text  , which was parsed automatically by the Charniak parser  , and   the Gigaword corpus of raw newswire text  , which we parsed ourselves with the Stanford parser.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "parsed by human annotators",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Charniak parser",
                "parsed automatically",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Lin  s similar word list for eat misses these but includes sleep   and sit  , because these have similar subjects to eat.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word list",
                "misses these but includes sleep and sit",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word list",
                "has similar subjects to eat",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We adopt their idea of an utterance as a description, generated from a communicative goal, and also use an ontologically promiscuous formalism for representing meaning  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "idea",
                "ontologically promiscuous formalism",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "formalism",
                "for representing meaning",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "When we have a junction tree for each document, we can efficiently perform belief propagation in order to compute argmax in Equation  , or the marginal probabilities of cliques and labels, necessary for the parameter estimation of machine learning classifiers, including perceptrons  , and maximum entropy models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "junction tree",
                "efficiently perform belief propagation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "machine learning classifiers",
                "including perceptrons and maximum entropy models",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As referring dataset, we used the PropBank corpora available at www.cis.upenn.edu/ace, along with the Penn TreeBank 2    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PropBank corpora",
                "available",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Penn TreeBank 2",
                "available",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We employ loglinear models   for the disambiguation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "loglinear models",
                "for disambiguation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "loglinear models",
                "for",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Verbs and possible senses in our corpus Both corpora were lemmatized and part-of-speech   tagged using Minipar   and Mxpost  , respectivelly.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "lemmatized and part-of-speech tagged",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Minipar and Mxpost",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We show that our semi-supervised approach yields improvements for fixed datasets by performing parsing experiments on the Penn Treebank   and Prague Dependency Treebank    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "yields improvements",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "datasets",
                "fixed",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Minimum Error Rate Training     under BLEU criterion is used to estimate 20 feature function weights over the larger development set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature function weights",
                "estimated over the larger development set",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU criterion",
                "used to",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The other intriguing issue is how our anchor-based method for shared argument identification can benefit from recent advances in coreference and zero-anaphora resolution  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "anchor-based method",
                "benefit from recent advances",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "coreference and zero-anaphora resolution",
                "recent advances",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Models of translational equivalence that are ignorant of indirect associations have \"\"a tendency  to be confused by collocates\"\"  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Models of translational equivalence",
                "tendency to be confused",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "collocates",
                "confused by",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "It forms a baseline for performance evaluations, but is prone to sparse data problems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "performance evaluations",
                "forms a baseline",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "sparse data problems",
                "is prone to",
                "LIMITATION",
                "negative",
                0.9
            ]
        ]
    },
    {
        "text": "The MT systems of   learn to generate text in the target language straight from the source language, without the aid of an explicit semantic representation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT systems",
                "generate text straight from the source language",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "semantic representation",
                "explicit",
                "METHODOLOGY",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "In his Xtract system, Smadja   first extracted significant pairs of words that consistently co-occur within a single syntactic structure using statistical scores called distance, strength and spread, and then examined concordances of the bi-grams to find longer frequent multiword units.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Xtract system",
                "using statistical scores",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "concordances of the bi-grams",
                "to find longer frequent multiword units",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The Xerox tagger   comes with a set of rules that assign an unknown word a set of possible pos-tags   on the basis of its ending segment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Xerox tagger",
                "set of rules",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "set of possible pos-tags",
                "on the basis of its ending segment",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Re-ordering effects across languages have been modeled in several ways, including word-based  , template-based   and syntax-based  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "re-ordering effects",
                "modeled in several ways",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "ways",
                "word-based, template-based, syntax-based",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Minor variants support voted perceptron   and MEMMs   with the same ef cient feature encoding.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Minor variants",
                "support",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ef cient feature encoding",
                "same",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In this case it is possible to perform the correct selection if we used only statistics about the cooccurrences of 'corruption' with either 'investigator' or 'researcher', without looking for any syntactic relation  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "cooccurrences",
                "statistics about the cooccurrences",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "selection",
                "correct",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Baseline We use the Moses MT system   as a baseline and closely follow the example training procedure given for the WMT-07 and WMT-08 shared tasks.4 In particular, we perform word alignment in each direction using GIZA++  , apply the grow-diag-finaland heuristic for symmetrization and use a maximum phrase length of 7.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Moses MT system",
                "as a baseline",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "grow-diag-finaland heuristic",
                "for symmetrization",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "More specifically, two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment   and pronoun references  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexical relations",
                "for resolving ambiguity",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "recent works",
                "suggested using",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules  , or using machine-learning methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "calculated by mining the parse trees",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "machine-learning methods",
                "using machine-learning methods",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Similar to work in image retrieval  , we cast the problem in terms of Machine Translation: given a paired corpus of words and a set of video event representations to which they refer, we make the IBM Model 1 assumption and use the expectation-maximization method to estimate the parameters  :  =+ = m j ajm jvideowordpl Cvideowordp 1 )| 1 |  This paired corpus is created from a corpus of raw video by first abstracting each video into the feature streams described above.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "paired corpus",
                "created from a corpus of raw video",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "IBM Model 1 assumption",
                "use the expectation-maximization method to estimate the parameters",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The first stage parser is a best-first PCFG parser trained on sections 2 through 22, and 24 of the Penn WSJ treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "best-first PCFG parser",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn WSJ treebank",
                "trained on sections 2 through 22, and 24",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The approach is in the spirit of Smadja   on retrieving collocations from text corpora, but is more integrated with parsing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "is in the spirit of Smadja",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "approach",
                "is more integrated with parsing",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "We participated in the multilingual track of the CoNLL 2007 shared task  , and evaluated the system on data sets of 10 languages  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CoNLL 2007 shared task",
                "multilingual track",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "data sets of 10 languages",
                "evaluated on",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Bergsma et al   proposed a distributional method in detecting non-anaphoric pronouns by first extracting the surrounding textual context of the pronoun, then gathering the distribution of words that occurred within that context from a large corpus and finally learning to classify these distributions as representing either anaphoric and non-anaphoric pronoun instances.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "distributional method",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "classification",
                "as representing anaphoric and non-anaphoric pronoun instances",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Table 1 shows the evaluation of all the systems in terms of BLEU score   with the best score highlighted.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "best score",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "systems",
                "evaluation",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Rather than explicit annotation, we could use latent annotations to split the POS tags, similarly to the introduction of latent annotations to PCFG grammars  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "latent annotations",
                "introduction to PCFG grammars",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "POS tags",
                "split",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As shown by McDonald and Nivre  , the Single Malt parser tends to suffer from two problems: error propagation due to the deterministic parsing strategy, typicallyaffectinglongdependenciesmorethan short ones, and low precision on dependencies originating in the artificial root node due to fragmented parses.9 The question is which of these problems is alleviatedbythemultipleviewsgivenbythecomponent parsers in the Blended system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Single Malt parser",
                "tends to suffer from two problems",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "multiple views given by the component parsers",
                "alleviate one of these problems",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The third exploits automatic subjectivity analysis in applications such as review classification  ), mining texts for product reviews  ), summarization  ), information extraction  ), 1Note that sentiment, the focus of much recent work in the area, is a type of subjectivity, specifically involving positive or negative opinion, emotion, or evaluation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "automatic subjectivity analysis",
                "in applications such as review classification, mining texts for product reviews, summarization, information extraction",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "sentiment",
                "type of subjectivity, specifically involving positive or negative opinion, emotion, or evaluation",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There exist many different string similarity measures: word overlap  , longest common subsequence  , Levenshteinedit distance  , word n-gramoverlap   etc. Semantic similarity measures are obtained by first computing the semantic similarity of the words containedin the sentencesbeing compared.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "string similarity measures",
                "many different",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "semantic similarity measures",
                "obtained by",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Aggregate models based on higher-order n-grams   might be able to capture multi-word structures such as noun phrases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-grams",
                "capture multi-word structures such as noun phrases",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "models",
                "based on higher-order",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It has been shown that both Nave Bayes and SVMs perform with similar accuracy on different sentiment tagging tasks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Nave Bayes",
                "perform with similar accuracy",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "SVMs",
                "perform with similar accuracy",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This obviously does not preclude using the audio-based system together with other features such as utterance position, length, speakers roles, and most others used in the literature  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "used in the literature",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "audio-based system",
                "together with other features",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "According to the statistical machine translation formalism  , the translation process is to search for the best sentence bE such that bE = arg max E P  = arg maxE P P  where P  is a translation model characterizing the correspondence between E and J; P , the English language model probability.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "characterizing the correspondence between E and J",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "P",
                "the English language model probability",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For instance, several studies have shown that BLEU correlates with human ratings on machine translation quality  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "correlates with human ratings on machine translation quality",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Table 2: The set of tags used to mark explicit morphemes in English Tag Meaning JJR Adjective, comparative JJS Adjective, superlative NNS Noun, plural POS Possessive ending RBR Adverb, comparative RBS Adverb, superlative VB Verb, base form VBD Verb, past tense VBG Verb, gerund or present participle VBN Verb, past participle VBP Verb, non3rd person singular present VBZ Verb, 3rd person singular present Figure 2: Morpheme alignment between a Turkish and an English sentence 4 Experiments We proceeded with the following sequence of experiments:   Baseline: As a baseline system, we used a pure word-based approach and used Pharaoh Training tool  , to train on the 22,500 sentences, and decoded using Pharaoh   to obtain translations for a test set of 50 sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pharaoh Training tool",
                "used to train",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Pharaoh",
                "used to obtain translations",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In the following section, we follow the notation in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "notation",
                "in",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "notation",
                "follows",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Liu and Gildea   also pointed out that due to the limited references for every MT output, using the overlapping ratio of n-grams longer than 2 did not improve sentence level evaluation performance of BLEU.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT output",
                "limited references",
                "LIMITATION",
                "negative",
                0.8
            ],
            [
                "n-grams longer than 2",
                "did not improve",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The recall problem is usually addressed by increasing the amount of text data for extraction  ) or by developing more surface patterns  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "text data",
                "amount of text data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "surface patterns",
                "more surface patterns",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We have achieved average results in the CoNLL domain adaptation track open submission  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "results",
                "average",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "CoNLL domain adaptation track open submission",
                "submission",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction A number of empirical studies have found bracketing to be a useful type of corpus annotation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bracketing",
                "be a useful type of corpus annotation",
                "INNOVATION",
                "positive",
                0.7
            ],
            [
                "corpus annotation",
                "be a type of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2.2 Creation of a Coarse-Grained Sense Inventory To tackle the granularity issue, we produced a coarser-grained version of the WordNet sense inventory3 based on the procedure described by Navigli  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet sense inventory",
                "coarser-grained version",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "procedure described by Navigli",
                "described",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The model scaling factors M1 are trained with respect to the final translation quality measured by an error criterion  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model scaling factors M1",
                "trained with respect to the final translation quality",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "error criterion",
                "measured by",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Model Bits / Character ASCII Huffman code each char Lempel-Ziv   Unigram   Trigram Human Performance 8 5 4.43 2.1   1.76   1.25   The cross entropy, H, of a code and a source is given by: H  = ~ ~ Pr  log 2 Pr  s h where Pr  is the joint probability of a symbol s following a history h given the source.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Huffman code",
                "each char Lempel-Ziv Unigram Trigram",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "cross entropy",
                "given by",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Distortion models were first proposed by   in the so-called IBM Models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Models",
                "proposed",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "distortion models",
                "first",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For better probability estimation, the model was extended to work with   word classes  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "work with word classes",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "probability estimation",
                "better",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Parse Parse score from Model 2 of the statistical parser  , normalized by the number of words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 2",
                "statistical parser",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Parse score",
                "normalized by the number of words",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The resulting memory limitations alone can prevent the practical learning of highly split grammars  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "memory limitations",
                "prevent practical learning of highly split grammars",
                "LIMITATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "There has thus been a trend recently towards robust wide-coverage semantic construction  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic construction",
                "robust wide-coverage",
                "INNOVATION",
                "positive",
                0.7
            ],
            [
                "trend",
                "recently",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This is related to the wellstudied problem of identifying paraphrases   and the more general variant of recognizing textual entailment, which explores whether information expressed in a hypothesis can be inferred from a given premise.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "paraphrases",
                "well-studied problem",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "textual entailment",
                "more general variant",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Our work is most similar to work using discriminative log-linear models for alignment, which is similar to discriminative log-linear models used for the SMT decoding   problem  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "12 As such, we resort to an approximation: Voted Perceptron training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Voted Perceptron training",
                "approximation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Voted Perceptron training",
                "training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "At one extreme are those, exemplified by that of Wu  , that have no dependence on syntactic theory beyond the idea that natural language is hierarchical.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wu",
                "no dependence on syntactic theory",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "natural language",
                "is hierarchical",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It has been argued that METEOR correlates better with human judgment due to higher weight on recall than precision  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "METEOR",
                "correlates better with human judgment",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "higher weight on recall than precision",
                "evaluation criterion",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However, Dunning   pointed out that for the purpose of corpus statistics, where the sparseness of data is an important issue, it is better to use the log-likelihood ratio.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Dunning",
                "pointed out",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "log-likelihood ratio",
                "better to use",
                "METHODOLOGY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "The modifications are made to deal with the efficiency issue due to the fact that there is a very large number of features and training samples in our task, compared to only 8 features used in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "very large number of",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "training samples",
                "very large number of",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In order to generate a value for each target-side factor, we use a sequence of mapping steps similar to Koehn and Hoang  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "target-side factor",
                "sequence of mapping steps similar to Koehn and Hoang",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "mapping steps",
                "similar to",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The segmentation is based on the guidelines, given in the Chinese national standard GB13715,   and the POS tagging specification was developed according to the Grammatical Knowledge-base of contemporary Chinese.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "guidelines",
                "given in the Chinese national standard GB13715",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "POS tagging specification",
                "developed according to the Grammatical Knowledge-base of contemporary Chinese",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Moreover, rather than predicting an intrinsic metric such as the PARSEVAL Fscore, the metric that the predictor learns to predict can be chosen to better fit the final metric on which an end-to-end system is measured, in the style of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metric",
                "chosen to better fit the final metric",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "predictor",
                "learns to predict",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Ordinary Prologstyle, backchaining deduction is augmented with the capability of making assumptions and of factoring two goal literals that are unifiable  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "goal literals",
                "are unifiable",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "backchaining deduction",
                "is augmented",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "gain we used Mohammad and Hirsts   method along with Lins   distributional measure to determine the distributional closeness of two thesaurus concepts",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Mohammad and Hirst's method",
                "method",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Lins' distributional measure",
                "distributional measure",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "5 Experiments We compare the performance of our forest reranker against n-best reranking on the Penn English Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn English Treebank",
                "is compared on",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "n-best reranking",
                "is compared against",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use BLEU scores   to measure translation accuracy.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU scores",
                "to measure translation accuracy",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Similarly, prototype-driven learning     optimizes the joint marginal likelihood of data labeled with prototype input features for each label.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prototype-driven learning",
                "optimizes the joint marginal likelihood",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "joint marginal likelihood",
                "data labeled with prototype input features for each label",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Banerjee and Lavie   introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Meteor metric",
                "more flexible match",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Meteor metric",
                "incorporates recall on the unigram level",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms  , weakly supervised learning algorithms  , unsupervised learning algorithms    , and knowledge based algorithms  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus based statistical methods",
                "have been proposed",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "supervised learning algorithms",
                "weakly supervised learning algorithms",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  to LFG parses, and by Liu and Gildea   to features derived from phrase-structure tress.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LFG parses",
                "by Liu and Gildea",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "features derived from phrase-structure trees",
                "to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Over the past five years progress in machine translation, and to a lesser extent progress in natural language generation tasks such as summarization, has been driven by optimizing against n-grambased evaluation metrics such as Bleu  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-grambased evaluation metrics",
                "such as Bleu",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "progress",
                "has been driven",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Turney   describes a method   that extracts subsequence patterns for noun pairs from a large corpus, using query expansion to increase the recall of the search and feature selection and dimensionality reduction to reduce the complexity of the feature space.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "extracts subsequence patterns for noun pairs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "feature selection and dimensionality reduction",
                "reduce the complexity of the feature space",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In cases where the number of gold tags is different than the number of induced tags, some must necessarily remain unassigned  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "gold tags",
                "different than the number of induced tags",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "unassigned",
                "necessarily remain",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We computed precision, recall and error rate on the entire set for each data set.6 For an initial alignment, we used GIZA++ in both directions   or Spanish  ), and also two different combined alignments: intersection of E-to-F and F-to-E; and RA using a heuristic combination approach called grow-diag-final  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "in both directions (English to Spanish and Spanish to English)",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "grow-diag-final",
                "heuristic combination approach",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our results agree, at least at the level of morphology, with  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "results",
                "agree",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "level of morphology",
                "at least",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The WSJ corpus is based on the WSJ part of the PENN TREEBANK  ; we used the first 10,000 sentences of section 2-21 as the pool set, and section 00 as evaluation set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSJ corpus",
                "based on WSJ part of the PENN TREEBANK",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "pool set",
                "first 10,000 sentences of section 2-21",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Expansion of the equivalent sentence set can be applied to automatic evaluation of machine translation quality  , for example.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "equivalent sentence set",
                "can be applied",
                "APPLICABILITY",
                "neutral",
                0.85
            ],
            [
                "machine translation quality",
                "evaluation",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 Language modelling with Bloom filters Recentwork presenteda scheme for associating static frequency information with a set of n-grams in a BF efficiently.1 3.1 Log-frequency Bloom filter The efficiency of the scheme for storing n-gram statistics within a BF presented in Talbot and Osborne   relies on the Zipf-like distribution of n-gramfrequencies: mosteventsoccuranextremely small number of times, while a small number are very frequent.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bloom filter",
                "efficiently",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Zipf-like distribution",
                "n-gram frequencies",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The data consist of sections of the Wall Street Journal   part of the Penn TreeBank  , with information on predicate-argument structures extracted from the PropBank corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn TreeBank",
                "part of",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "PropBank corpus",
                "extracted from",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example it has been used to measure centrality in hyperlinked web pages networks  , lexical networks  , and semantic networks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "centrality",
                "measure in hyperlinked web pages networks, lexical networks, and semantic networks",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "networks",
                "hyperlinked web pages, lexical, semantic",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "hese are the same distributions that are needed by previous POS-based language models   and POS taggers  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distributions",
                "needed by previous POS-based language models and POS taggers",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "POS-based language models",
                "POS taggers",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Many researchers build alignment links with bilingual corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bilingual corpora",
                "are built with",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "task",
                "identifying positive and negative sentiments",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "text",
                "a passage, a sentence, a phrase and even a word",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One of the theoretical problems with phrase based SMT models is that they can not effectively model the discontiguous translations and numerous attempts have been made on this issue  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase based SMT models",
                "can not effectively model",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "numerous attempts",
                "have been made",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Since their appearance, string-based evaluation metrics such as BLEU   and NIST   have been the standard tools used for evaluating MT quality.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "string-based evaluation metrics",
                "standard tools",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "BLEU and NIST",
                "have been the standard",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Conjunctions are a major source of errors for English chunking as well  9, and we plan to address them in future work.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "conjunctions",
                "a major source of errors",
                "METHODOLOGY",
                "negative",
                0.7
            ],
            [
                "future work",
                "plan to address",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We used the Berkeley Parser4 to learn such grammars from Sections 2-21 of the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Berkeley Parser4",
                "learn such grammars",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank",
                "Sections 2-21",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This definition is similar to that of minimal translation units as described in Quirk and Menezes  , although they allow null words on either side.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "minimal translation units",
                "similar to that of",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Quirk and Menezes",
                "allow null words on either side",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Zens and Ney   compute the viterbi alignments for German-English and French-English sentences pairs using IBM Model 5, and then measure how many of the resulting alignments fall within the hard constraints of both Wu   and Berger et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 5",
                "compute viterbi alignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignments",
                "fall within hard constraints",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In our SMT system implementation, this optimization procedure is performed by using a tool developed in-house, which is based on a simplex method  , and the BLEU score   is used as a translation quality measurement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tool",
                "developed in-house",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "BLEU score",
                "used as a translation quality measurement",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "ikipedia first sentence  : Kazama and Torisawa   used Wikipedia as an external knowledge to improve Named Entity Recognition",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wikipedia",
                "as an external knowledge",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Named Entity Recognition",
                "improve",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "EnglishChinese   and EnglishSpanish  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "EnglishChinese",
                "and EnglishSpanish",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "EnglishChinese",
                "EnglishSpanish",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "An additional consistent edge of a linear-chain conditional random field   explicitly models the dependencies between distant occurrences of similar words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linear-chain conditional random field",
                "explicitly models the dependencies",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "similar words",
                "distant occurrences of",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "However, our representation of the model conceptually separates some of the hyperparameters which are not separated in  , and we found that setting these hyperparameters with different values from one another was critical for improving performance.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hyperparameters",
                "separates some of the hyperparameters which are not separated in",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "hyperparameters",
                "was critical for improving performance",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "It has also obtained competitive scores on general GR evaluation corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GR evaluation corpora",
                "obtained competitive scores",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "GR evaluation corpora",
                "competitive scores",
                "PERFORMANCE",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Each i is a weight associated with feature i, and these weights are typically optimized using minimum error rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights",
                "optimized using minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "minimum error rate training",
                "typically used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Later taggers have managed to improve Brills figures a little bit, to just above 97% on the Wall Street Journal corpus using Hidden Markov Models, HMM and Conditional Random Fields, CRF; e.g., Collins   and Toutanova et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brills figures",
                "improved a little bit",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "Hidden Markov Models",
                "HMM and Conditional Random Fields, CRF",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.5 Regularization We apply lscript1 regularization   to make learning more robust to noise and control the effective dimensionality of the feature spacebysubtractingaweightedsumofabsolutevalues of parameter weights from the log-likelihood of the training data w = argmaxw LL  summationdisplay i Ci|wi|   We optimize the objective using a variant of the orthant-wise limited-memory quasi-Newton algorithm proposed by Andrew & Gao  .3 All values Ci are set to 1 in most of the experiments below, although we apply stronger regularization   to reordering features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "regularization",
                "make learning more robust to noise",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "objective",
                "optimize using a variant of the orthant-wise limited-memory quasi-Newton algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The quality of the translation output is evaluated using BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "evaluated using",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "translation output",
                "quality",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "All the feature weights   were trained using our implementation of Minimum Error Rate Training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "were trained using our implementation of Minimum Error Rate Training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Minimum Error Rate Training",
                "our implementation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One is distortion model   which penalizes translations according to their jump distance instead of their content.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distortion model",
                "penalizes translations according to their jump distance instead of their content",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translations",
                "according to their jump distance",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Illustrative clusterings of this type can also be found in Pereira, Tishby, and Lee  , Brown, Della Pietra, Mercer, Della Pietra, and Lai  , Kneser and Ney  , and Brill et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pereira, Tishby, and Lee",
                "Illustrative clusterings of this type",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Brill et al.",
                "Illustrative clusterings of this type",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2 Phrase-based Statistical MT Our baseline is a standard phrase-based SMT system  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based SMT system",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase-based SMT system",
                "standard",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Parameter Optimization We optimize the feature weights using a modified version of averaged perceptron learning as described by Collins  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "using a modified version of averaged perceptron learning",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Collins",
                "described by",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This system uses all featuresof conventionalphrase-basedSMT as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features of conventional phrase-based SMT",
                "as in",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "conventional phrase-based SMT",
                "features",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Other statistical systems that address word classification probleans do not emphasize the use of linguistic knowledge and do not deal with a specific word class\\ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical systems",
                "do not emphasize the use of linguistic knowledge",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "other statistical systems",
                "do not deal with a specific word class",
                "METHODOLOGY",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "A variety of unsupervised WSD methods, which use a machinereadable dictionary or thesaurus in addition to a corpus, have also been proposed  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "use a machine-readable dictionary or thesaurus in addition to a corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "have also been proposed",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The training samples are respectively used to create the models PT^G, PCHUNK, PBUILD, and PCMECK, all of which have the form: k p  = II _ij  j----1 where a is some action, b is some context, ~\"\" is a nor4 Model Categories Description Templates Used TAG See   CHUNK chunkandpostag * BUILD CHECK chunkandpostag * cons  cons * cons  T punctuation checkcons * checkcons * production surround * The word, POS tag, and chunk tag of nth leaf.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "have the form: k p  = II _ij  j----1",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "chunk tag",
                "of nth leaf",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However, work in that direction has so far addressed only parse reranking  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "addressed only parse reranking",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "direction",
                "that direction",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A conditional maximum entropy model q  for p has the parametric form  : q  = exp T f   y2Y  exp )   where  is a d-dimensional parameter vector and T f   is the inner product of the parameter vector and a feature vector.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameter vector",
                "d-dimensional",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "feature vector",
                "inner product",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he patterns will be manually constructed following the approach of Hearst   and Nakov and Hearst  .6 The example collection for each relation R will be passed to two independent annotators",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "patterns",
                "following the approach of Hearst and Nakov",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "example collection",
                "passed to two independent annotators",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  showed how to use the Voted Perceptron algorithm for learning W, and we use it for learning the global transliteration model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "According to our experience, the best performance is achieved when the union of the source-to-target and target-to-source alignment sets   is used for tuple extraction  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "union of the source-to-target and target-to-source alignment sets",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "performance",
                "best",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "For each co-occurring pair of word types u and v, these likelihoods are initially set proportional to their co-occurrence frequency n  and inversely proportional to their marginal frequencies n  and n  z, following   2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word types u and v",
                "proportional to their co-occurrence frequency and inversely proportional to their marginal frequencies",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "likelihoods",
                "following 2",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We compared this nonprobabilistic DOP model against tile probabilistic DOP model   on three different domains: tbe Penn ATIS treebank  , the Dutch OVIS treebank   and tile Penn Wall Street Journal   treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "DOP model",
                "nonprobabilistic",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "DOP model",
                "probabilistic",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "METEOR uses the Porter stemmer and synonymmatching via WordNet to calculate recall and precision more accurately  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Porter stemmer",
                "to calculate recall and precision more accurately",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "WordNet",
                "via WordNet",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.2 Cast3LB Function Tagging For the task of Cast3LB function tag assignment we experimented with three generic machine learning algorithms: a memory-based learner  , a maximum entropy classifier   and a Support Vector Machine classifier  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning algorithms",
                "generic",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "machine learning algorithms",
                "memory-based learner, maximum entropy classifier, Support Vector Machine classifier",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 The Penn Discourse TreeBank   The PDTB contains annotations of discourse relations and their arguments on the Wall Street Journal corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PDTB",
                "contains annotations of discourse relations and their arguments",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Wall Street Journal corpus",
                "is used as",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In practice, texts contain an enormous number of word sequences  , only a tiny fraction of which are NCCs, and it takes considerable computational effort to induce each translation model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word sequences",
                "enormous number of",
                "PERFORMANCE",
                "neutral",
                0.85
            ],
            [
                "induce each translation model",
                "considerable computational effort",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In other words, learning with L1 regularization naturally has an intrinsic effect of feature selection, which results in an 97 efficient and interpretable inference with almost the same performance as L2 regularization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "L1 regularization",
                "naturally has an intrinsic effect of feature selection",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "L2 regularization",
                "almost the same performance as",
                "PERFORMANCE",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "Use of sententially aligned corpora for word alignment has already been recommended in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sententially aligned corpora",
                "recommended",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "word alignment",
                "has been",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Dunning   has called attention to the log-likelihood ratio, G 2, as appropriate for the analysis of such contingency tables, especially when such contingency tables concern very low frequency words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-likelihood ratio, G 2",
                "called attention to",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "contingency tables",
                "concern very low frequency words",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We delete all links in the set {a, an, the}  {DF, GI} from Ainitial as a preprocessing step.7 2.4 Perceptron Training We set the feature weights  using a modified version of averaged perceptron learning with structured outputs  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ainitial",
                "delete all links",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "perceptron learning",
                "modified version of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Recently, several solutions to the problem of tagging unknown words have been presented  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "solutions",
                "have been presented",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "problem of tagging unknown words",
                "has several solutions",
                "LIMITATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Maximum Entropy   modeling has received a lot of attention in language modeling and natural language processing for the past few years  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy",
                "has received a lot of attention",
                "INNOVATION",
                "positive",
                0.85
            ],
            [
                "language modeling and natural language processing",
                "for the past few years",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In particular, previous work   has investigated the use of Markov random fields   or log-linear models as probabilistic models with global features for parsing and other NLP tasks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Markov random fields",
                "probabilistic models with global features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "log-linear models",
                "probabilistic models with global features",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In practice, we used MMR in our experiments, since the original MEAD considers also sentence positions 3 , which can always been added later as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MMR",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentence positions",
                "can always been added later",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ROUGE has been used in meeting summarization evaluation  , yet the question remained whether ROUGE is a good metric for the meeting domain.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE",
                "a good metric",
                "PERFORMANCE",
                "neutral",
                0.75
            ],
            [
                "ROUGE",
                "for the meeting domain",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "For tuning of the decoders parameters, including the language model weight, minimum error training   with respect to the BLEU score using was conducted using the development corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoders parameters",
                "minimum error training with respect to the BLEU score using",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "development corpus",
                "was conducted using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "enkova and Louis   investigate how summary length and the characteristics of the input influence the summary quality in multi-document summarization",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "summary length",
                "influence summary quality",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "input characteristics",
                "characteristics of the input",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4.1 NER features We used the features generated by the CRF package  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CRF package",
                "generated features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "features",
                "generated by CRF package",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  and   modify the distortion model of the HMM alignment model   to reflect tree distance rather than string distance;   modify an ITG aligner by introducing a penalty for induced parses that violate syntactic bracketing constraints.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMM alignment model",
                "reflect tree distance rather than string distance",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "ITG aligner",
                "introducing a penalty for induced parses that violate syntactic bracketing constraints",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We use MXPOST tagger   for POS tagging, Charniak parser   for extracting syntactic relations, and David Blei?s version of LDA1 for LDA training and inference.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MXPOST tagger",
                "for POS tagging",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Charniak parser",
                "for extracting syntactic relations",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The learning algorithm used is the IB1 algorithm   with k = 5, i.e. classification based on 5 nearest neighbors.4 Distances are measured using the modified value difference metric     for instances with a frequency of at least 3  , and classification is based on distance weighted class voting with inverse distance weighting  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IB1 algorithm",
                "classification based on 5 nearest neighbors",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "distance weighted class voting with inverse distance weighting",
                "classification method",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Syntactic Score   Some erroneous sentences often contain words and concepts that are locally correct but cannot form coherent sentences  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words and concepts",
                "cannot form coherent sentences",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "syntactic score",
                "some erroneous sentences",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These findings are in line with Collins & Roarks   results with incremental parsing with perceptrons, where it is suggested that a generative baseline feature provides the perceptron algorithm with a much better starting point for learning.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "In this paper, we follow this line of research and try to solve the problem by extending Collins perceptron algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins perceptron algorithm",
                "extend",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "research line",
                "follow",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "An open question in SMT is whether there existsclosed formexpressions   for P   and the counts in the EM iterations for models 3-5  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "P",
                "closed form expressions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "models 3-5",
                "EM iterations",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3 Length Model: Dynamic Programming Given the word fertility de nitions in IBM Models  , we can compute a probability to predict phrase length: given the candidate target phrase   eI1, and a source phrase   of length J, the model gives the estimation of P  via a dynamic programming algorithm using the source word fertilities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "dynamic programming algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probability",
                "estimation via dynamic programming",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Maximum Entropy models have been used to express the interactions among multiple feature variables  ), but within this framework no systematic study of interactions has been proposed.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy models",
                "have been used to express the interactions among multiple feature variables",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "systematic study of interactions",
                "has not been proposed",
                "INNOVATION",
                "negative",
                0.75
            ]
        ]
    },
    {
        "text": "The phoneme prediction and sequence modeling are considered as tagging problems and a Perceptron HMM   is used to model it.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Perceptron HMM",
                "is used to model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tagging problems",
                "are considered as",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "157 ena or the linguist's abstraction capabilities  , they tend to reach a 95-97% accuracy in the analysis of several languages, in particular English  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linguist's abstraction capabilities",
                "reach a 95-97% accuracy",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "several languages",
                "in particular English",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Haghighi and Klein s   prototype-driven approach requires just a few prototype examples for each POS tag, exploiting these labeled words to constrain the labels of their distributionally similar words when training a generative log-linear model for POS tagging.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prototype-driven approach",
                "requires just a few prototype examples",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "generative log-linear model",
                "training a",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This is important when LARGE CUT-OFF 0 5 100 NAIVE 541,721 184,493 35,617 SASH 10,599 8,796 6,231 INDEX 5,844 13,187 32,663 Table 4: Average number of comparisons per term considering that different tasks may require different weights and measures  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LARGE CUT-OFF 0 5 100 NAIVE",
                "NA",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "different tasks",
                "may require different weights and measures",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction Most recent approaches in SMT, eg  , use a log-linear model to combine probabilistic features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT",
                "log-linear model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "probabilistic features",
                "combine",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the multilingual parsing track, participants train dependency parsers using treebanks provided for ten languages: Arabic  , Basque  , Catalan  , Chinese  , Czech  , English  , Greek  , Hungarian  , Italian  , and Turkish  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treebanks",
                "provided for ten languages",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "participants",
                "train dependency parsers",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As our approach for incorporating unlabeled data, we basically follow the idea proposed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "basically follow",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "idea",
                "proposed in  ",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use Minimal Error Rate Training   to maximize BLEU on the complete development data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Minimal Error Rate Training",
                "maximize BLEU",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "complete development data",
                "on",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "WSD is one of the fundamental problems in natural language processing and is important for applications such as machine translation    , information retrieval  , etc. WSD is typically viewed as a classification problem where each ambiguous word is assigned a sense label   during the disambiguation process.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSD",
                "fundamental problem",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "WSD",
                "important for applications",
                "APPLICABILITY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "We also trained a baseline model with GIZA++   following a regimen of 5 iterations of Model 1, 5 iterations of HMM, and 5 iterations of Model 4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "following a regimen of 5 iterations of Model 1, 5 iterations of HMM, and 5 iterations of Model 4",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Model 1",
                "5 iterations",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The marginal relevance systems   used a simple selection mechanism which does not involve search, inspired by the maximal marginal relevance   approach  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "marginal relevance systems",
                "simple selection mechanism",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "maximal marginal relevance approach",
                "inspired by",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Therefore, the results are more informative than a simple agreement average  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "results",
                "more informative",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "results",
                "than a simple agreement average",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "arpuat and Wu   approached the issue as a Word Sense Disambiguation problem",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "arpuat and Wu",
                "approached the issue as a Word Sense Disambiguation problem",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Table 6 shows 3An exception is Golding  , who uses the entire Brown corpus for training   and 3/4 of the Wall Street Journal corpus   for testing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brown corpus",
                "entire",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Wall Street Journal corpus",
                "3/4",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "So we will engineer more such features, especially with lexicalization and soft alignments  , and study the impact of alignment quality on parsing improvement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "more such features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "alignment quality",
                "impact on parsing improvement",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Such measures as mutual information  , latent semantic analysis  , log-likelihood ratio   have been proposed to evaluate word semantic similarity based on the co-occurrence information on a large corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measures",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "corpus",
                "large",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Then the initial precision is 1 , citing  , actually uses a superficially different score that is, however, a monotone transform of precision, hence equivalent to precision, since it is used only for sorting.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "precision",
                "uses a superficially different score",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "precision",
                "equivalent to precision",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "c2008 Association for Computational Linguistics Refining Event Extraction through Cross-document Inference   Heng Ji Ralph Grishman Computer Science Department New York University New York, NY 10003, USA  @cs.nyu.edu       Abstract We apply the hypothesis of One Sense Per Discourse   to information extraction  , and extend the scope of discourse from one single document to a cluster of topically-related documents.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "One Sense Per Discourse",
                "hypothesis",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "cluster of topically-related documents",
                "scope of discourse",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "urney   has recently proposed a simpler SVM-based algorithm for analogical classification called PairClass",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SVM-based algorithm",
                "simpler",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "PairClass",
                "proposed",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Related Work The first application of log-linear models to parsing is the work of Ratnaparkhi and colleagues  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ratnaparkhi and colleagues",
                "first application",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "log-linear models",
                "to parsing",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For a more detailed introduction to maximum entropy estimation see  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy estimation",
                "see",
                "INNOVATION",
                "neutral",
                1.0
            ],
            [
                "introduction",
                "see",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Talbot and Brants   show that Bloomier filters   can be used to create perfect hash functions for language models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bloomier filters",
                "can be used to create perfect hash functions",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "language models",
                "perfect hash functions",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "We use the same preprocessing steps as Turian and Melamed  : during both training and testing, the parser is given text POS-tagged by the tagger of Ratnaparkhi  , with capitalization stripped and outermost punctuation removed.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "given text POS-tagged by the tagger of Ratnaparkhi",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tagger of Ratnaparkhi",
                "POS-tagged",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms  , information retrieval  , determining semantic orientation  , grading student essays  , measuring textual cohesion  , and word sense disambiguation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measures of attributional similarity",
                "have been studied extensively",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "word sense disambiguation",
                "measuring",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5.2.1 Generate English Annotated Corpus from Wikipedia Wikipedia provides a variety of data resources for NER and other NLP research  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wikipedia",
                "variety of data resources",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Wikipedia",
                "provides",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "8 Related Research Class-based LMs   or factored LMs   are very similar to our T+C scenario.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Class-based LMs",
                "are very similar",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "factored LMs",
                "are very similar",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Previous research has focused on classifying subjective-versus-objective expressions  , and also on accurate sentiment polarity assignment  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "focused on classifying",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentiment polarity assignment",
                "accurate",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "5.2 NP Chunking The goal of this task   is the identification of non-recursive NPs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NP Chunking",
                "identification of non-recursive NPs",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "goal",
                "identification",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Feature function weights in the loglinear model are set using Ochs minium error rate algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Feature function weights",
                "set using Ochs minium error rate algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Ochs minium error rate algorithm",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Methods 4.1 Experiment 1: Held out data To examine the generalizability of classifiers trained on the automatically generated data, a C4.5 decision tree classifier   was trained and tested on the held out test set described above.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "C4.5 decision tree classifier",
                "trained",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "held out test set",
                "described above",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The study is conducted on both a simple Air Travel Information System   corpus   and the more complex Wall Street Journal   corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "simple and complex",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "corpora",
                "Wall Street Journal",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "They mention that the resulting shallow parse tags are somewhat different than those used by Ramshaw and Marcus  , but that they found no significant accuracy differences in training on either set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "shallow parse tags",
                "different than those used by Ramshaw and Marcus",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "accuracy differences",
                "no significant",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " , or in more recent implementation, the MOSES MT system1  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MOSES MT system",
                "more recent implementation",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "MOSES MT system",
                "MT system",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The assumptions we made were the following:  a lexical token in one half of the translation unit   corresponds to at most one non-empty lexical unit in the other half of the TU; this is the 1:1 mapping assumption which underlines the work of many other researchers  , Brew and McKelvie  , Hiemstra  , Kay and Rscheisen  , Tiedmann  , Melamed   etc);  a polysemous lexical token, if used several times in the same TU, is used with the same meaning; this assumption is explicitly used by Gale and Church  , Melamed   and implicitly by all the previously mentioned authors;  a lexical token in one part of a TU can be aligned to a lexical token in the other part of the TU only if the two tokens have compatible types  ; in most cases, compatibility reduces to the same POS, but it is also possible to define other compatibility mappings  ;  although the word order is not an invariant of translation, it is not random either  ); when two or more candidate translation pairs are equally scored, the one containing tokens which are closer in relative position are preferred.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "1:1 mapping assumption",
                "underlines the work of many other researchers",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "compatible types",
                "compatible reduces to the same POS, but it is also possible to define other compatibility mappings",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Doing inference with representations close to natural language has also been advocated by Jerry Hobbs, as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "representations",
                "close to natural language",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Hobbs",
                "advocated",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Translation performance was measured using the automatic BLEU evaluation metric   on four reference translations",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation metric",
                "BLEU",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "reference translations",
                "four",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4 5 Experiments 5.1 Evaluation Measures We evaluated the proposed method using four evaluation measures, BLEU  , NIST  , WER , and PER .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation measures",
                "BLEU, NIST, WER, and PER",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "proposed method",
                "using four evaluation measures",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Mathematical details are fully described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "details",
                "are fully described",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Mathematical",
                "are fully described",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In  , the authors proposed a method to integrate the IBM translation model 2   with an ASR system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM translation model",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ASR system",
                "integrate with",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Method Source Spearman   Wikipedia 0.190.48   WordNet 0.330.35   Rogets 0.55   WordNet 0.55   Web corpus, WN 0.56   ODP 0.65   Wikipedia 0.75 SVM Web corpus, WN 0.78 Table 9: Comparison with previous work for WordSim353.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Spearman",
                "Method Source",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "WordSim353",
                "Comparison with previous work",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "All features encountered in the training data are ranked in the DL   according to the following loglikelihood ratio  : Log Pr  P j6=i Pr  We estimated probabilities via maximum likelihood, adopting a simple smoothing method  : 0.1 is added to both the denominator and numerator.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "ranked in the DL according to the following loglikelihood ratio",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "smoothing method",
                "adopting a simple",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We compare those algorithms to generalized iterative scaling    , non-preconditioned CG, and voted perceptron training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "generalized iterative scaling, non-preconditioned CG, and voted perceptron training",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "algorithms",
                "to",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4 Analysis of Experimental Data Most of the existing research in computational linguistics that uses human annotators is within the framework of classification, where an annotator decides, for every test item, on an appropriate tag out of the pre-specified set of tags  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "existing research",
                "uses human annotators",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "annotation framework",
                "within the framework of classification",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Discriminatively trained parsers that score entire trees for a given sentence have only recently been investigated  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing",
                "entire trees for a given sentence",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parsers",
                "have only recently been investigated",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "BLEU: BLEU score, which computes the ratio of n-gram for the translation results found in reference translations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "computes the ratio of n-gram for the translation results",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation results",
                "found in reference translations",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Church, K. and Hanks, P. ,   \"\"Word Association Norms, Mutual Information, and Lexicography,\"\" Computational Linguistics Vol.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Word Association Norms",
                "Mutual Information",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Mutual Information",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The traditional estimation method for word 98 alignment models is the EM algorithm   which iteratively updates parameters to maximize the likelihood of the data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "EM algorithm",
                "iteratively updates parameters to maximize the likelihood of the data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "EM algorithm",
                "traditional estimation method",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Many researchers have focused the related problem of predicting sentiment and opinion in text  , sometimes connected to extrinsic values like prediction markets  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "problem",
                "related problem of predicting sentiment and opinion in text",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "extrinsic values",
                "connected to",
                "APPLICABILITY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "To further emphasize the importance of morphology in MT to Czech, we compare the standard BLEU   of a baseline phrasebased translation with BLEU which disregards word forms  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "standard",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "BLEU",
                "which disregards word forms",
                "PERFORMANCE",
                "negative",
                0.75
            ]
        ]
    },
    {
        "text": "The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Riezler and Maxwell   combine transfer-based and statistical MT; they back off to the SMT translation when the grammar is inadequate, analysing the grammar to determine this.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transfer-based and statistical MT",
                "combine",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "grammar",
                "inadequate",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "This has been shown both in supervised settings   and unsupervised settings   in which constraints are used to bootstrap the model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "bootstrap",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "constraints",
                "are used",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Mean number of instances of paraphrase phenomena per sentence  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phenomena",
                "per sentence",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "instances of paraphrase",
                "number of",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Moses provides BLEU   and NIST  , but Meteor   and TER   can easily be used instead.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "is provided",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "Meteor and TER",
                "can easily be used instead",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.2 Learning Algorithm For learning coreference decisions, we used a Maximum Entropy   model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "Maximum Entropy",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Maximum Entropy",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Starting from the parallel training corpus, provided with direct and inverted alignments, the socalled union alignment   is computed.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parallel training corpus",
                "computed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "union alignment",
                "computed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Part-of-Speech   annotation for example can be seen as the task of choosing the appropriate tag for a word from an ontology of word categories  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word categories",
                "ontology of word categories",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tag",
                "choosing the appropriate tag",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "All submitted runs were evaluated with the automatic metrics: ROUGE  , which calculates the proportion of n-grams shared between the candidate summary and the reference summaries, and Basic Elements  , which compares the candidate to the models in terms of head-modifier pairs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE",
                "calculates the proportion of n-grams shared",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Basic Elements",
                "compares the candidate to the models",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In addition to portability experiments with the parsing model of  ,   provided a comprehensive analysis of parser portability.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing model",
                "provided a comprehensive analysis",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parser portability",
                "portability experiments",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A possible solution to his problem might be the use of more general morphological rules like those used in part-of-speech tagging models  ), where all suffixes up to a certain length are included.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "morphological rules",
                "those used in part-of-speech tagging models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "suffixes",
                "up to a certain length are included",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers   have used their class scheme and data set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "class scheme",
                "two-level hierarchy",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "class scheme",
                "5 classes at the first level and 30 classes at the second one",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We run the decoder with its default settings   and then use Koehn's implementation of minimum error rate training   to tune the feature weights on the de2 The full name of HTRDP is National High Technology Research and Development Program of China, also named as 863 Program.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "default settings",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "feature weights",
                "tune with Koehn's implementation of minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "hang and Clark   indicated that their results cannot directly compare to the results of Shi and Wang   due to different experimental settings",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "experimental settings",
                "different",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "results",
                "cannot directly compare",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Word association norms, mutual information, and lexicography, Computational Linguistics, 16 : 22-29 Marcus, M. et al. 1993.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Word association norms",
                "mutual information",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "lexicography",
                "Computational Linguistics",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": " ), concordancing for bilingual lexicography  , computerassisted language learning, corpus linguistics (Melby.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "concordancing",
                "for bilingual lexicography",
                "APPLICABILITY",
                "neutral",
                0.85
            ],
            [
                "corpus linguistics",
                "Melby",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We report BLEU scores   on untokenized, recapitalized output.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU scores",
                "on untokenized, recapitalized output",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Similarly,   tested his WSD algorithm on a dozen words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSD algorithm",
                "tested",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "words",
                "a dozen",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The TRIPS structure generally has more levels of structure   than the Penn Treebank analyses  , in particular for base noun phrases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TRIPS structure",
                "has more levels of structure",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank analyses",
                "has fewer levels of structure",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Nevertheless, EM sometimes fails to find good parameter values.2 The reason is that EM tries to assign roughly the same number of word tokens to each of the hidden states  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "EM",
                "tries to assign roughly the same number of word tokens to each of the hidden states",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "parameter values",
                "find good",
                "PERFORMANCE",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "ang and Lee   report 87.15% accuracy using a unigram-based SVM classifier combined with subjectivity detection",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifier",
                "unigram-based SVM",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "accuracy",
                "87.15%",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Their approaches include the use of a vector-based information retrieval technique   /bin/bash: line 1: a: command not found Our do- mains are more varied, which may results in more recognition errors.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "vector-based information retrieval technique",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "domains",
                "more varied",
                "METHODOLOGY",
                "neutral",
                0.75
            ]
        ]
    },
    {
        "text": "The former extracts collocations within a fixed window  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "extracts",
                "within a fixed window",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "window",
                "fixed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Results are reported using lowercase BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "reported using",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "BLEU",
                "lowercase",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "To avoid this problem, generative models for NLP tasks have often been manually designed to achieve an appropriate representation of the joint distribution, such as in the parsing models of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generative models",
                "achieve an appropriate representation of the joint distribution",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parsing models",
                "of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "While traditional approaches to syntax based MT were dependent on availability of manual grammar, more recent approaches operate within the resources of PB-SMT and induce hierarchical or linguistic grammars from existing phrasal units, to provide better generality and structure for reordering  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "traditional approaches to syntax based MT",
                "dependent on availability of manual grammar",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "PB-SMT",
                "provide better generality and structure for reordering",
                "PERFORMANCE",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "We believe the benefit to limiting the size of n is connected to Brown et al.s   observation that as n increases, the accuracy of an n-gram model increases, but the reliability of our parameter estimates, drawn as they must be from a limited training text, decreases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n",
                "increases",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parameter estimates",
                "reliability decreases",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "Collocation Dictionary of Modern Chinese Lexical Words, Business Publisher, China Yuan Liu, et al. 1993.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collocation Dictionary of Modern Chinese Lexical Words",
                "Business Publisher, China Yuan Liu, et al. 1993",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "Business Publisher",
                "China Yuan Liu, et al.",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Inter-sentential contexts as in our approach were used as a clue also for subjectivity analysis  , which is two-fold classification into subjective and objective sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Inter-sentential contexts",
                "as a clue",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "subjectivity analysis",
                "two-fold classification",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In Step 3, a simple perceptron update   is performed.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron update",
                "is performed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "perceptron update",
                "simple",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One aspect of VPCs that makes them dicult to extract  ) is that the verb and particle can be non-contiguous, e.g. hand the paper in and battle right on.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "VPCs",
                "make them difficult to extract",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "verb and particle",
                "can be non-contiguous",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In addition to sentence fusion, compression algorithms   and methods for expansion of a multiparallel corpus   are other instances of such methods.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "such methods",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpus",
                "multiparallel",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Note that all systems were optimized using a non-deterministic implementation of the Minimum Error Rate Training described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Minimum Error Rate Training",
                "non-deterministic implementation",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "systems",
                "were optimized",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Although LDD annotation is actually provided in Treebanks such as the Penn Treebank   over which they are typically trained, most probabilistic parsers largely or fully ignore this information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LDD annotation",
                "are actually provided",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "probabilistic parsers",
                "largely or fully ignore",
                "METHODOLOGY",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "These algorithms are usually applied to sequential labeling or chunking, but have also been applied to parsing  , machine translation   and summarization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "applied to sequential labeling or chunking, parsing, machine translation, and summarization",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithms",
                "usually",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.2 String-Based Evaluation We evaluate the output of our generation system against the raw strings of Section 23 using the Simple String Accuracy and BLEU   evaluation metrics.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generation system",
                "output",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Simple String Accuracy and BLEU evaluation metrics",
                "evaluation metrics",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "BLEU Score: BLEU is an automatic metric designed by IBM, which uses several references  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "designed by IBM",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "references",
                "several",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio   and outputted to a lexicon.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Longman English Language Corpus",
                "seven million word sample",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "association ratio",
                "outputted to a lexicon",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We observe that the tagging method exploits the one sense per collocation property  , which means that WSD based on collocations is probably finer than WSD based on simple words, since ambiguity is reduced  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagging method",
                "exploits the one sense per collocation property",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "WSD based on collocations",
                "is probably finer",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "ollocation map that is first suggested in   is a sigmoid belief network with words as probabilistic variables",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sigmoid belief network",
                "suggested in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "words",
                "probabilistic variables",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our first model   is based on disambiguating the MA output in the maximum entropy   framework  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "based on disambiguating the MA output",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "maximum entropy framework",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": ".2 Data sparseness Another facet of the general trade-off identified by Rapp   pertains to how limitations in862 herent in the combination of data and cooccurrence retrieval method are manifest",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data sparseness",
                "manifest",
                "LIMITATION",
                "neutral",
                0.95
            ],
            [
                "combination of data and cooccurrence retrieval method",
                "manifest",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Non-anaphoric definite descriptions have been detected using heuristics  ) and unsupervised methods  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "heuristics",
                "have been detected",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "unsupervised methods",
                "have been detected",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We collect substring rationales for a sentiment classification task   and use them to obtain significant accuracy improvements for each annotator.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rationales",
                "significant accuracy improvements",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "annotator",
                "each",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For the identification and labeling steps, we train a maximum entropy classifier   over sections 02-21 of a version of the CCGbank corpus   that has been augmented by projecting the Propbank semantic annotations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CCGbank corpus",
                "has been augmented by projecting the Propbank semantic annotations",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "maximum entropy classifier",
                "trained over sections 02-21",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Some of the early statistical terminology translation methods are  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical terminology translation methods",
                "early",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "statistical terminology translation methods",
                "some of",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "However, more recent work   has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank  , containing more than 1,000,000 words and 49,000 sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotation techniques",
                "evolving and scaling up",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Penn-II Treebank",
                "containing more than 1,000,000 words and 49,000 sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "During evaluation two performance metrics, BLEU   and NIST, were computed.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "performance metrics",
                "BLEU and NIST",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "performance metrics",
                "computed",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The Spanish corpus was parsed using the MST dependency parser   trained using dependency trees generated from the the English Penn Treebank   and Spanish CoNLL-X data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MST dependency parser",
                "trained using dependency trees generated from the English Penn Treebank and Spanish CoNLL-X data",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dependency trees",
                "generated from the English Penn Treebank and Spanish CoNLL-X data",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Various methods   of automatically acquiring synonyms have been proposed.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "have been proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "synonyms",
                "proposed",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Automatic identification of subjective content often relies on word indicators, such as unigrams   or predetermined sentiment lexica  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unigrams",
                "word indicators",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "predetermined sentiment lexica",
                "predetermined sentiment lexica",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "These sentences were parsed with the Collins parser  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "Collins parser",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentences",
                "parsed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As machine learners we used SVM-light1   and the MaxEnt decider from the Stanford Classifier2  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SVM-light",
                "machine learners used",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "MaxEnt decider",
                "from the Stanford Classifier",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Of particular relevance is other work on parsing the Penn WSJ Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn WSJ Treebank",
                "is of particular relevance",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "other work on parsing",
                "is other work",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "As the strength of relevance between a target compound noun t and its co-occurring word r, the feature value of r, w  is deflned by the log likelihood ratio   1 as follows.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "target compound noun t",
                "log likelihood ratio",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "feature value of r",
                "defined by",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The straight-forward way is to first generate the best BTG tree for each sentence pair using the way of  , then annotate each BTG node with linguistic elements by projecting source-side syntax tree to BTG tree, and finally extract rules from these annotated BTG trees.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BTG tree",
                "best",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "rules",
                "extract",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The Chinese text was tagged using the MXPOST maximum-entropy part of speech tagging tool   trained on the Penn Chinese Treebank 5.1; the English text was tagged using the TnT part of speech tagger   trained on the Wall Street Journal portion of the English Penn treebank.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MXPOST maximum-entropy part of speech tagging tool",
                "trained on Penn Chinese Treebank 5.1",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "TnT part of speech tagger",
                "trained on Wall Street Journal portion of English Penn treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The XEROX tagger comes with a list of built-in ending guessing rules  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "XEROX tagger",
                "built-in ending guessing rules",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "ending guessing rules",
                "comes with",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The limited contexts used in this model are similar to the previous methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "similar to the previous methods",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "methods",
                "previous",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Then, h   h  + Lmax, s  S. This epsilon1-admissible heuristic   bounds our search error by Lmax.3 3 Bitext Parsing In bitext parsing, one jointly infers a synchronous phrase structure tree over a sentence ws and its translation wt  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Lmax",
                "bounds our search error",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "bitext parsing",
                "infers a synchronous phrase structure tree",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Architecture of the system The goal of statistical machine translation   is to produce a target sentence e from a source sentence f. It is today common practice to use phrases as translation units   and a log linear framework in order to introduce several models explaining the translation process: e = argmaxp  = argmaxe {exp )}   The feature functions hi are the system models and the i weights are typically optimized to maximize a scoring function on a development set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "log linear framework",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "scoring function",
                "maximize on a development set",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "However, instead of estimating the probabilities for the production rules via EM as described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "production rules",
                "estimating the probabilities via EM",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "EM",
                "described in.",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "First, two maximum entropy classifiers   are applied, where the first predicts clause start labels and the second predicts clause end labels.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifiers",
                "predicts clause start labels and clause end labels",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "predicts",
                "labels",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  applied iterative refinement algorithms to sentence level alignment tasks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "iterative refinement",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentence level alignment tasks",
                "sentence level",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We also note that Turney   found movie reviews to be the most 2Indeed, although our choice of title was completely independent of his, our selections were eerily similar.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Turney",
                "found movie reviews",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "our selections",
                "eerily similar",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "We utilize maximum entropy   model   to design the basic classifier used in active learning for WSD and TC tasks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy model",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "basic classifier",
                "used in active learning",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "a list of pilot terms ranked from the most representative of the corpus to the least thanks to the Loglikelihood coefficient introduced by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pilot terms",
                "ranked from the most representative of the corpus to the least",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Loglikelihood coefficient",
                "introduced by",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Thus, Collins   also proposed an averaged perceptron, where the nal weight vector is 1Collins alsoprovidedproofthatguaranteedgood learning for the non-separable case.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron",
                "averaged perceptron",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "weight vector",
                "is 1",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Several researchers  ) work on reducing the granularity of sense inventories for WSD.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sense inventories",
                "reducing the granularity",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "researchers",
                "work on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "TheChinesesentencefromtheselected pair is used as the single reference to tune and evaluate the MT system with word-based BLEU-4  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TheChinesesentencefromtheselected pair",
                "single reference",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU-4",
                "word-based",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We also report the result of our translation quality in terms of both BLEU   and TER   against four human reference translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation quality",
                "in terms of BLEU and TER against four human reference translations",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "result",
                "reported",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The SemEval-2010 task we present here builds on thework ofNakov  , where NCs are paraphrased by combinations of verbs and prepositions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SemEval-2010 task",
                "builds on the work of",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Nakov",
                "work",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "The grammars were induced from sections 2-21 of the Penn Wall St. Journal Treebank  , and tested on section 23.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammars",
                "induced from sections 2-21 of the Penn Wall St. Journal Treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "grammars",
                "tested on section 23",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 Experimental Data The sense-tagged text and feature set used in these experiments are the same as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sense-tagged text",
                "same as in ",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "feature set",
                "same as in ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The other main difference is the apparently nonlocal nature of the problem, which motivates our choice of a Maximum Entropy   model for the tagging task  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy",
                "model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "nonlocal nature",
                "problem",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Most work on corpora of naturally occurring language 244 Michael R. Brent From Grammar to Lexicon either uses no a priori grammatical knowledge  , or else it relies on a large and complex grammar  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora of naturally occurring language",
                "uses no a priori grammatical knowledge",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "large and complex grammar",
                "relies on",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We train IBM Model-4 using GIZA++ toolkit   in two translation directions and perform different word alignment combination.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model-4",
                "using GIZA++ toolkit",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word alignment combination",
                "different",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "First, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases   and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "shallow parsing information",
                "sufficient",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "noun phrases and other syntactic sequences",
                "useful",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For the chunk part of the code, we adopt the Inside, Outside, and Between   encoding originating from  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunk part of the code",
                "Inside, Outside, and Between encoding originating from",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Inside, Outside, and Between encoding",
                "originating from",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "6 Related Work and Discussion There are several studies that used automatically extracted gazetteers for NER  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "gazetteers",
                "for NER",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "studies",
                "used automatically extracted",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "However, they use the   data set in a different training-test division   which makes it (tifficult to compare their results with others.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data set",
                "different training-test division",
                "LIMITATION",
                "negative",
                0.8
            ],
            [
                "results",
                "difficult to compare with others",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We used sections 220 of the Penn Treebank 2 Wall Street Journal corpus   for training, section 22 as development set and section 23 for testing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank 2 Wall Street Journal corpus",
                "used for training",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "section 220",
                "as development set",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": " , who retrain the Ratnaparkhi   tagger and reach accuracies of 93% using CTB-I.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ratnaparkhi tagger",
                "retrain",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "accuracies of 93%",
                "using CTB-I",
                "PERFORMANCE",
                "positive",
                0.95
            ]
        ]
    },
    {
        "text": "This paper demonstrates several of the characteristics and benefits of SemFrame  , a system that produces such a resource.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SemFrame",
                "produces such a resource",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "characteristics and benefits",
                "of",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Future work will include:   applying the method to retrieve other types of collocations  , and   evaluating the method using Internet directories.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "applying to retrieve other types of collocations",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "method",
                "evaluating using Internet directories",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The various extraction measures have been discussed in great detail in the literature  , their performance has been compared  , and the methods have been combined to improve overall performance  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "extraction measures",
                "have been discussed in great detail",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "methods",
                "have been combined to improve overall performance",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Summarizing spoken documents has been extensively studied over the past several years  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "spoke documents",
                "has been extensively studied",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "years",
                "past several",
                "LIMITATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "We used a loglinear model with no Markov dependency between adjacent tags,3 and trained the parameters of the model with the perceptron algorithm, with averaging to control for over-training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "loglinear model",
                "with no Markov dependency",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "perceptron algorithm",
                "with averaging to control for over-training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Many systems   use these relationships as an intermediate, form when determining the semantics of syntactically parsed text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "systems",
                "as an intermediate",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "semantics of syntactically parsed text",
                "when determining",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Here, we train word alignments in both directions with GIZA++  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "word alignments",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "GIZA++",
                "train",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One can imagine the same techniques coupled with more informative probability distributions, such as lexicalized PCFGs  , or even grammars not based upon literal rules, but probability distributions that describe how rules are built up from smaller components  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques",
                "coupled with more informative probability distributions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "probability distributions",
                "describe how rules are built up from smaller components",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "or colnparison~ we refer here to Smadja's method   because this method and the proposed method have much in connnon",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Smadja's method",
                "this method and the proposed method have much in common",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Smadja's method",
                "because",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2We use a POS tagger   trained on switchboard data with the additional tags of FP   and FRAG  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tagger",
                "trained on switchboard data with additional tags FP and FRAG",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "switchboard data",
                "additional tags FP and FRAG",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Text similarity has been also used for relevance feedback and text classification  , word sense disambiguation  , and more recently for extractive summarization  , and methods for automatic evaluation of machine translation   or text summarization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "text similarity",
                "used for relevance feedback and text classification",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods for automatic evaluation",
                "of machine translation or text summarization",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We have used three different algorithms: the nearest neighbour algorithm IB1IG, which is part of the Timbl software package  , the decision tree learner IGTREE, also from Timbl, and C5.0, a commercial version of the decision tree learner C4.5  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IB1IG",
                "part of the Timbl software package",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "C5.0",
                "commercial version of the decision tree learner C4.5",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4 Pattern switching The compositional translation presents problems which have been reported by  : Fertility SWTs and MWTs are not translated by a term of a same length.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pattern switching",
                "presents problems",
                "LIMITATION",
                "neutral",
                0.8
            ],
            [
                "Fertility SWTs and MWTs",
                "are not translated by a term of a same length",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ote that Row 3 of Table 3 corresponds to Marcu and Echihabi  s system which applies only word pair features",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Marcu and Echihabi's system",
                "applies only word pair features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Table 3",
                "Row 3",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "u   and Jones and Havrilla   have sought to more closely tie the allowed motion of constituents between languages to those syntactic transductions supported by the independent rotation of parse tree constituents",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "constituents",
                "allowed motion",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntactic transductions",
                "supported by independent rotation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Aligning parallel texts has recently received considerable attention  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "attention",
                "has recently received considerable",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "parallel texts",
                "has received considerable attention",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4 Data analysis To test the reliability of the annotation, we first considered the kappa statistic   which is used extensively in empirical studies of discourse  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa statistic",
                "used extensively",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "annotation",
                "tested for reliability",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This approach is similar to that of seed words  ) or hook words  ) in previous work.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "seed words",
                "similar to",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "previous work",
                "previous work",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We implemented an N-gram indexer/estimator using MPI inspired by the MapReduce implementation of N-gram language model indexing/estimation pipeline  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "N-gram indexer/estimator",
                "using MPI inspired by the MapReduce implementation of N-gram language model indexing/estimation pipeline",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "MPI",
                "inspired by",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Phrase-based SMT We use a phrase-based SMT system, Pharaoh,  , which is based on a log-linear formulation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pharaoh",
                "log-linear formulation",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "SMT system",
                "phrase-based",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We assign tags of part-of-speech   to the words with MXPOST that adopts the Penn Treebank tag set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MXPOST",
                "adopts the Penn Treebank tag set",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank tag set",
                "used",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger  , the YamCha chunker   and the Stanford Named Entity Recognizer  , the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnsons reranking parser   to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stanford POS tagger",
                "classical combination",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Charniak and Johnson's reranking parser",
                "assign POS tags",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The tree is produced by a state-of-the-art dependency parser   trained on the Wall Street Journal Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "state-of-the-art",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Recently, Bean and Riloff   presented an unsupervised approach to coreference resolution, which mined the co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "unsupervised",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpus",
                "large",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "More recent papers Hindle  , Pereira and Tishby   proposed to cluster nouns on the basis of a metric derived from the distribution of subject, verb and object in the texts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metric",
                "derived from the distribution of subject, verb and object in the texts",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "papers",
                "proposed to cluster nouns",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Barzilay and Lee   proposed to apply multiple-sequence alignment   for traditional, sentence-level PR.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "multiple-sequence alignment",
                "for traditional, sentence-level PR",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "traditional, sentence-level PR",
                "traditional",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Moreover, as P-DOP is formulated as an enrichment of the treebank Probabilistic Context-free Grammar  , it allows for much easier comparison to alternative approaches to statistical parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Probabilistic Context-free Grammar",
                "alternative approaches to statistical parsing",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "P-DOP",
                "formulated as an enrichment",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Riloff and Jones 1999) note that the bootstrapping algorithm works well but its performance can deteriorate rapidly when non-coreferring data enter as candidate heuristics",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "works well",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "performance",
                "can deteriorate rapidly",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Parse selection constitutes an important part of many parsing systems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing systems",
                "important part of",
                "APPLICABILITY",
                "neutral",
                0.85
            ],
            [
                "parse selection",
                "constitutes",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Any linguistic annotation required during the extraction process, therefore, is produced through automatic means, and it is only for reasons of accessibility and comparability with other research that we choose to work over the Wall Street Journal section of the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "Wall Street Journal section",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "research",
                "for reasons of accessibility and comparability with other",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "MET   was carried out using a development set, and the BLEU score evaluated on two test sets.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "evaluated on two test sets",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "development set",
                "carried out using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As a solution, a given amount of labeled training data is divided into two distinct sets, i.e., 4/5 for estimating , and the 667 remaining 1/5 for estimating   .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "labeled training data",
                "divided into two distinct sets",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "remaining 1/5",
                "for estimating",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This framework is 211 commonly used in generation and summarization applications where the selection process is driven by multiple constraints  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "framework",
                "commonly used",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "selection process",
                "driven by multiple constraints",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In comparison,   achieved 48 Table 1: A summary of the experimental results on four polysemous words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "experimental results",
                "summary",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "polysemous words",
                "four",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Second, we discuss the work done by   who use clustering of paraphrases to induce rewriting rules.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "who",
                "clustering of paraphrases",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "rewriting rules",
                "induced",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The model scaling factors 1,,5 and the word and phrase penalties are optimized with respect to some evaluation criterion  , e.g. BLEU score.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model scaling factors",
                "optimized",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word and phrase penalties",
                "optimized with respect to some evaluation criterion",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The first one, GIZA-Lex, is obtained by running the GIZA++2 implementation of the IBM word alignment models   on the initial parallel corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA-Lex",
                "obtained by running the GIZA++2 implementation of the IBM word alignment models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "GIZA++2 implementation",
                "of the IBM word alignment models",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction and Motivation Detecting contradictory statements is an important and challenging NLP task with a wide range of potential applications including analysis of political discourse, of scientific literature, and more  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "task",
                "important and challenging",
                "INNOVATION",
                "positive",
                0.7
            ],
            [
                "task",
                "wide range of potential applications",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In their presentation of the factored SMT models, Koehn and Hoang   describe experiments for translating from English to German, Spanish and Czech, using morphology tags added on the morphologically rich side, along with POS tags.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT models",
                "factored",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "morphology tags",
                "added on the morphologically rich side",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Obtained percent agreement of 0.988 and  coefficient   of 0.975 suggest high convergence of both annotations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "percent agreement",
                "0.988",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "coefficient of",
                "0.975",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "With IOB2 representation  , the problem of Chinese chunking can be regarded as a sequence labeling task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IOB2 representation",
                "regarded as",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sequence labeling task",
                "can be regarded",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "D. Hindle, Noun classification from predicate argument structures, in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "predicate argument structures",
                "from",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "noun classification",
                "from",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The discrepancy between DEV performance and TEST performance is due to temporal distance from TRAIN and high variance in BLEU score.11 We also compared our model with Pharaoh  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "temporal distance",
                "from TRAIN",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU score",
                "high variance",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Both techniques implement variations on the approaches of   and   for the purpose of differentiating between complement and adjunct.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "differentiating between complement and adjunct",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "techniques",
                "implement variations on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A ~ value of 0.8 or greater indicates a high level of reliability among raters, with values between 0.67 and 0.8 indicating only moderate agreement  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "value",
                "0.8 or greater",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "value",
                "values between 0.67 and 0.8",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Traditionally, maximum-likelihood estimation from relative frequencies is used to obtain conditional probabilities  , eg, p  = c /summationtexts c    and p  are symmetrical, we will usually refer only to p  for brevity).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "relative frequencies",
                "used to obtain conditional probabilities",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "p",
                "are symmetrical",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "arowsky   has used a few seeds and untagged sentences in a bootstrapping algorithm based on decision lists",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "seeds",
                "a few",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "bootstrapping algorithm",
                "based on decision lists",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Table 1 shows a summary of the results of our experiments with SVMpar and MBLpar, and also results obtained with the Charniak   parser, the Bikel   implementation of the Collins   parser, and the Ratnaparkhi   parser.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SVMpar",
                "and also results obtained with",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Charniak parser",
                "implementation of the Collins parser",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 SMT Experiments 5.1 Experimental Setup We used publicly available resources for all our tests: for decoding we used Moses   and our parallel data was taken from the Spanish-English section of Europarl.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Moses",
                "publicly available",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Similar to  , each word in the hypothesis is assigned with a rank-based score of 1/ r+ , where r is the rank of the hypothesis.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hypothesis",
                "rank-based score of 1/r+",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "rank",
                "r",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Instead of using Inversion Transduction Grammar     directly, we will discuss an ITG extension to accommodate gapping.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG extension",
                "accommodate gapping",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Inversion Transduction Grammar",
                "directly",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "arowsky   used both supervised and unsupervised WSD for correct phonetizitation of words in speech synthesis",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSD",
                "correct phonetizitation of words",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "WSD",
                "for speech synthesis",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "l lhmsetsu ideni,illcation is a ln'oblem similar to ohm,king   in other l;mguages.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "idemi,illcation",
                "similar to ohm,king",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "l;mguages",
                "other",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Ours is 772 similar to   in the use of dependency relationship as the word features, based on which word similarities are computed.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ours",
                "similar to",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word features",
                "based on which word similarities are computed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use the minimum-error rate training procedure by Och   as implemented in the Moses toolkit to set the weights of the various translation and language models, optimizing for BLEU.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation and language models",
                "weights set by Och's minimum-error rate training procedure",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU",
                "optimizing for",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "We tested the techniques described above with the previous Bakeoffs data5  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques",
                "described above",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "data5",
                "Bakeoffs data",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There are studies on learning subjective language  , identifying opinionated documents   and sentences  , and discriminating between positive and negative language  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "learning",
                "subjective language",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "language",
                "positive and negative",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The Log-Likelihood-Ratio Association Measure We base all our association-based word-alignment methods on the log-likelihood-ratio   statistic introduced to the NLP community by Dunning  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Log-Likelihood-Ratio Association Measure",
                "introduced by Dunning",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "log-likelihood-ratio statistic",
                "introduced to the NLP community",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "More specifically, the latter system uses the IBM-1 lexical parameters   for computing the translation probabilities of two possible new tuples: the one resulting when the null-aligned-word is attached to Table 6 Evaluation results for experiments on n-gram size incidence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM-1 lexical parameters",
                "computing translation probabilities",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "n-gram size incidence",
                "evaluation results",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Recent advances in these approaches include the use of a fully Bayesian HMM  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMM",
                "fully Bayesian",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approaches",
                "recent advances",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Mostcommonlyvariational   or sampling techniques are applied  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "variational",
                "sampling techniques",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sampling techniques",
                "are applied",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The Duluth Word Alignment System is a Perl implementation of IBM Model 2  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Duluth Word Alignment System",
                "Perl implementation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM Model 2",
                "implementation",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "A natural fit to the existing statistical machine translation framework  A metric that ranks a good translation high in an nbest list could be easily integrated in a minimal error rate statistical machine translation training framework  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metric",
                "ranks a good translation high",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "training framework",
                "minimal error rate statistical machine translation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 A bilingual language model  ITG Wu   has proposed a bilingual language model called Inversion Transduction Grammar  , which can be used to parse bilingual sentence pairs simultaneously.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG Wu",
                "proposed",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Inversion Transduction Grammar",
                "can be used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We design special inference algorithms, instead of general-purpose inference algorithms used in previous works  , by taking advantage of special properties of our task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "special inference algorithms",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "properties",
                "special properties of our task",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "6.4 Feature Selection Methods A number of previous papers   describe feature selection approaches for log-linear models applied to NLP problems.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature selection methods",
                "describe feature selection approaches",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "log-linear models",
                "applied to NLP problems",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This model can be seen as an extension of the standard Maximum Entropy Markov Model  ) with an extra dependency on the predicate label, we will henceforth refer to this model as MEMM+pred.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy Markov Model",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "MEMM+pred",
                "extension",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "  propose a MaxEnt-based reordering model for BTG   while Setiawan et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reordering model",
                "MaxEnt-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "reordering model",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Dependency relations are produced using a version of the Collins parser   that has been adapted for building dependencies.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins parser",
                "adapted for building dependencies",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "version of the Collins parser",
                "has been",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "We use the same set of binary features as in previous work on this dataset  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dataset",
                "same set of binary features",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "previous work",
                "on this dataset",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Overall % agreement among judges for 250 propositions 60.1 A commonly used metric for evaluating interrater reliability in categorization of data is the kappa statistic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "judges",
                "60.1",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "kappa statistic",
                "commonly used metric",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For evaluation, we use IBMs BLEU score   to measure the performance of the SMS normalization.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "to measure the performance",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "SMS normalization",
                "evaluation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The approach combines statistical and knowledge-based methods, but unlike many recent corpus-based approaches to sense disambiguation  , it takes as its starting point the assumption that senseannotated training text is not available.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "takes as its starting point the assumption that senseannotated training text is not available",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpus-based approaches",
                "many recent",
                "INNOVATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "We ran the baseline semisupervised system for two iterations  , and in contrast with   we found that the best symmetrization heuristic for this system was union, which is most likely due to our use of fully linked alignments which was discussed at the end of Section 3.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semisupervised system",
                "two iterations",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "symmetrization heuristic",
                "best",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "While in traditional word-based statistical models   the atomic unit that translation operates on is the word, phrase-based methods acknowledge the significant role played in language by multiword expressions, thus incorporating in a statistical framework the insight behind Example-Based Machine Translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "atomic unit",
                "word",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase-based methods",
                "acknowledge the significant role played in language by multiword expressions",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "In this paper, sentence pairs are extracted by a simple model that is based on the so-called IBM Model1  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model1",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "simple model",
                "simple",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Previous workthe generative models described in Collins   and the earlier version of these models described in Collins  conditioned on punctuation as surface features of the string, treating it quite differently from lexical items.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generative models",
                "conditioned on punctuation as surface features of the string",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "punctuation",
                "treating it quite differently from lexical items",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Tuning was done using Maximum BLEU hill-climbing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tuning",
                "Maximum BLEU hill-climbing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU",
                "Maximum",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "2A maximum-entropy-based part of speech tagger was used   without the adaptation to the biomedical domain.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum-entropy-based part of speech tagger",
                "was used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "adaptation to the biomedical domain",
                "without",
                "METHODOLOGY",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "oward a Task-based Gold Standard for Evaluation of NP Chunks and Technical Terms Nina Wacholder Rutgers University nina@scils.rutgers.edu Peng Song Rutgers University psong@paul.rutgers.edu Abstract We propose a gold standard for evaluating two types of information extraction output -noun phrase   chunks   and technical terms  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "gold standard",
                "for evaluating",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "technical terms",
                "two types of information extraction output",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  is one of the first works to use statistical methods of distributional analysis to induce clusters of words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "statistical methods of distributional analysis",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "works",
                "first to use",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "2.4 Factor Model Decomposition Factored translation models   extend the phrase-based model by integrating word level factors into the decoding process.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Factor Model Decomposition",
                "integrating word level factors",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase-based model",
                "extend",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "The algorithm of   translates the traces into corresponding re-entrancies in the f-structure representation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "translates the traces into corresponding re-entrancies",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "f-structure representation",
                "corresponding re-entrancies",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For the former we made use Decision Lists similar to Yarowskys method for Word Sense Disambiguation    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Decision Lists",
                "similar to Yarowskys method",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "method for Word Sense Disambiguation",
                "Yarowskys",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Once we obtain the augmented phrase table, we should run the minimum-error-rate training   with the augmented phrase table such that the model parameters are properly adjusted.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model parameters",
                "are properly adjusted",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "minimum-error-rate training",
                "with the augmented phrase table",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he other approach selected was Yarowsky's unsupervised algorithm  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Yarowsky's unsupervised algorithm",
                "unsupervised",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "unsupervised algorithm",
                "selected",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Similarly, Kazama and Torisawa   used Wikipedia, particularly the first sentence of each article, to create lists of entities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wikipedia",
                "particularly the first sentence of each article",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "entities",
                "create lists of",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Before parsing, POS tags are assigned to the input sentence using our reimplementation of the POStagger from Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POStagger",
                "from Collins",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "reimplementation",
                "of the POStagger",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "c2005 Association for Computational Linguistics Recognizing Paraphrases and Textual Entailment using Inversion Transduction Grammars Dekai Wu1 Human Language Technology Center HKUST Department of Computer Science University of Science and Technology, Clear Water Bay, Hong Kong dekai@cs.ust.hk Abstract We present first results using paraphrase as well as textual entailment data to test the language universal constraint posited by Wus   Inversion Transduction Grammar   hypothesis.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "We chose this inverse direction since it can be integrated directly into the decoder and, thus, does not rely on a two-pass approach using reranking, as it is the case for  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "does not rely on two-pass approach",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "two-pass approach",
                "using reranking",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Due to the parameter interdependencies introduced by the one-to-one assumption, we are unlikely to find a method for decomposing the assignments into parameters that can be estimated independently of each other as in Brown et al. \\ ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameter interdependencies",
                "introduced by the one-to-one assumption",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "method for decomposing assignments",
                "can be estimated independently of each other",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3.1 Exhaustive search by tree fragments This method generates all possible tree fragments rooted by each node in the source parse tree or forest, and then matches all the generated tree fragments against the source parts   of translation rules to extract the useful rules  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tree fragments",
                "generated by tree fragments rooted by each node",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tree fragments",
                "matches against the source parts of translation rules",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The POS data set and the CTS data set have previously been used for testing other adaptation methods  , though the setup there is different from ours.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS data set",
                "previously been used for testing other adaptation methods",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "setup",
                "is different from ours",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We adopted the chunk representation proposed by Ramshaw and Marcus   and used four different tags: B-NUC and B-SAT for nucleus and satellite-initial tokens, and I-NUC and I-SAT for non-initial tokens, i.e., tokens inside a nucleus and satellite span.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunk representation",
                "proposed by Ramshaw and Marcus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tags",
                "B-NUC and B-SAT for nucleus and satellite-initial tokens, and I-NUC and I-SAT for non-initial tokens",
                "METHODOLOGY",
                "neutral",
                0.98
            ]
        ]
    },
    {
        "text": "The candidates were then ranked according to the scores assigned by four association measures: the log-likelihood ratio G2  , Pearsons chi-squared statistic X2  , the t-score statistic t  , and mere cooccurrence frequency f.4 TPs were identified according to the definition of Krenn  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "association measures",
                "four",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "scores",
                "assigned by four association measures",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Most of the early work in this area was based on postulating generative probability models of language that included parse structures  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generative probability models",
                "postulating generative probability models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parse structures",
                "included parse structures",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.1 Paraphrase Identification A few unsupervised metrics have been applied to automatic paraphrase identification and extraction  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unsupervised metrics",
                "applied to automatic paraphrase identification and extraction",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "metrics",
                "a few",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "203 Estimating the parameters for these models is more difficult   than with the models considered in the previous section: rather than simply being able to count the word pairs and alignment relationships and estimate the models directly, we must use an existing model to compute the expected counts for all possible alignments, and then use these counts to update the new model.7 This training strategy is referred to as expectationmaximization   and is guaranteed to always improve the quality of the prior model at each iteration  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "more difficult",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "training strategy",
                "guaranteed to always improve the quality",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Large volumes of training data of this kind are indispensable for constructing statistical translation models  , acquiring bilingual lexicon  , and building example-based machine translation   systems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training data",
                "indispensable",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "statistical translation models",
                "constructing",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Baron and Hirst   extracted collocations with Xtract   and classified the collocations using the orientations of the words in the neighboring sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Xtract",
                "collocations",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "orientations of the words in the neighboring sentences",
                "using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Next, we learn our polarity classifier using positive and negative reviews taken from two movie 611 review datasets, one assembled by Pang and Lee   and the other by ourselves.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifier",
                "learned using positive and negative reviews",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "reviews",
                "taken from two movie 611 review datasets",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The classifier consists of two components based on the averaged multiclass perceptron  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifier",
                "based on the averaged multiclass perceptron",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "multiclass perceptron",
                "averaged",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "he algorithm proposed by Turney   is labeled as Turney-PairClass",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Turney-PairClass",
                "is labeled",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "proposed by Turney",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.3 Previous Randomized LMs Recent work   has used lossy encodings based on Bloom filters   to represent logarithmically quantized corpus statistics for language modeling.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lossy encodings",
                "based on Bloom filters",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpus statistics",
                "logarithmically quantized",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Pure statistical machine translation   mltst in principle recover the most probable alignment out of all possible alignments between the input and a translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mltst",
                "recover the most probable alignment",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignments",
                "between the input and a translation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 Probability Model This paper takes a \"\"history-based\"\" approach   where each tree-building procedure uses a probability model p , derived from p , to weight any action a based on the available context, or history, b. First, we present a few simple categories of contextual predicates that capture any information in b that is useful for predicting a. Next, the predicates are used to extract a set of features from a corpus of manually parsed sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probability model",
                "derived from p",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "contextual predicates",
                "capture any information in b",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Effectiveness Comparison 5.1 English-Chinese ATIS Models Both the transfer and transducer systems were trained and evaluated on English-to-Mandarin Chinese translation of transcribed utterances from the ATIS corpus  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "systems",
                "were trained and evaluated",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We implement some global constraints and include unary constraints which were largely imported from the list of seed words used in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "global constraints",
                "implemented",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "unary constraints",
                "largely imported",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The second attempts to instill knowledge of collocations in the data; we use the technique described by   to compute multi-word expressions and then mark words that are commonly used as such with a feature that expresses this fact.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "multi-word expressions",
                "compute",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "words that are commonly used",
                "mark with a feature",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The translation models and lexical scores were estimated on the training corpus whichwasautomaticallyalignedusingGiza++  in both directions between source and target and symmetrised using the growing heuristic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Giza++",
                "was automatically aligned",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "lexical scores",
                "estimated on the training corpus",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The {ij}j=1m weights are estimated during the training phase to maximize the likelihood of the data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights",
                "estimated during the training phase",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "data",
                "likelihood of",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "From the same treebank, Cahill and van Genabith   automatically extracted wide-coverage LFG approximations for a PCFG-based generation model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treebank",
                "same treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "PCFG-based generation model",
                "LFG approximations",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  described symmetrized training of a 1-toN log-linear model and a M-to-1 log-linear model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-linear model",
                "symmetrized training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "model",
                "log-linear",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We evaluate this method over the part of speech tagged portion of the Penn Treebank corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "part of speech tagged",
                "portion of",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We ran the trainer with its default settings  , and then used Koehns implementation of minimumerror-rate training   to tune the feature weights to maximize the systems BLEU score on our development set, yielding the values shown in Table 2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "trainer",
                "default settings",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "feature weights",
                "maximize the systems BLEU score",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "We attribute the difference in M3/4 scores to the fact we use a Viterbi-like training procedure   while GIZA uses pegging   to sum over a set of likely hidden variable configurations in EM.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Viterbi-like training procedure",
                "use",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "GIZA's pegging",
                "sum over likely hidden variable configurations",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "6 The Experimental Results We used the Penn Treebank   to perform empirical experiments on this parsing model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "perform empirical experiments",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parsing model",
                "used to perform",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For a detailed description of each algorithm, readers are referred to Collins   for the boosting algorithm, Collins   for perceptron learning, and Gao et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "boosting algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "perceptron learning",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Using dictionaries as network of lexical items or senses has been quite popular for word sense disambiguation   before losing ground to statistical approaches, even though   tried a revival of such methods.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dictionaries as network of lexical items or senses",
                "has been quite popular",
                "METHODOLOGY",
                "positive",
                0.7
            ],
            [
                "statistical approaches",
                "lost ground",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": ".2.1 Proxy items There is a potential risk of redundancy if we represent related statistics using the log-frequency BF scheme presented in Talbot and Osborne  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-frequency BF scheme",
                "presented in Talbot and Osborne",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "risk of redundancy",
                "potential risk",
                "LIMITATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "a1 Graduated in March 2006 Standard phrase-based translation systems use a word distance-based reordering model in which non-monotonic phrase alignment is penalized based on the word distance between successively translated source phrases without considering the orientation of the phrase alignment or the identities of the source and target phrases  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reordering model",
                "penalized based on word distance",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "phrase alignment",
                "without considering orientation or identities",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ollins and Roark   presented a linear parsing model trained with an averaged perceptron algorithm",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing model",
                "trained with an averaged perceptron algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "averaged perceptron",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  where K is the number of distinct nonternfinal symbols in the gramma.r G. We ca.n expect a. very etfide.nt pa.rser tbr our pa.tterns, r The input string ca.n a.lso be scanned to reduce the number of relewmt gramma.r rules before pa.rsing, e The combined process is a.lso known as offlineparsing in LTAC,.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "G",
                "grammar",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "input string",
                "scanned to reduce the number of relevant grammar rules",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "4.2 Models with Prior Distributions Minimum discrimination information models   are exponential models with a prior distribution q : p  = q exp ) Z    The central issue in performance prediction for MDI models is whether q  needs to be accounted for.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Models",
                "exponential models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "performance prediction",
                "needs to be accounted for",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We tagged all the sentences in the training and devset3 using a maximum entropy-based POS tagger MXPOST  , trained on the Penn English and Chinese Treebanks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MXPOST",
                "trained on the Penn English and Chinese Treebanks",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "POS tagger",
                "maximum entropy-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The decoding process is very similar to those described in  : It starts from an initial empty hypothesis.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoding process",
                "very similar",
                "METHODOLOGY",
                "positive",
                0.7
            ],
            [
                "initial empty hypothesis",
                "",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As, Rapp   observes, choosing a window size involves making a trade-off between various qualities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "window size",
                "involves making a trade-off",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "qualities",
                "various",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Nevertheless, as   and others have argued, semantic representations for natural language need not be higher-order in that ontological promiscuity can solve the problem.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic representations",
                "not be higher-order",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "ontological promiscuity",
                "solve the problem",
                "PERFORMANCE",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "They first extract English collocations using the Xtract systetn  , and theu look for French coutlterparts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Xtract system",
                "English collocations",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "coutlterparts",
                "French counterparts",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Allomorphs   are also automatically identified in  , but the general problem of recognizing highly irregular forms is examined more extensively in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Allomorphs",
                "are also automatically identified",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "general problem of recognizing highly irregular forms",
                "is examined more extensively",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "6.2 Translation Results For the translation experiments, we report the two accuracy measures BLEU   and NIST   as well as the two error rates word error rate   and positionindependent word error rate  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU and NIST",
                "accuracy measures",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "word error rate and position-independent word error rate",
                "error rates",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.1 Learning Chunk-based Translation We learn chunk alignments from a corpus that has been word-aligned by a training toolkit for wordbased translation models: the Giza++   toolkit for the IBM models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "has been word-aligned by a training toolkit for word-based translation models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Giza++ toolkit",
                "for the IBM models",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "aume III & Marcu   argue that generic sentence fusion is an ill-defined task",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "task",
                "ill-defined",
                "INNOVATION",
                "negative",
                0.8
            ],
            [
                "sentence fusion",
                "is",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Examples of statistical and machine learning approaches that have been used for tagging include transformation based learning  , memory based learning  , and maximum entropy models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transformation based learning",
                "used for tagging",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "maximum entropy models",
                "used for tagging",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In our SRL system, we select maximum entropy   as a classi er to implement the semantic role labeling system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy",
                "as a classifier",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "semantic role labeling system",
                "to implement",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this paper, we bring forward the first idea by studying the issue of how to utilize structured syntactic features for phrase reordering in a phrase-based SMT system with BTG   constraints  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based SMT system",
                "with BTG constraints",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "structured syntactic features",
                "for phrase reordering",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In order to avoid this problem we implemented a simple bootstrapping procedure in which a seed data set of 100 instances of each of the eight categories was hand tagged and used to generate a decision list classifier using the C4.5 algorithm   with the word frequency and topic signature features described below.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "C4.5 algorithm",
                "was used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "decision list classifier",
                "was generated",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 Maximum Entropy Classifier For local classifiers, we used a maximum entropy model which is a common choice for incorporating various types of features for classification problems in natural language processing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy Classifier",
                "common choice for incorporating various types of features",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "maximum entropy model",
                "for classification problems in natural language processing",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A tight integration of morphosyntactic information into the translation model was proposed by   where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "information",
                "translated separately and combined on the output side",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "While this approach exploits only syntactic and lexical information, Jing and McKeown   also rely on cohesion information, derived from word distribution in a text: Phrases that are linked to a local context are retained, while phrases that have no such links are dropped.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Phrases",
                "are retained",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Phrases",
                "have no such links are dropped",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 perform the following maximization: eI1 = argmax eI1 fPr Pr g   This approach is referred to as source-channel approach to statistical MT. Sometimes, it is also referred to as the fundamental equation of statistical MT  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "source-channel approach",
                "approach to statistical MT",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "fundamental equation of statistical MT",
                "fundamental equation",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It is an extension of Pharaoh  , and supports factor training and decoding.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pharaoh",
                "extension of",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "factor training and decoding",
                "supports",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Our baseline model follows Chiangs hierarchical model   in conjunction with additional features:  conditional probabilities in both directions: P  and P ;  lexical weights   in both directions: Pw  and Pw ; 21  word counts |e|;  rule counts |D|;  target n-gram language model PLM ;  glue rule penalty to learn preference of nonterminal rewriting over serial combination through Eq.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "conditional probabilities",
                "in both directions: P and P",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "target n-gram language model",
                "PLM",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the following experiments, we run two machine learning classifiers: Bayes Point Machines    , and the maximum entropy model    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bayes Point Machines",
                "machine learning classifiers",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "maximum entropy model",
                "machine learning classifiers",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The techniques examined are Structural Correspondence Learning     and Self-training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques",
                "examined",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "Structural Correspondence Learning",
                "Self-training",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: Hindles   measure, the weighted Lin measure  , the -Skew divergence measure  , the Jensen-Shannon   divergence measure  , Jaccards coef cient   and the Confusion probability  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "similarity measures",
                "commonly used",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "Hindles measure",
                "used",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "arowsky has proposed an algorithm that requires as little user input as one seed word per sense to start the training process  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "requires one seed word per sense",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "user input",
                "as little as",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Results from Collins, Schapire and Singer   show that under these definitions the following guarantee holds: LogLossUpda,k, BestWtk, a C20 BestLossk, a So it can be seen that the update from a to Upda, k, BestWtk, a is guaranteed to decrease LogLoss by at least  W  k q C0  W C0 k qC16C17 2 . From these results, the algorithms in Figures 3 and 4 could be altered to take the revised definitions of W  k and W C0 k into account.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LogLossUpda,k, BestWtk, a C20 BestLossk, a",
                "decrease",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "algorithms",
                "could be altered",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Formally, the approach we take can be thought of as a noisier channel, where an observed signal o gives rise to a set of source-language strings fprime  F  and we seek e = arg maxe max fprimeF  Pr    = arg maxe max fprimeF  Pr Pr    = arg maxe max fprimeF  Pr Pr Pr .  Following Och and Ney  , we use the maximum entropy framework   to directly model the posterior Pr  with parameters tuned to minimize a loss function representing 1012 the quality only of the resulting translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy framework",
                "to directly model the posterior",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parameters tuned to minimize a loss function",
                "representing the quality of the resulting translations",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Thus, we used the five taggers, MBL  , MXPOST  , fnTBL  , TnT, and IceTagger3, in the same manner as described in  , but with the following minor changes.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "taggers",
                "five taggers",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "TnT",
                "used in same manner",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge, e.g.,    , or require human-annotated training data with relation information for each domain  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "labeled relations",
                "require substantial hand-created linguistic or domain knowledge",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "relation information",
                "human-annotated training data",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he noun phrase extraction module uses Brill's POS tagger  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brill's POS tagger",
                "uses",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "noun phrase extraction module",
                "uses",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features   , Druck et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "assign labels to",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "user",
                "assign labels to features",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Rather than learning how strings in one language map to strings in another, however, translation now involves learning how systematic patterns of errors in ESL learners English map to corresponding patterns in native English 2.2 A Noisy Channel Model of ESL Errors If ESL error correction is seen as a translation task, the task can be treated as an SMT problem using the noisy channel model of  : here the L2 sentence produced by the learner can be regarded as having been corrupted by noise in the form of interference from his or her L1 model and incomplete language models internalized during language learning.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ESL errors",
                "systematic patterns of errors",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Noisy Channel Model",
                "noise in the form of interference",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Also, PMI-IR is useful for calculating semantic orientation and rating reviews  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PMI-IR",
                "useful for calculating semantic orientation and rating reviews",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "PMI-IR",
                "calculating semantic orientation and rating reviews",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "11 However, modeling word order under translation is notoriously difficult  , and it is unclear how much improvement in accuracy a good model of word order would provide.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "modeling",
                "notoriously difficult",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "improvement",
                "good model of word order",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "aume III   further augments the feature space on the instances of both domains",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature space",
                "augments the instances of both domains",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "instances of both domains",
                "of both domains",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Our method is thus related to previous work based on Harris  s distributional hypothesis.2 It has been used to determine both word and syntactic path similarity  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "based on Harris's distributional hypothesis",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word and syntactic path similarity",
                "determine",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "6 Discourse Context   pointed out that the sense of a target word is highly consistent within any given document  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sense of a target word",
                "is highly consistent",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "document",
                "any given",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "One option is what Johnson   calls many-to-one   accuracy, in which each induced tag is labeled with its most frequent gold tag.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "induced tag",
                "labeled with its most frequent gold tag",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Johnson's accuracy",
                "many-to-one accuracy",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  and section 23 for testing  ; we only tested on sentences _< 40 words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentences",
                "_< 40 words",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "testing",
                "only tested",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For each differently tokenized corpus, we computed word alignments by a HMM translation model   and by a word alignment refinement heuristic of grow-diagfinal  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMM translation model",
                "by a HMM translation model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word alignment refinement heuristic of grow-diagfinal",
                "word alignment refinement heuristic",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The current approach does not use specialized probability features as in   in any stage during decoder parameter training.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder parameter training",
                "does not use specialized probability features",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "decoder parameter training",
                "in any stage during",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This clustering was created automatically with the aid of a methodology described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methodology",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "clustering",
                "created automatically",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  shows that baseNP recognition   is easier than finding both NP and VP chunks   and that increasing the size of the training data increases the performance on the test set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseNP recognition",
                "is easier",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "increasing the size of the training data",
                "increases the performance",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As noted in Talbot and Osborne  , errors for this log-frequency BF scheme are one-sided: frequencies will never be underestimated.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-frequency BF scheme",
                "errors are one-sided",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "frequencies",
                "will never be underestimated",
                "PERFORMANCE",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "1510 5 Related Work In recent years, many research has been done on extracting relations from free text  ); however, almost all of them require some language-dependent parsers or taggers for English, which restrict the language of their extractions to English only  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "done on extracting relations from free text",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "parsing or tagging tools",
                "language-dependent",
                "METHODOLOGY",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "This is applied to maximize coverage, which is similar as the final in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "coverage",
                "similar",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "maximize",
                "coverage",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "7 Related work Similarly to  , the rules discussed in this paper are equivalent to productions of synchronous tree substitution grammars.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rules",
                "equivalent to productions of synchronous tree substitution grammars",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "productions of synchronous tree substitution grammars",
                "",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.2 Features For our experiments, we use a feature set analogous to the default feature set of Pharaoh  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature set",
                "analogous to the default feature set of Pharaoh",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Pharaoh",
                "default feature set",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexica from a resource-rich language to the target language  , or a domain that is similar enough to the target domain  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus/machine translation engine",
                "resource-rich language",
                "APPLICABILITY",
                "neutral",
                0.85
            ],
            [
                "domain",
                "similar enough to the target domain",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The supervised training described in   uses manually annotated data for the estimation of the weight coefficients .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training",
                "manually annotated data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "weight coefficients",
                "estimation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ean and Riloff   proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "caseframe networks",
                "kind of contextual role knowledge",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "anaphora resolution",
                "proposed for",
                "PERFORMANCE",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "For the correct identification of phrases in a Korean query, it would help to identify the lexical relations and produce statistical information on pairs of words in a text corpus as in Smadja  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "text corpus",
                "statistical information on pairs of words",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "lexical relations",
                "identify",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "An alternative would be using a vector space model for classi cation where calltypes and utterances are represented as vectors including word a2 -grams  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "calltypes and utterances",
                "are represented as vectors including word a2-grams",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "vector space model",
                "for classification",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In information retrieval, word similarity can be used to identify terms for pseudo-relevance feedback  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word similarity",
                "used to identify terms for pseudo-relevance feedback",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "terms for pseudo-relevance feedback",
                "for pseudo-relevance feedback",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Our hierarchical system is Hiero  , modified to construct rules from a small sample of occurrences of each source phrase in training as described by Lopez  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hiero",
                "modified",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "rules",
                "constructed from a small sample of occurrences",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "  grow the set of word links by appending neighboring points, while Och and Hey   try to avoid both horizontal and vertical neighbors.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word links",
                "appending neighboring points",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Och and Hey",
                "try to avoid",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4.3 Baselines 4.3.1 Word Alignment We used the GIZA++ implementation of IBM word alignment model 4   for word alignment, and the heuristics described in   to derive the intersection and refined alignment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ implementation of IBM word alignment model",
                "for word alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "heuristics described in",
                "to derive the intersection and refined alignment",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2.1.2 Research on Syntax-Based SMT A number of researchers   have proposed models where the translation process involves syntactic representations of the source and/or target languages.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "researchers",
                "proposed models",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "translation process",
                "involves syntactic representations",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In statistical computational linguistics, maximum conditional likelihood estimators have mostly been used with general exponential or maximum entropy models because standard maximum likelihood estimation is usually computationally intractable  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum likelihood estimation",
                "is usually computationally intractable",
                "PERFORMANCE",
                "negative",
                0.75
            ],
            [
                "maximum conditional likelihood estimators",
                "have mostly been used with general exponential or maximum entropy models",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The data consists of 2,544 main clauses from the Wall Street Journal Treebank corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal Treebank corpus",
                "consists of 2,544 main clauses",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Sentiment classification is a well studied problem   and in many domains users explicitly 1We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "aspect",
                "properties of an object that can be rated by a user",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "domain",
                "many domains",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Most current statistical models   treat the aligned sentences in the corpus as sequences of tokens that are meant to be words; the goal of the alignment process is to find links between source and target words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical models",
                "treat the aligned sentences in the corpus as sequences of tokens",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "alignment process",
                "find links between source and target words",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Also relevant is previous work that applied machine learning approaches to MT evaluation, both with human references   and without  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning approaches",
                "applied",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "MT evaluation",
                "without human references",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The usual Chinese NLP architecture first preprocesses input text through a word segmentation module  , but, clearly, bilingual parsing will be hampered by any errors arising from segmentation ambiguities that could not be resolved in the isolated monolingual context because even if the Chinese segmentation is acceptable monolingually, it may not agree with the words present in the English sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word segmentation module",
                "segmentation ambiguities",
                "METHODOLOGY",
                "negative",
                0.75
            ],
            [
                "Chinese segmentation",
                "not agree with the words present in the English sentence",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This paper explores an alternative approach to parsing, based on the perceptron training algorithm introduced in Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron training algorithm",
                "introduced in Collins",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alternative approach to parsing",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  from the Penn Treebank   WSJ corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "WSJ corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "WSJ corpus",
                "used in",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This is comparable to the accuracy of 96.29% reported by   on the newswire domain.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "accuracy",
                "reported by 96.29%",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "accuracy",
                "comparable to",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Amount of works have been done on sentimental classification in different levels  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "works",
                "done",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "sentimental classification",
                "different levels",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For example, consider a case of observation bias   for a first-order left-toright CMM.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CMM",
                "first-order left-to-right",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "observation bias",
                "for a case",
                "LIMITATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Finally, in order to formally evaluate the method and the different heuristics, a large-scale evaluation on the BioMed Corpus is under way, based on computing the ROUGE measures  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation",
                "large-scale evaluation",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "ROUGE measures",
                "computing the",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In future work, we will expand all of the above types of features and employ techniques to reduce dimensionality along the lines suggested in   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "expand all of the above types",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "dimensionality reduction techniques",
                "along the lines suggested",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "However, searching the space of all possible alignments is intractable for EM, so in practice the procedure is bootstrapped by models with narrower search space such as IBM Model 1   or Aachen HMM  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 1",
                "models with narrower search space",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "Aachen HMM",
                "models with narrower search space",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Automatic NE transliteration is an important component in many cross-language applications, such as Cross-Lingual Information Retrieval   and Machine Translation   .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NE transliteration",
                "important component",
                "INNOVATION",
                "positive",
                0.85
            ],
            [
                "cross-language applications",
                "many",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The research of opinion mining began in 1997, the early research results mainly focused on the polarity of opinion words   and treated the text-level opinion mining as a classification of either positive or negative on the number of positive or negative opinion words in one text  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "mainly focused on the polarity of opinion words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "text-level opinion mining",
                "as a classification of either positive or negative",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "If the language model Pr  = p   depends on parameters and the translation model Pr  = p   depends on parameters, then the optimal parameter values are obtained by maximizing the likelihood on a parallel training corpus fS1;eS1  :  = argmax SY s=1 p      = argmax SY s=1 p     Computational Linguistics  , Philadelphia, July 2002, pp.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "obtained by maximizing the likelihood",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parameters",
                "depends on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Given a weight vector w, the score wf  ranks possible labelings of x, and we denote by Yk,w  the set of k top scoring labelings for x. We use the standard B,I,O encoding for named entities  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weight vector w",
                "ranks possible labelings",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "standard B,I,O encoding",
                "named entities",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The significance values are obtained using the loglikelihood measure assuming a binomial distribution for the unrelatedness hypothesis  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "loglikelihood measure",
                "assuming a binomial distribution",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "unrelatedness hypothesis",
                "for the",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "146 2.3 Approximating ISBNs   proposes two approximations for inference in ISBNs, both based on variational methods.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ISBNs",
                "two approximations",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "variational methods",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Clustering algorithms have been previously shown to work fairly well for the classification of words into syntactic and semantic classes  , but determining the optimum number of classes for a hierarchical cluster tree is an ongoing difficult problem, particularly without prior knowledge of the item classification.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "With this model, we can provide not only qualitative textual summarization such as good food and bad service, but also a numerical scoring of sentiment, i.e., how good the food is and how bad the service is. 2 Related Work There have been many studies on sentiment classification and opinion summarization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentiment",
                "how good the food is and how bad the service is",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "sentiment classification and opinion summarization",
                "many studies on",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  of running GIZA++   in both directions and then merging the alignments using the grow-diag-final heuristic.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "running in both directions and then merging using the grow-diag-final heuristic",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "alignments",
                "merging using the grow-diag-final heuristic",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Word alignment models were first introduced in statistical machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "introduced in statistical machine translation",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "models",
                "introduced",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "On the British National Corpus  , using Lins   similarity method, we retrieve the following neighbors for the first and second sense, respectively: 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "British National Corpus",
                "using Lins similarity method",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "neighbors",
                "for the first and second sense",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The process of phrase extraction is difficult to optimize in a non-discriminative setting: many heuristics have been proposed  , but it is not obvious which one should be chosen for a given language pair.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "heuristics",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "which one",
                "chosen for a given language pair",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In recent years, sentiment classification has drawn much attention in the NLP field and it has many useful applications, such as opinion mining and summarization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentiment classification",
                "has many useful applications",
                "INNOVATION",
                "positive",
                0.75
            ],
            [
                "NLP field",
                "drawn much attention",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Similar to BLEU score, we also use the similar Brevity Penalty BP   to penalize the short translations in computing RAcc.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brevity Penalty",
                "to penalize the short translations",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "BLEU score",
                "similar to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In  , shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal decoder.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "shallow syntactic analysis",
                "was incorporated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrasal decoder",
                "such as POS tagging and morphological analysis",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We used a non-projective model, trained using an application of the matrix-tree theorem   for the first-order Czech models, and projective parsers for all other models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "non-projective",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parsers",
                "projective",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In   non-terminals in a standard PCFG model are augmented with latent variables.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "non-terminals",
                "augmented with latent variables",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "PCFG model",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One of the applications is in automatic summarization in order to compress sentences extracted for the summary  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentences",
                "extracted for the summary",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "automatic summarization",
                "in order to compress",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This test set was tagged using MXPOST   which was itself trained on Switchboard.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MXPOST",
                "trained on Switchboard",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Switchboard",
                "used for training",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In this method, each training sentence is decoded and weights are updated at every iteration  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training sentence",
                "decoded",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "weights",
                "updated",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The first approach is to reuse the components of a generative model, but tune their relative weights in a discriminative fashion  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "components of a generative model",
                "reuse and tune",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "relative weights",
                "in a discriminative fashion",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 Related Research Ramshaw and Marcus  , Munoz et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ramshaw and Marcus",
                "related research",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Munoz et al.",
                "related research",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "4 Experiments The experiments described here were conducted using the Wall Street Journal Penn Treebank corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal Penn Treebank corpus",
                "was used",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "To be able identify that adjacent blocks   can be merged into larger blocks, our model infers binary   trees reminiscent of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "infers binary trees",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "binary trees",
                "reminiscent of",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The huge increase in computational and storage cost of including longer phrases does not provide a signi cant improvement in quality   as the probability of reappearance of larger phrases decreases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "computational and storage cost",
                "huge increase in",
                "LIMITATION",
                "neutral",
                0.85
            ],
            [
                "quality",
                "does not provide a signi cant improvement in",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Automatic approaches to creating a semantic orientation lexicon and, more generally, approaches for word-level sentiment annotation can be grouped into two kinds:   those that rely on manually created lexical resourcesmost of which use WordNet  ; and   those that rely on text corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicon",
                "manually created lexical resources",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpora",
                "rely on text",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 Related work Word collocation Various collocation metrics have been proposed, including mean and variance  , the t-test  , the chi-square test, pointwise mutual information    , and binomial loglikelihood ratio test    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocation metrics",
                "have been proposed",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "t-test",
                "the t-test",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Responsiveness differs from other measures of summary content such as SEE coverage   and Pyramid scores   in that it does not compare a peer summary against a set of known human summaries.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "summary content",
                "does not compare a peer summary against a set of known human summaries",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "measures",
                "such as SEE coverage and Pyramid scores",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 and Mancini 1991; Meteer, Schwartz, and Weischedel 1991; Merialdo 1991; Pelillo, Moro, and Refice 1992; Weischedel et al. 1993; Wothke et al. 1993).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Association for Computational Linguistics",
                "Computational Linguistics Volume 21, Number 2",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "Mancini 1991",
                "Mancini 1991",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "204 4.2.2 Correlation between TREC nuggets and non-text features Analyzing the features used could let us understand summarization better  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "used",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "summarization",
                "better",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "Using BLEU   as a metric, our method achieves an absolute improvement of 0.06   as compared with the standard model trained with 5,000 L f -L e sentence pairs for French-Spanish translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "achieves an absolute improvement of 0.06",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "standard model",
                "trained with 5,000 L f -L e sentence pairs",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Although some work has been done on syllabifying orthographic forms  , syllables are, technically speaking, phonological entities that can only be composed of strings of phonemes.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syllables",
                "phonological entities that can only be composed of strings of phonemes",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "orthographic forms",
                "technically speaking",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "From this point of view, some of the measures used in the evaluation of Machine Translation systems, such as BLEU  , have been imported into the summarization task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "have been imported",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "summarization task",
                "used in the evaluation of Machine Translation systems",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3 Experimental Results Whereas stochastic modelling is widely used in speech recognition, there are so far only a few research groups that apply stochastic modelling to language translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "stochastic modelling",
                "widely used",
                "PERFORMANCE",
                "positive",
                0.7
            ],
            [
                "research groups",
                "few",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "SD that use information gathered from raw corpora      ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SD",
                "use information gathered from raw corpora",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpora",
                "raw",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "After unioning the Viterbi alignments, the stems were replaced with their original words, and phrase-pairs of up to five foreign words in length were extracted in the usual fashion  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Viterbi alignments",
                "usual fashion",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase-pairs",
                "up to five foreign words in length",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "uo and Zitouni   proposed a coreference resolution approach which also explores the information from the syntactic parse trees",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "coreference resolution approach",
                "explores information from syntactic parse trees",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "information from syntactic parse trees",
                "explored",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5.1 Relationship to \"\"supervised\"\" training To illustrate the relationship between the above symbolic training method for preference scoring and corpus-based methods, perhaps the easiest way is to compare it to an adaptation   of the perceptron training method to the problem of obtaining a best parse  , because the two methods are analogous in a number of ways.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "symbolic training method for preference scoring",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "methods",
                "analogous in a number of ways",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "methods for syntactic SMT held to this assumption in its entirety  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods for syntactic SMT",
                "held to this assumption in its entirety",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "assumption",
                "in its entirety",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he data set that has become standard for evaluation machine learning approaches is the one first used by Ramshaw and Marcus  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data set",
                "standard",
                "APPLICABILITY",
                "positive",
                0.8
            ],
            [
                "machine learning approaches",
                "evaluation",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "English POS tags were assigned by MXPOST  , which was trained on the training data described in Section 4.1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MXPOST",
                "was trained on the training data",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "training data",
                "described in Section 4.1",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Different models have been presented in the literature, see for instance  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "presented in the literature",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "instance",
                "see for instance",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We would like to apply our learning approach to the large data set mentioned in  : Wall Street Journal corpus sections 2-21 as training material and section 0 as test material.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data set",
                "large",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "training material",
                "as test material",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The score combination weights are trained by a minimum error rate training procedure similar to  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "score combination weights",
                "trained by a minimum error rate training procedure",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "minimum error rate training procedure",
                "similar to ",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We also compared the MSR algorithm to two of the state-of-the-art discriminative training methods: Boosting in Row 3 is an implementation of the improved algorithm for the boosting loss function proposed in  , and Perceptron in Row 4 is an implementation of the averaged perceptron algorithm described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MSR algorithm",
                "state-of-the-art",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Boosting in Row 3",
                "implementation of the improved algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A key example is that of class-based language models   where clustering approaches are used in order to partition words, determined to be similar, into sets.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "class-based language models",
                "clustering approaches",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "words, determined to be similar",
                "partition into sets",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Statistical Model In SIFT's statistical model, augmented parse trees are generated according to a process similar to that described in Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SIFT's statistical model",
                "similar to that described in Collins",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "augmented parse trees",
                "generated according to a process",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As such, discourse markers play an important role in the parsing of natural language discourse  , and their correspondence with discourse relations can be exploited for the unsupervised learning of discourse relations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "discourse markers",
                "play an important role",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "correspondence with discourse relations",
                "can be exploited",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The corpus was aligned with GIZA++   and symmetrized with the grow-diag-finaland heuristic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "symmetrized with the grow-diag-finaland heuristic",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpus",
                "aligned with",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  and Jansche  , who discuss maximum expected F-score training of decision trees and logistic regression models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decision trees",
                "training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "logistic regression models",
                "training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To compare the performance of system, we recorded the total training time and the BLEU score, which is a standard automatic measurement of the translation quality .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "standard automatic measurement of translation quality",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "training time",
                "recorded",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Within NLP, applications include sentiment-analysis problems   and content selection for text generation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "applications",
                "sentiment-analysis problems",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "content selection",
                "for text generation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, in John saw Mary yesterday at the station, only John and Mary are required arguments while the other constituents are optional  .3 The problem of SF identification using statistical methods has had a rich discussion in the literature    ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "John",
                "required arguments",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "statistical methods",
                "had a rich discussion",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Whereas most of the work on English has been based on constituency-based representations,partly inuenced by the availability of data resources such as the Penn Treebank  ,it has been argued that free constituent order languages can be analyzed more adequately using dependency-based representations,which is also the kind of annotation found, for example,in the Prague Dependency Treebank of Czech  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "availability of data resources",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "Prague Dependency Treebank of Czech",
                "kind of annotation found",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As other researchers pursued efficient default unification  , we also propose another definition of default unification, which we call lenient default unification.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "default unification",
                "efficient",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "lenient default unification",
                "another definition",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We used MXPOST  , and in order to discover more general patterns, we map the tag set down after tagging, e.g. NN, NNP, NNPS and NNS all map to NN.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MXPOST",
                "tagging",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tag set",
                "map to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There are similarities with dependency grammars here because such constraint graphs are also produced by dependency grammars    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "constraint graphs",
                "produced by dependency grammars",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "dependency grammars",
                "similarities with",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  describes various strategies for the decomposition of the decoding into multiple translation models using the Moses decoder.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Moses decoder",
                "using",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translation models",
                "multiple",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There are many choices for modeling co-occurrence data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parent phrase-marker",
                "many other features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "lexical-head",
                "famously",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "opez   gives indirect experimental evidence that this difference affects performance",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "difference",
                "affects performance",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "experimental evidence",
                "indirect",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "three models in   are susceptible to the method.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "susceptible to the method",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "method",
                "susceptible",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The interpolation weights a65   are trained using discriminative training   using ROUGEa129 as the objective function, on the development set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "interpolation weights",
                "trained using discriminative training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "objective function",
                "ROUGE as the objective function",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this paper, Stanford Named Entity Recognizer   is used to classify noun phrases into four semantic categories: PERSON, LOCATION, ORGANIZARION and MISC.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stanford Named Entity Recognizer",
                "is used to classify",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "noun phrases",
                "into four semantic categories",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ch   described the use of minimum error training directly optimizing the error rate on automatic MT evaluation metrics such as BLEU",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "minimum error training",
                "directly optimizing the error rate",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "automatic MT evaluation metrics",
                "such as BLEU",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In addition to this phrase translation probability feature, Hieros feature set includes the inverse phrase translation probability log p , lexical weights lexwt  and lexwt , which are estimates of translation quality based on word-level correspondences  , and a rule penalty allowing the model to learn a preference for longer or shorter derivations; see   for details.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase translation probability log p",
                "estimates of translation quality",
                "PERFORMANCE",
                "neutral",
                0.85
            ],
            [
                "rule penalty",
                "preference for longer or shorter derivations",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this paper, we make a direct comparison of a syntactically unsupervised alignment model, based on Wu  , with a syntactically supervised model, based on Yamada and Knight  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment model",
                "based on Wu",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "syntactically supervised model",
                "based on Yamada and Knight",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  from Sections 2-21 of the Wall Street Journal   in the Penn Treebank   and its subsets.3 We then converted them into strongly equivalent HPSG-style grammars using the grammar conversion described in Section 2.1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal",
                "from Sections 2-21",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Penn Treebank",
                "and its subsets",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In this paper we use the phrase-based system of   as our underlying model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based system",
                "as our underlying model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase-based system",
                "underlying model",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To this end, the translational correspondence is described within a translation rule, i.e.,    , rather than a translational phrase pair; and the training data will be derivation forests, instead of the phrase-aligned bilingual corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation rule",
                "described within",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "training data",
                "derivation forests",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The average senior high school student achieves 57% correct  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "student",
                "achieves 57% correct",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "senior high school student",
                "average",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this paper, we investigate the effectiveness of structural correspondence learning     in the domain adaptation task given by the CoNLL 2007.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "structural correspondence learning",
                "in the domain adaptation task given by the CoNLL 2007",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "CoNLL 2007",
                "given by",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "To compare the performance of different taggers learned by different mechanisms, one can measure the precision, recall and F-measure, given by precision = # correct predictions# predicted gene mentions recall = # correct predictions# true gene mentions F-measure = a96a15a14 precision a14 recallprecision a44 recall In our evaluation, we compared the proposed semi-supervised learning approach to the state of the art supervised CRF of McDonald and Pereira  , and also to self-training  , using the same feature set as  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "precision",
                "given by",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "F-measure",
                "precision a96a15a14 precision a14 recall",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The training and decoding system of our SMT used the publicly available Pharaoh  2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT",
                "used the publicly available Pharaoh 2",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Pharaoh 2",
                "publicly available",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The acquisition of clues is a key technology in these research efforts, as seen in learning methods for document-level SA   and for phraselevel SA  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "acquisition of clues",
                "key technology",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "learning methods",
                "for document-level SA and for phrase-level SA",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "urney   later addressed the same problem using 8000 automatically generated patterns",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "patterns",
                "automatically generated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "8000",
                "large-scale",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "2.2 Closed Challenge Setting The organization provided training, development and test sets derived from the standard sections of the Penn TreeBank   and PropBank   corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "Penn TreeBank and PropBank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "standard sections",
                "derived from",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our goal is to come up with a mechanism that, given an input string, identifies the phrases in this string, this is a fundamental task with applications in natural language  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "input string",
                "identifies phrases",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "mechanism",
                "comes up with",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However, feature/class functions are traditionally deflned as binary  ; hence, explicitly incorporating frequencies would require difierent functions for each count  , making training impractical.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature/class functions",
                "traditionally defined as binary",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "different functions",
                "making training impractical",
                "PERFORMANCE",
                "negative",
                0.9
            ]
        ]
    },
    {
        "text": "2.2 Using Log-Likelihood-Ratios to Estimate Word Translation Probabilities Our method for computing the probabilistic translation lexicon LLR-Lex is based on the the Log2http://www.fjoch.com/GIZA++.html Likelihood-Ratio   statistic  , which has also been used by Moore   and Melamed   as a measure of word association.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Log-Likelihood-Ratios",
                "has also been used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Likelihood-Ratio",
                "as a measure of word association",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.2 Xerox Tagger The Xerox Tagger 1, XT,   is a statistical tagger made by Doug Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun in Xerox PARC.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Xerox Tagger",
                "statistical tagger",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Xerox PARC",
                "made by",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "6.3 Comparison with re-ranking approach Finally, we compared our algorithm with the reranking approach  , where we rst generate the n-best candidates using a model with only local features   and then re-rank the candidates using a model with non-local features  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "with local features",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "reranking approach",
                "with non-local features",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Syntactic-oriented evaluation metrics We investigated the following metrics oriented on the syntactic structure of a translation output:  POSBLEU The standard BLEU score   calculated on POS tags instead of words;  POSP POS n-gram precision: percentage of POS ngrams in the hypothesis which have a counterpart in the reference;  POSR Recall measure based on POS n-grams: percentage of POS n-grams in the reference which are also present in the hypothesis;  POSF POS n-gram based F-measure: takes into account all POS n-grams which have a counter29 part, both in the reference and in the hypothesis.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "standard",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "POS n-gram based F-measure",
                "takes into account",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The text was split at the sentence level, tokenized and PoS tagged, in the style of the Wall Street Journal Penn TreeBank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal Penn TreeBank",
                "in the style of",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentence level",
                "tokenized and PoS tagged",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Generative methods   treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization   algorithm.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word alignment",
                "hidden process",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "expectation maximization algorithm",
                "",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Motivation In the past, work has been done in the area of characterizing words and phrases according to their emotive tone  , but in many domains of text, the values of individual phrases may bear little relation to the overall sentiment expressed by the text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words and phrases",
                "according to their emotive tone",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "individual phrases",
                "may bear little relation",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Unlike with factored models   or additional translation lexicons  , we do not generate the surface form back from the lemma translation, which means that tense, gender and number information are 151 news-dev2009a representation OOV % METEOR BLEU NIST baseline surface form only 2.24 49.05 20.45 6.135 decoding lemma backoff 2.13 49.12 20.44 6.143 word alignment lemma+POS for all 2.24 48.87 20.36 6.145 lemma+POS for adj 2.25 48.94 20.46 6.131 lemma+POS for verbs 2.21 49.05 20.47 6.137 decoding + alignment backoff + all 2.10 48.97 20.36 6.147 backoff + adj 2.12 49.05 20.48 6.140 backoff + verbs 2.08 49.15 20.50 6.148 news-dev2009b representation OOV % METEOR BLEU NIST baseline surface form only 2.52 49.60 21.10 6.211 decoding lemma backoff 2.43 49.66 21.02 6.210 word alignment lemma+POS for all 2.53 49.56 21.03 6.199 lemma+POS for adj 2.52 49.74 21.00 6.213 lemma+POS for verbs 2.47 49.73 21.10 6.217 decoding+alignment backoff + all 2.44 49.59 20.92 6.194 backoff + adj 2.43 49.80 21.03 6.217 backoff + verbs 2.39 49.80 21.03 6.217 Table 2: Evaluation of the decoding backoff strategy, the modified word alignment strategy and their combination Input Meme sil demissionnait, la situation ne changerait pas.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoding backoff strategy",
                "modified word alignment strategy and their combination",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "lemma translation",
                "surface form back from the lemma translation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The user can select characters by their frequencies  , the top or bottom N%  , their ranks   and by their frequencies above two standard deviations phlS the mean    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "characters",
                "by their frequencies",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "characters",
                "above two standard deviations from the mean",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Obtaining a word-aligned corpus usually involves training a word-based translation models   in each directions and combining the resulting alignments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word-based translation models",
                "training",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "resulting alignments",
                "combining",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Sentiment classification at the sentence-level has also been studied  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentiment classification",
                "has been studied",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence-level",
                "has also been studied",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "should appear with at most one value in each announcement, although the field and value may be repeated  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "announcement",
                "have at most one value",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "value",
                "may be repeated",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "INTRODUCTION Class-based language models  have been proposed for dealing with two problems confronted by the well-known word n-gram language models   data sparseness: the amount of training data is insufficient for estimating the huge number of parameters; and   domain robustness: the model is not adaptable to new application domains.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "huge number of parameters",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "domain robustness",
                "not adaptable to new application domains",
                "LIMITATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "6 Related Work Other work combining supervised and unsupervised learning for parsing includes  ,  , and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work combining supervised and unsupervised learning",
                "Other work",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Other work combining supervised and unsupervised learning",
                "includes",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Other work aims to do truly unsupervised learning of taggers, such as Goldwater and Griffiths   and Johnson  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "taggers",
                "truly unsupervised learning",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "taggers",
                "unsupervised",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Like the data used by  , this data was retagged by the Brill tagger in order to obtain realistic part-of-speech   tags 3.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brill tagger",
                "obtain realistic part-of-speech tags",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "data",
                "used by  and retagged",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This corpus of 29 million words was provided to us by Michael Collins, and was automatically parsed with the parser described in Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "described in Collins",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpus",
                "provided by Michael Collins",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We guess it is an acronym for the authors of  : Michel Galley, Mark Hopkins, Kevin Knight and Daniel Marcu.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "authors",
                "the authors of",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "authors",
                "Michel Galley, Mark Hopkins, Kevin Knight and Daniel Marcu",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In summary, the strength of our approach is to exploit extremely precise structural clues, and to use 5 Semantic Orientation in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "extremely precise structural clues",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "Semantic Orientation",
                "in ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The syntactic parameters are the same as in Section 5.1 and are smoothed as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic parameters",
                "same as in Section 5.1",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parameters",
                "smoothed as in  ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, if the lexicon contains an adjective excellent, it matches every adjective phrase that includes excellent such as view-excellent etc. As a baseline, we built lexicon similarly by using polarity value of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicon",
                "contains an adjective excellent",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "polarity value",
                "of ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We solve this using the local search defined in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "local search",
                "defined in",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "local search",
                "defined in",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Weeds and Weir   discuss the influence of bias towards highor low-frequency items for different tasks  , and it would not be surprising if the different high-frequency bias were leading to different results.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bias",
                "different high-frequency bias",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "results",
                "different",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Studies on self-training have focused mainly on generative, constituent based parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing",
                "generative, constituent based",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "studies",
                "have focused mainly",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Automatic measures like BLEU   or NIST   do so by counting sequences of words in such paraphrases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "counting sequences of words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "NIST",
                "counting sequences of words",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "ote that the algorithm from Collins   was designed for discriminatively training an HMM-style tagger",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "discriminatively training an HMM-style tagger",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "HMM-style tagger",
                "",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We implement this algorithm using the perceptron framework, as it can be easily modified for structured prediction while preserving convergence guarantees  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron framework",
                "can be easily modified for structured prediction while preserving convergence guarantees",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "algorithm",
                "preserving convergence guarantees",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "e optimize the model weights using a modified version of averaged perceptron learning as described by Collins  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model weights",
                "modified version of averaged perceptron learning",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Collins",
                "described by",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.1 Data Sets Our results are based on syntactic data drawn from the Penn Treebank  , specifically the portion used by CoNLL 2000 shared task  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "used by CoNLL 2000 shared task",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "CoNLL 2000 shared task",
                "specifically",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Inspired by previous work on syntax-driven semantic parsing  , and syntax-based machine translation  , we postulate that syntactically similar sentences with the same predicate also share similar semantic roles.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntax-driven semantic parsing",
                "previous work",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "syntactically similar sentences",
                "share similar semantic roles",
                "PERFORMANCE",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "For practical reasons, the maximum size of a token was set at three for Chinese, andfour forKorean.2 Minimum error rate training   was run on each system afterwardsand BLEU score   was calculated on the test sets.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "token",
                "set at three for Chinese and four for Korean",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU score",
                "calculated on the test sets",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The parser is coupled with an on-line averaged perceptron   as the learning method.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "on-line averaged perceptron",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "learning method",
                "as",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Task Description 2.1 Data Representation Ramshaw and Marcus   gave mainly two kinds of base NPs representation  the open/close bracketing and IOB tagging",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "base NPs",
                "mainly two kinds",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "IOB tagging",
                "gave",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "have been used in statistical machine translation  , terminology research and translation aids  , bilingual lexicography  , word-sense disambiguation   and information retrieval in a multilingual environment  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical machine translation",
                "has been used",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "bilingual lexicography",
                "has been used",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER  , which employs a version of edit distance for word substitution and reordering; METEOR  , which uses stemming and WordNet synonymy; and a linear regression model developed by  , which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CDER",
                "employs a version of edit distance for word substitution and reordering",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "METEOR",
                "uses stemming and WordNet synonymy",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Variations of SCFGs go back to Aho and Ullman  s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu  , which restrict grammar rules to be binary, the synchronous grammars in Chiang  , which use only a single nonterminal symbol, and the Multitext Grammars in Melamed  , which allow independent rewriting, as well as other tree-based models such as Yamada and Knight   and Galley et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SCFGs",
                "go back to Aho and Ullman's Syntax-Directed Translation Schemata",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Multitext Grammars",
                "allow independent rewriting",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "After that, several million instances of people, locations, and other facts were added  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "instances",
                "several million",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "facts",
                "added",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this paper we present a novel PCFG-based architecture for probabilistic generation based on wide-coverage, robust Lexical Functional Grammar   approximations automatically extracted from treebanks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PCFG-based architecture",
                "novel",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "Lexical Functional Grammar approximations",
                "automatically extracted from treebanks",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Most systems extract co-occurrence and syntactic information from the words surrounding the target term, which is then converted into a vector-space representation of the contexts that each target term appears in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "systems",
                "extract co-occurrence and syntactic information",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "vector-space representation",
                "converted into",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Surprisingly, though, rather little work has been devoted to learning local syntactic patterns, mostly noun phrases  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Factored models are introduced in   for better integration of morphosyntactic information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "factored models",
                "for better integration of morphosyntactic information",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "morphosyntactic information",
                "integration",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We used GIZA++ package   to train IBM translation models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ package",
                "train IBM translation models",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "IBM translation models",
                "used",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "To address this issue, many syntax-based approaches   tend to integrate more syntactic information to enhance the non-contiguous phrase modeling.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntax-based approaches",
                "integrate more syntactic information",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase modeling",
                "non-contiguous",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A similar approach is used here, including a collapsed version of the Treebank POS tag set  , with additions for specific words  , compound punctuation  , and a general emoticon tag, resulting in a total of 41 tags.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Treebank POS tag set",
                "collapsed version",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "41 tags",
                "resulting in",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "While we have shown an increase in performance over a purely syntactic baseline model  ), there are a number of avenues to pursue in extending this work.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseline model",
                "purely syntactic",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "performance",
                "increase",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "3.1 Regeneration with Re-decoding One way of regeneration is by running the decoding again to obtain new hypotheses through a re-decoding process  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "regeneration",
                "through a re-decoding process",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "decoding",
                "running again",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he tag propagation/elimination scheme is adopted from  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tag propagation/elimination scheme",
                "is adopted from",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tag propagation/elimination scheme",
                "is adopted from",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The only requirement will be that a parallel corpus exist for the language under consideration and one or more other languages.2 Induction of grammars from parallel corpora is rarely viewed as a promising task in its own right; in work that has addressed the issue directly  , the synchronous grammar is mainly viewed as instrumental in the process of improving the translation model in a noisy channel approach to statistical MT.3 In the present paper, we provide an important prerequisite for parallel corpus-based grammar induction work: an efficient algorithm for synchronous parsing of sentence pairs, given a word alignment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parallel corpus",
                "exist for the language under consideration and one or more other languages",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "synchronous grammar",
                "mainly viewed as instrumental in the process of improving the translation model",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Unlike  , in the shared task we used only the simplest feed-forward approximation, which replicates the computation of a neural network of the type proposed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "neural network",
                "simplest feed-forward approximation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "computation of a neural network",
                "proposed in",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "While earlier approaches for text compression were based on symbolic reduction rules  , more recent approaches use an aligned corpus of documents and their human written summaries to determine which constituents can be reduced  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "symbolic reduction rules",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "constituents",
                "can be reduced",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Methods such as  ,   and   employ a synchronous parsing procedure to constrain a statistical alignment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Methods",
                "synchronous parsing procedure",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical alignment",
                "constrain",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Some methods parse two flat strings at once using a bitext grammar  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bitext grammar",
                "parse two flat strings at once",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "bitext grammar",
                "using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "6 Experiment 6.1 Setup The experiments we report were done on the Penn Treebank WSJ Corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank WSJ Corpus",
                "was done on",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank WSJ Corpus",
                "used in Experiment 6.1 Setup",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Evaluating Heterogeneous Parser Output Two commonly reported shallow parsing tasks are Noun-Phrase   Chunking   and the CoNLL-2000 Chunking task  , which extends the NPChunking task to recognition of 11 phrase types1 annotated in the Penn Treebank.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Parser Output",
                "Two commonly reported shallow parsing tasks",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Penn Treebank",
                "annotated in",
                "APPLICABILITY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "1142 We show that by using a variant of SVM  Anchored SVM Learning   with a polynomial kernel, one can learn accurate models for English NP-chunking  , base-phrase chunking  , and Dutch Named Entity Recognition  , on a heavily pruned feature space.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SVM Anchored SVM Learning",
                "variant of",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "feature space",
                "heavily pruned",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This merging of contexts is different than clustering words  , but is applicable, as word clustering relies on knowing which contexts identify the same category.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "context merging",
                "is different than clustering words",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word clustering",
                "relies on knowing which contexts identify the same category",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "17 The justification for this is that there is an estimated 3% error rate in the hand-assigned POS tags in the treebank  , and we didnt want this noise to contribute to dependency errors.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treebank",
                "hand-assigned POS tags",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "error rate",
                "estimated 3% error rate",
                "PERFORMANCE",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "3.1.2 Kappa Kappa   is an evaluation measure which is increasingly used in NLP annotation work  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa Kappa",
                "is increasingly used",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "evaluation measure",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  present a knowledge-lean algorithm that uses multiple-sequence alignment to 177 learn generate sentence-level paraphrases essentially from unannotated corpus data alone.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "multiple-sequence alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "generate sentence-level paraphrases",
                "essentially from unannotated corpus data alone",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Since  , numerous works have used patterns for discovery and identification of instances of semantic relationships  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "patterns",
                "used for discovery and identification",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "semantic relationships",
                "instances of",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The evaluation metric is case-sensitive BLEU-4  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation metric",
                "case-sensitive BLEU-4",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "BLEU-4",
                "case-sensitive",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "GIZA++ toolkit   is used to perform word alignment in both directions with default settings, and the intersect-diag-grow method is used to generate symmetric word alignment refinement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ toolkit",
                "default settings",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "intersect-diag-grow method",
                "used to generate symmetric word alignment refinement",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For causativity, the same counting scripts were used for both groups of verbs, but the input to the counting programs was determined by manual inspection of the corpus for verbs belonging to group 1, while it was extracted automatically from a parsed corpus for group 2  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "counting scripts",
                "same for both groups of verbs",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "input extraction",
                "manual inspection for group 1, automatic for group 2",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "sentence length: The longer the sentence is, the poorer the parser performs  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "performs poorer",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "sentence length",
                "longer",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We also show that the domain adaptation work of  , which is presented as an ad-hoc preprocessing step, is actually equivalent to our formal model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "domain adaptation work",
                "is equivalent to our formal model",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "ad-hoc preprocessing step",
                "is presented as",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "POS tagging and phrase chunking in English were done using the trained systems provided with the fnTBL Toolkit  ; both were trained from the annotated Penn Treebank corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "fnTBL Toolkit",
                "provided",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank corpus",
                "annotated",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We evaluated the translation quality using the BLEU metric  , as calculated by mteval-v11b.pl with its default setting except that we used case-sensitive matching of n-grams.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mteval-v11b.pl",
                "default setting",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU metric",
                "calculated",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Probabilistic generative models like IBM 1-5  , HMM  , ITG  , and LEAF   define formulas for P  or P , with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1: Word alignment exercise  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilistic generative models",
                "define formulas",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM 1-5, HMM, ITG, and LEAF",
                "define formulas",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In a test set of 756 utterances containing 26 repairs  , they obtained a detection recall rate of 42% and a precision of 84.6%; for correction, they obtained a recall rate of 30% and a precision rate of 62%.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "detection recall rate",
                "42%",
                "PERFORMANCE",
                "neutral",
                0.5
            ],
            [
                "precision",
                "84.6%",
                "PERFORMANCE",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "In Ratnaparkhi  , a maximum entropy tagger is presented.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy tagger",
                "is presented",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tagger",
                "maximum entropy",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The simplest   uses constit  to denote a NP spanning positions 35 in the English string that is aligned with an NP spanning positions 48 in the Chinese string.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "uses",
                "constit",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "positions 35",
                "spanning",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The IBM source-channel model for statistical machine translation   plays a central role in our system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM source-channel model",
                "plays a central role",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "system",
                "central role in",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "of Linguistics University of Potsdam kuhn@ling.uni-potsdam.de Abstract The empirical adequacy of synchronous context-free grammars of rank two    , used in syntaxbased machine translation systems such as Wu  , Zhang et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "synchronous context-free grammars of rank two",
                "used in syntaxbased machine translation systems",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Wu, Zhang et al.",
                "syntaxbased machine translation systems",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "With hand-labeled data, {m} can be learnt via generalized iterative scaling algorithm     or improved iterative scaling    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "generalized iterative scaling",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "improved iterative scaling",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Therefore, including a model based on surface forms, as suggested  , is also necessary.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "based on surface forms",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "including",
                "necessary",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In this work, we employ a syntax-based model that applies a series of tree/string   rules   to a source language string to produce a target language phrase structure tree.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntax-based model",
                "applies a series of tree/string rules",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tree/string rules",
                "to produce a target language phrase structure tree",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "his tagging scheme is the IOB scheme originally put forward by Ramshaw and Marcus  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagging scheme",
                "originally put forward",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IOB scheme",
                "by Ramshaw and Marcus",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 Schone & Jurafsky's results indicate similar results for log-likelihood & T-score, and strong parallelism among information-theoretic measures such as ChiSquared, Selectional Association  , Symmetric Conditional Probability   and the Z-Score  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "results",
                "similar results",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "information-theoretic measures",
                "strong parallelism",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "It is possible to recognize a common structure of these works, based on a typical bootstrap schema  : Step 1: Initial unsupervised categorization.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bootstrap schema",
                "typical",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "unsupervised categorization",
                "Initial",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Some of them have been fully tested in real size texts  ,  ,  , knowledge based methods  ,  , or mixed methods  ,  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "knowledge based methods",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "methods",
                "mixed methods",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "7 Experiments To show the effectiveness of cross-language mention propagation information in improving mention detection system performance in Arabic, Chinese and Spanish, we use three SMT systems with very competitive performance in terms of BLEU11  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT systems",
                "very competitive performance",
                "PERFORMANCE",
                "positive",
                0.75
            ],
            [
                "mention detection system performance",
                "improving",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "his algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs Collins  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "large-margin version of the perceptron algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "for structured outputs",
                "APPLICABILITY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "For further information on these parameter settings, confer  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameter settings",
                "confer",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parameter settings",
                "for further information",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "It is dubious whether SWD is useful regarding recall-oriented metrics like METEOR  , since SWD removes information in source sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SWD",
                "removes information in source sentences",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "recall-oriented metrics like METEOR",
                "dubious whether useful",
                "PERFORMANCE",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "P-STM -l This metric corresponds to the STM metric presented by Liu and Gildea  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "STM metric",
                "presented by Liu and Gildea",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "STM metric",
                "corresponds to",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Usually in 1 In our experiments, we set negative PMI values to 0, because Church and Hanks  , in their seminal paper on word association ratio, show that negative PMI values are not expected to be accurate unless co-occurrence counts are made from an extremely large corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PMI values",
                "are not expected to be accurate",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "corpus",
                "extremely large",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "e propose Smith and Eisners   quasi-synchronous grammar   as a general solution and the Jeopardy model   as a specific instance",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Smith and Eisner's quasi-synchronous grammar",
                "as a general solution",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Jeopardy model",
                "as a specific instance",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Another motivation to evaluate the performance of a phrase translation model that contains only syntactic phrases comes from recent efforts to built syntactic translation models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase translation model",
                "contains only syntactic phrases",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntactic translation models",
                "built",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Our experience suggests that disjunctive LFs are an important capability, especially as one seeks to make grammars reusable across applications, and to employ domain-specific, sentence-level paraphrases  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "disjunctive LFs",
                "important capability",
                "INNOVATION",
                "positive",
                0.75
            ],
            [
                "grammars",
                "reusable across applications",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "We set all weights by optimizing Bleu   using minimum error rate training     on a separate development set of 2,000 sentences  , and we used them in a beam search decoder   to translate 2,000 test sentences   into English.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights",
                "optimized using minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "beam search decoder",
                "used to translate",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Moreover, this evaluation concern dovetails with a frequent engineering concern, that sentence-level scores are useful at various points in the MT pipeline: for example, minimum Bayes risk decoding  , selecting oracle translations for discriminative reranking  , and sentenceby-sentence comparisons of outputs during error analysis.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentence-level scores",
                "useful at various points in the MT pipeline",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "minimum Bayes risk decoding",
                "selecting oracle translations for discriminative reranking",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4.1 The base line For our base line parse accuracy, we used the now standard division of the WSJ   with sections 2 through 21 for training (approx.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "base line",
                "now standard",
                "INNOVATION",
                "positive",
                0.85
            ],
            [
                "WSJ",
                "with sections 2 through 21",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Word correspondence was further developed in IBM Model-1   for statistical machine translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model-1",
                "statistical machine translation",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word correspondence",
                "further developed",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "So we propose forest reranking, a technique inspired by forest rescoring   that approximately reranks the packed forest of exponentially many parses.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "technique",
                "inspired by forest rescoring",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parses",
                "exponentially many",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The alignment of sentences can be done sufficiently well using cues such as sentence length   or cognates  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "cues",
                "sentence length or cognates",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignment of sentences",
                "can be done sufficiently well",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "First, hierarchical word clusters are derived from unlabeled data using the Brown et al. clustering algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brown et al. clustering algorithm",
                "derived from unlabeled data",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Brown et al. clustering algorithm",
                "using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Results in terms of word-error-rate   and BLEU score   are reported in Table 4 for those sentences that contain at least one unknown word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word-error-rate",
                "reported in Table 4",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "BLEU score",
                "reported in Table 4",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Examples of such methods are the introduction of information weights as in the NIST measure or the comparison of stems or synonyms, as in METEOR  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "information weights",
                "NIST measure",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "comparison",
                "METEOR",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Like Haghighi and Klein  , we give our model information about the basic types of pronouns in English.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "information about basic types of pronouns in English",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Haghighi and Klein",
                "basic types of pronouns in English",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "6 Related work Evidence from the surrounding context has been used previously to determine if the current sentence should be subjective/objective   and adjacency pair information has been used to predict congressional votes  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "context",
                "surrounding context",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence",
                "should be subjective/objective",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "A total of 216 collocations were extracted, shown in Appendix A. We compared the collocations in Appendix A with the entries for the above 10 words in the NTC's English Idioms Dictionary    , which contains approximately 6000 definitions of idioms.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocations",
                "extracted",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "English Idioms Dictionary",
                "contains approximately 6000 definitions of idioms",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Hindle   used noun-verb syntactic relations, and Hatzivassiloglou and McKeown   used coordinated adjective-adjective modifier pairs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hindle",
                "noun-verb syntactic relations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Hatzivassiloglou and McKeown",
                "coordinated adjective-adjective modifier pairs",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": " ), while exploring word-to-expression   relations has connections to techniques that employ more of a global-view of corpus statistics  ).1 While most previousresearch exploits only one or the other type of relation, we propose a unified method that can exploit both types of semantic relation, while adapting a general purpose polarity lexicon into a domain specific one.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus statistics",
                "global-view of",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "polarity lexicon",
                "domain specific",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": " ,  ,  ,  ,  ,  ), and to pick those ingredients which are known to be con~i)utationally 'tractable' in some sense.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ingredients",
                "known to be con~i)utationally tractable",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "ingredients",
                "tractable",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Many of the previous studies of Bio-NER tasks have been based on machine learning techniques including Hidden Markov Models    , the dictionary HMM model   and Maximum Entropy Markov Models    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning techniques",
                "including Hidden Markov Models, the dictionary HMM model, and Maximum Entropy Markov Models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dictionary HMM model",
                "HMM model",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size  ,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "unable to distinguish between distinct n-grams",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "parameter",
                "store in constant space independent of both n and the vocabulary size",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The construction is defined in Fillmore's   Construction Grammar as \"\"a pairing of a syntactic pattern with a meaning structure\"\"; they are similar to signs in HPSG   and pattern-concept pairs  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Construction",
                "a pairing of a syntactic pattern with a meaning structure",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Fillmore's Construction Grammar",
                "similar to signs in HPSG and pattern-concept pairs",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "available): SCISSOR  , an integrated syntactic-semantic parser; KRISP  , an SVM-based parser using string kernels; WASP  , a system based on synchronous grammars; Z&C  3, a probabilistic parser based on relaxed CCG grammars; and LU  , a generative model with discriminative reranking.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SCISSOR",
                "integrated syntactic-semantic parser",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "WASP",
                "system based on synchronous grammars",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Several approaches for learning from both labeled and unlabeled data have been proposed   where the unlabeled data is utilised to boost the performance of the algorithm.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "for learning from both labeled and unlabeled data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "performance is boosted",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "16In fact, we have experimented with other tagger combinations and configurations as wellwith the TnT  , MaxEnt   and TreeTagger  , with or without the Morce tagger in the pack; see below for the winning combination.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger combinations",
                "TnT, MaxEnt, TreeTagger, Morce tagger",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tagger combination",
                "winning combination",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "899 To alleviate overfitting on the training examples, we use the refinement strategy called averaged parameters   to the algorithm in Algorithm 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "averged parameters",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "refinement strategy",
                "to alleviate overfitting",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3 The statistical model We use the Xerox part-of-speech tagger  , a statistical tagger made at the Xerox Palo Alto Research Center.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Xerox part-of-speech tagger",
                "statistical tagger made at the Xerox Palo Alto Research Center",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Xerox part-of-speech tagger",
                "statistical tagger",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use MER   to tune the decoders parameters using a development data set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MER",
                "tune the decoders parameters",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "development data set",
                "using",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "First, as originally advocated by Hobbs  , we adopt an ONTOLOGICALLY PROMISCUOUS representation that includes a wide variety of types of entities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "representation",
                "ONTOLOGICALLY PROMISCUOUS",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "types of entities",
                "wide variety",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A simple example is shown in Figure 1, where the arc between a and hat indicates that hat is the head of a. Current statistical dependency parsers perform better if the dependency lengthes are shorter  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency parsers",
                "perform better",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "dependency lengthes",
                "shorter",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he benefits of using grammatical information for automatic WSD were first explored by Yarowsky   and Resnik   in unsupervised approaches to disambiguating single words in context",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Yarowsky and Resnik",
                "unsupervised approaches",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "WSD",
                "automatic",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For each word in LDV, three existing thesauri are consulted: Rogets Thesaurus  , Collins COBUILD Thesaurus  , and WordNet  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "thesauri",
                "existing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "WordNet",
                "no opinion term found",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Some stem from work on graphical models,includingloopybeliefpropagation , Gibbs sampling  , sequential Monte Carlo methods such as particle filtering  , and variational inference  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "he corresponding weight is trained through minimum error rate method  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weight",
                "trained through minimum error rate method",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "minimum error rate method",
                "used for training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "These words and phrases are usually compiled using different approaches (Hatzivassiloglou and McKeown, 1997; Kaji and Kitsuregawa, 2006; Kanayama and Nasukawa, 2006; Esuli and Sebastiani, 2006; Breck et al, 2007; Ding, Liu and Yu.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "different",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Kaji and Kitsuregawa, 2006",
                "novel",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "  proposed using GIZA++   to align words between the backbone and hypothesis.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "to align words",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "backbone and hypothesis",
                "align",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For each training direction, we run GIZA++  , specifying 5 iterations of Model 1, 4 iterations of the HMM model  , and 4 iterations of Model 4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "specifying 5 iterations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "HMM model",
                "4 iterations",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Based on the observations in  , we also limited the phrase length to 3 for computational reasons.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase length",
                "limited to 3",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "computational reasons",
                "no opinion term found",
                "LIMITATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "All words occurring less than 3 times in the training data, and words in test data that were not seen in training, are unknown words and are replaced with the UNKNOWN token. Note this threshold is smaller than the one used in   since the corpora used in our experiments are smaller.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "occurring less than 3 times",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "threshold",
                "smaller than the one used",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Shen et al.,   report an accuracy of 97.33% on the same data set using a perceptron-based bidirectional tagging model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bidirectional tagging model",
                "perceptron-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "accuracy",
                "97.33%",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Pang et al. proposed a method of classifying movie reviews into positive and negative ones  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "classifying movie reviews",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "reviews",
                "positive and negative ones",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The generator used in our experiments is an instance of the second type, using a probability model defined over Lexical Functional Grammar c-structure and f-structure annotations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generator",
                "probability model defined over Lexical Functional Grammar c-structure and f-structure annotations",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probability model",
                "defined over Lexical Functional Grammar c-structure and f-structure annotations",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "While Schiitze and Pedersen  , Brown et al   and Futrelle and Gauch   all demonstrate the ability of their systems to identify word similarity using clustering on the most frequently occurring words in their corpus, only Grefenstette   demonstrates his system by generating word similarities with respect to a set of target words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Schiitze and Pedersen",
                "ability to identify word similarity",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "Grefenstette",
                "generating word similarities",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "6.1 Interand Intra-annotator agreement We measured pairwise agreement among annotators in computational linguistics for measuring agreement in category judgments  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotators",
                "pairwise agreement",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "computational linguistics",
                "measuring agreement in category judgments",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5We use deterministic sampling, which is useful for reproducibility and for minimum error rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "deterministic sampling",
                "is useful for reproducibility and for minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "deterministic sampling",
                "is useful",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "PB, available at www.cis.upenn.edu/ace, is used along with the Penn TreeBank 2    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PB",
                "available at www.cis.upenn.edu/ace",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Penn TreeBank 2",
                "used along with",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For each language pair, we use two development sets: one for Minimum Error Rate Training  , and the other for tuning the scale factor for MBR decoding.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "development sets",
                "one for Minimum Error Rate Training and the other for tuning the scale factor for MBR decoding",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "scale factor",
                "tuning",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In particular, we need to develop a backoff strategy for unseen pairs in the relational similarity tasks, that, following Turney  , could be based on constructing surrogate pairs of taxonomically similar words found in the CxLC space.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CxLC space",
                "found in",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "surrogate pairs",
                "based on constructing",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We compare system performance between   and our framework in Section 5.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system performance",
                "between and our framework",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "our framework",
                "our",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Finally, it should be noted that in the current implementation, we have not applied any of the possible optimizations that appear in the literature   to speed up normalization of the probability distribution q. These improvements take advantage of a models structure to simplify the evaluation of the denominator in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probability distribution q",
                "normalization",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "optimizations in the literature",
                "speed up normalization",
                "METHODOLOGY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "Unknown words were not identified in   as a useful predictor for the benefit of self-training.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unknown words",
                "useful predictor",
                "PERFORMANCE",
                "neutral",
                0.5
            ],
            [
                "self-training",
                "benefit",
                "APPLICABILITY",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "2 Related Research Several researchers   have already proposed methods for binarizing synchronous grammars in the context of machine translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "synchronous grammars",
                "in the context of machine translation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "section 20 Majority voting         accuracy precision O:98.10% C:98.29% 93.63% O:98.1% C:98.2% 93.1% 97.58% 92.50% 97.37% 91.80% 91.6% recall FZ=I 92.89% 93.26 92.4% 92.8 92.25% 92.37 92.27% 92.03 91.6% 91.6 section 00 accuracy precision Majority voting 0:98.59% C:98.65% 95.04% r   98.04% 93.71%   97.8% 93.1% recall FB=I 94.75% 94.90 93.90% 93.81 93.5% 93.3 Table 3: The results of majority voting of different data representations applied to the two standard data sets put forward by   compared with earlier work.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Majority voting",
                "accuracy 98.10%",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "majority voting",
                "precision 98.29%",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Example of such algorithms are   and   that use syntactic features in the vector definition.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "use syntactic features in the vector definition",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntactic features",
                "in the vector definition",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Dagan, Church, and Gale   expanded on this idea by replacing Brown et al.'s   word alignment parameters, which were based on absolute word positions in aligned segments, with a much smaller set of relative offset parameters.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word alignment parameters",
                "based on absolute word positions in aligned segments",
                "METHODOLOGY",
                "neutral",
                0.7
            ],
            [
                "relative offset parameters",
                "much smaller set",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "3 Experimental Results and Discussion We test our parsing models on the CONLL-2007   data set on various languages including Arabic, Basque, Catalan, Chinese, English, Italian, Hungarian, and Turkish.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CONLL-2007 data set",
                "various languages",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "languages",
                "including Arabic, Basque, Catalan, Chinese, English, Italian, Hungarian, and Turkish",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  simplify these probability distributions, as given in Equations 9 and 10.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probability distributions",
                "simplify",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probability distributions",
                "as given in Equations 9 and 10",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Since we also adopt a linear scoring function in Equation  , the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training   algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "combination model",
                "can also be tuned on a development data set",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Minimum Error Rate Training algorithm",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The orientation model is related to the distortion model in  , but we do not compute a block alignment during training.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "orientation model",
                "related to the distortion model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "block alignment",
                "do not compute",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One such technique is bootstrapping, which was recently presented in  ,   as an ideal framework for text learning tasks that have knowledge seeds.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bootstrapping",
                "ideal framework",
                "METHODOLOGY",
                "positive",
                0.7
            ],
            [
                "text learning tasks",
                "have knowledge seeds",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For a given choice of q and f, the IIS algorithm   can be used to find maximum likelihood values for the parameters ~.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IIS algorithm",
                "can be used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parameters ~",
                "find maximum likelihood values",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The reported results for the full parse tree   are recall/precision of 88.1/87.5  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "recall/precision",
                "of 88.1/87.5",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "results",
                "are reported",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "They have used the   representation as well  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "representation",
                "as well",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "representation",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our decoder is a phrase-based multi-stack implementation of the log-linear model similar to Pharaoh  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "phrase-based multi-stack implementation of the log-linear model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Pharaoh",
                "similar to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the sequel, we use Collinss statistical parser   as our canonical automated approximation of the Treebank.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collinss statistical parser",
                "as our canonical automated approximation of the Treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Treebank",
                "canonical",
                "APPLICABILITY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Heuristic approaches obtain word alignments by using various similarity functions between the types of the two languages  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "similarity functions",
                "various",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word alignments",
                "obtain by",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Some researchers   use manually designed rules to take into account the grammatical role of the antecedent candidates as well as the governing relations between the candidate and the pronoun, while others use features determined over the parse tree in a machine-learning approach  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "antecedent candidates",
                "grammatical role",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parse tree features",
                "machine-learning approach",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A class of training criteria that provides a tighter connection between the decision rule and the final error metric is known as Minimum Error Rate Training   and has been suggested for SMT in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Minimum Error Rate Training",
                "provides a tighter connection",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "SMT",
                "has been suggested",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "7 For a more detailed discussion, see Berger, Della Pietra, and Della Pietra   and Ratnaparkhi  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Berger, Della Pietra, and Della Pietra, and Ratnaparkhi",
                "more detailed discussion",
                "INNOVATION",
                "neutral",
                0.5
            ],
            [
                "Berger, Della Pietra, and Della Pietra, and Ratnaparkhi",
                "see",
                "APPLICABILITY",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "This has been now an active research area for a couple of decades  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research area",
                "active research area",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "decades",
                "couple of decades",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In an attempt to provide a quantitative evaluation of our results, for each of the 12 ambiguous words shown in table 1 we manually assigned the top 30 first-order associations to one of the two senses provided by Yarowsky  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ambiguous words",
                "top 30 first-order associations",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "Yarowsky",
                "provided by",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This is seen in that each time we check for the nearest intersection to the current 1-best for some n-best list l, we Algorithm 1 Och  s line search method to find the global minimum in the loss, lscript, when starting at the point w and searching along the direction d using the candidate translations given in the collection of n-best lists L. Input: L, w, d, lscript I {} for l L do for e  l do m{e} e.features d b{e} e.features w end for bestn argmaxel m{e}{b{e} breaks ties} loop bestn+1 = argminel max parenleftBig 0, b{bestn}b{e}m{e}m{bestn} parenrightBig intercept  max parenleftBig 0, b{bestn}b{bestn+1}m{bestn+1}m{bestn} parenrightBig if intercept > 0 then add  else break end if end loop end for add +2epsilon1) ibest = argminiI evallscript d) return w+ d must calculate its intersection with all other candidate translations that have yet to be selected as the 1-best.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Algorithm 1 Och s line search method",
                "to find the global minimum",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "n-best lists L",
                "given in the collection",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For mutual information  , we use two different equations: one for two-element compound nouns   and the other for three-element compound nouns  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "compound nouns",
                "two-element or three-element",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "equations",
                "different",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "6 Concluding remarks Our work presents a set of improvements on previous state of the art of Grammar Association: first, by providing better language models to the original system described in  ; second, by setting the technique into a rigorous statistical framework, clarifying which kind of probabilities have to be estimated by association models; third, by developing a novel and especially adequate association model: Loco C. On the other hand, though experimental results are quite good, we find them particularly relevant for pointing out directions to follow for further improvement of the Grammar Association technique.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Grammar Association",
                "set of improvements on previous state of the art",
                "INNOVATION",
                "positive",
                0.85
            ],
            [
                "experimental results",
                "quite good",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Candidate translations are scored by a linear combination of models, weighted according to Minimum Error Rate Training or MERT  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "weighted according to Minimum Error Rate Training or MERT",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translations",
                "scored by a linear combination",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, Weeds     took verbs as contexts for nouns in object position: so they regarded two nouns to be similar to the extent that they occur as direct objects of the same set of verbs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Weeds",
                "took verbs as contexts for nouns in object position",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "nouns",
                "similar to the extent that they occur as direct objects of the same set of verbs",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Of these, only feature weights can be trained, for which we used minimum error rate training with version 1.04 of IBM-style BLEU   in case-insensitive mode.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "can be trained",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM-style BLEU",
                "version 1.04",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In each iteration of local search, we look in the neighborhood of the current best alignment for a better alignment  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "better alignment",
                "better",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "current best alignment",
                "current",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Bidirectional Dependency Networks When building probabilistic models for tag sequences, we often decompose the global probability of sequences using a directed graphical model   or a conditional Markov model    ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilistic models",
                "decompose the global probability of sequences",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "directed graphical model",
                "or a conditional Markov model",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This set of context vectors is then clustered into a predetermined number of coherent clusters or context groups using Buckshot  , a combination of the EM algorithm and agglomerative clustering.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Buckshot",
                "combination of the EM algorithm and agglomerative clustering",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "context vectors",
                "clustered into a predetermined number of coherent clusters",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It is an important and growing field of natural language processing with applications in areas such as transferbased machine translation   and sentence condensation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "natural language processing",
                "important and growing field",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "machine translation",
                "transfer-based",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Several representations to encode region information are proposed and examined  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "representations",
                "proposed and examined",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "region information",
                "encode",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 This problem is also a central concern in the work by Bean and Riloff  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work by Bean and Riloff",
                "central concern",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "problem",
                "central concern",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4 Evaluation The evaluation is conducted with all four corpora from Bakeoff-3  , as summarized in Table 1 with corpus size in number of characters.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "from Bakeoff-3",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "corpora size",
                "number of characters",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In the second experiment, the basic learning model is Collinss   Model 2 parser, which uses a history-based learning algorithm that takes statistics directly over the treebank.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collinss Model 2 parser",
                "basic learning model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "history-based learning algorithm",
                "uses statistics directly over the treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this paper we use a non-projective dependency tree CRF  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CRF",
                "non-projective dependency tree",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "dependency tree",
                "non-projective",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Translation rules can:  look like phrase pairs with syntax decoration: NPB  NNP  NNP  NNP ) BUFDFKEUBWAZ  carry extra contextual constraints: VP  x0:SBAR-C) DKx0    be non-constituent phrases: VP  SBAR-C  x0:S-C)) DKx0 VP  PRT ) x0:SBAR-C) DXGPx0  contain non-contiguous phrases, effectively phrases with holes: PP  NP-C  x0:NNP)) NN ))) GRx0 EVABG6 PP  NP-C  NN ) x0:PP)) GRx0 EVEVABABG6  be purely structural  : S x0 x1  re-order their children: NP-C  x0:NN) PP  x1:NP-C)) x1 DFx0 Decoding with this model produces a tree in the target language, bottom-up, by parsing the foreign string using a CYK parser and a binarized rule set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Translation rules",
                "look like phrase pairs with syntax decoration",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Decoding with this model",
                "produces a tree in the target language, bottom-up",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Deterministic Annealing: In this system, instead of using the regular MERT   whose training objective is to minimize the onebest error, we use the deterministic annealing training procedure described in Smith and Eisner  , whose objective is to minimize the expected error  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MERT",
                "regular",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "deterministic annealing training procedure",
                "minimize the expected error",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "im and Hovy   make a similar assumption",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hovy",
                "make a similar assumption",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "assumption",
                "similar",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We utilise the automatic annotation algorithm of   to derive a version of Penn-II where each node in each tree is annotated with an LFG functional annotation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "automatic annotation algorithm",
                "LFG functional annotation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn-II",
                "each node in each tree is annotated",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Phrases are then extracted from the word alignments using the method described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "described in",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "phrases",
                "extracted from word alignments",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Typical approaches to conversion of constituent structures into dependencies are based on handconstructed head percolation rules, an idea that has its roots in lexicalized constituent parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "head percolation rules",
                "an idea that has its roots in lexicalized constituent parsing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "handconstructed",
                "purely descriptive",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The quality of the translation output is mainly evaluated using BLEU, with NIST   and METEOR   as complementary metrics.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "BLEU, NIST, METEOR",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "translation output",
                "evaluated using",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "arowsky   used the one sense per collocation property as an essential ingredient for an unsupervised Word-SenseDisambiguationalgorithm",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "one sense per collocation property",
                "essential ingredient",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Word-SenseDisambiguation algorithm",
                "unsupervised",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, Smadja   suggests a basic characteristic of collocations and multiword units is recurrent, domain-dependent and cohesive lexical clusters.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocations and multiword units",
                "recurrent, domain-dependent and cohesive lexical clusters",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "lexical clusters",
                "cohesive",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The simplest one is the BIO representation scheme  , where a B denotes the first item of an element and an I any non-initial item, and a syllable with tag O is not a part of any element.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BIO representation scheme",
                "simplest",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "syllable with tag O",
                "not a part of any element",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However, CHECK moves are almost always about some information which the speaker has been told \\  -a description that models the backward looking functionality of a dialogue act.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CHECK moves",
                "backward looking functionality",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "speaker has been told",
                "description that models",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "This was a difcult challenge as many participants in the task failed to obtain any meaningful gains from unlabeled data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "participants",
                "failed to obtain any meaningful gains",
                "PERFORMANCE",
                "negative",
                0.75
            ],
            [
                "unlabeled data",
                "was difcult to obtain meaningful gains from",
                "METHODOLOGY",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "A very common case of this in the CoNLL dataset is that of documents containing references to both The China Daily, a newspaper, and China, the country  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CoNLL dataset",
                "common case",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "China Daily",
                "newspaper",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "After maximum BLEU tuning   on a held-out tuning set, we evaluate translation quality on a held-out test set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "tuning",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "translation quality",
                "evaluate",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "In a second top-down pass similar to Huang and Chiang  , we can recalculate psyn  for alternative derivations in the hypergraph; potentially correcting search errors made in the first pass.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "psyn",
                "alternative derivations in the hypergraph",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "search errors",
                "made in the first pass",
                "LIMITATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "73 1.2.2 Baseline System and Experimental Setup We take BBNs HierDec, a string-to-dependency decoder as described in  , as our baseline for the following two reasons:  It provides a strong baseline, which ensures the validity of the improvement we would obtain.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BBNs HierDec",
                "provides a strong baseline",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "baseline",
                "ensures the validity of the improvement",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We measure semantic similarity using the shortest path length in WordNet   as implemented in the WordNet Similarity package  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet Similarity package",
                "as implemented",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "shortest path length",
                "in WordNet",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "aximum Entropy models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence but otherwise is as uniform as possible  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Entropy models",
                "is as uniform as possible",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Entropy models",
                "consistent with the set of constraints imposed by the evidence",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Identification of Terms To-be Transliterated   must not be confused with recognition of Named Entities    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Terms To-be Transliterated",
                "must not be confused with",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Named Entities",
                "recognition of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The WSJNPVP set consists of part-of speech tagged Wall Street Journal material  , supplemented with syntactic tags indicating noun phrase and verb phrase boundaries  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSJNPVP set",
                "part-of speech tagged Wall Street Journal material",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntactic tags",
                "indicating noun phrase and verb phrase boundaries",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "6 Related Work As suggested in  , an alternative method for the optimization of the unsmoothed error count is Powells algorithm combined with a grid-based line optimization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Powells algorithm",
                "alternative method",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "grid-based line optimization",
                "combined with",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Dependency models   use the parsed dependency structure of sentences to build the language model as in grammatical trigrams  , structured language models  , and dependency language models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency models",
                "parsed dependency structure of sentences",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dependency language models",
                "structured language models",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.5 Consistency of Annotations In order to assess the consistency of annotation, we follow Carletta   in using Cohen's ~, a chancecorrected measure of inter-rater agreement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Cohen's ~",
                "chance-corrected measure of inter-rater agreement",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Carletta",
                "in using",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Dredze et al. yielded the second highest score1 in the domain adaptation track  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "domain adaptation track",
                "second highest score",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Dredze et al.",
                "yielded",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Much work has gone into methods for measuring synset similarity; early work in this direction includes  , which attempted to discover sense similarities between dictionary senses.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "measuring synset similarity",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "early work",
                "attempted to discover sense similarities",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Therefore, Lin and Och   introduced skip-bigram statistics for the evaluation of machine translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "skip-bigram statistics",
                "evaluation of machine translation",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Lin and Och",
                "introduced",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Much of the recent work in word alignment has focussed on improving the word alignment quality through better modeling   or alternative approaches to training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word alignment quality",
                "improving",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "word alignment",
                "better modeling or alternative approaches",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The experimental results in   show a negative impact on the parsing accuracy from too long dependency relation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency relation",
                "too long",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "parsing accuracy",
                "negative impact",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "In retrospect, however, there are perhaps even greater similarities to that of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "similarities",
                "greater similarities",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "similarities",
                "to that of  ",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Methods that use bigrams   or trigrams   cluster words considering as a word's context the one or two immediately adjacent words and employ as clustering criteria the minimal loss of average 836 nmtual information and the perplexity improvement respectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "cluster words considering as a word's context the one or two immediately adjacent words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "clustering criteria",
                "minimal loss of average 836 nmtual information and the perplexity improvement respectively",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Shallow parsing has received a reasonable amount of attention in the last few years  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "shallow parsing",
                "received a reasonable amount of attention",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "last few years",
                "has received attention",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Our methods are most influenced by IBMs Model 1  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "influenced by IBM's Model 1",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Model 1",
                "IBM's",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Further, it has been shown   that performance of Lins distributional similarity score decreases more significantly than other measures for low frequency nouns.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Lins distributional similarity score",
                "decreases more significantly",
                "PERFORMANCE",
                "negative",
                0.75
            ],
            [
                "low frequency nouns",
                "other measures",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "5 Parsing experiments 5.1 Data and setup We used the standard partitions of the Wall Street Journal Penn Treebank  ; i.e., sections 2-21 for training, section 22 for development and section 23 for evaluation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "standard partitions",
                "of the Wall Street Journal Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sections 2-21",
                "for training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "They can be seen as extensions of the simpler IBM models 1 and 2  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM models 1 and 2",
                "extensions of",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM models 1 and 2",
                "simpler",
                "METHODOLOGY",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "The IBM translation models   describe word reordering via a distortion model defined over word positions within sentence pairs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM translation models",
                "describe word reordering via a distortion model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "distortion model",
                "defined over word positions within sentence pairs",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ord Alignment Quality Metrics 3.1 Alignment Error Rate is Not a Useful Measure We begin our study of metrics for word alignment quality by testing AER  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Alignment Error Rate",
                "is Not a Useful Measure",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "AER",
                "is tested",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In training process, we use GIZA++ 4 toolkit for word alignment in both translation directions, and apply grow-diag-final method to refine it  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ 4 toolkit",
                "word alignment in both translation directions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "grow-diag-final method",
                "refine it",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, the maximum entropy   was found to yield higher accuracy than nave Bayes in a subsequent comparison by Klein and Manning  , who used a different subset of either Senseval-1 or Senseval-2 English lexical sample data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bayes",
                "yield higher accuracy",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "Senseval-1 or Senseval-2 English lexical sample data",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.2 Translation quality Table 2 presents the impact of parse quality on a treelet translation system, measured using BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parse quality",
                "on a treelet translation system",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "BLEU",
                "measured using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This is also true for reranking and discriminative training, where the k-best list of candidates serves as an approximation of the full set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "k-best list of candidates",
                "serves as an approximation of the full set",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "reranking and discriminative training",
                "true for",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Previous research in automatic acquisition focuses primarily on the use of statistical techniques, such as bilingual alignment   or extraction of syntactic constructions from online dictionaries and corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical techniques",
                "use of statistical techniques",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "online dictionaries and corpora",
                "extraction of syntactic constructions from",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Probabilistic translation models generally seek to find the translation string e that maximizes the probability Pra5 ea6fa7, given the source string f  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Probabilistic translation models",
                "seek to find the translation string that maximizes the probability",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation string",
                "maximizes the probability",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Both were 5gram models with modified Kneser-Ney smoothing, lossily compressed using a perfect-hashing scheme similar to that of Talbot and Brants   but using minimal perfect hashing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kneser-Ney smoothing",
                "modified",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "perfect-hashing scheme",
                "similar to that of Talbot and Brants but using minimal",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The maximum entropy classier   used is Le Zhang's Maximum Entropy Modeling Toolkit and the L-BFGS parameter estimation algorithm with gaussian prior smoothing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy Modeling Toolkit",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "L-BFGS parameter estimation algorithm",
                "with gaussian prior smoothing",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Therefore, the Viterbi alignment is comlmted only approximately using the method described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Viterbi alignment",
                "is comlmted only approximately",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "method described in  ",
                "approximate",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The task originally emerged as an intermediate result of training the IBM translation models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM translation models",
                "training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "task",
                "emerged as an intermediate result",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3.2 Probability structure of the original model We use p to denote the unlexicalized nonterminal corresponding to P, and similarly for li, ri and h. We now present the top-level generation probabilities, along with examples from 4The inclusion of the word feature in the BBN model was due to the work described in  , where word features helped reduce part of speech ambiguity for unknown words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "p",
                "nonterminal",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word features",
                "helped reduce part of speech ambiguity",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "2 Phrase-based statistical machine translation Phrase-based SMT uses a framework of log-linear models   to integrate multiple features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Phrase-based SMT",
                "uses a framework of log-linear models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "multiple features",
                "to integrate",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 minority report 2 box office 3 scooby doo 4 sixth sense 5 national guard 6 bourne identity 7 air national guard 8 united states 9 phantom menace 10 special effects 11 hotel room 12 comic book 13 blair witch project 14 short story 15 real life 16 jude law 17 iron giant 18 bin laden 19 black people 20 opening weekend 21 bad guy 22 country bears 23 mans man 24 long time 25 spoiler space 26 empire strikes back 27 top ten 28 politically correct 29 white people 30 tv show 31 bad guys 32 freddie prinze jr 33 monsters ball 34 good thing 35 evil minions 36 big screen 37 political correctness 38 martial arts 39 supreme court 40 beautiful mind Figure 7: Result of re-ranking output from the phrase extension module 6.4 Revisiting unigram informativeness An alternative approach to calculate informativeness from the foreground LM and the background LM is just to take the ratio of likelihood scores, a11 fga9a54a86 a15 a23 a11 bga9a54a86 a15 . This is a smoothed version of relative frequency ratio which is commonly used to find subject-specific terms  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "foreground LM",
                "commonly used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "relative frequency ratio",
                "smoothed version",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Online baselines include Top-1 Perceptron  , Top-1 Passive-Aggressive  , and k-best PA  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Perceptron",
                "Top-1",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Passive-Aggressive",
                "Top-1",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Berger et al. 1996 presented a way of computing conditional maximum entropy models directly by modifying equation 6 as follows   ): i ~Cx~) = ~ f~  * ~  ~ ~ .~  * ~  * pCy I ~) = p    x6X yEY xEX yEY where ~  is an empirical probability of a joint configuration   of certain instantiated factor I variables with certain instantiated behavior variables.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "conditional maximum entropy models",
                "directly by modifying equation 6",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "empirical probability",
                "of a joint configuration of certain instantiated factor I variables with certain instantiated behavior variables",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.4 GermanEnglish For GermanEnglish, we additionally incorporated rule-based reordering  We parse the input using the Collins parser   and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins parser",
                "parse the input",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "reordering rules",
                "re-arrange the German sentence",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Rules have the form X  e, f, where e and f are phrases containing terminal symbols   and possibly co-indexed instances of the nonterminal symbol X.2 Associated with each rule is a set of translation model features, i ; for example, one intuitively natural feature of a rule is the phrase translation  probability   = log p  , directly analogous to the corresponding feature in non-hierarchical phrase-based models like Pharaoh  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model features",
                "log p, directly analogous to the corresponding feature in non-hierarchical phrase-based models like Pharaoh  ",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "rule",
                "has the form X  e, f, where e and f are phrases containing terminal symbols   and possibly co-indexed instances of the nonterminal symbol X",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Daume III   divided features into three classes: domainindependent features, source-domain features and target-domain features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "into three classes",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "features",
                "domain-independent, source-domain, target-domain",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.2 Generalization pseudocode In order to identify the portions in common between the patterns, and to generalize them, we apply the following pseudocode  : 1All the PoS examples in this paper are done with Penn Treebank labels  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank labels",
                "are done with",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "pseudocode",
                "identify the portions in common",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We employ a robust statistical parser   to determine the constituent structure for each sentence, from which subjects  , objects  , and relations other than subject or object   are identified.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "robust statistical",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "constituent structure",
                "identified",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "440 respondence learning   domain adaptation algorithm   for use in sentiment classification.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "respondence learning domain adaptation algorithm",
                "for use in sentiment classification",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "respondence learning",
                "domain adaptation algorithm",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "We analyze our results using syntactic features extracted from a parse tree generated by Collins parser   and compare those to models built using features extracted from FrameNets human annotations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins parser",
                "generated by",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "models built using features extracted from FrameNets human annotations",
                "human annotations",
                "PERFORMANCE",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "2This can explain why previous attempts to use WordNet for generating sentence-level paraphrases   were unsuccessful.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet",
                "unsuccessful",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "previous attempts",
                "were unsuccessful",
                "LIMITATION",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "From this aligned training corpus, we extract the phrase pairs according to the heuristics in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "heuristics",
                "in ",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase pairs",
                "according to",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It is sometimes assumed that estimates of entropy   upper bound of 1.75 bits per character for printed English) are directly 3There are some cases where words are deliberately misspelled in order to get better output from the synthesizer, such as coyote spelled kiote.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "entropy estimates",
                "upper bound of 1.75 bits per character for printed English",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "output from the synthesizer",
                "better",
                "PERFORMANCE",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "5-gram word language models in English are trained on a variety of monolingual corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "5-gram word language models",
                "trained on a variety of monolingual corpora",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "monolingual corpora",
                "variety of",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In the following sections, we present the best performing set of feature templates as determined on the development data set using only the supervised training setting; our feature templates have thus not been influenced nor extended by the unsupervised data.13 11The full list of tags, as used by  , also makes the underlying Viterbi algorithm unbearably slow.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature templates",
                "have not been influenced nor extended by unsupervised data",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "Viterbi algorithm",
                "unbearably slow",
                "METHODOLOGY",
                "negative",
                0.9
            ]
        ]
    },
    {
        "text": "7 In the models described in Collins  , there was a third question concerning punctuation:   Does the string contain 0, 1, 2 or more than 2 commas?",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "described in Collins",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "string",
                "contain 0, 1, 2 or more than 2 commas",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "We use the default configuration of the measure in WordNet::Similarity-0.12 package  , and, with a single exception, the measure performed below Gic; see BP in table 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measure",
                "default configuration",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "measure",
                "performed below Gic",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "4.2 Translation Results The evaluation metrics used in our experiments are WER  , PER   and BLEU    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation metrics",
                "WER, PER, and BLEU",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "WER, PER, and BLEU",
                "evaluation metrics used in our experiments",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Building upon the large body of research to improve tagging performance for various languages using various models  ) and the recent work on PCFG grammars with latent annotations  , we will investigate the use of fine-grained latent annotations for Chinese POS tagging.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "large body of research",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "PCFG grammars",
                "recent work",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "3 Incremental Parsing Method Based on Adjoining Operation In order to avoid the problem of infinite local ambiguity, the previous works have adopted the following approaches:   a beam search strategy  ,   limiting the allowable chains to those actually observed in the treebank  , and   transforming the parse trees with a selective left-corner transformation   before inducing the allowable chains and allowable triples  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Incremental Parsing Method",
                "based on adjoining operation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "beam search strategy",
                "limiting the allowable chains",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Feature function scaling factors m are optimized based on a maximum likely approach   or on a direct error minimization approach  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "m",
                "optimized",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "approach",
                "maximum likely or direct error minimization",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The majority of this research was done on extending the tree structure   or enriching WN with new relationships  ) rather than improving the quality of existing concept/synset nodes.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tree structure",
                "extending",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "concept/synset nodes",
                "quality",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Combining statistical and parsing methods has been done by   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "statistical and parsing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "has been done by",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "To simulate real world scenario, we use n-best lists from ISIs state-of-the-art statistical machine translation system, AlTemp  , and the 2002 NIST Chinese-English evaluation corpus as the test corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "AlTemp",
                "state-of-the-art",
                "METHODOLOGY",
                "positive",
                0.85
            ],
            [
                "2002 NIST Chinese-English evaluation corpus",
                "test corpus",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The observation that shallow syntactic information can be extracted using local information by examining the pattern itself, its nearby context and the local part-of-speech information has motivated the use of learning methods to recognize these patterns  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pattern",
                "can be extracted using local information",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "learning methods",
                "recognize these patterns",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Others use sentence cohesion  , agreement/disagreement between speakers  , or structural adjacency.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentence cohesion",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "agreement/disagreement between speakers",
                "or",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Practically all data-driven models that have been proposed for dependency parsing in recent years can be described as either graph-based or transitionbased  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "graph-based or transition-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "models",
                "proposed for dependency parsing",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As the training data from DVDs is much more similar to books than that from kitchen  , we should give the data from DVDs a higher weight.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training data",
                "from DVDs",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "weight",
                "a higher",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ang et al   considered the same problem and presented a set of supervised machine learning approaches to it",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning approaches",
                "set of supervised",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "problem",
                "same",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The current version of the dataset gives semantic tags for the same sentencesas inthe PennTreebank  , whichareexcerptsfromtheWallStreetJournal.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PennTreebank",
                "excerpts from the Wall Street Journal",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "semantic tags",
                "gives",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Moreover, an F-score optimization method for logistic regression has also been proposed  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "F-score optimization method",
                "has also been proposed",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "logistic regression",
                "has also been proposed",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "  that draws on a stochastic tagger   for details) as well as the SPECIALIST Lexicon5, a large syntactic lexicon of both general and medical English that is distributed with the UMLS.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SPECIALIST Lexicon",
                "large syntactic lexicon of both general and medical English",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "UMLS",
                "distributed with",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The features used by the decoder were the English language model log probability, logf , the lexical translation log probabilities in both directions  , and a word count feature.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "English language model log probability, logf, lexical translation log probabilities in both directions, and word count feature",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "decoder",
                "uses",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Alignment is often used in training both generative and discriminative models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Alignment",
                "used in training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "generative and discriminative models",
                "both",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For instance, both Pang and Lee   and Turney   consider the thumbs up/thumbs down decision: is a film review positive or negative?",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "thumbs up/thumbs down decision",
                "positive or negative",
                "PERFORMANCE",
                "neutral",
                0.5
            ],
            [
                "film review",
                "positive or negative",
                "INNOVATION",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "Finally, inducing lexical semantics from distributional data  ) is also a form of surface cueing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexical semantics",
                "distributional data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "surface cueing",
                "is also a form of",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "t is known that ITGs do not induce the class of inside-out alignments discussed in Wu  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITGs",
                "do not induce",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "class of inside-out alignments",
                "discussed in Wu",
                "INNOVATION",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "These blocks are used to compute the results in the fourth column: the BLEU score   with a153 reference translation using a153 -grams along with 95% confidence interval is reported 4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "with a153 reference translation using a153-grams along with 95% confidence interval",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "confidence interval",
                "95%",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "  present a chart generator using wide-coverage PCFG-based LFG approximations automatically acquired from treebanks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PCFG-based LFG approximations",
                "automatically acquired from treebanks",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "chart generator",
                "using wide-coverage",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The preprocessed training data was filtered for length and aligned using the GIZA++ implementation of IBM Model 4   in both directions and symmetrized using the grow-diag-final-and heuristic.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ implementation of IBM Model 4",
                "aligned using",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "heuristic",
                "symmetrized using",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "But there is also extensive research focused on including linguistic knowledge in metrics   among others.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linguistic knowledge",
                "including",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "metrics",
                "among others",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2004) use an information extraction engine to extract linguistic features from documents relevant to the target term",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "information extraction engine",
                "extract linguistic features",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "target term",
                "documents relevant",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2.2 Word Alignment Aligning below the sentence level is usually done using statistical models for machine translation   where any word of the targetlanguageistakentobeapossibletranslation for each source language word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical models",
                "for machine translation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word of the target language",
                "to be a possible translation",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The prime public domain examples of such implementations include the TrigramsnTags tagger  , Xerox tagger   and LT POS tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TrigramsnTags tagger",
                "tagger",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Xerox tagger",
                "tagger",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "bney   presented a thorough discussion on the Yarowsky algorithm",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Yarowsky algorithm",
                "thorough discussion",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "algorithm",
                "presented",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Methods for doing so, for stochastic parser output, are described by Johnson   and Cahill et al  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser output",
                "described by Johnson and Cahill et al",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "methods",
                "for doing so",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "hese weights or scaling factors can be optimized with respect to some evaluation criterion  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights or scaling factors",
                "optimized with respect to some evaluation criterion",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In analyzing opinions  , judging document-level subjectivity  , and answering opinion questions  , the output of a sentence-level subjectivity classification can be used without modification.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "output of a sentence-level subjectivity classification",
                "can be used without modification",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "sentence-level subjectivity classification",
                "can be used without modification",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Similarly, Murdock and Croft   adopted a simple translation model from IBM model 1   and applied it to QA.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "simple",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IBM model 1",
                "adopted",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Due to the positive results in Ando  , Blitzer et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Consider the following example  : This film should be brilliant.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "film",
                "should be brilliant",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Expectation Evaluation is the soul of parameter estimation  ,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameter estimation",
                "is the soul of",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "parameter estimation",
                "Expectation Evaluation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "imilarities are captured from different viewpoints: DP-HWC -l This metric corresponds to the HWC metric presented by Liu and Gildea  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metric",
                "corresponds to the HWC metric",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "HWC metric",
                "presented by Liu and Gildea",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Since there is no well-agreed to definition of what an utterance is, we instead focus on intonational phrases  , which end with an acoustically signaled boundary lone.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "utterance",
                "no well-agreed to definition",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "intonational phrases",
                "end with an acoustically signaled boundary",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The first approaches are used for Penn Treebank   and the KAIST language resource  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "is used",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "KAIST language resource",
                "is used",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "2 Related Work Syntax-based translation models engaged with SCFG have been actively investigated in the literature  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SCFG",
                "have been actively investigated",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "translation models",
                "engaged with",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.1 Selecting Coreference Systems A learning-based coreference system can be defined by four elements: the learning algorithm used to train the coreference classifier, the method of creating training instances for the learner, the feature set 2Examples of such scoring functions include the DempsterShafer rule   and Bean and Riloff  ) and its variants (see Harabagiu et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "learning algorithm",
                "used to train the coreference classifier",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "feature set",
                "scoring functions",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These include cube pruning  , cube growing  , early pruning  , closing spans  , coarse-to-fine methods  , pervasive laziness  , and many more.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "cube pruning, cube growing, early pruning, closing spans, coarse-to-fine methods, pervasive laziness, and many more",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "methods",
                "many more",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "2.2 Three Treebanks The Treebanks that we used in this paper are the English Penn Treebank II  , the Chinese Penn Treebank  , and the Korean Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Treebanks",
                "English Penn Treebank II, Chinese Penn Treebank, Korean Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Treebanks",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Reported and direct speech are certainly important in discourse  ; we do not believe, however, that they enter discourse relations of the type that RST attempts to capture.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "RST",
                "attempts to capture",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "discourse relations",
                "enter",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "759 For all models used in our experiments, both wordand class-based, the smoothing method used was Stupid Backoff  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "smoothing method",
                "Stupid Backoff",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "models",
                "used in our experiments",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "We can find some other machine-learning approaches that use more sophisticated LMs, such as Decision Trees   , memory-based approaclms to learn special decision trees  , maximmn entropy approaches that combine statistical information from different sources  , finite state autonmt2 inferred using Grammatical Inference  , etc. The comparison among different al)t)roaches is dif ficult due to the nmltiple factors that can be eonsid614 ered: tile languagK, tile mmfl)er and tyt)e of the tags, the size of tilt vocabulary, thK ambiguity, the diiticulty of the test ski, Kte.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine-learning approaches",
                "more sophisticated LMs",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "finite state autonmt2",
                "inferred using Grammatical Inference",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Agglomerative clustering   iteratively merges the most similar clusters into bigger clusters, which need to be labeled.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "clusters",
                "most similar clusters",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "clusters",
                "need to be labeled",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We adopted log-likelihood ratio  , which gave the best pertbrmance among crude non-iterative methods in our test experiments 6 .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pertbrmance",
                "best",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "log-likelihood ratio",
                "non-iterative methods",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "By labelling Treeb~n~ nodes with Gr~ramar rule names, and not with phrasal and clausal n~raes, as in other   treebanks'  , we gain access to all information provided by the Grammar regarding each ~reebank node.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Treebank nodes",
                "with Gr~ramar rule names",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "other treebanks",
                "with phrasal and clausal names",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We provide results using a range of automatic evaluation metrics: BLEU  , Precision and Recall  , and Wordand Sentence Error Rates.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation metrics",
                "range of automatic evaluation metrics",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "Wordand Sentence Error Rates",
                "Error Rates",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Most empirical work in translation analyzes models and algorithms using BLEU   and related metrics.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models and algorithms",
                "using BLEU and related metrics",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "BLEU and related metrics",
                "metrics",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "field of sentiment classification",
                "has received considerable attention",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "researchers",
                "in recent years",
                "APPLICABILITY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "These findings are somehow surprising since it was eventually believed by the community that adding large amounts of bitexts should improve the translation model, as it is usually observed for the language model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bitexts",
                "should improve the translation model",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "community",
                "believed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Results on the provided 2000sentence development set are reported using the BLEU metric  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metric",
                "is used",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "2000sentence development set",
                "provided",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For the statistics-based approaches, Bean and Riloff   developed a statistics-based method for automatically identifying existential definite NPs which are non-anaphoric.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bean and Riloff's method",
                "statistics-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "existential definite NPs",
                "non-anaphoric",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "10Both Pharoah and our system have weights trained using MERT   on sentences of length 30 words or less, to ensure that training and test conditions are matched.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MERT",
                "trained using MERT",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "training and test conditions",
                "matched",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As has been pointed out by Dunning  , the calculation of log  assumes a binomial distribution.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "calculation",
                "assumes a binomial distribution",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "log",
                "assumes a binomial distribution",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 Augmenting the corpus with an extracted dictionary Previous research   has shown that including word aligned data during training can improve translation results.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "augmenting with an extracted dictionary",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "translation results",
                "can improve",
                "PERFORMANCE",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "2 Decoding The decoding problem in SMT is one of finding the most probable translation e in the target language of a given source language sentence f in accordance with the Fundamental Equation of SMT  : e = argmaxe Pr Pr .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoding problem",
                "most probable translation",
                "PERFORMANCE",
                "neutral",
                0.5
            ],
            [
                "Fundamental Equation of SMT",
                "Pr Pr",
                "METHODOLOGY",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "Starting from a N-Best list generated from a translation decoder, an optimizer, such as Minimum Error Rate     training, proposes directions to search for a better weight-vector  to combine feature functions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "optimizer",
                "Minimum Error Rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "weight-vector",
                "to combine feature functions",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "There are also automatic methods for summary evaluation, such as ROUGE  , which gives a score based on the similarity in the sequences of words between a human-written model summary  and  the  machine  summary.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE",
                "gives a score",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ROUGE",
                "based on similarity in sequences of words",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For comparison purposes, we also computed the value of R 2 for fluency using the BLEU score formula given in  , for the 7 systems using the same one reference, and we obtained a similar value, 78.52%; computing the value of R 2 for fluency using the BLEU scores computed with all 4 references available yielded a lower value for R 2, 64.96%, although BLEU scores obtained with multiple references are usually considered more reliable.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU scores",
                "obtained with multiple references are usually considered more reliable",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "R 2 for fluency",
                "lower value",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Far from full syntactic complexity, we suggest to go back to the simpler alignment methods first described by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment methods",
                "simpler",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "simpler",
                "alignment methods",
                "INNOVATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "The piecewise linearity observation made in   is no longer applicable since we cannot move the log operation into the expected value.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "piecewise linearity observation",
                "no longer applicable",
                "METHODOLOGY",
                "negative",
                0.75
            ],
            [
                "log operation",
                "cannot move into expected value",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "According to one account   the majority of errors arise because of the statistical filtering process, which is reported to be particularly unreliable for low frequency SCFs  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical filtering process",
                "is reported to be particularly unreliable",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "low frequency SCFs",
                "unreliable",
                "METHODOLOGY",
                "negative",
                0.75
            ]
        ]
    },
    {
        "text": "This helps solve the sparse data problem since the number of classes is usually much smaller than the number of words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "number of classes",
                "is usually much smaller",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "number of words",
                "is usually much larger",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Most previous work exploiting unsupervised training data for inferring POS tagging models has focused on semi-supervised methods in the in which the learner is provided with a lexicon specifying the possible tags for each word   or a small number of prototypes for each POS  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "semi-supervised methods",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "lexicon",
                "specifying possible tags",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "BLEU   is a canonical example: in matching n-grams in a candidate translation text with those in a reference text, the metric measures faithfulness by counting the matches, and fluency by implicitly using the reference n-grams as a language model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "measures faithfulness and fluency",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "n-grams",
                "as a language model",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.1 The Corpus The systems are applied to examples from the Penn Treebank   a corpus of over 4.5 million words of American English annotated with both part-of-speech and syntactic tree information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "annotated with part-of-speech and syntactic tree information",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Yarowsky proposed the unsupervised learning method for WSD .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "unsupervised learning method",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "WSD",
                "proposed for",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "arzilay & Lee   also identify paraphrases in their paraphrased sentence generation system",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "paraphrases",
                "in their paraphrased sentence generation system",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "sentence generation system",
                "paraphrased",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The empirical probability for each sentence pair is estimated by maximum likelihood estimation over the training data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training data",
                "estimated by maximum likelihood estimation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentence pair",
                "empirical probability",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We shall take HMM-based word alignment model   as an example and follow the notation of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMM-based word alignment model",
                "example",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "notation of  ",
                "notation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Aligning parallel text, i.e. automatically setting the sentences or words in one text into correspondence with their equivalents in a translation, is a very useful preprocessing step for a range of applications, including but not limited to machine translation  , cross-language information retrieval  , dictionary creation   and induction of NLP-tools  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parallel text",
                "very useful",
                "APPLICABILITY",
                "positive",
                0.9
            ],
            [
                "preprocessing step",
                "useful",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "They are also used for inducing alignments  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignments",
                "inducing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "inducing",
                "alignments",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This tolerant search uses the well known concept of Levenshtein distance in order to obtain the most similar string for the given prefix   for more details).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Levenshtein distance",
                "well known concept",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "prefix",
                "given prefix",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "As the first method, we learn phrase alignments from a corpus that has been word-aligned by a training toolkit for a word-based translation model: the Giza++  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Giza++",
                "training toolkit for a word-based translation model",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "phrase alignments",
                "learned from a corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The tagger is a Hidden Markov Model trained with the perceptron algorithm introduced in  , which applies Viterbi decoding and is regularized using averaging.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger",
                "trained with the perceptron algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Viterbi decoding",
                "applies",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Past work has synchronously binarized such rules for efficiency  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rules",
                "for efficiency",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "efficiency",
                "synchronously binarized",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "2 Inside-out alignments Wu   identified so-called inside-out alignments, two alignment configurations that cannot be induced by binary synchronous context-free grammars; these alignment configurations, while infrequent in language pairs such as EnglishFrench  , have been argued to be frequent in other language pairs, incl.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "inside-out alignments",
                "cannot be induced by binary synchronous context-free grammars",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "alignment configurations",
                "have been argued to be frequent",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "This is con rmed by the translation experiments in which the evaluation data sets were translated using the servers translation engines and the translation quality was evaluated using the standard automatic evaluation metrics BLEU   and METEOR   where scores range between 0   and 1  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "servers translation engines",
                "standard automatic evaluation metrics",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "translation quality",
                "evaluated using BLEU and METEOR",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, the coding manual for the Switchboard DAMSL dialogue act annotation scheme   states that kappa is used to assess labelling accuracy, and Di Eugenio and Glass   relate reliability to the objectivity of decisions, whereas Carletta   regards reliability as the degree to which we understand the judgments that annotators are asked to make.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa",
                "assess labelling accuracy",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "reliability",
                "objectivity of decisions",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The MBT POS tagger   is used to provide POS information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MBT POS tagger",
                "provide POS information",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "POS information",
                "provided by MBT POS tagger",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In the future, we will experiment with semantic   clustering of premoditiers, using techniques such as those proposed in \\ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "premoditiers",
                "semantic clustering",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "techniques",
                "those proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Three recent papers in this area are Church and Hanks  , Hindle  , and Smadja and McKeown  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "papers",
                "recent",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Church and Hanks",
                "Hanks",
                "PERFORMANCE",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "We generated for each phrase pair in the translation table 5 features: phrase translation probability  , lexical weighting     and phrase penalty  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase pair",
                "translation table",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase penalty",
                "phrase penalty",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In our future work we plan to investigate the effect of more sophisticated and, probably, more accurate filtering methods   on the QA results.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "filtering methods",
                "more sophisticated and, probably, more accurate",
                "METHODOLOGY",
                "positive",
                0.7
            ],
            [
                "QA results",
                "on",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In the last decade or so research on lexical semantics has focused more on sub-problems like word sense disambiguation  , named entity recognition  , and vocabulary construction for information extraction  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "focused on sub-problems",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "sub-problems",
                "like word sense disambiguation, named entity recognition, and vocabulary construction for information extraction",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our system is a re-implementation of the phrase-based system described in Koehn  , and uses publicly available components for word alignment  1, decoding  2, language modeling  3 and finite-state processing  4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based system",
                "described in Koehn",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "publicly available components",
                "for word alignment, decoding, language modeling, and finite-state processing",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "he studies presented by Goldwater and Griffiths   and Johnson   differed in the number of states that they used",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "studies",
                "differed in the number of states",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "studies",
                "presented by Goldwater and Griffiths and Johnson",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This is the way the Maximum Entropy tagger   runs if one uses the binary version from the website  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy tagger",
                "runs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "binary version",
                "from the website",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our approach is based on earlier work on LFG semantic form extraction   and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LFG semantic form extraction",
                "earlier work",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Penn-II and Penn-III Treebanks",
                "annotating with LFG f-structures",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Word associations   have a wide range of applications including: speech recognition, optical character recognition, and information retrieval    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Word associations",
                "range of applications",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "speech recognition",
                "optical character recognition",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Automatic methods for this often make use of lexicons of words tagged with positive and negative semantic orientation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicons",
                "words tagged with positive and negative semantic orientation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "semantic orientation",
                "positive and negative",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word alignment",
                "was first proposed",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "statistical machine translation",
                "intermediate result",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Recent work   on this task explored a variety of methodologies to address this issue.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methodologies",
                "address this issue",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "various",
                "explored",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Both for the training and for the testing of our algorithm, we used the syntactically analysed sentences of the Brown Corpus  , which have been manually semantically tagged   into semantic concordance files  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brown Corpus",
                "syntactically analysed sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "semantic concordance files",
                "manually semantically tagged",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To this end, we adopt techniques from statistical machine translation   and use statistical alignment to learn the edit patterns.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques",
                "from statistical machine translation",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "edit patterns",
                "learned using statistical alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The skip-chain CRFs   model the long distance dependency between context and answer sentences and the 2D CRFs   model the dependency between contiguous questions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "skip-chain CRFs",
                "model the long distance dependency",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "2D CRFs",
                "model the dependency between contiguous questions",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "While other systems, such as  , have addressed these tasks to some degree, OPINE is the first to report results.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "OPINE",
                "report results",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "other systems",
                "addressed these tasks",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Two baseNP data sets have been put forward by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseNP data sets",
                "have been put forward",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "data sets",
                "by  ",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In Machine Translation, for example, sentences are produced using application-specific decoders, inspired by work on speech recognition  , whereas in Summarization, summaries are produced as either extracts or using task-specific strategies  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoders",
                "inspired by work on speech recognition",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "strategies",
                "task-specific",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "incorporate syntactic information",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical machine translation systems",
                "currently a great deal of interest",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Therefore, an increasing attention has been recently given to semi-supervised learning, where large amounts of unlabeled data are used to improve the models learned from a small training set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "attention",
                "given",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "models",
                "learned from a small training set",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Within the machine learning paradigm, IL has been incorporated as a technique for bootstrapping an extensional learning algorithm, as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IL",
                "technique for bootstrapping an extensional learning algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "extensional learning algorithm",
                "as in ",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "So far, these techniques have focused on phrasebased models using contiguous phrases  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques",
                "phrase-based models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase-based models",
                "contiguous phrases",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ollins   proposed the perceptron as an alternative to the CRF method for HMM-style taggers",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron",
                "as an alternative",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "CRF method",
                "for HMM-style taggers",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A generative parsing model can be used on its own, and it was shown in Collins and Roark   that a discriminative parsing model can be used on its own.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generative parsing model",
                "can be used on its own",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "discriminative parsing model",
                "can be used on its own",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Adopting the SCF acquisition system of Briscoe and Carroll, we have experimented with an alternative hypothesis test, the binomial log-likelihood ratio   test  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SCF acquisition system",
                "SCF acquisition system of Briscoe and Carroll",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "binomial log-likelihood ratio test",
                "alternative hypothesis test",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  discusses the recovery of one kind of empty node, viz.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "empty node",
                "viz",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "kind of empty node",
                "one kind of",
                "INNOVATION",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The Dublin Core Metadata Initiative3 established a de facto standard for the Semantic Web.4 For   linguistics proper, syntactic annotation schemes, such as the one from the Penn Treebank  , or semantic annotations, such as the one underlying ACE  , are increasingly being used in a quasi standard way.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Dublin Core Metadata Initiative",
                "established a de facto standard",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "semantic annotations",
                "increasingly being used in a quasi standard way",
                "APPLICABILITY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": " , which assume that certain words in a source language sentence tend to trigger the usage of certain words in a target language translation of that sentence.)",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words in a source language sentence",
                "tend to trigger",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "target language translation",
                "certain words",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3.1 Maximum Entropy This section presents a brief description of ME. A more detailed and informative description can be found in Berger   4, Ratnaparkhi  , Manning and Shutze   to name just a few.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ME",
                "description",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "description",
                "informative",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 Maximum Entropy Taggers The taggers are based on Maximum Entropy tagging methods  , and can all be trained on new annotated data, using either GIS or BFGS training code.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "taggers",
                "based on Maximum Entropy tagging methods",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "training code",
                "GIS or BFGS",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Several incremental parsing methods have been proposed so far  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "have been proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "several incremental parsing",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Labelling was carried out by three computational linguistics graduate students with 89% agreement resulting in a Kappa statistic of 0.87, which is a satisfactory indication that our corpus can be labelled with high reliability using our tag set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa statistic",
                "is a satisfactory indication",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "corpus",
                "can be labelled with high reliability",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Log-likelihood ratio     with respect to a large reference corpus, Web 1T 5-gram Corpus  , is used to capture the contextually relevant nouns.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Log-likelihood ratio",
                "with respect to a large reference corpus, Web 1T 5-gram Corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "nouns",
                "contextually relevant",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The disambiguation algorithms also require that the semantic relatedness measures WordNet::Similarity   be installed.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet::Similarity",
                "be installed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "semantic relatedness measures",
                "require",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Table 2 shows the unknown word tags for chunking, which are known as the IOB2 model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IOB2 model",
                "known as",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "unknown word tags",
                "for chunking",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 2.1 Word Alignment Adaptation Bi-directional Word Alignment In statistical translation models  , only one-to-one and more-to-one word alignment links can be found.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical translation models",
                "only one-to-one and more-to-one word alignment links can be found",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word alignment links",
                "can be found",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ald, 2008), and is also similar to the Pred baseline for domain adaptation in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pred baseline",
                "is similar",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "Pred baseline",
                "for domain adaptation",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  1We follow the notations in   for English-French, i.e., e  f, although our models are tested, in this paper, for English-Chinese.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "notations",
                "in for English-French",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "models",
                "tested for English-Chinese",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Recent approaches to statistical machine translation   piggyback on the central concepts of phrasebased SMT   and at the same time attempt to improve some of its shortcomings by incorporating syntactic knowledge in the translation process.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based SMT",
                "central concepts",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "syntactic knowledge",
                "in the translation process",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "For example, the feature 1 On the ATR English Grammar, see below; for a detailed description of a precursor to the Gr-r~raar, see  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ATR English Grammar",
                "see below",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Gr-r~raar",
                "see precursor",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3.1 Experiments The model described in section 2 has been tested on the Brown corpus  , tagged with the 45 tags of the Penn treebank tagset  , which constitute the initial tagset T0.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brown corpus",
                "tagged with the 45 tags of the Penn treebank tagset",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "Penn treebank tagset",
                "constitute the initial tagset T0",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, with their system trained on the medical corpus and then tested on the Wall Street Journal corpus  , they achieve an overall prediction accuracy of only 54%.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "trained on the medical corpus and then tested on the Wall Street Journal corpus",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "prediction accuracy",
                "only 54%",
                "PERFORMANCE",
                "negative",
                0.6
            ]
        ]
    },
    {
        "text": "Instead, we opt to utilize the Stanford NER tagger   over the sentences in a document and annotate each NP with the NER label assigned to that mention head.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stanford NER tagger",
                "utilize",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "NER label",
                "assigned to that mention head",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " , and the third type is a mixture of the first and second type, employing n-gram and grammarbased features, e.g.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-gram and grammarbased features",
                "employing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "mixture of the first and second type",
                "is",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Note, that for our example the effect of the uniform additional conditioning on mother grammatical function has the same effect as the generation grammar transform of  , but without the need for the gramF-Struct Feats Grammar Rules {PRED=PRO,NUM=SG PER=3, GEN=FEM, SUBJ} PRP   she {PRED=PRO,NUM=SG PER=3, GEN=FEM, OBJ} PRP   her Table 7: Lexical item rules.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "gramF-Struct Feats Grammar Rules",
                "without the need for",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "generation grammar transform",
                "has the same effect as",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Maximum entropy estimation for translation of individual words dates back to Berger et al  , and the idea of using multi-class classifiers to sharpen predictions normally made through relative frequency estimates has been recently reintroducedundertherubricofwordsensedisambiguation and generalized to substrings  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation of individual words",
                "dates back to Berger et al",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "multi-class classifiers",
                "sharpen predictions",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "More details about the re-ranking algorithm are presented in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "re-ranking algorithm",
                "presented in",
                "INNOVATION",
                "neutral",
                0.5
            ],
            [
                "details",
                "More details",
                "METHODOLOGY",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "hey are a subset of the features used in Ratnaparkhi  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "used in Ratnaparkhi",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "features",
                "subset of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.3 Accuracy Results   describe a model for unknown words that uses four features, but treats the features ms independent.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model for unknown words",
                "uses four features, but treats the features as independent",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "features",
                "independent",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 OverviewofExtractionWork 3.1 English As one mightexpect,the bulk of the collocation extractionwork concernsthe English language:  , amongmany others1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "English language",
                "bulk of the collocation extraction work",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "many others",
                "among many others",
                "APPLICABILITY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "ROUGE-N   This measure compares n-grams of two summaries, and counts the number of matches.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE-N",
                "compares n-grams of two summaries, and counts the number of matches",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "n-grams",
                "compares",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "  explored the use a formalism called quasisynchronous grammar   in order to find a more explicit model for matching the set of dependencies, and yet still allow for looseness in the matching.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "quasisynchronous grammar",
                "formalism",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "matching the set of dependencies",
                "more explicit",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Empirical evaluation has been done with the ERG on a small set of texts from the Wall Street Journal Section 22 of the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ERG",
                "on a small set of texts",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank",
                "Section 22",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In Table 1, the MALINE row 3  shows that the English name has a palato-alveolar modification   2 As   point out, these insights are not easy to come by: These rules are based on first author Dr. Andrew Freemans experience with reading and translating Arabic language texts for more than 16 years  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MALINE",
                "row 3",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "English name",
                "palato-alveolar modification",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "We use the neural network approximation   to perform inference in our model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "neural network approximation",
                "to perform inference",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "model",
                "our",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ther representative collocation research can be found in Church and Hanks   and Smadja  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Church and Hanks",
                "research",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Smadja",
                "research",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To compare different clustering algorithms, results with the standard method of     are also reported.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method of",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "results",
                "with the standard method",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction In the multilingual track of the CoNLL 2007 shared task on dependency parsing, a single parser must be trained to handle data from ten different languages: Arabic  , Basque  , Catalan,  , Chinese  , Czech  , English  , Greek  , Hungarian  , Italian  , and Turkish  .1 Our contribution is a study in multilingual parser optimization using the freely available MaltParser system, which performs 1For more information about the task and the data sets, see Nivre et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MaltParser system",
                "freely available",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parser",
                "handle data from ten different languages",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The more recent set of techniques includes mult iplicative weightupdate algorithms  , latent semantic analysis  , transformation-based learning  , differential grammars  , decision lists  , and a variety of Bayesian classifiers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "multiplicative weightupdate algorithms",
                "more recent set of techniques",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Bayesian classifiers",
                "variety of",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "De-En En-De Baseline 26.95 20.16 Factored baseline 27.43 20.27 Submitted system 27.63 20.46 Table 1: Bleu scores for Europarl   De-En En-De Baseline 19.54 14.31 Factored baseline 20.16 14.37 Submitted system 20.61 14.77 Table 2: Bleu scores for News Commentary   5 Results Case-sensitive Bleu scores4   for the Europarl devtest set   are shown in table 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bleu scores",
                "are shown in table 1",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "factored baseline",
                "outperforms baseline",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "he SENSEVAL '~tan    and Schiitze    ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SENSEVAL",
                "standard evaluation benchmark",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "evaluation",
                "standard",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Mihalcea   demonstrates that manual mappings can be created for a small number of words with relative ease, but for a very large number of words the e ort involved in mapping would approach presented involves no be considerable.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Mihalcea",
                "manual mappings can be created",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "e ort involved in mapping",
                "would approach considerable",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "BLEU score In order to measure the extent to which whole chunks of text from the prompt are reproduced in the student essays, we used the BLEU score, known from studies of machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "known from studies of machine translation",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "BLEU score",
                "used to measure the extent",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Here, we present experiments performed using two complex corpora, C1 and C2, extracted from the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "C1 and C2",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank",
                "extracted from",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "decades like n-gram back-off word models  , class models  , structured language models   or maximum entropy language models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-gram back-off word models",
                "class models",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "structured language models",
                "maximum entropy language models",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In our experiments we use a grammar with a start symbol S, a single preterminal C, and two nonterminals A and B used to ensure that only one parse can generate any given word-level alignment    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar",
                "with a start symbol S, a single preterminal C, and two nonterminals A and B",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parse",
                "can generate any given word-level alignment",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In our implementation, the IBM Model 1   is used.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 1",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IBM Model 1",
                "IBM Model 1",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Further, we can learn the channel probabilities in an unsupervised manner using a variant of the EM algorithm similar to machine translation  , and statistical language understanding  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "EM algorithm",
                "similar to machine translation",
                "METHODOLOGY",
                "positive",
                0.75
            ],
            [
                "EM algorithm",
                "unsupervised manner",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In many applications, it has been shown that sentences with subjective meanings are paid more attention than factual ones  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentences",
                "are paid more attention",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "sentences with subjective meanings",
                "are paid more attention",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "First, word frequencies, context word frequencies in surrounding positions   are computed following a statistics-based metrics, the log-likelihood ratio  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word frequencies",
                "computed following statistics-based metrics",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "log-likelihood ratio",
                "statistics-based metrics",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ujii and Ishikawa   also work with arguments",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ujii and Ishikawa",
                "work with arguments",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "arguments",
                "no opinion term found",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However, the study of Weeds and Weir   provides interesting insights into what makes a good distributional similarity measure in the contexts of semantic similarity prediction and language modeling.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distributional similarity measure",
                "what makes a good",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "study",
                "provides interesting insights",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "This problem can be cast as an instance of synchronous ITG parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG parsing",
                "synchronous",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "instance",
                "of",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilities",
                "below a certain threshold to negligible values",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translation model induction algorithms",
                "sets all",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It is available in several formats, and in this paper, we use the Penn Treebank   format of NEGRA.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank format",
                "of NEGRA",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "format",
                "available in several",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Wu   proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Inversion Transduction Grammars",
                "parallel parsing of the source and target language",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "synchronized grammar",
                "synchronized",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The node mapping function f for the entire tree thus has a different role from the alignment function in the IBM statistical translation model  ; the role of the latter includes the linear ordering of words in the target string.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "node mapping function f",
                "different role",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "alignment function",
                "linear ordering of words",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This approach is usually referred to as the noisy source-channel approach in statistical machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "noisy source-channel approach",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "approach",
                "statistical machine translation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "roposals have recently been made for protocols for the collection of human discourse segmentation data   and for how to evaluate the validity of judgments so obtained  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "proposals",
                "collection of human discourse segmentation data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "validity of judgments",
                "evaluate",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Work in   modeled the limited information available at phrase-boundaries.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "limited information",
                "available at phrase-boundaries",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "limited",
                "limited",
                "LIMITATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  also includes a brief discussion of crossing constraints that can be derived from phrase structure correspondences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase structure correspondences",
                "can be derived",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "crossing constraints",
                "derived from",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Policy #Shift #Left #Right Start over 156545 26351 27918 Stay 117819 26351 27918 Step back 43374 26351 27918 Table 1: The number of actions required to build all the trees for the sentences in section 23 of Penn Treebank   as a function of the focus point placement policy.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "focus point placement policy",
                "placement policy",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "trees",
                "required to build",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "8.1 The Averaged Perceptron Algorithm with Separating Plane The averaged perceptron algorithm   has previously been applied to various NLP tasks   for discriminative reranking.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "The Averaged Perceptron Algorithm",
                "has previously been applied to various NLP tasks",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "averaged perceptron algorithm",
                "for discriminative reranking",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The f are optimized by Minimum-Error Training    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "f",
                "optimized by Minimum-Error Training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Training",
                "Minimum-Error",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Until now, translation models have been evaluated either subjectively   or using relative metrics, such as perplexity with respect to other models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "subjectively or using relative metrics",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "metrics",
                "perplexity with respect to other models",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This approach is similar to conventional techniques for automatic thesaurus construction  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "conventional techniques",
                "for automatic thesaurus construction",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "this approach",
                "is similar to",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Unlike our technique, in most cases researchers have focused on the scenario where labeled training data is available in both the source and the target domain  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "technique",
                "focused on scenario",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "labeled training data",
                "available in both domains",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "1993; Chang et al. , 1992; Collins and Brooks, 1995; Fujisaki, 1989; Hindle and Rooth, 1991; Hindle and Rooth, 1993; Jelinek et al. , 1990; Magerman and Marcus, 1991; Magerman, 1995; Ratnaparkhi et al. , 1994; Resnik, 1993; Su and Chang, 1988).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Chang et al.",
                "1988",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Collins and Brooks",
                "1995",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Ramshaw and Marcus   state that a baseNP aims to identify essentially the initial portions of nonrecursive noun phrases up to the head, including determiners but not including postmodifying prepositional phrases or clauses . However, work on baseNPs has essentially always proceeded via algorithmic extraction from fully parsed corpora such as the Penn Treebank.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseNP",
                "initial portions of nonrecursive noun phrases up to the head, including determiners but not including postmodifying prepositional phrases or clauses",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithmic extraction",
                "from fully parsed corpora such as the Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Statistical approaches, which depend on a set of unknown parameters that are learned from training data, try to describe the relationship between a bilingual sentence pair  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Statistical approaches",
                "try to describe the relationship",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "training data",
                "learned from",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Instead of interpolating the two language models, we explicitly used them in the decoder and optimized their weights via minimumerror-rate   training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "language models",
                "explicitly used",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "weights",
                "optimized via minimum-error-rate training",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Because our algorithm does not consider the context given by the preceding sentences, we have conducted the following experiment to see to what extent the discourse context could improve the performance of the wordsense disambiguation: Using the semantic concordance files  , we have counted the occurrences of content words which previously appear in the same discourse file.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "does not consider context",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "discourse context",
                "could improve performance",
                "PERFORMANCE",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "The heuristic estimator employs word-alignment     and a few thumb rules for defining phrase pairs, and then extracts a multi-set of phrase pairs and estimates their conditional probabilities based on the counts in the multi-set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "heuristic estimator",
                "employs word-alignment and a few thumb rules",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase pairs",
                "estimates their conditional probabilities based on counts in the multi-set",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Statistical or probabilistic methods are often used to extract semantic clusters from corpora in order to build lexical resources for ANLP tools  ,  ,  , or for automatic thesaurus generation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "used to extract semantic clusters",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "resources",
                "for ANLP tools or automatic thesaurus generation",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Related Work The recent availability of large amounts of bilingual data has attracted interest in several areas, including sentence alignment  , word alignment  , alignment of groups of words  , and statistical translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bilingual data",
                "large amounts of",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "statistical translation",
                "several areas",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "37 3 Semi-supervised Domain Adaptation 3.1 Structural Correspondence Learning Structural Correspondence Learning   exploits unlabeled data from both source and target domain to find correspondences among features from different domains.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Structural Correspondence Learning",
                "find correspondences among features from different domains",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "unlabeled data",
                "from both source and target domain",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, Barzilay and Lee   applied multiple-sequence alignment   to parallel news sentences and induced paraphrasing patterns for generating new sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "multiple-sequence alignment",
                "applied to parallel news sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "paraphrasing patterns",
                "for generating new sentences",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Such a technique has been used with TER to combine the output of multiple translation systems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation systems",
                "multiple",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "output",
                "combine",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "4 The Experiments For the experiments, we used PropBank   along with PennTreeBank5 2    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PropBank",
                "along with PennTreeBank5",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "PennTreeBank5",
                "along with PropBank",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Class-based methods   cluster words into classes of similar words, so that one can base the estimate of a word pair's probability on the averaged cooccurrence probability of the classes to which the two words belong.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "cluster words into classes of similar words",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "classes",
                "one can base the estimate",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ur statistical tagging model is modified from the standard bigrams   using Viterbi search plus onthe-fly extra computing of lexical probabilities for unknown morphemes",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical tagging model",
                "modified from the standard bigrams",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Viterbi search plus onthe-fly extra computing of lexical probabilities",
                "unknown morphemes",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The parser is trained on dependencies extracted from the English Penn Treebank version 3.0   by using the head-percolation rules of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank version 3.0",
                "English",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "head-percolation rules",
                "using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Some of them are based upon syntactic structure, with PropBank   being one of the most relevant, building the annotation upon the syntactic representation of the TreeBank corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PropBank",
                "being one of the most relevant",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "TreeBank corpus",
                "syntactic representation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For instance, BLEU and ROUGE   are based on n-gram precisions, METEOR   and STM   use word-class or structural information, Kauchak   leverages on paraphrases, and TER   uses edit-distances.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "based on n-gram precisions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "METEOR",
                "use word-class or structural information",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We say that wv and nq are semantically related if w~i and nq are semantically related and   and   are semantically similar  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "wv",
                "are semantically related",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "nq",
                "are semantically similar",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "models implement the intuition that the best model will be the one that is consistent with the set of constrains imposed by the evidence, but otherwise is as uniform as possible  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "as uniform as possible",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "model",
                "consistent with the set of constraints imposed by the evidence",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "guchi & Lavrenko   propose the use of probabilistic language models for ranking the results not only by sentiment but also by the topic relevancy",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilistic language models",
                "for ranking the results not only by sentiment but also by topic relevancy",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "results",
                "not only by sentiment but also by topic relevancy",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Antonyms often indicate the discourse relation of contrast  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Antonyms",
                "indicate the discourse relation of contrast",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "We use   and  for straight and inverted combinations respectively, following the ITG notation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG notation",
                "following the ITG notation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "straight and inverted combinations",
                "respectively",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The weights 1,,M are typically learned to directly minimize a standard evaluation criterion on development data  ) using numerical search  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights",
                "learned to directly minimize a standard evaluation criterion",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "numerical search",
                "using numerical search",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Although ITA rates and system performance both significantly improve with coarse-grained senses  , the question about what level of granularity is needed remains.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITA rates",
                "significantly improve",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "system performance",
                "significantly improve",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Collins head words finder rules have been modified to extract semantic head word  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "head words finder rules",
                "modified",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "semantic head word",
                "",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Evaluations are typically carried out on newspaper texts, i.e. on section 23 of the Penn Treebank    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "section 23",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "newspaper texts",
                "typically carried out",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "84 5.2 Machine translation on Europarl corpus We further tested our WDHMM on a phrase-based machine translation system to see whether our improvement on word alignment can also improve MT accuracy measured by BLEU score  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based machine translation system",
                "phrase-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU score",
                "measured by",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Since so many concepts used in discourse are $q'aindependent, a theory of granularity is also fundamental  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "concepts",
                "q'aindependent",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "theory of granularity",
                "fundamental",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "2.3 Online Learning Again following  , we have used the single best MIRA  , which is a margin aware variant of perceptron   for structured prediction.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MIRA",
                "margin aware variant of perceptron",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "MIRA",
                "single best",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For instance, we may find metrics which compute similarities over shallow syntactic structures/sequences  , constituency trees   and dependency trees  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "compute similarities over shallow syntactic structures/sequences",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "dependency trees",
                "constituent",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Some of this work focuses on classifying the semantic orientation of individual words or phrases, using linguistic heuristics or a pre-selected set of seed words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linguistic heuristics",
                "pre-selected set of seed words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "semantic orientation",
                "classifying individual words or phrases",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "For comparison, we use the MT training program, GIZA++  , the phrase-base decoder, Pharaoh  , and the wordbased decoder, Rewrite  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "MT training program",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Pharaoh",
                "phrase-base decoder",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.1 Conditional Random Field for Alignment Our conditional random field   for alignment has a graphical model structure that resembles that of IBM Model 1  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "conditional random field for alignment",
                "has a graphical model structure that resembles that of IBM Model 1",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IBM Model 1",
                "resembles",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "3 Parse Tree Features We tagged each candidate transcription with   part-of-speech tags, using the tagger documented in Collins  ; and   a full parse tree, using the parser documented in Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger",
                "documented in Collins",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parser",
                "documented in Collins",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The results were evaluated using the CoNLL shared task evaluation tools 5 . The approaches tested were Error Driven Pruning     and Transformational Based Learning of IOB tagging    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CoNLL shared task evaluation tools",
                "used for evaluation",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "Transformational Based Learning of IOB tagging",
                "tested",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A number of other re532 searchers   have described previous work on preprocessing methods.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "preprocessing methods",
                "described",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "previous work",
                "on",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Stress is an attribute of syllables, but syllabification is a non-trivial task in itself  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syllables",
                "non-trivial task",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "syllabification",
                "is a non-trivial task",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This is the same complexity as the ITG alignment algorithm used by Wu   and others, meaning complete Viterbi decoding is possible without pruning for realistic-length sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG alignment algorithm",
                "same complexity",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Viterbi decoding",
                "possible without pruning",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction Distributional Similarity has been an active research area for more than a decade  ,  ,  ,  ,  ,  ,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research area",
                "active research area",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "research area",
                "more than a decade",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "ean and Riloff   present a system called BABAR that uses contextual role knowledge to do coreference resolution",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BABAR",
                "uses contextual role knowledge",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "coreference resolution",
                "does",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Following recent research about disambiguation models on linguistic grammars  , we apply a log-linear model or maximum entropy model   on HPSG derivations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-linear model",
                "on HPSG derivations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "maximum entropy model",
                "on HPSG derivations",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Besides the the case-sensitive BLEU-4   used in the two experiments, we design another evaluation metrics Reordering Accuracy   for forced decoding evaluation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU-4",
                "used in the two experiments",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Reordering Accuracy",
                "for forced decoding evaluation",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Although the relative success of previous disambiguation systems   suggests that this should be the case, the effect has usually not been quantified as the emphasis was on a task-based evaluation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "success",
                "relative",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "emphasis",
                "on a task-based evaluation",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "2.3 Measuring the similarity between classes   In step 3, we measure the similarity between two primitive classes by using the method given by Hindle  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "4 Experiments and Results We use the standard corpus for this task, the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "standard corpus",
                "Penn Treebank",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "corpus",
                "for this task",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature set",
                "current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word",
                "being tagged",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In general, Agold / Acandidates; following   and   for parse reranking and   for translation reranking, we define Aoracle as alignment in Acandidates that is most similar to Agold.8 We update each feature weight i as follows: i = i + hAoraclei hA1-besti .9 Following  , after each training pass, we average all the feature weight vectors seen during the pass, and decode the discriminative training set using the vector of averaged feature weights.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Aoracle",
                "most similar to Agold",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "feature weight vectors",
                "averaged",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We then tested the best models for each vocabulary size on the testing set.4 Standard measures of performance are shown in table 1.5 3We used a publicly available tagger   to provide the tags used in these experiments, rather than the handcorrected tags which come with the corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "vocabulary size",
                "tested on the testing set",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tagger",
                "publicly available",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Recent research in open information extraction   has shown that we can extract large amounts of relational data from open-domain text with high accuracy.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "relational data",
                "large amounts of",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "open-domain text",
                "high accuracy",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "7 Related Work The trigger labeling task described in this paper is in part a task of word sense disambiguation  , so we have used the idea of sense consistency introduced in  , extending it to operate across related documents.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "task",
                "word sense disambiguation",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "idea",
                "sense consistency",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Parsing research has also begun to adopt discriminative methods from the Machine Learning literature, such as the perceptron   and the largemargin methods underlying Support Vector Machines  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "discriminative methods",
                "from the Machine Learning literature",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "largemargin methods",
                "underlying Support Vector Machines",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Research prototypes exist for applications such as personal email and calendars, travel and restaurant information, and personal banking   inter alia.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "applications",
                "personal email and calendars, travel and restaurant information, and personal banking",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "prototypes",
                "exist",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "P  =producttextiP    The actions are also sometimes split into a sequence of elementary decisions Di = di1,,din, as discussed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "actions",
                "split into a sequence of elementary decisions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sequence",
                "of elementary decisions",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In contrast, semi-supervised domain adaptation   is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semi-supervised domain adaptation",
                "scenario in which",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "target domain data",
                "no labeled",
                "APPLICABILITY",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "Probability estimates of the RHS given the LHS are often smoothed by making a Markov assumption regarding the conditional independence of a category on those more than k categories away  : P = P  nY i=2 P   P  nY i=2 P .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Markov assumption",
                "conditional independence",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "probability estimates",
                "smoothed",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction The last few decades have seen the emergence of multiple treebanks annotated with different grammar formalisms, motivated by the diversity of languages and linguistic theories, which is crucial to the success of statistical parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treebanks",
                "annotated with different grammar formalisms",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "diversity of languages and linguistic theories",
                "crucial to the success of statistical parsing",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Many methods have been proposed to compute distributional similarity between words, e.g.,  ,  ,   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "distributional similarity",
                "computed",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Moreover, the overall BLEU   and METEOR   scores, as well as numbers of exact string matches   are higher for the hypertagger-seeded realizer than for the preexisting realizer.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "scores",
                "are higher",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "realizer",
                "preexisting",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The first one is a hypotheses testing approach   while the second one is closer to a model estimating approach  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "hypotheses testing",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "approach",
                "model estimating",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We trained and tested the parser on the Wall Street Journal corpus of the Penn Treebank   using the standard split: sections 2-21 were used for training, section 22 was used for development and tuning of parameters and features, and section 23 was used for testing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal corpus",
                "Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "standard split",
                "used for training, development, and testing",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": " , we treat the word identities as a further refinement of the POS tags; thus we build a word classification tree for each POS tag.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word identities",
                "further refinement",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "POS tags",
                "word classification tree",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Since an existing study incorporates these relations ad hoc  , they are apparently crucial in accurate disambiguation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "relations",
                "apparently crucial in accurate disambiguation",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "relations",
                "incorporates ad hoc",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Chiang   distinguishes statistical MT approaches that are  syntactic in a formal sense, going beyond the  nite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.1 The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are  nite state but informed by linguistic annotation prior to training  ), and also include systems employing contextfree models trained on parallel text without bene t of any prior linguistic analysis (e.g.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic modeling",
                "syntactic in a formal sense",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntactic modeling",
                "taking advantage of a priori language knowledge",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "We are currently investigating caching and optimizations that will enable the use of our metric for MT parameter tuning in a Minimum Error Rate Training setup  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metric",
                "for MT parameter tuning",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "caching and optimizations",
                "will enable",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "In other words,   can be used in substitution of  , whereas   cannot, so easily 41n  , a value of K between .8 and I indicates good agreement; a value between .6 and .8 indicates some agreement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "We use a tagger based on Adwait Ratnaparkhi's method  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger",
                "based on Adwait Ratnaparkhi's method",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "of Words Person names 803 1749 Organization names 312 867 Location names 345 614 The BLEU score   with a single reference translation was deployed for evaluation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "was deployed",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "single reference translation",
                "was deployed",
                "INNOVATION",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "utomatic segmentation of spontaneous speech is an open research problem in its own right  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research problem",
                "open",
                "LIMITATION",
                "neutral",
                0.8
            ],
            [
                "spontaneous speech",
                "its own right",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this year, CoNLL-2007 shared task   focuses on multilingual dependency parsing based on ten different languages   and domain adaptation for English   without taking the languagespecific knowledge into consideration.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CoNLL-2007 shared task",
                "multilingual dependency parsing",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "domain adaptation",
                "without taking languagespecific knowledge into consideration",
                "LIMITATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "Since text planners cannot generate either the requisite syntactic variation or quantity of text,  , a corpus that includes texts from newspapers such as the Wall Street Journal, and which have been hand-annotated for syntax by linguists.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "includes texts from newspapers such as the Wall Street Journal",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "text planners",
                "cannot generate requisite syntactic variation or quantity of text",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "However, as Categorial Grammar formalisms do not usually change the lexical entries of words to deal with movement, but use further rules  , the lexicons learned here will be valid over corpora with movement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Categorial Grammar formalisms",
                "do not usually change the lexical entries of words",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "lexicons",
                "will be valid over corpora with movement",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "We held out 300 sentences for minimum error rate training     and optimised the parameters of the feature functions of the decoder for each experimental run.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "parameters of the feature functions",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "experimental run",
                "optimised",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We use the same feature processing as Haghighi and Klein  , with the addition of context features in a window of3.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Haghighi and Klein",
                "same feature processing",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "context features",
                "in a window of 3",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, our system configuration for the shared task incorporates a wrapper around GIZA++   for word alignment and a wrapper around Moses   for decoding.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "word alignment",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Moses",
                "decoding",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Feature-based methods   use pre-defined feature sets to extract features to train classification models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature sets",
                "pre-defined",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "classification models",
                "train",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We created a dependency training corpus based on the Penn Treebank  , or more specifically on the HPSG Treebank generated from the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "generated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "HPSG Treebank",
                "generated",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Despite these difficulties, some work has shown it worthwhile to minimize error directly  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "shown worthwhile",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "error",
                "minimize directly",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "BLEU   is one of the methods for automatic evaluation of translation quality.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "automatic evaluation of translation quality",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "BLEU",
                "one of the methods",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "While the need for annotation by multiple raters has been well established in NLP tasks  , most previous work in error detection has surprisingly relied on only one rater to either create an annotated corpus of learner errors, or to check the systems output.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotation",
                "by multiple raters",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "previous work",
                "relied on only one rater",
                "METHODOLOGY",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "A summary of the differences between our proposed approach and that of   would include:  The reliance of BLEU on the diversity of multiple reference translations in order to capture some of the acceptable alternatives in both word choice and word ordering that we have shown above.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "reliance on diversity of multiple reference translations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approach",
                "capture acceptable alternatives in word choice and word ordering",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We obtained word alignments of training data by first running GIZA++   and then applying the refinement rule grow-diag-final-and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "running",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "grow-diag-final-and",
                "applying the refinement rule",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 Results and Discussion The system with online learning and Nivres parsing algorithm was trained on the data released by CoNLL Shared Task Organizers for all the ten languages  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Nivres parsing algorithm",
                "Nivres parsing algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "CoNLL Shared Task Organizers",
                "releasing data",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "3.2 System Combination Scheme In our work, we use a sentence-level system combination model to select best translation hypothesis from the candidate pool     . This method can also be viewed to be a hypotheses reranking model since we only use the existing translations instead of performing decoding over a confusion network as done in the word-level combination method  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system combination model",
                "select best translation hypothesis",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word-level combination method",
                "decoding over a confusion network",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Banerjee and Lavie   and Chan and Ng   use WordNet, and Zhou et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet",
                "use",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Zhou et al.",
                "use",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "This does not seem to be the case, however, for common feature weighting functions, such as Point-wise Mutual Information  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weighting functions",
                "common",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Point-wise Mutual Information",
                "",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.5 ITG Constraints Another type of reordering can be obtained using Inversion Transduction Grammars    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG Constraints",
                "reordering can be obtained using",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Inversion Transduction Grammars",
                "type of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Thus, Nakagawa   and Hall   both try to overcome the limited feature scope of graph-based models by adding global features, in the former case using Gibbs sampling to deal with the intractable inference problem, in the latter case using a re-ranking scheme.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "graph-based models",
                "limited feature scope",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "Gibbs sampling",
                "intractable inference problem",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4 Method-2: Simple Chunk-based Extraction To overcome the shortcomings of the Brill tagger in identifying particles, we next look to full chunk 2Note, this is the same as the maximum span length of 5 used by Smadja  , and above the maximum attested NP length of 3 from our corpus study  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brill tagger",
                "shortcomings in identifying particles",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "chunk 2",
                "maximum span length of 5",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This sort of problem can be solved in principle by conditional variants of the Expectation-Maximization algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Expectation-Maximization algorithm",
                "conditional variants",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "problem",
                "can be solved",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Second, the word alignment is refined by a grow-diag-final heuristic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word alignment",
                "refined by a grow-diag-final heuristic",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "grow-diag-final heuristic",
                "used for refinement",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Syntax based statistical MT approaches began with  , who introduced a polynomial-time solution for the alignment problem based on synchronous binary trees.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Syntax based statistical MT approaches",
                "introduced a polynomial-time solution",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "alignment problem",
                "based on synchronous binary trees",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he output of GIZA++ is then post-processed using the three symmetrization heuristics described in Och and Ney  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "post-processed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "symmetrization heuristics",
                "described in Och and Ney",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Many NLP systems use the output of supervised parsers   for QA,   for IE,   for SRL,   for Textual Inference and   for MT).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NLP systems",
                "use the output of supervised parsers",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "output of supervised parsers",
                "for various tasks (QA, IE, SRL, Textual Inference, MT)",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This is in contrast to standard summarization models that look to promote sentence diversity in order to cover as many important topics as possible  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "standard summarization models",
                "promote sentence diversity",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentence diversity",
                "cover as many important topics as possible",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The intuition is that the produced clusters will be less sense-conflating than those produced by other graph-based approaches, since collocations provide strong and consistent clues to the senses of a target word  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocations",
                "strong and consistent clues",
                "METHODOLOGY",
                "positive",
                0.85
            ],
            [
                "produced clusters",
                "less sense-conflating",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We set all feature weights by optimizing Bleu   directly using minimum error rate training     on the tuning part of the development set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "optimized directly using minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "development set",
                "tuning part",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "At the end we ran our models once on TEST to get final numbers.2 4 Models Our experiments used phrase-based models  , which require a translation table and language model for decoding and feature computation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "phrase-based models",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "phrase-based models",
                "require a translation table and language model",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Note that the translation direction is inverted from what would be normally expected; correspondingly the models built around this equation are often called invertedtranslationmodels  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation direction",
                "inverted from what would be normally expected",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "models",
                "built around this equation are often called inverted translation models",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "However, because these estimates are too sparse to be relied upon, we use interpolated estimates consisting of mixtures of successively lowerorder estimates  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "estimates",
                "are too sparse",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "estimates",
                "consisting of mixtures of successively lower-order estimates",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This simplified version does not take word classes into account as described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word classes",
                "does not take into account",
                "METHODOLOGY",
                "negative",
                0.7
            ],
            [
                "description",
                "as described in",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This method is very similar to some ideas in domain adaptation  , but we argue that the underlying problems are quite different.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "similar to some ideas in domain adaptation",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "problems",
                "quite different",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For example, sentence alignment of bilingual texts are performed just by measuring sentence lengths in words or in characters  , or by statistically estimating word level correspondences  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentence alignment",
                "measuring sentence lengths in words or in characters",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word level correspondences",
                "statistically estimating",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "On the other hand, integrating an additional component into a baseline SMT system is notoriously tricky as evident in the research on integrating word sense disambiguation   into SMT systems: different ways of integration lead to conflicting conclusions on whether WSD helps MT performance  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSD",
                "conflicting conclusions on whether WSD helps MT performance",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "integration ways",
                "lead to conflicting conclusions",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "These range from twoword to multi-word, with or without syntactic structure  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "twoword",
                "to multi-word",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "without syntactic structure",
                "syntactic structure",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this paper we present MapReduce implementations of training algorithms for two kinds of models commonly used in statistical MT today: a phrasebased translation model   and word alignment models based on pairwise lexical translation trained using expectation maximization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrasebased translation model",
                "phrasebased translation model",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "expectation maximization",
                "expectation maximization",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Independently, in AI an effort arose to encode large amounts of commonsense knowledge  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "AI",
                "encode large amounts of commonsense knowledge",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "large amounts of commonsense knowledge",
                "encode",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Following Ramshaw and Marcus  , the current dominant approach is formulating chunking as a classification task, in which each word is classified as the  eginning,  nside or  outside of a chunk.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunking",
                "as a classification task",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word",
                "classified as the beginning, inside or outside of a chunk",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We used the preprocessed data to train the phrase-based translation model by using GIZA++   and the Pharaoh tool kit  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based translation model",
                "trained by using GIZA++ and Pharaoh tool kit",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "GIZA++ and Pharaoh tool kit",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars   to general tree-to-string transducers   and have increased in size by including more synchronous tree fragments  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation grammars",
                "grown in complexity",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation grammars",
                "increased in size",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Word Alignment Framework A statistical translation model   describes the relationship between a pair of sentences in the source and target languages   using a translation probability P .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "describes the relationship",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation probability P",
                "using",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The algorithm is slightly different from other online training algorithms   in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a metric, i.e. BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "slightly different",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU",
                "metric",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "6 Smaller Tagset and Incomplete Dictionaries Previously, researchers working on this task have also reported results for unsupervised tagging with a smaller tagset  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Smaller Tagset",
                "smaller tagset",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "results",
                "reported",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Some statistical model to estimate the part of speech of unknown words from the case of the first letter and the prefix and suffix is proposed  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical model",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "part of speech of unknown words",
                "from the case of the first letter and the prefix and suffix",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "IBM Model 4 parameters are then estimated over this partial search space as an approximation to EM  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 4 parameters",
                "estimated over this partial search space",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "EM",
                "approximation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "rown et al   put forward and discussed n-gram models based on classes of words",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-gram models",
                "based on classes of words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "models",
                "put forward and discussed",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "On the other hand, Kazama and Torisawa   extracted hyponymy relations, which are independent of the NE categories, from Wikipedia and utilized it as a gazetteer.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kazama and Torisawa",
                "extracted hyponymy relations",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "hyponymy relations",
                "are independent of the NE categories",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For example, in this work we use loglikelihood ratio   to determine the SoA between a word sense and co-occurring words, and cosine to determine the distance between two DPWSs log likelihood vectors  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "loglikelihood ratio",
                "to determine the SoA",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "cosine",
                "to determine the distance",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Post-editing of automatic annotation has been pursued in various projects  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotation",
                "pursued",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "projects",
                "various",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As reported previously, the standard left-corner grmninar embeds sufficient non-local infornlation in its productions to significantly improve the labelled precision and recall of its MLPs with respect to MLPs of the PCFG estimated from the untransfornmd trees  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MLPs",
                "sufficient non-local information",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "MLPs",
                "significantly improve",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "We selected 580 short sentences of length at most 50 characters from the 2002 NIST MT Evaluation test set as our development corpus and used it to tune s by maximizing the BLEU score  , and used the 2005 NIST MT Evaluation test set as our test corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NIST MT Evaluation test set",
                "2002 and 2005",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU score",
                "maximizing",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Among the several proposals, we mention here the models presented in  ,  ,  ,   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "presented",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "several proposals",
                "mention here",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "it constitutes a bijection between source and target sentence positions, since the intersecting alignments are functions according to their definition in   3.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignments",
                "functions according to their definition",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence positions",
                "constitutes a bijection",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3.1 Data The English data set consists of the Wall Street Journal sections 2-24 of the Penn treebank  , converted to dependency format.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn treebank",
                "converted to dependency format",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Wall Street Journal sections 2-24",
                "English data set consists of",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Others, such as Turney  , Pang and Vaithyanathan  , have examined the positive or negative polarity, rather than presence or absence, of affective content in text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Turney",
                "examined",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Pang and Vaithyanathan",
                "examined",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  describe the voted perceptron applied to the named-entity data in this paper, but using kernel-based features rather than the explicit features described in this paper.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "voted perceptron",
                "applied to named-entity data",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "kernel-based features",
                "rather than explicit features",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.3 Corpora Our labeled data comes from the Penn Treebank   and consists of about 40,000 sentences from Wall Street Journal   articles 153 annotated with syntactic information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "labeled data comes from",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Wall Street Journal articles",
                "annotated with syntactic information",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Inter-annotator agreement was determined for six pairs of two annotators each, resulting in kappa values  ) ranging from 0.62 to 0.82 for the whole database (Carlson et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa values",
                "ranging from 0.62 to 0.82",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "annotators",
                "six pairs of two",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Reliability metrics   are designed to give a robust measure of how well distinct sets of data agree with, or replicate, one another.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "give a robust measure",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sets of data",
                "agree with, or replicate",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We follow the method used by Kazama and Torisawa  , which encodes the matching with a gazetteer entity using IOB tags, with the modication for Japanese.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kazama and Torisawa's method",
                "used by",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IOB tags",
                "with the modification for Japanese",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There has also been previous work on determining whether a given text is factual or expresses opinion  ; again this work uses a binary distinction, and supervised rather than unsupervised approaches.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "binary distinction",
                "binary distinction",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "supervised rather than unsupervised approaches",
                "approaches",
                "METHODOLOGY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "As well as the sentiment expressions leading to evaluations, there are many semantic aspects to be extracted from documents which contain writers opinions, such as subjectivity  , comparative sentences  , or predictive expressions  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic aspects",
                "subjectivity",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "expressions",
                "predictive",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Stage 2 processing is then free to assign to the compound any bracketing for which it 3The design of this level of Lucy is influenced by Hobbs  , which advocates a level of \"\"surfaey\"\" logical form with predicates close to actual English words and a structure similar to the syntactic structure of the sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Lucy",
                "influenced by Hobbs",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "logical form",
                "with predicates close to actual English words and a structure similar to the syntactic structure of the sentence",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The model cleanly incorporates both syntax and lexical semantics using quasi-synchronous dependency grammars  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "We retrained the parser on lowercased Penn Treebank II  , to match the lowercased output of the MT decoder.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "lowercased",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "output of the MT decoder",
                "lowercased",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "By associating natural language with concepts as they are entered into a knowledge A Model Of Semantic Analysis All of the following discussion is based on a model of semantic analysis similar to that proposed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model Of Semantic Analysis",
                "similar to that proposed",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "knowledge",
                "entered into",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use the same alignment data for the five language pairs Chinese/English, Romanian/English, Hindi/English, Spanish/English, and French/English  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment data",
                "same",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "language pairs",
                "Chinese/English, Romanian/English, Hindi/English, Spanish/English, and French/English",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Typicality was measured using the log-likelihood ratio test  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-likelihood ratio test",
                "was measured using",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "log-likelihood ratio test",
                "is typically used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Chen & Martin   introduced one of those similarity schemes, ?two-level SoftTFIDF??",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "similarity schemes",
                "two-level SoftTFIDF",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "two-level SoftTFIDF",
                "introduced",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This is the traditional approach for glass-box smoothing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "traditional",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "smoothing",
                "glass-box",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For getting the syntax trees, the latest version of Collins parser   was used.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins parser",
                "latest version",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntax trees",
                "getting",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ch   developed a training procedure that incorporates various MT evaluation criteria in the training procedure of log-linear MT models",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT evaluation criteria",
                "incorporates in the training procedure",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "log-linear MT models",
                "developed a training procedure",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Barzilay and Lee   learned paraphrasing patterns as pairs of word lattices, which are then used to produce sentence level paraphrases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "patterns",
                "pairs of word lattices",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentence level paraphrases",
                "used to produce",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.1 Data The starting corpus we use is formed by a mix of three different sources of data, namely the Penn Treebank corpus  , the Los Angeles Times collection, as provided during TREC conferences1, and Open Mind Common Sense2, a collection of about 400,000 commonsense assertions in English as contributed by volunteers over the Web.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "starting corpus",
                "formed by mix of three different sources",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "SR thus adopts the method proposed by Och  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "proposed by Och",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Och's method",
                "",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "6.2 Experiment 2: Yarowskys Words We also conducted translation on seven of the twelve English words studied in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Yarowskys Words",
                "studied in 6.2 Experiment 2",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "seven",
                "of the twelve English words",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Not only many combinations are found in the corpus, many of them have very similar mutual information values to that of 318 Table 2: economic impact verb economic financial political social budgetary ecological economic economic economic economic economic economic economic economic economic object impact impact impact impact impact impact effect implication consequence significance fallout repercussion potential ramification risk mutual freq info 171 1.85 127 1.72 46 0.50 15 0.94 8 3.20 4 2.59 84 0.70 17 0.80 59 1.88 10 0.84 7 1.66 7 1.84 27 1.24 8 2.19 17 -0.33 nomial distribution can be accurately approximated by a normal distribution  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information values",
                "similar",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "nominal distribution",
                "can be accurately approximated by a normal distribution",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Some work identifies inflammatory texts  ) or classifies reviews as positive or negative  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "inflammatory texts",
                "identifies",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "reviews",
                "as positive or negative",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "7 Model Structure In our statistical model, trees are generated according to a process similar to that described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model Structure",
                "generated according to a process similar to that described in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "trees",
                "generated",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "e measured associations using the log-likelihood measure   for each combination of target category and semantic class by converting each cell of the contingency into a 22 contingency table",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-likelihood measure",
                "for each combination of target category and semantic class",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "contingency table",
                "converting each cell of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Most previous research in translation knowledge acquisition is based on parallel corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "previous research",
                "is based on",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parallel corpora",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "6 Bracketing of Compound Nouns The first analysis task we consider is the syntactic disambiguation of compound nouns, which has received a fair amount of attention in the NLP literature  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "compound nouns",
                "has received a fair amount of attention",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "NLP literature",
                "has received attention",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he third function is an original variant of the second; the fourth is original; and the fifth is prompted by the arguments of Dunning  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "function",
                "original",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "arguments of Dunning",
                "prompted",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4 Optimizing Metric Parameters The original version of Meteor   has instantiated values for three parameters in the metric: one for controlling the relative weight of precision and recall in computing the Fmean score  ; one governing the shape of the penalty as a function of fragmentation   and one for the relative weight assigned to the fragmentation penalty  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metric parameters",
                "instantiated values for three parameters",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "penalty",
                "shape of the penalty as a function of fragmentation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "arzilay and Lee   also used newspaper articles on the same event as comparable corpora to acquire paraphrases",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "newspaper articles",
                "comparable corpora",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "acquire paraphrases",
                "same event",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The second method considers the means and variance of the distance between two words, and can compute flexible collocations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distance between two words",
                "can compute flexible collocations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "distance between two words",
                "means and variance",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The Attr cells summarize the performance of the 6 models on the wiki table that are based on attributional similarity only  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "based on attributional similarity only",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "wiki table",
                "summarize the performance of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The trigger-based lexicon model used in this work follows the training procedure introduced in   and is integrated directly in the decoder instead of being applied in n-best list reranking.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "trigger-based lexicon model",
                "used in this work",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "training procedure",
                "introduced in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Bilingual Bracketing In     A ! < AA >   A ! f=e   A ! f=null   A ! null=e   Where f and e are words in the target vocabulary Vf and source vocabulary Ve respectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bilingual Bracketing",
                "In A! < AA >",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "target vocabulary Vf and source vocabulary Ve",
                "respectively",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This task is quite common in corpus linguistics and provides the starting point to many other algorithms, e.g., for computing statistics such as pointwise mutual information  , for unsupervised sense clustering  , and more generally, a large body of work in lexical semantics based on distributional profiles, dating back to Firth   and Harris  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus linguistics",
                "provides the starting point",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "algorithms",
                "for computing statistics",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Due to its popularity for unsupervised POS induction research   and its often-used tagset, for our initial research, we use the Wall Street Journal   portion of the Penn Treebank  , with 36 tags  , and we use sections 00-18, leaving held-out data for future experiments.4 Defining frequent frames as those occurring at 4Even if we wanted child-directed speech, the CHILDES database   uses coarse POS tags.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "Wall Street Journal portion",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tagset",
                "often-used",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "e borrow the idea of classifying definites occurring in the first sentence as chain starting from Bean and Riloff  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bean and Riloff",
                "idea",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "chain starting from Bean and Riloff",
                "definites occurring in the first sentence as",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Close to the problem studied here is Jing and McKeowns   cut-and-paste method founded on EndresNiggemeyers observations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Jing and McKeown's cut-and-paste method",
                "founded on EndresNiggemeyers observations",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "cut-and-paste method",
                "founded on observations",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2.3 Feature Functions Our phrase-based model uses a standard pharaoh feature functions listed as follows  :  Relative-count based phrase translation probabilities in both directions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based model",
                "standard pharaoh feature functions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase translation probabilities",
                "in both directions",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We tested several measures, such as ROUGE   and the cosine distance.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measures",
                "ROUGE and cosine distance",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ROUGE",
                "and cosine distance",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction Word compositions have long been a concern in lexicography , and now as a specific kind of lexical knowledge, it has been shown that they have an important role in many areas in natural language processing, e.g., parsing, generation, lexicon building, word sense disambiguation, and information retrieving, etc. .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Word compositions",
                "long been a concern",
                "LIMITATION",
                "neutral",
                0.8
            ],
            [
                "lexical knowledge",
                "has an important role",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "For example, incremental CFG parsing algorithms can be used with the CFGs produced by this transform, as can the Inside-Outside estimation algorithm   and more exotic methods such as estimating adjoined hidden states  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CFG parsing algorithms",
                "can be used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Inside-Outside estimation algorithm",
                "more exotic methods",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Furthermore, these systems have tackled the problem at different levels of granularity, from the document level  , sentence level  , phrase level  , as well as the speaker level in debates  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "levels of granularity",
                "different levels of granularity",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "speaker level",
                "in debates",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he usefulness of likelihood ratios for collocation detection has been made explicit by Dunning   and has been confirmed by an evaluation of various collocation detection methods carried out by Evert and Krenn  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Dunning",
                "has made explicit",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Evert and Krenn",
                "have confirmed",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "After line 17, we can employ the one-sense-per-discourse heuristic to further classify unclassified data, as proposed in Yarowsky  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "one-sense-per-discourse heuristic",
                "proposed in Yarowsky",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "uncategorized data",
                "further classify",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Besides continued research on improving MT techniques, one line of research is dedicated to better exploitation of existing methods for the combination of their respective advantages  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT techniques",
                "improving",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "methods",
                "respective advantages",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The subset was the neighboring alignments   of the Viterbi alignments discovered by Model 1 and Model 2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 1 and Model 2",
                "discovered by",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Viterbi alignments",
                "neighborhood of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The pipeline extracts a Hiero-style synchronous context-free grammar  , employs suffix-array based rule extraction  , and tunes model parameters with minimum error rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hiero-style synchronous context-free grammar",
                "extracts",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "model parameters",
                "tunes with minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The tagger from   first annotates sentences of raw text with a sequence of partof-speech tags.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger",
                "annotates sentences of raw text",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "part-of-speech tags",
                "sequence of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The optimal weights for the different columns can then be assigned with the help of minimum error rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights",
                "assigned with the help of minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "minimum error rate training",
                "help",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A la Ramshaw and Marcus  , and Kudo and Matsumato  , we use the IOB tagging style for modeling and classification.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IOB tagging style",
                "for modeling and classification",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Ramshaw and Marcus",
                "Kudo and Matsumato",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "4.1 Extraction from Definition Sentences Definition sentences in the Wikipedia article were used for acquiring hyponymy relations by   for named entity recognition.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Definition sentences",
                "used for acquiring hyponymy relations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "named entity recognition",
                "for",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Maximum Entropy In this bakeoff, our basic model is based on the framework described in the work of Ratnaparkhi   which was applied for English POS tagging.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "based on the framework described in the work of Ratnaparkhi",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Ratnaparkhi's work",
                "applied for English POS tagging",
                "METHODOLOGY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "As a result, we can use collocation measures like point-wise mutual information   or the log-likelihood ratio   to predict the strong association for a given cue.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "The proxy slot denotes a semantic individual which serves the role of an event instance in a partially Davidsonian scheme, as in   or  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "proxy slot",
                "semantic individual",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "partially Davidsonian scheme",
                "semantic individual",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our approach is data-driven: following the methodology in  , we automatically convert the English PennII treebank and the Chinese Penn Treebank   into f-structure banks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PennII treebank",
                "English PennII treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank",
                "Chinese Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Furthermore, early work on class-based language models was inconclusive  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "class-based language models",
                "inconclusive",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "However, certain properties of the BLEU metric can be exploited to speed up search, as described in detail by Och  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metric",
                "can be exploited",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "properties of the BLEU metric",
                "can be exploited",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction Supervised statistical parsers attempt to capture patterns of syntactic structure from a labeled set of examples for the purpose of annotating new sentences with their structure  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsers",
                "capture patterns of syntactic structure",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "labeled set of examples",
                "annotating new sentences with their structure",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Instead we report BLEU scores   of the machine translation system using different combinations of wordand classbased models for translation tasks from English to Arabic and Arabic to English.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine translation system",
                "different combinations of wordand classbased models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU scores",
                "report",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Since the word support model and triple context matching model have been proposed in our previous work   at the SIGHAN bakeoff 2005   and 2006  , the major descriptions of this paper is on the WBT model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word support model",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "WBT model",
                "major descriptions",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Lee & Barzilay  , for example, use MultiSequence Alignment   to build a corpus of paraphrases involving terrorist acts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MultiSequence Alignment",
                "build a corpus of paraphrases",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "terrorist acts",
                "involving",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "5.3 Experimental setup We used the Stanford Parser   for both languages, Penn English Treebank   and Penn Arabic Treebank set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stanford Parser",
                "Penn English Treebank and Penn Arabic Treebank set",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn English Treebank and Penn Arabic Treebank set",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "An alternative approach to extracting the informal phrases is to use a bootstrapping algorithm  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "bootstrapping",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "approach",
                "alternative",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Our method is similar to  ,  , and   in the use of dependency relationships as the word features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency relationships",
                "use of dependency relationships as word features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "method",
                "similar to other methods",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Table 3 shows the differences between the treebank~ utilized in   on the one hand, and in the work reported here, on the other, is Table 4 shows relevant lSFigures for Average Sentence Length   and Training Set Size, for the IBM ManuaLs Corpus, are approximate, and cz~e fzom  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treebank",
                "utilized",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Table 4",
                "relevant lSFigures for Average Sentence Length and Training Set Size",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In Smadja's collocation algorithm Xtract, the lowest-frequency words are effectively discarded as well  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "lowest-frequency",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "words",
                "are effectively discarded",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Most previous work on compositionality of MWEs either treat them as collocations  , or examine the distributional similarity between the expression and its constituents  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MWEs",
                "treat them as collocations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "distributional similarity",
                "between the expression and its constituents",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As association measure we apply log-likelihood ratio   to normalized frequency.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-likelihood ratio",
                "to normalized frequency",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "frequency",
                "normalized",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": " , we used the MXPOST   tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10way jackknifing to generate tags for the training set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MXPOST tagger",
                "trained on training data",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "10way jackknifing",
                "generate tags",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The tagger was tested on two corpora-the Brown corpus  ) and the Wall Street Journal corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "Brown corpus and Wall Street Journal corpus",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "tagger",
                "was tested on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Machine translation has code-like characteristics, and indeed, the initial models of   took a word-substitution/transposition approach, trained on a parallel text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "took a word-substitution/transposition approach",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parallel text",
                "trained on",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We used the MXPOST tagger   for POS annotation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MXPOST tagger",
                "POS annotation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tagger",
                "for",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "p and pt are feature weights set by performing minimum error rate training as described in Och  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "set by performing minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Och",
                "described in",
                "LIMITATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The progress in parsing technology are noteworthy, and in particular, various statistical dependency models have been proposed ,,  ,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "progress in parsing technology",
                "noteworthy",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "statistical dependency models",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Recently, Wikipedia is emerging as a source for extracting semantic relationships  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wikipedia",
                "emerging as a source",
                "APPLICABILITY",
                "neutral",
                0.5
            ],
            [
                "semantic relationships",
                "extracting",
                "PERFORMANCE",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "Some researchers have concentrated on producing WSD systems that base results on a limited number of words, for example Yarowsky   and Schtitze   who quoted results for 12 words, and a second group, including Leacock, Towell, and Voorhees   and Bruce and Wiebe  , who gave results for just one, namely interest.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSD systems",
                "base results on a limited number of words",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "results for one word",
                "just one, namely interest",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "urney   describes a method of sentiment classification using two human-selected seed words   in conjunction with a very large text corpus; the semantic orientation of phrases is computed as their association with the seed words  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "using two human-selected seed words",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "semantic orientation",
                "computed as their association with the seed words",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Since this relation can often be determined automatically for a given text  , we can readily use it to improve rank prediction.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "relation",
                "can often be determined automatically",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "rank prediction",
                "can be improved",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "This sequential property is well suited to HMMs  , in which the jumps from the current aligned position can only be forward.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMMs",
                "well suited",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "jumps",
                "can only be forward",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The relatedness between two word senses is computed using a measure of semantic relatedness defined in the WordNet::Similarity software package  , which is a suite of Perl modules implementing a number WordNet-based measures of semantic relatedness.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet::Similarity software package",
                "WordNet-based measures of semantic relatedness",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "semantic relatedness measure",
                "defined in the WordNet::Similarity software package",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The features we use are shown in Table 2, which are based on the features used by Ratnaparkhi   and Uchimoto et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "We can use a linear-time algorithm   to detect non-ITG movement in our high-confidence links, and remove the offending sentence pairs from our training corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "linear-time",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence pairs",
                "offending",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "There are three major types of models: Heuristic models as in  , generative models as the IBM models   and discriminative models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "Heuristic models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "models",
                "IBM models",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "he extraction procedure utilizes a head percolation table as introduced by Magerman   in combination with a variation of Collinss   approach to the differentiation between complement and adjunct",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "head percolation table",
                "introduced by Magerman",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Collins' approach",
                "differentiation between complement and adjunct",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The hallucination process is motivated by the use of NULL alignments into Markov alignment models as done by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NULL alignments",
                "into Markov alignment models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Markov alignment models",
                "done by  ",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2.1 Model 2 of   Both parsing models discussed in this paper inherit a great deal from this model, so we briefly describe its \"\"progenitive\"\" features here, describing only how each of the two models of this paper differ in the subsequent two sections.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 2",
                "progenitive features",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "models",
                "differ",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ang & Lee   propose the use of language models for sentiment analysis task and subjectivity extraction",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "language models",
                "for sentiment analysis task and subjectivity extraction",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "sentiment analysis task and subjectivity extraction",
                "are proposed",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We show translation results in terms of the automatic BLEU evaluation metric   on the MT03 Arabic-English DARPA evaluation test set consisting of a212a89a212a89a87 sentences with a98a89a212a161a213a89a214a89a215 Arabic words with a95 reference translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT03 Arabic-English DARPA evaluation test set",
                "consisting of",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "BLEU evaluation metric",
                "automatic",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Section 4 concludes the paper with a critical assessment of the proposed approach and a discussion of the prospects for application in the construction of corpora comparable in size and quality to existing treebanks   or the TIGER Treebank for German  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prospects",
                "application in the construction of corpora comparable in size and quality",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "proposed approach",
                "critical assessment",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Two are conditionalized phrasal models, each EM trained until performance degrades:  C-JPTM3 as described in    Phrasal ITG as described in Section 4.1 Three provide alignments for the surface heuristic:  GIZA++ with grow-diag-final    Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder   with the SMT Shared Task baseline system  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "EM trained until performance degrades",
                "performance degrades",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "Pharaoh decoder",
                "with the SMT Shared Task baseline system",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Since it is not feasible to maximise the likelihood of the observations directly, we maximise the expected log likelihood by considering the EM auxiliary function, in a similar manner to that used for modelling contextual variations of phones for ASR  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "EM auxiliary function",
                "similar manner to that used for modelling contextual variations of phones for ASR",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "likelihood",
                "maximise the expected log likelihood",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Like the data used by Ramshaw and Marcus  , this data was retagged by the Brill tagger in order to obtain realistic part-of speech   tags 5.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data",
                "retagged by the Brill tagger",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tags",
                "realistic",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "his hypothesized relationship between distributional similarity and semantic similarity has given rise to a large body of work on automatic thesaurus generation  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distributional similarity",
                "hypothesized relationship",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "automatic thesaurus generation",
                "large body of work",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Both systems rely on the OpenNlp maximum-entropy part-of-speech tagger and chunker  , but KNOWITALL applies them to pages downloaded from the Web based on the results of Google queries, whereas KNOWITNOW applies them once to crawled and indexed pages.6 Overall, each of the above elements of KNOWITALL and KNOWITNOW are the same to allow for controlled experiments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "his upper bound is consistent with the upper limit of 50% found by Daume III and Marcu   which takes into account stemming differences",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "upper bound",
                "consistent with the upper limit of 50%",
                "PERFORMANCE",
                "neutral",
                0.7
            ],
            [
                "Daume III and Marcu's method",
                "takes into account stemming differences",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The experiments were performed using the Wall Street Journal   corpus of the University of Pennsylvania   modified as described in   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal corpus",
                "of the University of Pennsylvania",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "corpus",
                "modified as described in and",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Precision and recall rates were 92.4% on the same data used in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Precision and recall rates",
                "92.4% on the same data",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "data",
                "used in ",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "From wordlevel alignments, such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages  , or with the the word-level alignments alone without reference to external syntactic analysis  , which is the scenario we address here.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Some of the differences between our approach and those of Turney   are mentioned below: ??objectives: Turney   aims at binary text classification, while our objective is six class classification of one-liner headlines.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "six class classification of one-liner headlines",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "objective",
                "binary text classification",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "These IBM models and more recent refinements   as well as algorithms that bootstrap from these models like the HMM algorithm described in   are unsupervised algorithms.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM models",
                "unsupervised algorithms",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "HMM algorithm",
                "unsupervised algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  was proposed in the original work to solve the LMR Tagging problem.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "proposed work",
                "solve the LMR Tagging problem",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "original work",
                "solves the LMR Tagging problem",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The Kappa statistic   is typically used to measure the human interrater agreement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa statistic",
                "is typically used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "human interrater agreement",
                "to measure",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he original intention of assignment 2 was that students then use this maxent classifier as a building block of a maxent part-of-speech tagger like that of Ratnaparkhi  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maxent classifier",
                "as a building block",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "Ratnaparkhi's part-of-speech tagger",
                "like that of",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However, the aforementioned SDT techniques require word classes  to help prevent data fragmentation, and a sophisticated smoothing algorithm to mitigate the effects of any fragmentation that occurs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SDT techniques",
                "require word classes",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "smoothing algorithm",
                "mitigate the effects of fragmentation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4 Maxilnum Entropy The model used here for sentence-boundary detection is based on the maximum entropy model used for POS tagging in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "maximum entropy model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "maximum entropy model",
                "used for POS tagging",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Statistical Model In SIFTs statistical model, augmented parse trees are generated according to a process similar to that described in Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SIFTs statistical model",
                "according to a process similar to that described in Collins",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "augmented parse trees",
                "generated",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Decoding with an SCFG   can be cast as a parsing problem  , in which case we need to binarize a synchronous rule with more than two nonterminals to achieve polynomial time algorithms  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SCFG",
                "polynomial time algorithms",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "rule",
                "more than two nonterminals",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Moreover, our approach integrates the abbreviation translation component into the baseline system in a natural way, and thus is able to make use of the minimum-error-rate training   to automatically adjust the model parameters to reflect the change of the integrated system over the baseline system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model parameters",
                "to reflect the change of the integrated system over the baseline system",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "approach",
                "integrates the abbreviation translation component into the baseline system in a natural way",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Syntactic criteria are relevant, but clearly not decisive, as can be observed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Syntactic criteria",
                "are relevant",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Syntactic criteria",
                "are not decisive",
                "METHODOLOGY",
                "negative",
                0.75
            ]
        ]
    },
    {
        "text": "Thus, equation   can be rewritten as  = i p i iii i i eppfef )| | |  4.2 Lexical Weight Given a phrase pair ), .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "equation",
                "can be rewritten",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase pair",
                "Given a phrase",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Existing automatic evaluation measures such as BLEU   and ROUGE (Lin 2The collections are available.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "and ROUGE",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "collections",
                "available",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4 Experiments and evaluation We carried out an evaluation on the local rephrasing of French sentences, using English as the pivot language.2 We extracted phrase alignments of up to 7 word forms using the Giza++ alignment tool   and the grow-diag-final-and heuristics described in   on 948,507 sentences of the French-English part of the Europarl corpus   and obtained some 42 million phrase pairs for which probabilities were estimated using maximum likelihood estimation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Giza++ alignment tool",
                "described in on 948,507 sentences",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "phrase alignments",
                "extracted using maximum likelihood estimation",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We follow the approach by Turney  , who note that the semantic orientation of an adjective depends on the noun that it modifies and suggest using adjective-noun or adverb-verb pairs to extract semantic orientation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Turney",
                "note that the semantic orientation",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "adjective-noun or adverb-verb pairs",
                "suggest using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Following   we can avoid unnecessary false positives by not querying for the longer n-gram in such cases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-gram",
                "longer n-gram",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "querying",
                "unnecessary false positives",
                "PERFORMANCE",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "We will employ the structural correspondence learning   domain adaption algorithm used in   for linking the translated text and the natural text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "structural correspondence learning domain adaption algorithm",
                "used in",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translated text and natural text",
                "linking",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We first added sister-head dependencies for NPs   original proposal) and then for PPs, which are flat in Negra, and thus similar in structure to NPs  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NPs",
                "similar in structure to NPs",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "PPs",
                "flat in Negra",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The statistical methods are based on distributional analysis  , and cluster analysis  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical methods",
                "based on distributional analysis and cluster analysis",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "methods",
                "based on",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "To improve raw output from decoding, Portage relies on a rescoring strategy: given a list of n-best translations from the decoder, the system reorders this list, this time using a more elaborate loglinear model, incorporating more feature functions, in addition to those of the decoding model: these typically include IBM-1 and IBM-2 model probabilities   and an IBM-1-based feature function designed to detect whether any word in one language appears to have been left without satisfactory translation in the other language; all of these feature functions can be used in both language directions, i.e. source-to-target and target-to-source.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "reorders this list",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "loglinear model",
                "more elaborate",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2.1 Data-based Methods Data-based approaches extract their information directly from texts and are divided into supervised and unsupervised methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "data-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "methods",
                "supervised and unsupervised",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The approach is based on the hypothesis that positive words co-occur more than expected by chance, and so do negative words; this hypothesis was validated, at least for strong positive/negative words, in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hypothesis",
                "positive words co-occur more than expected by chance",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "hypothesis",
                "negative words",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Recently, it has gained renewed attention as empirical methods in parsing have emphasized the importance of relations between words  ), which is what dependency grammars model explicitly, but context-free phrase-structure grammars do not.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency grammars",
                "model explicitly",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "context-free phrase-structure grammars",
                "do not",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4.1 Applications to phrase-based SMT Aphrase-basedtranslationmodelcanbeestimated in two stages: first a parallel corpus is aligned at the word-level and then phrase pairs are extracted  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based translation model",
                "estimated in two stages",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase pairs",
                "extracted",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Stochastic ITGs are parameterized like their PCFG counterparts  ; productions A  X are assigned probability Pr .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITGs",
                "parameterized like PCFG counterparts",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "productions",
                "assigned probability",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Turney   also selects patterns based on the number of pairs that generate them, but the number of selected patterns is a constant  , independent of the number of input word pairs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "selected patterns",
                "constant",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "number of selected patterns",
                "independent of the number of input word pairs",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the above equation, P  and P  are estimated by the maximum-likelihood method, and the probability of a POC tag ti, given a character wi  ) is estimated using ME models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "P and P",
                "estimated by the maximum-likelihood method",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ME models",
                "estimated using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "??word class: Turney   measures polarity using only adjectives, however in our approach we consider the noun, the verb, the adverb and the adjective content words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Turney",
                "measures polarity using only adjectives",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "our approach",
                "consider the noun, the verb, the adverb and the adjective content words",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 Comparison with related work Preliminary work on SF extraction from coq~ora was done by   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SF extraction",
                "preliminary work",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "coq~ora",
                "by and ",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Basic Approaches 2.1 Cross-Lingual Approach Our cross-lingual approach   is based on  , who used a modified Levenshtein string edit-distance algorithm to match Arabic script person names against their corresponding English versions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Levenshtein string edit-distance algorithm",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Arabic script person names",
                "corresponding English versions",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In all the experiments, our source side language is English, and the Stanford Named Entity Recognizer   was used to extract NEs from the source side article.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stanford Named Entity Recognizer",
                "was used to extract NEs",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "source side article",
                "to extract NEs",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.1 Synchronous derivations The derivations for syntactic dependency trees are the same as specified in  , which are based on the shift-reduce style parser of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "derivations",
                "specified in",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "parser",
                "shift-reduce style parser",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "These data sets were based on the Wall Street Journal corpus in the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal corpus",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank",
                "in the",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For all non-LEAF systems, we take the best performing of the union, refined and intersection symmetrization heuristics   to combine the 1-to-N and M-to-1 directions resulting in a M-to-N alignment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "union, refined and intersection symmetrization heuristics",
                "combine the 1-to-N and M-to-1 directions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "symmetrization heuristics",
                "resulting in a M-to-N alignment",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For content selection, discourse-level considerations were proposed by Daume III and Marcu  , who explored the use of Rhetorical Structure Theory  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Daume III and Marcu",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Rhetorical Structure Theory",
                "use of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Related work Our work is closest in spirit to the two papers that inspired us   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "papers",
                "inspired us",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "work",
                "closest in spirit",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "These include the perceptron   and its large-margin variants  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron",
                "large-margin variants",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "perceptron",
                "its large-margin variants",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction A recent theme in parsing research has been the application of statistical methods to linguistically motivated grammars, for example LFG  , HPSG  , TAG   and CCG  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "statistical methods",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "grammars",
                "linguistically motivated",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Background: Overview of BLEU This section briefly describes the original BLEU  1, which was designed for English translation evaluation, so English sentences are used as examples in this section.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "designed for English translation evaluation",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "English sentences",
                "used as examples",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The Baseline Maximum Entropy Model We started with a maximum entropy based tagger that uses features very similar to the ones proposed in Ratnaparkhi  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Baseline Maximum Entropy Model",
                "uses features very similar to the ones proposed in Ratnaparkhi",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "features",
                "very similar",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The topic signatures are automatically generated for each specific term by computing the likelihood ratio   between two hypotheses  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "topic signatures",
                "are automatically generated",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "likelihood ratio",
                "between two hypotheses",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Agglomerative clustering  ) can produce hierarchical word categories from an unannotated corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Agglomerative clustering",
                "can produce hierarchical word categories",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "BLEU   was devised to provide automatic evaluation of MT output.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "automatic evaluation of MT output",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "MT output",
                "provide",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Treebank  , six of which are errors.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Treebank",
                "six of which are errors",
                "METHODOLOGY",
                "neutral",
                0.5
            ],
            [
                "errors",
                "six",
                "PERFORMANCE",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "Recently, Snow, Jurafsky and Ng   generated tens of thousands of hypernym patterns and combined these with noun clusters to generate high-precision suggestions for unknown noun insertion into WordNet  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hypernym patterns",
                "generated tens of thousands of",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "suggestions",
                "high-precision",
                "PERFORMANCE",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "The simple idea that words in a source chunk are typically aligned to words in a single possible target chunk is used to discard alignments which link words from 2We use IBM-1 to IBM-5 models   implemented with GIZA++  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM-1 to IBM-5 models",
                "implemented with GIZA++",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "alignments",
                "link words from 2",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Recent work on reordering has been on trying to find smart ways to decide word order, using syntactic features such as POS tags   , parse trees   to name just a few, and synchronized CFG  , again to name just a few.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic features",
                "POS tags and parse trees",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "CFG",
                "synchronized",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Most existing methods treat word tokens as basic alignment units  , however, many languages have no explicit word boundary markers, such as Chinese and Japanese.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word tokens",
                "basic alignment units",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "languages",
                "have no explicit word boundary markers",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, in the WSJ corpus, part of the Penn Treebank 3 release  , the string in   is a variation 12-gram since off is a variation nucleus that is tagged preposition   in one corpus occurrence and particle   in another.1 Dickinson   shows that examining those cases with identical local contextin this case, lookingat ward off aresultsinanestimated error detection precision of 92.5%.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSJ corpus",
                "part of the Penn Treebank 3 release",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "error detection precision",
                "92.5%",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Since the texts in the RST Treebank are taken from the syntactically annotated Penn Treebank  , it is natural to ask what the relation is between the discourse structures in the RST Treebank and the syntactic structures of the Penn Treebank.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "RST Treebank",
                "taken from Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntactic structures of the Penn Treebank",
                "relation is between",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "While close attention has been paid to multi-document summarization technologies  , the inherent properties of humanwritten multi-document summaries have not yet been quantified.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "multi-document summarization technologies",
                "not yet been quantified",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "human-written multi-document summaries",
                "inherent properties",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In our experiments we use standard methods in phrase-based systems   to define the set of phrase entries for each sentence in training data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase entries",
                "standard methods",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "phrase-based systems",
                "standard",
                "METHODOLOGY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "In contrast, more recent research has focused on stochastic approaches that model discourse coherence at the local lexical   and global levels  , while preserving regularities recognized by classic discourse theories  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classic discourse theories",
                "recognized by",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "stochastic approaches",
                "model discourse coherence",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Previous work from   showed improvements in perplexity-oriented measures using mixture-based translation lexicon  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation lexicon",
                "mixture-based",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "perplexity-oriented measures",
                "improvements",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Demonstrating the inadequacy of such approaches, Al-Onaizan and Papineni   showed that even given the words in the reference translation, and their alignment to the source words, a decoder of this sort charged with merely rearranging them into the correct target-language order could achieve a BLEU score   of at best 69%and that only when restricted to keep most words very close to their source positions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "inadequate",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "decoder",
                "achieving BLEU score of at best 69%",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For a full discussion of previous work, please see  , or see   for work relating to synonym resolution.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "previous work",
                "please see",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "work relating to synonym resolution",
                "for work relating to",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Feature selection Berger et al   proposed an iterative procedure of adding news features to feature set driven by data",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature set",
                "driven by data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "features",
                "adding to feature set",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 Inversion Transduction Grammars While our approach applies in principle to a variety of machine translation systems  , we will use the inversion transduction grammar   approach of Wu   to facilitate comparison with previous work  aswellastofocuson language model complexity.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "inversion transduction grammar",
                "approach of Wu",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "machine translation systems",
                "variety of",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Regarding error detection in corpora, Ratnaparkhi   discusses inconsistencies in the Penn Treebank and relates them to interannotator differences in tagging style.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "inconsistencies",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "interannotator differences",
                "tagging style",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Ramshaw and Marcus   first assigned a chunk tag to each word in the sentence: I for inside a chunk, O for outside a chunk, and 240 type precision B tbr inside a chunk, but tile preceding word is in another chunk.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunk tag",
                "inside a chunk",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "precision",
                "240 type precision B",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The data set consisting of 249,994 TFSs was generated by parsing the Figure 3: The size of Dpi; for the size of the data set 800 bracketed sentences in the Wall Street Journal corpus   in the Penn Treebank   with the XHPSG grammar  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TFSs",
                "generated by parsing",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Wall Street Journal corpus",
                "widely used",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "8See formula in appendix B. We use   implementation with a minor alteration  see Beigman Klebanov  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "implementation",
                "with a minor alteration",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "formula",
                "in appendix B",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This finding has been previously reported, among others, in Liu and Gildea  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "finding",
                "previously reported",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "report",
                "others",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Estimation of the parameters has been described elsewhere  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "has been described elsewhere",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "description",
                "elsewhere",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " , and Magerman   can suffer from very similar problems to the label bias or observation bias problem observed in tagging models, as described in Lafferty, McCallum, and Pereira   and Klein and Manning  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagging models",
                "observed in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "problems",
                "similar to label bias or observation bias problem",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We adopted an N-best hypothesis approach   to train.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "N-best hypothesis approach",
                "to train",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "approach",
                "to train",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Aspect-based sentiment analysis summarizes sentiments with diverse attributes, so that customers may have to look more closely into analyzed sentiments  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "attributes",
                "diverse",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "customers",
                "have to look more closely",
                "LIMITATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "There is evidence that this leads to better performance on some part-of-speech induction metrics  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "part-of-speech induction metrics",
                "better performance",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "part-of-speech induction metrics",
                "some",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4 Evaluation As our algorithm works in open domains, we were able to perform a corpus-based evaluation using the Penn WSJ Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "works in open domains",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn WSJ Treebank",
                "corpus-based evaluation",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "These distributions are modeled using a maximum entropy formulation  , using training data which consists of human judgments of question answer pairs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "formulation",
                "maximum entropy",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "training data",
                "human judgments of question answer pairs",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": ".3 Grid Line Search Our implementation of a grid search is a modified version of that proposed in  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Grid Line Search",
                "modified version",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "implementation",
                "of that proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, syntactic features   can be computed this way and are used in our system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic features",
                "can be computed this way",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntactic features",
                "are used in our system",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The next two methods are heuristic   in   and grow-diagonal   proposed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "heuristic",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "grow-diagonal",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "sister head tag X Table 4: Linguistic features in the current model compared to the models of Carroll and Rooth  , Collins  , and Charniak   Negra, based on Collinss   model for nonrecursive NPs in the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "for nonrecursive NPs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "models of Carroll and Rooth",
                "compared to",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The implementation is similar to the idea of lexical weight in  : all points in the alignment matrices of the entire training corpus are collected to calculate the probabilistic distribution, P , of some TL word 3Some readers may prefer the expression the subtree rooted at node N to node N. The latter term is used in this paper for simplicity.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "implementation",
                "similar to the idea of lexical weight",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "expression the subtree rooted at node N to node N",
                "the latter term is used for simplicity",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Following  , we computed the skip bi-gram score using both the sentence pool and the query pool.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "skip bi-gram score",
                "using both the sentence pool and the query pool",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "sentence pool",
                "and the query pool",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "iebe   uses Lin   style distributionally similar adjectives in a cluster-and-label process to generate sentiment lexicon of adjectives",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "adjectives",
                "distributionally similar",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentiment lexicon",
                "generate",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Following  , we call the first the source domain, and the second the target domain.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "source domain",
                "the first",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "target domain",
                "the second",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "84.12 only PTB   83.58 1st   83.42 2nd   83.38 3rd   83.08 third row lists the three highest scores of the domain adaptation track of the CoNLL 2007 shared task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "domain adaptation track of the CoNLL 2007 shared task",
                "highest scores",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "three highest scores",
                "lists",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "5.2 Adding lexical information Gildea   shows that removing the lexical dependencies in Model 1 of Collins    decreases labeled precision and recall by only 0.5%.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 1 of Collins",
                "decreases labeled precision and recall",
                "PERFORMANCE",
                "negative",
                0.75
            ],
            [
                "lexical dependencies",
                "removing",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In constrast with many previous approaches  , our model does not try to capture how Source sentences can be mapped into Target sentences, but rather how Source and Target sentences can be generated simultaneously.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "many previous approaches",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "model",
                "generated simultaneously",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We utilize the OpenNLP MaxEnt implementation2 of the maximum entropy classification algorithm   to train classification models for each lemma and part-of-speech combination in the training corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "OpenNLP MaxEnt implementation",
                "maximum entropy classification algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "classification models",
                "each lemma and part-of-speech combination",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Kupiec   has proposed an estimation method for the N-gram language model using the Baum-Welch reestimation algorithm   from an untagged corpus and Cutting et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Baum-Welch reestimation algorithm",
                "using",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "N-gram language model",
                "proposed estimation method",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A la Ramshaw and Marcus  , they represent the words as a sequence of labeled words with IOB annotations, where the B marks a word at the beginning of a chunk, I marks a word inside a chunk, and O marks those words   that are outside chunks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "labeled words with IOB annotations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IOB annotations",
                "marks a word at the beginning of a chunk, I marks a word inside a chunk, and O marks those words that are outside chunks",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "translation systems   and use Moses   to search for the best target sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation systems",
                "best target sentence",
                "INNOVATION",
                "neutral",
                0.5
            ],
            [
                "Moses",
                "use",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Statistical Word Alignment According to the IBM models  , the statistical word alignment model can be generally represented as in Equation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "4.2 Experiments To build all alignment systems, we start with 5 iterations of Model 1 followed by 4 iterations of HMM  , as implemented in GIZA++  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 1",
                "5 iterations",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "HMM",
                "as implemented in GIZA++",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Two error rates: the sentence error rate   and the word error rate   that we seek to minimize, and BLEU  , that we seek to maximize.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "error rates",
                "seek to minimize",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "BLEU",
                "seek to maximize",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Similarly to  , the tree-to-string alignment templates discussed in this paper are actually transformation rules.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tree-to-string alignment templates",
                "are actually transformation rules",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "transformation rules",
                "discussed in this paper",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "While we have observed reasonable results with both G 2 and Fisher's exact test, we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information   measure  : I  --log 2 P    P P  In  , y is the seed term and x a potential target word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "G 2",
                "reasonable results",
                "PERFORMANCE",
                "positive",
                0.7
            ],
            [
                "Fisher's exact test",
                "reasonable results",
                "PERFORMANCE",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "For the Brown corpus, we based our division on  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "division",
                "on",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Brown corpus",
                "no opinion term found",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A Head Percolation Table has previously been used in several statistical parsers   to find heads of phrases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Head Percolation Table",
                "used in several statistical parsers",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrases",
                "find heads of",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The standard split of the corpus into training  , validation  , and testing   was performed.2 As in   we used a publicly available tagger   to provide the part-of-speech tag for each word in the sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "split into training, validation, and testing",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tagger",
                "publicly available",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "When different decoder settings are applied to the same model, MERT weights   from the unprojected single pass setup are used and are kept constant across runs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MERT weights",
                "are used and are kept constant",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "decoder settings",
                "are applied to the same model",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "More recently, the problem has been tackled using statistics-based   and learning-based   methods.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "statistics-based and learning-based",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "tackled using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To cope with this problem we 898 use the concept of class proposed for a word n-gram model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "concept of class",
                "proposed for a word n-gram model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "n-gram model",
                "word n-gram model",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Correlation Coefficient with Human Judgement   Human-Likeness Classifier Accuracy   Figure 1: This scatter plot compares classifiers accuracy with their corresponding metrics correlations with human assessments been previously observed by Liu and Gildea  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Correlation Coefficient with Human Judgement",
                "been previously observed by Liu and Gildea",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Human-Likeness Classifier Accuracy",
                "corresponding metrics correlations with human assessments",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We benchmark our results against a model   which was directly trained to optimise BLEUNIST using the standard MERT algorithm   and the full set of translation and lexical weight features described for the Hiero model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MERT algorithm",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Hiero model",
                "described",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Other linear time algorithms for rank reduction are found in the literature  , but they are restricted to the case of synchronous context-free grammars, a strict subclass of the LCFRS with f = 2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linear time algorithms",
                "found in the literature",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "synchronous context-free grammars",
                "strict subclass of the LCFRS with f = 2",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "omokiyo and Hurst   use pointwise KLdivergence between multiple language models for scoring both phraseness and informativeness of phrases",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "language models",
                "pointwise KLdivergence between multiple",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phraseness and informativeness of phrases",
                "scoring",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "To evaluate the polarity and strength of opinions, most of the existing approaches rely either on training from human-annotated data  , or use linguistic resources   like WordNet, or rely on co-occurrence statistics   between words that are unambiguously positive   and unambiguously negative  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "human-annotated data",
                "rely on",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "WordNet",
                "linguistic resources",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The parser induction algorithm used in all of the experiments in this paper was a distribution of Collins's model 2 parser  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins's model 2 parser",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parser induction algorithm",
                "distribution of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In our experiments using BLEU   as the metric, the interpolated synthetic model achieves a relative improvement of 11.7% over the best RBMT system that is used to produce the synthetic bilingual corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "is used as the metric",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "RBMT system",
                "is used to produce the synthetic bilingual corpora",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Co-Training has been used before in applications like word-sense disambiguation  , web-page classification   and namedentity identification  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Co-Training",
                "used before",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word-sense disambiguation, web-page classification, namedentity identification",
                "applications",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Some regarded Wikipedia as the corpora and applied hand-crafted or machine-learned rules to acquire semantic relations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hand-crafted or machine-learned rules",
                "applied to acquire semantic relations",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Some work has been done on adding new terms and relations to WordNet   and FACTOTUM  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet",
                "adding new terms and relations",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "FACTOTUM",
                "new terms and relations",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For this reason, to our knowledge, all discriminative models proposed to date either side-step the problem by choosing simple model and feature structures, such that spurious ambiguity is lessened or removed entirely  , or else ignore the problem and treat derivations as translations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "simple model and feature structures",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "problem",
                "ignoring and treating as translations",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "This knowledge is represented in axiomatic form, using the notation proposed in   and previously implemented in TACITUS.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "notation",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "TACITUS",
                "previously implemented",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Though inter-rater reliability using the kappa statistic   may be calculated for each group, the distribution of categories in the contribution group was highly skewed and warrants further discussion.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa statistic",
                "may be calculated for each group",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "distribution of categories",
                "warrants further discussion",
                "LIMITATION",
                "negative",
                0.9
            ]
        ]
    },
    {
        "text": "In 2004, Conroy   tested Maximal Marginal Relevance   as well as QR decomposition.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximal Marginal Relevance",
                "tested as well as",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "QR decomposition",
                "tested as well as",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For 1-best search, we use the cube pruning technique   which approximately intersects the translation forest with the LM.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "cube pruning technique",
                "approximately intersects the translation forest with the LM",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation forest",
                "approximately intersects",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We also combine our basic algorithm   with  s algorithm in order to resolve the modifier-function traces.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "basic algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "s algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "s To set weights on the components of the loglinear model, we implemented Ochs algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ochs algorithm",
                "implemented",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "loglinear model",
                "set weights on components",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Many 649 similarity measures and weighting functions have been proposed for distributional vectors; comparative studies include Lee  , Curran   and Weeds and Weir  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "similarity measures and weighting functions",
                "have been proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "comparative studies",
                "include Lee, Curran, and Weeds and Weir",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A number of alignment techniques have been proposed, varying from statistical methods   to lexical methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment techniques",
                "varying from statistical methods to lexical methods",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "statistical methods",
                "statistical methods",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The methods for calculating relative frequencies   and lexical weights   are also adapted for the weighted matrix case.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "adapting for weighted matrix case",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "calculating relative frequencies and lexical weights",
                "adapted",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The loglinear model weights are learned using Chiangs implementation of the maximum BLEU training algorithm  , both for the baseline, and the WSD-augmented system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "loglinear model weights",
                "learned using Chiang's implementation of the maximum BLEU training algorithm",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "WSD-augmented system",
                "both for the baseline and",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We utilize a maximum entropy   model   to design the basic classifier used in active learning for WSD.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy model",
                "design the basic classifier",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "basic classifier",
                "used in active learning for WSD",
                "APPLICABILITY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "6 Parameter Estimation From the duality of ME and maximum likelihood  , optimal parameters  for model   can be found by maximizing the log-likelihood function over a training sample {  : t = 1,,N}, i.e.:  = argmax  Nsummationdisplay t=1 logp .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "optimal parameters",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "log-likelihood function",
                "maximizing",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, the pb features yields no noticeable improvement unlike in prefect lexical choice scenario; this is similar to the findings in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pb features",
                "yields no noticeable improvement",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "findings",
                "similar to the findings in ",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5.5 Dependency validity features Like  , we extract the dependency path from the question word to the common word  , and the path from candidate answer   to the common word for each pair of question and candidate sentence using Stanford dependency parser  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stanford dependency parser",
                "dependency path",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "dependency path",
                "from question word to common word",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Solving this first methodological issue, has led to solutions dubbed hereafter as unlexicalized statistical parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methodological issue",
                "solutions dubbed",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "statistical parsing",
                "dubbed",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Substring-based transliteration with a generative hybrid model is very similar to existing solutions for phrasal SMT  , operating on characters rather than words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Substring-based transliteration",
                "very similar to existing solutions",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "operating on characters rather than words",
                "purely descriptive",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "he idea of tracing polarity through adjective cooccurrence is adopted by Turney   for the binary   classification of text reviews",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "idea",
                "adopted by Turney",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "binary classification",
                "of text reviews",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this paper, we take the framework for acquiring multi-level syntactic translation rules of   from aligned tree-string pairs, and present two main extensions of their approach: first, instead of merely computing a single derivation that minimally explains a sentence pair, we construct a large number of derivations that include contextually richer rules, and account for multiple interpretations of unaligned words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "framework for acquiring multi-level syntactic translation rules",
                "from aligned tree-string pairs",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "derivations",
                "include contextually richer rules",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "In this paper, we propose an alignment algorithm between English and Korean conceptual units   in English-Korean technical term pairs based on IBM Model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model",
                "based on IBM Model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "English-Korean technical term pairs",
                "alignment algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Some of them use human reference translations, e.g., the BLEU method  , which is based on comparison of N-gram models in MT output and in a set of human reference translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU method",
                "based on comparison of N-gram models in MT output and in a set of human reference translations",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "BLEU method",
                "based on comparison of N-gram models",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Two LUs close in the space are likely to be in a paradigmatic relation, i.e. to be close in a is-a hierarchy  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LUs",
                "close in the space",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "paradigmatic relation",
                "is-a hierarchy",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "However, Klein and Manning   showed that for natural language and text processing tasks, conditional models are usually better than joint likelihood models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "usually better",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "joint likelihood models",
                "usually worse",
                "PERFORMANCE",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction Motivation: Sharing basic intuitions and longterm goals with other tasks within the area of Webbased information extraction  , the task of acquiring class attributes relies on unstructured text available on the Web, as a data source for extracting generally-useful knowledge.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data source",
                "unstructured text available on the Web",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "knowledge",
                "generally-useful",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "It has a lower bound of 0, no upper bound, better scores indicate better translations, and it tends to be highly correlated with the adequacy of outputs ;  mWER   or Multiple Word Error Rate is the edit distance in words between the system output and the closest reference translation in a set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "scores",
                "better",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "adequacy of outputs",
                "tends to be highly correlated",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 We then run Collins parser  , using just the sentence pairs where parsing succeeds with a negative log likelihood below 200.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "using just the sentence pairs where parsing succeeds with a negative log likelihood below 200",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Collins parser",
                "just the sentence pairs where parsing succeeds",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "7.2 Minimum-Risk Training Adjusting  or  changes the distribution p. Minimum error rate training     tries to tune  to minimize the BLEU loss of a decoder that chooses the most probable output according to p.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU loss",
                "minimize",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "decoder",
                "chooses the most probable output according to p",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As described in Section 4, we define the problem of term variation identifica1484 tion as a binary classification task, and build two types of classifiers according to the maximum entropy model   and the MART algorithm  , where all term similarity metrics are incorporated as features and are jointly optimized.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "term similarity metrics",
                "are incorporated as features and are jointly optimized",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "classifiers",
                "built according to the maximum entropy model and the MART algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "illmann and Zhang   trained their feature set using an online discriminative algorithm",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature set",
                "trained using an online discriminative algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "discriminative",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training   as implemented by Venugopal and Vogel  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights",
                "set using the 500-sentence tuning set",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "minimum error rate training",
                "as implemented",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A common choice for the local probabilistic classifier is maximum entropy classifiers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy classifiers",
                "common choice",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "maximum entropy classifiers",
                "for the local probabilistic classifier",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The method uses a translation model based on IBM Model 1  , in which translation candidates of a phrase are generated by combining translations and transliterations of the phrase components, and matching the result against a large corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "based on IBM Model 1",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpus",
                "large",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ur work in sentence reformulation is different from cut-and-paste summarization   in many ways",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "different from cut-and-paste summarization",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "reformulation",
                "is different",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Dynamic programming is applied to bilingual sentence alignment in most of previous works  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Dynamic programming",
                "applied to bilingual sentence alignment",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "previous works",
                "most of",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For now, we consider it to be one where:  Every foreign word is aligned exactly once  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "foreign word",
                "aligned exactly once",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Still, however, such techniques often require seeds, or prototypes  ) which are used to prune search spaces or direct learners.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "seeds",
                "are used to prune search spaces or direct learners",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "techniques",
                "require seeds",
                "METHODOLOGY",
                "negative",
                0.6
            ]
        ]
    },
    {
        "text": "Comparatively,   propose to use the N-gram Overlap metric to capture similarities between sentences and automatically create paraphrase corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "N-gram Overlap metric",
                "capture similarities",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "paraphrase corpora",
                "automatically create",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In our search procedure, we use a mixture-based alignment model that slightly differs from the model introduced as Model 2 in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "slightly differs from Model 2",
                "METHODOLOGY",
                "neutral",
                0.7
            ],
            [
                "model",
                "introduced as Model 2",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "While movie reviews have been the most studied domain, sentiment analysis has extended to a number of new domains, ranging from stock message boards to congressional floor debates  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "domain",
                "number of new domains",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "message boards",
                "to",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "5.1 Experimental setup The baseline model was Hiero with the following baseline features  :  two language models  phrase translation probabilities p  and p   lexical weighting in both directions    word penalty  penalties for:  automatically extracted rules  identity rules    two classes of number/name translation rules  glue rules The probability features are base-100 logprobabilities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hiero",
                "baseline model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase translation probabilities p and p",
                "probability features are base-100 logprobabilities",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Alternatively, one can train them with respect to the final translation quality measured by an error criterion  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation quality",
                "measured by an error criterion",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "error criterion",
                "measured by",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Two more recent investigations are by Yarowsky,  , and later, Mihalcea,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Yarowsky",
                "more recent",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Mihalcea",
                "more recent",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Appendix A: Derivation of the Probability of RWE We take a noisy channel approach, which is a common technique in NLP  ), including spellchecking  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "technique",
                "noisy channel approach",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "technique",
                "common in NLP",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "It is mentioned that the limitation is largely caused by inconsistencies in the corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "inconsistencies",
                "LIMITATION",
                "neutral",
                0.9
            ],
            [
                "inconsistencies",
                "are largely caused by",
                "LIMITATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4 Related Work 4.1 Acquisition of Classes of Instances Although some researchers focus on re-organizing or extending classes of instances already available explicitly within manually-built resources such as Wikipedia   or WordNet   or both  , a large body of previous work focuses on compiling sets of instances, not necessarily labeled, from unstructured text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Numerous approaches have been explored for exploiting situations where some amount of annotated data is available and a much larger amount of data exists unannotated, e.g. Marialdo's HMM part-of-speech tagger training  , Charniak's parser retraining experiment  , Yarowsky's seeds for word sense disambiguation   and Nigam et al's   topic classifier learned in part from unlabelled documents.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Marialdo's HMM part-of-speech tagger",
                "training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Charniak's parser",
                "retraining experiment",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Penn Treebank   the HPSG LinGo Redwoods Treebank  , and a smaller dependency treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "HPSG LinGo Redwoods Treebank",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "dependency treebank",
                "smaller",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "A different approach in evaluating nonparametric Bayesian models for NLP is statesplitting  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "nonparametric Bayesian models",
                "statesplitting",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "approach",
                "different",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "herefore in Collins   grammar rules are already factorized into a set of probabilities",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar rules",
                "are already factorized into a set of probabilities",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "probabilities",
                "set of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Related Work There has been extensive research in opinion mining at the document level, for example on product and movie reviews  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "extensive",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "opinion mining",
                "at the document level",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Following  , we do not distinguish rare words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "do not distinguish",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "rare words",
                "are not distinguished",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To perform minimum error rate training   to tune the feature weights to maximize the systems BLEU score on development set, we used optimizeV5IBMBLEU.m  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "maximize the systems BLEU score",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "optimizeV5IBMBLEU.m",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As an additional baseline, we compare against a phrasal SMT decoder, Pharaoh  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pharaoh",
                "phrasal SMT decoder",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Pharaoh",
                "additional baseline",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation   and consists of the Pharaoh decoder  , SRILM  , GIZA++  , mkcls  , Carmel,1 and a phrase model training code.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pharaoh decoder",
                "decoder",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "phrase model training code",
                "phrase model training code",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We have applied it to the two data sets mentioned in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data sets",
                "mentioned in ",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "it",
                "applied to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We finally also include as alignment candidates those word pairs that are transliterations of each other to cover rare proper names  , which is important for language pairs that dont share the same alphabet such as Arabic and English.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment candidates",
                "transliterations of each other",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "language pairs",
                "dont share the same alphabet",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use a statistical POS tagging system built on Arabic Treebank data with MaxEnt framework  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Arabic Treebank data",
                "statistical POS tagging system",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "MaxEnt framework",
                "built on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he use of such relations   for various purposes has received growing attention in recent research  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "relations",
                "has received growing attention",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "research",
                "has received growing attention",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Formally, by distributional similarity   of two words w 1 and w 2 , we mean that they tend to occur in similar contexts, for some definition of context; or that the set of words that w 1 tends to co-occur with is similar to the set that w 2 tends to co-occur with; or that if w 1 is substituted for w 2 in a context, its plausibility   is unchanged.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distributional similarity",
                "tend to occur in similar contexts",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "plausibility",
                "is unchanged",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We then used the kappa statistic   to assess the level of agreement between the three coders with respect to the 2 An agent holds the task initiative during a turn as long as some utterance during the turn directly proposes how the agents should accomplish their goal, as in utterance  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa statistic",
                "assess the level of agreement",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "utterance",
                "proposes how the agents should accomplish their goal",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrase-based approach  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based approach",
                "conventional",
                "INNOVATION",
                "positive",
                0.7
            ],
            [
                "quasi-syntactic structure",
                "naturally capture",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "owever morphosyntactic features alone cannot verify the terminological status of the units extracted since they can also select non terms  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "units extracted",
                "can also select non terms",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "morphosyntactic features",
                "alone cannot verify",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We could also introduce new variables, e.g., nonterminal refinements  , or secondary linksMij   that augment the parse with representations of control, binding, etc.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "nonterminal refinements",
                "augment the parse with representations of control, binding, etc.",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parse",
                "with representations of control, binding, etc.",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "00: the current input token and the previous one have the same parent  90: one ancestor of the current input token and the previous input token have the same parent  09: the current input token and one ancestor of the previous input token have the same parent  99 one ancestor of the current input token and one ancestor of the previous input token have the same parent Compared with the B-Chunk and I-Chunk used in Ramshaw and Marcus ~, structural relations 99 and 90 correspond to B-Chunk which represents the first word of the chunk, and structural relations 00 and 09 correspond to I-Chunk which represents each other in the chunk while 90 also means the beginning of the sentence and 09 means the end of the sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "B-Chunk",
                "represents the first word of the chunk",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "I-Chunk",
                "represents each other in the chunk",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The second approximation proposed in   takes into consideration the fact that, after each decision is made, all the preceding latent variables should have their means i updated.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "latent variables",
                "means updated",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "decision",
                "made",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The likelihood ratio is obtained by treating word and Ic as a bigram and computed with the formula in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word and Ic",
                "as a bigram",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "formula",
                "in  ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Each word i in the context vector of w is then weighted with a measure of its association with w. We chose the loglikelihood ratio test,  , to measure this association the context vectors of the target words are then translated with our general bilingual dictionary, leaving the weights unchanged   the similarity of each source word s, for each target word t, is computed on the basis of the cosine measure the similarities are then normalized to yield a probabilistic translation lexicon, P .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "loglikelihood ratio test",
                "measure of its association with w",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "cosine measure",
                "similarity of each source word s",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The starting point is the log likelihood ratio  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log likelihood ratio",
                "is the starting point",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "log likelihood ratio",
                "defined as",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The translation model is estimated via the EM algorithm or approximations that are bootstrapped from the previous model in the sequence as introduced in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "estimated via EM algorithm or approximations",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "previous model",
                "bootstrapped from",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The resulting corpus contains 385 documents of American English selected from the Penn Treebank  , annotated in the framework of Rhetorical Structure Theory.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "contains 385 documents of American English",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank",
                "annotated in the framework of Rhetorical Structure Theory",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our evaluation metric is BLEU   with caseinsensitive matching from unigram to four-gram.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "with case-insensitive matching from unigram to four-gram",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "metric",
                "is BLEU",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Recently, specific probabilistic tree-based models have been proposed not only for machine translation  , but also for This work was supported by DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilistic tree-based models",
                "have been proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450",
                "This work was supported by",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This situation is very similar to the training process of translation models in statistical machine translation  , where parallel corpus is used to find the mappings between words from different languages by exploiting their co-occurrence patterns.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parallel corpus",
                "used to find mappings between words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "co-occurrence patterns",
                "exploiting",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Words are encoded through an automatic clustering algorithm   while tags, labels and extensions are normally encoded using diagonal bits.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "automatic clustering",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "bits",
                "diagonal",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Different news articles reporting on the same event are commonly used as monolingual comparable corpora, from which both paraphrase patterns and phrasal paraphrases can be derived  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "monolingual comparable corpora",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "phrasal paraphrases",
                "can be derived",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "To extract semantic information of words such as synonyms and antonyms from corpora, previous research used syntactic structures  , response time to associate synonyms and antonyms in psychological experiments  , or extracting related words automatically from corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "used syntactic structures",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "syntactic structures",
                "previous research",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 Evaluation of Algorithms All four algorithms were run on a 3900 utterance subset of the Penn Treebank annotated corpus   provided by Charniak and Ge  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "run on a 3900 utterance subset",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " , Pereira and Tishby  , and Pereira, Tishby, and Lee   propose methods that derive classes from the distributional properties of the corpus itself, while other authors use external information sources to define classes: Resnik   uses the taxonomy of WordNet; Yarowsky   uses the categories of Roget's Thesaurus, Slator   and Liddy and Paik   use the subject codes in the LDOCE; Luk   uses conceptual sets built from the LDOCE definitions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "derive classes from the distributional properties of the corpus itself",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Resnik",
                "uses the taxonomy of WordNet",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "First, it recognizes non-recursive Base Noun Phrase    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Base Noun Phrase",
                "non-recursive",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Base Noun Phrase",
                "recognizes",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "orest reranking with a language model can be performed over this n-ary forest using the cube growing algorithm of Huang and Chiang  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "cube growing algorithm",
                "of Huang and Chiang",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "language model",
                "can be performed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4 Architecture of the SMT system The goal of statistical machine translation   is to produce a target sentence e from a source sentence f. It is today common practice to use phrases as translation units   and a log linear framework in order to introduce several models explaining the translation process: e = argmaxp  = argmaxe {exp )}   The feature functions hi are the system models and the i weights are typically optimized to maximize a scoring function on a development set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature functions",
                "system models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "weights",
                "optimized to maximize",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.3 Tree Transducer Grammars Syntactic machine translation   uses tree transducer grammars to translate sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Tree Transducer Grammars",
                "uses",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "sentences",
                "translate",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The noun phrase chunking   module uses the basic NP chunker software from 483   to recognize the noun phrases in the question.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "noun phrase chunking module",
                "uses the basic NP chunker software from 483",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "noun phrases",
                "in the question",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Giza++ is a freely available implementation of IBM Models 1-5   and the HMM alignment  , along with various improvements and modifications motivated by experimentation by Och & Ney  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Giza++",
                "implementation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "various improvements and modifications",
                "motivated by experimentation",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "173 The standard features for genre classification models include words, part-of-speech   tags, and punctuation  , but constituent-based syntactic categories have also been explored  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "standard features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "constituent-based syntactic categories",
                "have also been explored",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "ts distribution is asymptotic to a  2 distribution and can hence be used as a test statistic  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ts distribution",
                "asymptotic to a 2 distribution",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "test statistic",
                "can be used",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2 Related Work Automatic Paraphrasing and Entailment Our work is closely related to research in automatic paraphrasing, in particular, to sentence level paraphrasing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "sentence level paraphrasing",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "research",
                "automatic paraphrasing",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"\"crummy\"\" MT on the World Wide Web  , certain machine-assisted translation tools (e.g.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation lexicon",
                "sufficient and preferable",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "machine-assisted translation tools",
                "certain",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Templates for local features are similar to the ones employed by Ratnaparkhi   for POS-tagging  , though as our input already includes POStags, we can make use of part-of-speech information as well.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Templates for local features",
                "similar to the ones employed by Ratnaparkhi",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "part-of-speech information",
                "can make use of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Researchers have focused on learning adjectives or adjectival phrases   and verbs  , but no previous work has focused on learning nouns.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "nouns",
                "no previous work has focused on learning",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "verbs",
                "focused on learning",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We refer to a3a16a5a7 as the source language string and a10 a11a7 as the target language string in accordance with the noisy channel terminology used in the IBM models of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM models",
                "noisy channel terminology",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "source language string",
                "a3a16a5a7",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In the past two or three years, this kind of verification has been attempted for other aspects of semantic interpretation: by Passonneau and Litman   for segmentation and by Kowtko, Isard, and Doherty   and Carletta et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Passonneau and Litman",
                "for segmentation",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Kowtko, Isard, and Doherty and Carletta et al.",
                "for other aspects of semantic interpretation",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "using Spearmans rank correlation coefficient and Pearsons rank correlation coefficient  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Spearmans rank correlation coefficient",
                "Pearsons rank correlation coefficient",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Spearmans rank correlation coefficient",
                "no opinion term",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our method for identifying paraphrases is an extension of recent work in phrase-based statistical machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based statistical machine translation",
                "recent work",
                "INNOVATION",
                "neutral",
                0.7
            ],
            [
                "method",
                "extension of",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We tuned the parameters of these features with Minimum Error Rate Training     on the NIST MT03 Evaluation data set  , and then test the MT performance on NIST MT03 and MT05 Evaluation data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NIST MT03 Evaluation data set",
                "used for tuning parameters",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "NIST MT03 and MT05 Evaluation data",
                "used for testing MT performance",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "159 2.1 Baseline System The baseline system is a phrase-based SMT system  , built almost entirely using freely available components.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseline system",
                "built almost entirely using freely available components",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase-based SMT system",
                "phrase-based",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Learning We model the problem of selecting the best derivation as a structured prediction problem  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "derivation",
                "structured prediction problem",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "problem",
                "selecting the best",
                "APPLICABILITY",
                "neutral",
                0.75
            ]
        ]
    },
    {
        "text": "Such methods were presented in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "presented in ",
                "METHODOLOGY",
                "neutral",
                0.5
            ],
            [
                "methods",
                "were",
                "METHODOLOGY",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "We used treebank grammars induced directly from the local trees of the entire WSJ section of the Penn Treebank    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treebank grammars",
                "induced directly from local trees of the entire WSJ section of the Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "WSJ section",
                "entire",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The second baseline is our implementation of the relevant part of the Wikipedia extraction in  , taking the first noun after a be verb in the definition sentence, denoted as WikiBL.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wikipedia extraction",
                "relevant part",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "WikiBL",
                "our implementation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our system is actually designed as a hybrid of the classic phrase-based SMT model   and the kernel regression model as follows: First, for each source sentence a small relevant set of sentence pairs are retrieved from the large-scale parallel corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "hybrid of the classic phrase-based SMT model and the kernel regression model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence pairs",
                "retrieved from the large-scale parallel corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Early examples of this work include  ; more recent models include  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "more recent",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "models",
                "earlier",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "For these experiments, we have implemented an alignment package for IBM Model 4 using a hillclimbing search and Viterbi training as described in  , and extended this to use new submodels.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment package",
                "using hillclimbing search and Viterbi training",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "alignment package",
                "extended to use new submodels",
                "INNOVATION",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "Similarly to MERT, Tillmann and Zhang estimate the parameters of a weight vector on a linear combination of   features using a global objective function correlated with BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters of a weight vector",
                "estimate using a global objective function",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "global objective function",
                "correlated with BLEU",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "It is potentially useful in other natural language processing tasks, such as the problem of estimating n-gram models   or the problem of semantic tagging  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-gram models",
                "problem of estimating",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "semantic tagging",
                "problem of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This example is adapted from Resnik  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Resnik",
                "adapted from",
                "INNOVATION",
                "neutral",
                1.0
            ],
            [
                "example",
                "adapted",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "2.1 Data representation We have compared four complete and three partial data representation formats for the baseNP recognition task presented in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data representation formats",
                "four complete and three partial",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "baseNP recognition task",
                "presented in  ",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use the adaptation of this algorithm to structure prediction, first proposed by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "first proposed by  ",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "prediction",
                "structure prediction",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "918 English For English we used the Wall Street Journal section of the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "Wall Street Journal section",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "English",
                "used",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "6 Results We trained on the standard Penn Treebank WSJ corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank WSJ corpus",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "training",
                "on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It is a reimplementation of the averaged perceptron described in  , which uses such features that it behaves like an HMM tagger and thus the standard Viterbi decoding is possible.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron",
                "described in",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Viterbi decoding",
                "standard",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Significant neighbor-based co-occurrence: As discussed in  , it is possible to measure the amount of surprise to see two neighboring words in a corpus at a certain frequency under the assumption of independence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "at a certain frequency under the assumption of independence",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "words",
                "under the assumption of independence",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Introduction Verb subcategorizafion probabilities play an important role in both computational linguistic applications   and psycholinguisfic models of language processing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Verb subcategorizafion probabilities",
                "play an important role",
                "INNOVATION",
                "positive",
                0.85
            ],
            [
                "computational linguistic applications",
                "and psycholinguisfic models of language processing",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Adaptations to the algorithms in the presence of ngram LMs are discussed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "in the presence of ngram LMs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "adaptations",
                "are discussed",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "2 Related Work A large amount of previous research on clustering has been focused on how to find the best clusters  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "on how to find the best clusters",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "previous research",
                "focused on",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Building on the annotations from the Wall Street Journal   portion of the Penn Treebank  , the project added several new layers of semantic annotations, such as coreference information, word senses, etc. In its first release   through the Linguistic Data Consortium  , the project manually sense-tagged more than 40,000 examples belonging to hundreds of noun and verb types with an ITA of 90%, based on a coarse-grained sense inventory, where each word has an average of only 3.2 senses.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "portion",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "sense-tagged",
                "more than 40,000 examples",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " , this model is symmetric, because both word bags are generated together from a joint probability distribution.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "symmetric",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word bags",
                "generated together from a joint probability distribution",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Perhaps this was not observed earlier since   studied only base NPs, most of which are short.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NPs",
                "most of which are short",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "",
                "studied only base",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "It is therefore desirable to have dedicated servers to load parts of the LM3  an idea that has been exploited by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LM3",
                "desirable",
                "APPLICABILITY",
                "positive",
                0.75
            ],
            [
                "idea",
                "exploited",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In particular, it shows systematically better F-Measure and Accuracy measures over all other metrics showing an improvement of   at least 2.86% in terms of F-Measure and 3.96% in terms of Accuracy and   at most 6.61% in terms of FMeasure and 6.74% in terms of Accuracy compared to the second best metric which is also systematically the word N-gram overlap similarity measure used by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "F-Measure",
                "shows systematically better",
                "PERFORMANCE",
                "positive",
                0.95
            ],
            [
                "Accuracy",
                "shows systematically better",
                "PERFORMANCE",
                "positive",
                0.95
            ]
        ]
    },
    {
        "text": "Therefore, whenever we have access to a large amount of labeled data from some source  , but we would like a model that performs well on some new target domain  , we face the problem of domain adaptation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "labeled data",
                "from some source",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "model",
                "performs well on some new target domain",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.3 System evaluation Since both the system translations and the reference translations are available for the tuning 43 set, we first compare each output to the reference translation using BLEU   and METEOR   and a combined scoring scheme provided by the ULC toolkit  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system translations",
                "compared to the reference translation",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "combined scoring scheme",
                "provided by the ULC toolkit",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Related Works Generally speaking, approaches to MWE extraction proposed so far can be divided into three categories: a) statistical approaches based on frequency and co-occurrence affinity, b) knowledgebased or symbolic approaches using parsers, lexicons and language filters, and c) hybrid approaches combining different methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "statistical approaches based on frequency and co-occurrence affinity",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "approaches",
                "knowledge-based or symbolic approaches using parsers, lexicons and language filters",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1998; Goldman and Zhou, 2000) that has been used previously to train classifiers in applications like word-sense disambiguation  , document classification   and named-entity recognition   and apply this method to the more complex domain of statistical parsing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifiers",
                "in applications like word-sense disambiguation, document classification, and named-entity recognition",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "method",
                "apply this method to the more complex domain of statistical parsing",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Thus, one conclusion from that line of work is that as soon as there is a reasonable   amount of labeled target data, it is often more fruitful to either just use that, or to apply simple adaptation techniques  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "target data",
                "reasonable amount of labeled",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "adaptation techniques",
                "simple",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3.2 Domain Adaptation Track As mentioned previously, the source data is drawn from a corpus of news, specifically the Wall Street Journal section of the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "source data",
                "drawn from a corpus of news",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ustejovsky confronted with the problem of automatic acquisition more extensively in \\ ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "automatic acquisition",
                "more extensively",
                "INNOVATION",
                "neutral",
                0.7
            ],
            [
                "problem",
                "confronted with",
                "APPLICABILITY",
                "neutral",
                0.6
            ]
        ]
    },
    {
        "text": "arletta   and Ros6   point out the importance of taking into account the expected chance agreement among judges when computing whether or not judges agree significantly",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "judges",
                "expected chance agreement",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "computing whether or not judges agree significantly",
                "importance",
                "APPLICABILITY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Co-training   can be informally described in the following manner: #0F Pick two   views of a classification problem.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "views",
                "two",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "description",
                "informally",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Finally, other approaches rely on reviews with numeric ratings from websites   and train  supervised learning algorithms to classify reviews as positive or negative, or in more fine-grained scales  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "rely on reviews with numeric ratings from websites",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "reviews",
                "as positive or negative, or in more fine-grained scales",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Most existing work to capture labelconsistency, has attempted to create all parenleftbign2parenrightbig pairwise dependencies between the different occurrences of an entity,  , where n is the number of occurrences of the given entity.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "entity",
                "different occurrences of an entity",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "pairwise dependencies",
                "between the different occurrences",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "6 Related Work In machine translation, the concept of packed forest is first used by Huang and Chiang   to characterize the search space of decoding with language models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Huang and Chiang",
                "characterize the search space of decoding with language models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "packed forest",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Trained and tested using the same technique as  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "technique",
                "same",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "technique",
                "same",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Roget's has been used as the sense division in two recent WSD works   more or less as is, except for a small number of senses added to fill gaps.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sense division",
                "as is",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "number of senses",
                "added to fill gaps",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "After building the chunker, students were asked to 4 choose a verb and then analyze verb-argument structure  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunker",
                "build",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "verb-argument structure",
                "analyze",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Wu   demonstrated that for pairs of sentences that are less than 16 words, the ITG alignment space has a good coverage over all possibilities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG alignment space",
                "has a good coverage over all possibilities",
                "PERFORMANCE",
                "positive",
                0.75
            ],
            [
                "pairs of sentences",
                "less than 16 words",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Mutual information compares the probability of the co-occurence of words a and b with the independent probabilities of occurrence of a and b  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probability of the co-occurence of words a and b",
                "compares the probability",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "probability of occurrence of a and b",
                "independent probabilities",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The theory has been applied in probabilistic language modeling  , natural language processing  , as well as computational vision  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "theory",
                "has been applied in probabilistic language modeling",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "theory",
                "has been applied in natural language processing",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Similar to the work of Barzilay and Lee  , who have applied paraphrase generation techniques to comparable corpora consisting of different newspaper articles about the same event, we are currently attempting to solve the data sparseness problem by extending our approach to non-parallel corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "comparable corpora consisting of different newspaper articles about the same event",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "approach",
                "extending our approach to non-parallel corpora",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction Recent work in learning semantics has focused on mapping sentences to meaning representations   given aligned sentence/meaning pairs as training data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "learning semantics",
                "focusing on",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "sentence/meaning pairs",
                "as training data",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.2.2 Features We used eight features   and their weights for the translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "eight features and their weights",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translations",
                "for the translations",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We could also use the value of semantic similarity and relatedness measures   or the existence of hypernym or hyponym relations as features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic similarity and relatedness measures",
                "as features",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "hypernym or hyponym relations",
                "as features",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": " , a trigram target language model, an order model, word count, phrase count, average phrase size functions, and whole-sentence IBM Model 1 logprobabilities in both directions  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "trigram target language model",
                "IBM Model 1 logprobabilities",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "average phrase size functions",
                "word count, phrase count",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "TheauthorsapplySO-PMI-IR  to extract and determine the polarity of adjectives.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SO-PMI-IR",
                "apply",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "adjectives",
                "extract and determine polarity of",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 To train their system, R&M used a 200k-word chunk of the Penn Treebank Parsed Wall Street Journal   tagged using a transformation-based tagger   and extracted base noun phrases from its parses by selecting noun phrases that contained no nested noun phrases and further processing the data with some heuristics   to flatten the recursive structure of the parse.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank Parsed Wall Street Journal",
                "tagged using a transformation-based tagger",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "heuristics",
                "to flatten the recursive structure of the parse",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To reduce the knowledge engineering burden on the user in constructing and porting an IE system, unsupervised learning has been utilized, e.g. Riloff  , Yangarber et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unsupervised learning",
                "has been utilized",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IE system",
                "constructing and porting",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Still, it is in our next plans and part of our future work to embed in our model some of the interesting WSD approaches, like knowledgebased  , corpus-based  , or combinations with very high accuracy  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSD approaches",
                "very high accuracy",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "model",
                "embed some WSD approaches",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, the word alignment computed by GIZA++ and used as a basis to extract the TTS templates in most SSMT systems has been observed to be a problem for SSMT  , due to the fact that the word-based alignment models are not aware of the syntactic structure of the sentences and could produce many syntax-violating word alignments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "computed word alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word-based alignment models",
                "produce many syntax-violating word alignments",
                "METHODOLOGY",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "olan   observed that sense division in MRD is frequently too free for the purpose of WSD",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sense division",
                "frequently too free",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "purpose of WSD",
                "purpose of",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The standard Minimum Error Rate training   was applied to tune the weights for all feature types.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature types",
                "tune the weights",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Minimum Error Rate training",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.3 Syntax based approach An alternative to the Window and Document-oriented approach is to use syntactical information  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "to use syntactical information",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntax",
                "based",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "To find these pairs automatically, wetrainedanon-sequentiallog-linearmodel that achieves a .902 accuracy  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Likelihood ratios are particularly useful when comparing common and rare events  , making them natural here given the rareness of most question categories and the frequency of contributions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rare events",
                "making them natural",
                "INNOVATION",
                "positive",
                0.7
            ],
            [
                "question categories",
                "rareness",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": ".1 Background Smith and Eisner   introduced the quasisynchronous grammar formalism",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "quasisynchronous grammar formalism",
                "introduced by Smith and Eisner",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "quasisynchronous grammar formalism",
                "formalism",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It consists of sections 15-18 of the Wall Street Journal part of the Penn Treebank II   as training data   and section 20 as test data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank II",
                "part of the Penn Treebank II",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "section 15-18",
                "as training data",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "section 20",
                "as test data",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  argue that these restrictions reduce our ability to model translation equivalence effectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "restrictions",
                "reduce our ability to model translation equivalence effectively",
                "METHODOLOGY",
                "negative",
                0.75
            ]
        ]
    },
    {
        "text": "Other corpus-based methods determine associations between words  , which yields a basis for computing thesauri, or dictionaries of terminological expressions and multiword lexemes  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus-based methods",
                "determine associations between words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "thesauri",
                "basis for computing",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": ".3 Systematic Sense Shift Ostler and Atkins   contend that there is strong evidence to suggest that a large part of word sense ambiguity is not arbitrary but follows regular patterns",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Systematic Sense Shift Ostler and Atkins",
                "strong evidence",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "word sense ambiguity",
                "follows regular patterns",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The basic engine used to perform the tagging in these experiments is a direct descendent of the maximum entropy   tagger of   which in turn is related to the taggers of   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger",
                "direct descendent of the maximum entropy tagger",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "taggers",
                "related to the taggers of and ",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Prototype-drive learning   specifies prior knowledge by providing a few prototypes   for each label.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prior knowledge",
                "providing a few prototypes",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "label",
                "each label",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " , and others identifying non-anaphoric definite descriptions  ] and unsupervised techniques  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unsupervised techniques",
                "unsupervised",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "non-anaphoric definite descriptions",
                "identifying",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A CHECK move requests the partner to confirm information that the speaker has some reason to believe, but is not entirely sure about \\ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "speaker",
                "has some reason to believe",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "information",
                "is not entirely sure about",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  and Lee  ) can be generally divided into three types: discounting  , class-based smoothing  , and distance-weighted averaging  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "discounting",
                "discounting",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "distance-weighted averaging",
                "distance-weighted averaging",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.1 Translation Model Form We first assume the general hypergraph setting of Huang and Chiang  , namely, that derivations under our translation model form a hypergraph.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "form a hypergraph",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "hypergraph setting",
                "general hypergraph setting of Huang and Chiang",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To deal with the difficulties in parse-to-parse matching, Wu   utilizes inversion transduction grammar   for bilingual parsing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wu",
                "utilizes inversion transduction grammar",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "bilingual parsing",
                "difficulties in parse-to-parse matching",
                "PERFORMANCE",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "The translation quality is evaluated by case-sensitive NIST   and BLEU  2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NIST",
                "case-sensitive",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU",
                "2",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "These fourteen scores are weighted and linearly combined  ; their respective weights are learned on development data so as to maximize the BLEU score.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "scores",
                "learned on development data",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "weights",
                "maximize the BLEU score",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based  , ontology-based  , information-based   or distributional  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "Various",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "distributional",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Model performance is evaluated using the standard BLEU metric   which measures average n-gram precision, n 4, and we use the NIST definition of the brevity penalty for multiple reference test sets.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metric",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "n-gram precision",
                "average",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  looked at Golomb Coding and Brants et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Golomb Coding",
                "described",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Brants et al",
                "mentioned",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The modified version of the Roark parser, trained on the Brown Corpus section of the Penn Treebank  , was used to parse the different narratives and produce the word by word measures.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Roark parser",
                "trained on the Brown Corpus section of the Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word by word measures",
                "produced by the modified version",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We used four different system summaries for each of the 6 meetings: one based on the MMR method in MEAD  , the other three are the system output from  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system summaries",
                "based on the MMR method in MEAD",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "system output",
                "from ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The left-to-right parser would likely improve if we were to use a left-corner transform  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "use a left-corner transform",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "improve",
                "likely",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "We tuned our system on the development set devtest2006 for the EuroParl tasks and on nc-test2007 for CzechEnglish, using minimum error-rate training   to optimise BLEU score.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "devtest2006",
                "EuroParl tasks",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "BLEU score",
                "to optimise",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "These parameters 1 8 are tuned by minimum error rate training   on the dev sets.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "tuned by minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dev sets",
                "on",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As an alternative to the often used sourcechannel approach  , we directly model the posterior probability Pr   .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sourcechannel approach",
                "often used",
                "APPLICABILITY",
                "neutral",
                0.5
            ],
            [
                "posterior probability",
                "Pr",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Although the parser is not yet complete, we expect that its breath of coverage of the language will be substantially larger than that of other Government-binding parsers recently reported in the literature  , Kuhns  , Sharp  , and Wehrli  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "breath of coverage of the language",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Government-binding parsers",
                "recently reported in the literature",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "To optimize the system towards a maximal BLEU or NIST score, we use Minimum Error Rate   Training as described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Minimum Error Rate Training",
                "described in ",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU or NIST score",
                "maximal",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This idea of employing n-gram co-occurrence statistics to score the output of a computer system against one or more desired reference outputs has its roots in the BLEU metric for machine translation   and the ROUGE   metric for summarization.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-gram co-occurrence statistics",
                "score the output of a computer system against one or more desired reference outputs",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "BLEU metric",
                "for machine translation",
                "INNOVATION",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "We use a bootstrap approach in which we first extract 1-to-n word alignments using an existing word aligner, and then estimate the confidence of those alignments to decide whether or not the n words have to be grouped; if so, this group is conwould thus be completely driven by the bilingual alignment process   for related considerations).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word aligner",
                "existing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "confidence",
                "estimate",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "421 Teufel and Moens Summarizing Scientific Articles We use the kappa coefficient K   to measure stability and reproducibility, following Carletta  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa coefficient",
                "to measure stability and reproducibility",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Carletta",
                "following",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "task  , and reported errors in the range of 26% are common.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "task",
                "reported errors in the range of 26%",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "errors",
                "common",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Computational approaches to prosodic modeling of DAs have aimed to automatically extract various prosodic parameters--such as duration, pitch, and energy patterns--from the speech signal  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prosodic parameters",
                "such as duration, pitch, and energy patterns",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "speech signal",
                "extracted automatically",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The feature weights are tuned by the modified Koehns MER   trainer.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "tuned by the modified Koehns MER trainer",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Koehns MER trainer",
                "modified",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Most of them rely on the concept of alignment: a mapping from words or groups of words in a sentence into words or groups in the other   the mapping goes from rules in a grammar for a language into rules of a grammar for the other language).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment",
                "a mapping from words or groups of words",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "grammar for a language",
                "into rules of a grammar for the other language",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "If the alignments are not available, they can be automatically generated; e.g., using GIZA++  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "can be automatically generated",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignments",
                "are not available",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "Translation performance is measured using the automatic BLEU   metric, on one reference translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metric",
                "automatic",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "translation",
                "one reference translation",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Incremental Sigmoid Belief Networks   differ from simple dynamic SBNs in that they allow the model structure to depend on the output variable values.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model structure",
                "depend on the output variable values",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Incremental Sigmoid Belief Networks",
                "differ from simple dynamic SBNs",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Generalized Forward Backward Reestimation Generalization of the Forward and Viterbi Algorithm In English part of speech taggers, the maximization of Equation   to get the most likely tag sequence, is accomplished by the Viterbi algorithm  , and the maximum likelihood estimates of the parameters of Equation   are obtained from untagged corpus by the ForwardBackward algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Viterbi algorithm",
                "accomplishes maximization",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "ForwardBackward algorithm",
                "obtains maximum likelihood estimates",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Based on these grammars, a great number of SMT models have been recently proposed, including string-to-string model    , tree-to-string model    , string-totree model    , tree-to-tree model     and so on.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT models",
                "great number of",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "SMT models",
                "have been recently proposed",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "uch an approach contrasts with the log-linear HMM/Model-4 combination proposed by Och and Ney  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMM/Model-4 combination",
                "proposed by Och and Ney",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "approach",
                "contrasts with",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Base noun phrases  , broadly the initial portions of non-recursive noun phrases up to the head  , are valuable pieces of linguistic structure which minimally extend beyond the scope of named entities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "noun phrases",
                "valuable pieces of linguistic structure",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "named entities",
                "minimally extend beyond the scope of",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  a. Please move your car Her sadness moves him b. John enjoys the book John enjoys reading the book e. The two alibis do not accord They accorded him a warm welcome d. John swam for hours John swam across the channel Although the precise nrechanisms which govern lexical knowledge are still largely unknown, there is strong evidence that word sense extensibi\\[ity is not arbitrary  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexical knowledge",
                "govern",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word sense extensibility",
                "is not arbitrary",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4.1 Evaluation of Different Features and Models In pilot experiments on a subset of the features, we provide a comparison of HM-SVM with other two learning models, maximum entropy   model   and SVM model  , to test the effectiveness of HMSVM on function labeling task, as well as the generality of our hypothesis on different learning 58 Table 3: Features used in each experiment round.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HM-SVM",
                "with other two learning models",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "HM-SVM",
                "on function labeling task",
                "PERFORMANCE",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "2 The WFST Reordering Model The Translation Template Model   is a generative model of phrase-based translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Translation Template Model",
                "is a generative model of phrase-based translation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "WFST Reordering Model",
                "The Translation Template Model",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To tune the decoder parameters, we conducted minimum error rate training   with respect to the word BLEU score   using 2.0K development sentence pairs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder parameters",
                "minimum error rate training with respect to the word BLEU score",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "development sentence pairs",
                "2.0K",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To simplify, the plausibility of a detected esl is roughly inversely proportional to the number of mutually excluding syntactic structures in the text segment that generated the esl   for details).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic structures",
                "mutually excluding",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "plausibility of a detected esl",
                "roughly inversely proportional",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction Early works,  , and to a certain extent  , presented methods to ex~.:'~.ct bi'_.'i~gua!",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "to a certain extent presented",
                "METHODOLOGY",
                "neutral",
                0.75
            ],
            [
                "works",
                "Early",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "We used the Maximum Entropy approach5   as a machine learner for this task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy approach",
                "as a machine learner",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "machine learner",
                "for this task",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Overview 2.1 The word segmentation problem As statistical machine translation systems basically rely on the notion of words through their lexicon models  , they are usually capable of outputting sentences already segmented into words when they translate into languages like Chinese or Japanese.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical machine translation systems",
                "basically rely on the notion of words through their lexicon models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "systems",
                "are usually capable of outputting sentences already segmented into words",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The second voting model is a maximum entropy model  , since Klein and Manning   found that this model yielded higher accuracy than naive Bayes in a subsequent comparison of WSD performance.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy model",
                "yields higher accuracy",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "naive Bayes",
                "lower accuracy",
                "PERFORMANCE",
                "negative",
                0.75
            ]
        ]
    },
    {
        "text": "The word-based edit distance heuristic yields pairs that are relatively clean but offer relatively minor rewrites in generation, especially when compared to the MSA model of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word-based edit distance heuristic",
                "relatively clean",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "pairs",
                "offer relatively minor rewrites",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For example, in our previous work  , we have used a statistical translation memory of phrases in conjunction with a statistical translation model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical translation memory",
                "used in conjunction with",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical translation model",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "n alternative training criterion therefore directly optimizes translation quality as measured by an automatic evaluation criterion  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "criterion",
                "directly optimizes translation quality",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "evaluation criterion",
                "measured by",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To determine the target distribution we classified 171   randomly selected utterances from the TownInfo data, that were used as a development set.2 In Table 1 we can see that 15.2 % of the trees in the artificial corpus will be NP NSUs.3 4 Data generation We constructed our artificial corpus from sections 2 to 21 of the Wall Street Journal   section of the Penn Treebank corpus   2We discarded very short utterances   since they dont need parsing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "utterances",
                "randomly selected",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "artificial corpus",
                "constructed from sections 2 to 21 of the Wall Street Journal section of the Penn Treebank corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A number of bootstrapping methods have been proposed for NLP tasks  , Collins and Singer  , Riloff and Jones  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bootstrapping methods",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "NLP tasks",
                "for",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The hierarchical translation operations introduced in these methods call for extensions to the traditional beam decoder  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "introduced in these methods",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "beam decoder",
                "traditional",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "6.1.2 ROUGE evaluation Table 4 presents ROUGE scores   of each of human-generated 250-word surveys against each other.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE scores",
                "of each of human-generated 250-word surveys",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "ROUGE scores",
                "against each other",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To address this, standard measures like precision and recall could be used, as in some previous research  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measures",
                "like precision and recall",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "previous research",
                "some",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "The models are trained using the Margin Infused Relaxed Algorithm or MIRA   instead of the standard minimum-error-rate training or MERT algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MIRA",
                "Margin Infused Relaxed Algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "MERT",
                "standard minimum-error-rate training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We compare our methods with both the averaged perceptron   and conditional random fields   using identical predicate sets.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "both the averaged perceptron and conditional random fields",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "predicate sets",
                "identical",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This program differs from earlier work in its almost complete lack of hand-crafting, relying instead on a very small corpus of Penn Wall Street Journal Tree-bank text   that has been marked with co-reference information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hand-crafting",
                "almost complete lack of",
                "METHODOLOGY",
                "negative",
                0.85
            ],
            [
                "corpus of Penn Wall Street Journal Tree-bank text",
                "has been marked with co-reference information",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "503 Bikel Intricacies of Collins Parsing Model Table 4 Overall parsing results using only details found in Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins Parsing Model",
                "details found in Collins",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Overall parsing results",
                "using only",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Feature weights of both systems are tuned on the same data set.3 For Pharaoh, we use the standard minimum error-rate training  ; and for our system, since there are only two independent features  , we use a simple grid-based line-optimization along the language-model weight axis.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Feature weights",
                "tuned on the same data set",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "language-model weight axis",
                "along",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use a hand-written competence grammar, combined with performance-driven disambiguation obtained from the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar",
                "hand-written competence grammar",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "disambiguation",
                "performance-driven disambiguation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 Margin Perceptron Algorithm for Sequence Labeling Weextendedaperceptronwithamargin  to sequence labeling in this study, as Collins   extended the perceptron algorithm to sequence labeling.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron algorithm",
                "extended to sequence labeling",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Margin Perceptron Algorithm",
                "3",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "distance   and the maximum swap segment size   ranging from 0 to 10 and evaluated the translations with the BLEU7 metric  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distance",
                "ranging from 0 to 10",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translations",
                "evaluated with the BLEU7 metric",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "108 To follow related work and to focus on the effects of the language model, we present translation resultsunderaninversiontransductiongrammar  translation model   trained on the Europarl corpus  , described in detail in Section 3, and using a trigram language model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "trained on Europarl corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "language model",
                "trigram",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This includes both the parsers that attach probabilities to parser moves  , but also those of the lexicalized PCFG variety  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsers",
                "attach probabilities to parser moves",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parsers",
                "lexicalized PCFG variety",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This formula follows the convention of   in letting so designate the null state.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "null state",
                "designate the null state",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "formula",
                "follows the convention",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However,   show that, in phrase-based translation, improvements in AER or f-measure do not necessarily correlate with improvements in BLEU score.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "AER or f-measure",
                "improvements in",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "BLEU score",
                "improvements in",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The disambiguation model of this parser is based on a maximum entropy model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy model",
                "is based on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Besides precision, recall and   F-measure, we also include an F-measure variant strongly biased towards recall  , which   found to be best to tune their LEAF aligner for maximum MT accuracy.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "F-measure variant",
                "strongly biased towards recall",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "LEAF aligner",
                "best to tune for maximum MT accuracy",
                "METHODOLOGY",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "2.1 EM parameter estimation We train using Expectation Maximisation  , optimising the log probability of the training setfe ,f gSs=1  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "EM parameter estimation",
                "optimising the log probability of the training set",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "log probability",
                "of the training set",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": " ), better language-specific preprocessing   and restructuring  , additional feature functions such as word class language models, and minimum error rate training   to optimize parameters.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature functions",
                "word class language models",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parameters",
                "optimize",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  presents an automatic approach for mapping between sense inventories; here similarities in gloss definition and structured relations between the two sense inventories are exploited in order to map between WordNet senses and distinctions made within the coarser-grained Oxford English Dictionary.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sense inventories",
                "similarities in gloss definition and structured relations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Oxford English Dictionary",
                "coarser-grained",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The tool set for TEA is constantly being extended, recent additions include a prototype symbolic classifier, shallow parser  , sentence segmentation algorithm   and a POS tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tool set",
                "being extended",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "prototype symbolic classifier",
                "shallow parser",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Since this transform takes a probabilistic grammar as input, it can also easily accommodate horizontal and vertical Markovisation   as described by Collins   and subsequently.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transform",
                "accommodate horizontal and vertical Markovisation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "grammar",
                "probabilistic",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction A number of wide-coverage TAG, CCG, LFG and HPSG grammars   have been extracted from the Penn Treebank  , and have enabled the creation of widecoverage parsers for English which recover local and non-local dependencies that approximate the underlying predicate-argument structure  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammars",
                "wide-coverage",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "parsers",
                "recover local and non-local dependencies",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Weights on the loglinear features are set using Och's algorithm   to maximize the system's BLEU score on a development corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights",
                "set using Och's algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "system's BLEU score",
                "maximize",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "arowsky   has proposed a bootstrapping method for word sense disambiguation",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bootstrapping method",
                "for word sense disambiguation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "method",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.4 Syntactic Similarity We have incorporated, with minor modifications, some of the syntactic metrics described by Liu and Gildea   and Amigo et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic metrics",
                "described by Liu and Gildea and Amigo et al.",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "metrics",
                "some of the",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The standard solution is to approximate the maximum probability translation using a single derivation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "derivation",
                "single",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Another current topic of machine translation is automatic evaluation of MT quality  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT quality",
                "evaluation",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "machine translation",
                "current topic",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For these words, we first used a POS tagger   to determine the correct POS.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tagger",
                "determine the correct POS",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "POS tagger",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4 Features For our experiments we use the features proposed, motivated and described in detail by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "proposed, motivated and described in detail",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "features",
                "proposed, motivated and described in detail",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "They roughly fall into three categories according to what is used for supervision in learning process:   using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus,  ,   exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora    ,   bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "thesaurus or lexicons",
                "to disambiguate word senses",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "bilingual corpora",
                "use of",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Inter-annotator agreement was measured using the kappa   statistics   on 1,502 instances   marked by two annotators who followed specific written guidelines.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotators",
                "followed specific written guidelines",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "kappa statistics",
                "used for measurement",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In addition to individual seed words, Kanayama and Nasukawa   used more complicated syntactic patterns that were manually created.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic patterns",
                "manually created",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "individual seed words",
                "more complicated",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Using the values computed above: p1 = k1n 1 p2 = k2n 2 p = k1+k2n 1+n2 Taking these probabilities to be binomially distributed, the log likelihood statistic   is given by: 2 log = 2  where, log L =k log p+  log  According to this statistic, the greater the value of 2 log for a particular pair of observed frame and verb, the more likely that frame is to be valid SF of the verb.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log likelihood statistic",
                "given by 2 log",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "frame",
                "to be valid SF of the verb",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "In fact, we found that it doesnt do so badly at all: the bitag HMM estimated by EM achieves a mean 1-to1 tagging accuracy of 40%, which is approximately the same as the 41.3% reported by   for their sophisticated MRF model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bitag HMM",
                "achieves a mean 1-to1 tagging accuracy of 40%",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "MRF model",
                "reported by... 41.3%",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We report precision, recall and balanced F-measure  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "precision, recall and balanced F-measure",
                "are reported",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Such studies follow the empiricist approach to word meaning summarized best in the famous dictum of the British 3 linguist J.R. Firth: You shall know a word by the company it keeps.   Context similarity has been used as a means of extracting collocations from corpora, e.g. by Church & Hanks   and by Dunning  , of identifying word senses, e.g. by Yarowski   and by Schutze  , of clustering verb classes, e.g. by Schulte im Walde  , and of inducing selectional restrictions of verbs, e.g. by Resnik  , by Abe & Li  , by Rooth et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Firth's dictum",
                "summarized best",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "Church & Hanks",
                "used as a means of extracting collocations",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation    ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bilingual word alignment",
                "is first introduced",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "statistical machine translation",
                "as an intermediate result",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our baseline is the phrase-based MT system of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based MT system",
                "of  ",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "MT system",
                "baseline",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Hyperparameter  is automatically selected from 2Although Kanayama and Nasukawa   that  for their dataset similar to ours was 0.83, this value cannot be directly compared with our value because their dataset includes both individual words and pairs of words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hyperparameter",
                "automatically selected",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Kanayama and Nasukawa's dataset",
                "includes both individual words and pairs of words",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "dependency lengths: Long-distance dependencies exhibit bad performance  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency lengths",
                "exhibit bad performance",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "dependencies",
                "exhibit",
                "PERFORMANCE",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "In   the WSJ PennTreebank corpus   is analyzed and a very detailed list of syntactic patterns that correspond to different roles of commas is created.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSJ PennTreebank corpus",
                "analyzed",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "list of syntactic patterns",
                "very detailed",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The automatic assessment of the translation quality has been carried out using the BiLingual Evaluation Understudy    , and the Translation Error Rate    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BiLingual Evaluation Understudy",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Translation Error Rate",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Using this alignment strategy, we follow   and compute one alignment for each translation direction  , and then combine them.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment strategy",
                "compute one alignment for each translation direction",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "alignment",
                "combine them",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2005 Association for Computational Linguistics Automatic identification of sentiment vocabulary: exploiting low association with known sentiment terms Michael Gamon Anthony Aue Natural Language Processing Group Natural Language Processing Group Microsoft Research Microsoft Research mgamon@microsoft.com anthaue@microsoft.com Abstract We describe an extension to the technique for the automatic identification and labeling of sentiment terms described in Turney   and Turney and Littman  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentiment terms",
                "known sentiment terms",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "technique",
                "automatic identification and labeling",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There has been recent work on discovering allomorphic phenomena automatically  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "allomorphic phenomena",
                "automatically",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "work",
                "recent",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As expected, we see that MST does better than Malt for all categories except nouns and pronouns  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MST",
                "does better",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "Malt",
                "does better",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "For the combined set  , we also show the 95% BLEU confidence interval computed using bootstrap resampling  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU confidence interval",
                "computed using bootstrap resampling",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "95%",
                "confidence interval",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "7For details about the Bleu evaluation metric, see  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bleu evaluation metric",
                "details about",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "evaluation metric",
                "see",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Lexical Weighting:   the lexical weight a27 a14a12a91 a29 a92a93a21 of the block a9 a72 a14a12a91 a19a86a92a93a21 is computed similarly to  , details are given in Section 3.4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexical weight",
                "computed similarly",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "details",
                "given in Section 3.4",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A hierarchical alignment algorithm is a type of synchronous parser where, instead of constraining inferences by the production rules of a grammar, the constraints come from word alignments and possibly other sources  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "constraints come from word alignments and possibly other sources",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parser",
                "type of synchronous parser",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level  , instead of aiming at dictionary annotation as we do.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "mostly used",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "dictionary annotation",
                "do",
                "APPLICABILITY",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "In comparison with shallow semantic analysis tasks, such as wordsense disambiguation   and semantic role labeling  , which only partially tackle this problem by identifying the meanings of target words or finding semantic roles of predicates, semantic parsing   pursues a more ambitious goal  mapping natural language sentences to complete formal meaning representations  , where the meaning of each part of a sentence is analyzed, including noun phrases, verb phrases, negation, quantifiers and so on.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic analysis tasks",
                "only partially tackle",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "semantic parsing",
                "pursues a more ambitious goal",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "In addition to adapting the idea of Head Word Chains  , we also compared the input sentences argument structures against the treebank for certain syntactic categories.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Head Word Chains",
                "idea of",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "treebank",
                "certain syntactic categories",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "nother possible comparison could be with a version of Turney's   sentiment classification method applied to Chinese",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Turney's sentiment classification method",
                "applied to Chinese",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "version",
                "sentiment classification method",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Work on learning with hidden variables can be used for both CRFs   and for inference based learning algorithms like those used in this work  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "learning with hidden variables",
                "used for both CRFs and for inference",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithms",
                "like those used in this work",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Research on the automatic classification of movie or product reviews as positive or negative  ) is perhaps the most similar to our work.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classification",
                "automatic classification",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "work",
                "most similar",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in Collins  , using the same data splits, and a larger error reduction of 12.1% from the more similar best previous loglinear model in Toutanova and Manning  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "best",
                "PERFORMANCE",
                "positive",
                0.9
            ],
            [
                "previous loglinear model",
                "more similar best",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "  study the shortest hyperpath problem and Nielsen et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Nielsen et al",
                "Nielsen et al",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "shortest hyperpath problem",
                "study",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our chunks and functions are based on the annotations in the third release of the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "third release",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "annotations",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our approach was to identify a parallel corpus of manually and automatically transcribed documents, the TDT2 corpus, and then use a statistical approach   to identify tokens with significantly Table 5: Impact of recall and precision enhancing devices.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TDT2 corpus",
                "manually and automatically transcribed documents",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "statistical approach",
                "to identify tokens with significantly",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Moore and Quirk   share the goal underlying our own research: improving, rather than replacing, Ochs MERT procedure.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ochs MERT procedure",
                "improving",
                "PERFORMANCE",
                "neutral",
                0.5
            ],
            [
                "Ochs MERT procedure",
                "replacing",
                "PERFORMANCE",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "MXPOST  , and in order to discover more general patterns, we map the tag set down after tagging, e.g. NN, NNP, NNPS and NNS all map to NN.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tag set",
                "map to NN",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "patterns",
                "discover more general",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Charniak   and Johnson   annotated each node with its parent and grandparent nonterminals, to more precisely reflect its outside context.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "node",
                "precisely reflect its outside context",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "nonterminals",
                "more precisely",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Text chunking has been one of the most interesting problems in natural language learning community since the first work of   using a machine learning method.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "natural language learning community",
                "one of the most interesting problems",
                "APPLICABILITY",
                "neutral",
                0.7
            ],
            [
                "machine learning method",
                "using",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The sentences included in the gold standard were chosen at random from the BNC, subject to the condition that they contain a verb which does not occur in the training sections of the WSJ section of the PTB  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BNC",
                "randomly chosen",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "training sections of the WSJ section of the PTB",
                "does not occur",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As the most concise definition we take the first sentence of each article, following  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "definition",
                "first sentence of each article",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentence",
                "following",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "All our experiments used the standard BIO encoding   with different feature sets and learning procedures.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BIO encoding",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "feature sets and learning procedures",
                "different",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.1 Alignment Sentences from different systems are aligned in pairs using a modified version of the METEOR   matcher.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "METEOR matcher",
                "modified version",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignment sentences",
                "aligned in pairs",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Current methods for large-scale information extraction take advantage of unstructured text available from either Web documents   or, more recently, logs of Web search queries   to acquire useful knowledge with minimal supervision.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "take advantage of unstructured text",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "knowledge",
                "acquire with minimal supervision",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Thus we rank each sense wsi WSw using Prevalence Score wsi =    njNw dssnj  wnss  wsiWSw wnss  where the WordNet similarity score   is defined as: wnss = max nsxNSnj  ) 2.2 Building the Thesaurus The thesaurus was acquired using the method described by Lin  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Prevalence Score",
                "defined as",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "WordNet similarity score",
                "max nsxNSnj  ) 2.2",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "madja  finds significant bigrams using an estimate of z-score  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "madja",
                "significant bigrams",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "estimate of z-score",
                "estimate",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Second, the automatic approach, in which the model is automatically obtained from corpora   1, and consists of n-grams  , rules   or neural nets  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "automatically obtained from corpora",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "n-grams, rules, or neural nets",
                "consists of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Kappa is a better measurement of agreement than raw percentage agreement   because it factors out the level of agreement which would be reached by random annotators using the same distribution of categories as the real coders.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa",
                "is a better measurement of agreement than raw percentage agreement",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "random annotators",
                "factors out the level of agreement which would be reached",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As previously observed in the literature  , such components include a clause in the clause conjunction, relative clauses, and some elements within a clause  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "components",
                "such as a clause in the clause conjunction, relative clauses, and some elements within a clause",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "elements",
                "some elements within a clause",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "1 Introduction on measures for inter-rater reliability  , on frameworks for evaluating spoken dialogue agents   and on the use of different corpora in the development of a particular system (The Carnegie-Mellon Communicator, Eskenazi et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measures",
                "for inter-rater reliability",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "frameworks",
                "for evaluating spoken dialogue agents",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In the field of eomputationa.1 linguistics, mutual information \\  are suggested.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information",
                "are suggested",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "eomputationa",
                "linguistics",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use the Penn Treebank Wall Street Journal corpus as the large corpus and individual sections of the Brown corpus as the target corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank Wall Street Journal corpus",
                "large corpus",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Brown corpus",
                "target corpora",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A quite different approach from our hypotheses testing implemented in the TREQ-AL aligner is taken by the model-estimating aligners, most of them relying on the IBM models   described in the   seminal paper.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model-estimating aligners",
                "relying on IBM models",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "TREQ-AL aligner",
                "implemented in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Hypotheses for unknown words, both stochastic  , and connectionist   have been applied to unlimited vocabulary taggers.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hypotheses",
                "stochastic and connectionist",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "taggers",
                "unlimited vocabulary",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "For instance, work has been done in Chinese using the Penn Chinese Treebank  , in Czech using the Prague Dependency Treebank  , in French using the French Treebank  , in German using the Negra Treebank  , and in Spanish using the UAM Spanish Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Treebank",
                "Penn Chinese",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Treebank",
                "Prague Dependency",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Motivation Question Answering has emerged as a key area in natural language processing   to apply question parsing, information extraction, summarization, and language generation techniques  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Question Answering",
                "key area",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "question parsing, information extraction, summarization, and language generation techniques",
                "techniques",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The supervised methods are based on Maximum Entropy    , neural network using the Learning Vector Quantization algorithm   and Specialized Hidden Markov Models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy",
                "using the Learning Vector Quantization algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "neural network",
                "Specialized Hidden Markov Models",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "amshaw and Marcus   approached chunking by using a machine learning method",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning method",
                "using a machine learning method",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "chunking",
                "approached chunking",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Statistical machine translation is based on the noisy channel model, where the translation hypothesis is searched over the space defined by a translation model and a target language  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation hypothesis",
                "searched over the space defined by a translation model and a target language",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translation model",
                "defined by",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Logics for the IBM Models   would be similar to our logics for phrase-based models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "logics",
                "similar",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "phrase-based models",
                "our",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "1 Introduction Given a source-language   sentence f, the problem of machine translation is to automatically produce a target-language   translation e. The mathematics of the problem were formalized by  , and re-formulated by   in terms of the optimization e = arg maxe Msummationdisplay m=1 mhm    where fhm g is a set of M feature functions and fmg a set of weights.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature functions",
                "set of M feature functions",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "optimization",
                "arg maxe Msummationdisplay m=1 mhm",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "While the BBN model does not perform at the level of Model 2 of   on Wall Street Journal text, it is also less language-dependent, eschewing the distance metric   in favor of the \"\"bigrams on nonterminals\"\" model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BBN model",
                "does not perform at the level of Model 2",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "bigrams on nonterminals",
                "eschewing the distance metric",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Given a set of features and a training corpus, the ME estimation process produces a model in which every feature fi has a weight i. From  , we can compute the conditional probability as: p  = 1Z productdisplay i fi i   Z  =summationdisplay o productdisplay i fi i   The probability is given by multiplying the weights of active features   = 1).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "has a weight i",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probability",
                "multiplying the weights of active features",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "e solve SAT analogies with a simplified version of the method of Turney  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method of Turney",
                "simplified version",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "method of Turney",
                "simplified",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Och   claimed that this approximation achieved essentially equivalent performance to that obtained when directly using the loss as the objective, O = lscript.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approximation",
                "essentially equivalent performance",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "loss",
                "objective",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Hindle uses the observed frequencies within a specific syntactic pattern   to derive a cooccu,> rence score which is an estimate of mutual information  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic pattern",
                "to derive a cooccurrence score",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "cooccurrence score",
                "an estimate of mutual information",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Most statistical parsing research, such as Collins  , has centered on training probabilistic context-free grammars using the Penn Treebank.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins",
                "centered on training probabilistic context-free grammars",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Penn Treebank",
                "used for training",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The model parameters are trained using minimum error-rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model parameters",
                "minimum error-rate training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "minimum error-rate training",
                "trained using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Table 2: Corpora and Modalities CORPUS MODALITY ACE asserted, or other TIMEML must, may, should, would, or could Prasad et al., 2006 assertion, belief, facts or eventualities Saur et al., 2007 certain, probable, possible, or other Inui et al., 2008 affirm, infer, doubt, hear, intend, ask, recommend, hypothesize, or other THIS STUDY S/O, necessity, hope, possible, recommend, intend   Table 3: Markup Scheme   Tag Definition   R Remedy, Medical operation   T Medical test, Medical examination   D Deasese, Symptom   M Medication, administration of a drug   A patient action   V Other verb     2 Related Works 2.1 Previous Markup Schemes In the NLP field, fact identification has not been studied well to date.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Corpora and Modalities",
                "asserted, or other",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "fact identification",
                "has not been studied well to date",
                "INNOVATION",
                "negative",
                0.9
            ]
        ]
    },
    {
        "text": "Differences in behavior of WSD systems when applied to lexical-sample and all-words datasets have been observed on previous Senseval and Semeval competitions  : supervised systems attain results on the high 80s and beat the most frequent baseline by a large margin for lexical-sample datasets, but results on the all-words datasets were much more modest, on the low 70s, and a few points above the most frequent baseline.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSD systems",
                "results on high 80s",
                "PERFORMANCE",
                "positive",
                0.9
            ],
            [
                "results on all-words datasets",
                "on the low 70s",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Unfortunately, a counterexample illustrated in   shows that the max function does not produce valid kernels in general.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "max function",
                "does not produce valid kernels",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "kernels",
                "in general",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Previous SMT systems   used a word-based translation model which assumes that a sentence can be translated into other languages by translating each word into one or more words in the target language.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word-based translation model",
                "assumes that a sentence can be translated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation model",
                "into one or more words in the target language",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this paper we report case-insensitive Bleu scores  , unless otherwise stated, calculated with the NIST tool, and caseinsensitive Meteor-ranking scores, without WordNet  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NIST tool",
                "used to calculate Bleu scores",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Meteor-ranking scores",
                "without WordNet",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These lists are rescored with the following models:   the different models used in the decoder which are described above,   two different features based on IBM Model 1  ,   posterior probabilities for words, phrases, n-grams, and sentence length  , all calculated over the Nbest list and using the sentence probabilities which the baseline system assigns to the translation hypotheses.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "different models used in the decoder",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "features",
                "two different features based on IBM Model 1",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In order to be able to compare the edit distance with the other metrics, we have used the following formula whichnormalisesthe minimum edit distance by the length of the longest questionand transformsit into a similaritymetric: normalisededitdistance = 1 edit dist max  Word Ngram Overlap This metric compares the word n-gramsin both questions: ngramoverlap = 1N Nsummationdisplay n=1 | Gn   Gn  | min  |,| Gn  |) where Gn  is the set of n-grams of length n in question q and N usually equals 4  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "normalisededitdistance",
                "normalises the minimum edit distance by the length of the longest question and transforms it into a similarity metric",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ngramoverlap",
                "compares the word n-grams in both questions",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For that purpose, syntactical  , statistical   and hybrid syntaxicostatistical methodologies   have been proposed.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methodologies",
                "have been proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Transformation-based error-driven learning has been applied to a number of natural language problems, including part of speech tagging, prepositional phrase attachment disambiguation, speech generation and syntactic parsing \\ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "error-driven learning",
                "applied to a number of natural language problems",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "natural language problems",
                "including part of speech tagging, prepositional phrase attachment disambiguation, speech generation and syntactic parsing",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Inside/Outside This representation was first introduced in  , and has been applied for base NP chunking.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "representation",
                "first introduced",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "NP chunking",
                "applied for",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  provide four sets of annotation principles, one for non-coordinate configurations, one for coordinate configurations, one for traces   and a final catch all and clean up phase.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotation principles",
                "four sets of",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "catch all and clean up phase",
                "final",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3 A Categorization of Block Styles In  , multi-word cepts   are discussed and the authors state that when a target sequence is sufficiently different from a word by word translation, only then should the target sequence should be promoted to a cept.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "multi-word cepts",
                "are discussed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "target sequence",
                "should be promoted",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We measured stability   and reproducibility  , using the Kappa coefficient K  , which controls agreement P  for chance agreement P : K = PA)-P  1-P  Kappa is 0 for if agreement is only as would be expected by chance annotation following the same distribution as the observed distribution, and 1 for perfect agreement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "For all performance metrics, we show the 70% confidence interval with respect to the MAP baseline computed using bootstrap resampling  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MAP baseline",
                "computed using bootstrap resampling",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "confidence interval",
                "with respect to",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Thus, we are focusing on Inversion Transduction Grammars   which are an important subclass of SCFG.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Inversion Transduction Grammars",
                "are an important subclass of SCFG",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "SCFG",
                "subclass of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "From this LFG annotated treebank, large-scale unification grammar resources were automatically extracted and used in parsing   and generation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LFG annotated treebank",
                "large-scale",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "unification grammar resources",
                "used in parsing and generation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 proposed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "base forms and synonyms of",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "metric",
                "correlate better to human judgements",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For the future, the joint model would benefit from lexical weighting like that used in the standard model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "lexical weighting",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "standard model",
                "used in",
                "METHODOLOGY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "Work in this area includes that of Lin and Hovy   and Pastra and Saggion  , both of whom inspect the use of Bleu-like metrics   in summarization.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bleu-like metrics",
                "inspect the use of",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "summarization",
                "in summarization",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "When updating model parameters, we employ a memorizationvariant of a local updating strategy   in which parameters are optimized toward a set of good translations found in the k-best list across iterations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model parameters",
                "optimized toward a set of good translations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "local updating strategy",
                "memorizationvariant",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Given the parallel corpus, we tagged the English words with a publicly available maximum entropy tagger  , and we used an implementation of the IBM translation model   to align the words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy tagger",
                "publicly available",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IBM translation model",
                "implementation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Related Work Sentiment Classi cation Traditionally, categorization of opinion texts has been cast as a binary classication task  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Sentiment Classi cation",
                "binary classication task",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Traditionally",
                "cast as",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Appendix B gives a sketch of one such approach, which is based on results from Collins, Schapire, and Singer  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins, Schapire, and Singer",
                "results",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "approach",
                "based on results",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To optimize the parameters of the decoder, we performed minimum error rate training on IWSLT04 optimizing for the IBM-BLEU metric  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "IBM-BLEU metric",
                "optimizing for",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Using these patterns, we introduced verb form errors into AQUAINT, then re-parsed the corpus  , and compiled the changes in the disturbed trees into a catalog.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "AQUAINT",
                "verb form errors were introduced",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "corpus",
                "was re-parsed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As pointed out by Johnson  , in effect this expression adds to c a small value that asymptotically approaches  0.5 as c approaches , and 0 as c approaches 0.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "c",
                "approaches 0.5",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "c",
                "approaches 0",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "number of words in target string These statistics are combined into a log-linear model whose parameters are adjusted by minimum error rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "number of words in target string",
                "combined into a log-linear model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parameters",
                "adjusted by minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Systems which are able to acquire a small number of verbal subcategorisation classes automatically from corpus text have been described by Brent  , and Ushioda et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Systems",
                "acquire a small number of verbal subcategorisation classes",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "subcategorisation classes",
                "automatically from corpus text",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  described the use of a biased PageRank over the WordNet graph to compute word pair semantic relatedness using the divergence of the probability values over the graph created by each word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PageRank",
                "biased",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probability values",
                "divergence",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  So far, none of the studies in sentiment detection   or opinion extraction   have specifically looked at the role of superlatives in these areas.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "studies",
                "specifically looked",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "superlatives",
                "in these areas",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "All model weights were trained on development sets via minimum-error rate training     with 200 unique n-best lists and optimizing toward BLEU.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model weights",
                "trained on development sets",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "training process",
                "optimizing toward BLEU",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Church and Hanks 1990; Klavans, Chodorow, and Wacholder 1990; Wilks et al. 1993; Smadja 1991a, 1991b; Calzolari and Bindi 1990).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Church and Hanks 1990",
                "Church and Hanks' work",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Church and Hanks 1990",
                "Church and Hanks' methodology",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Essentially, we follow Hobbs   in using a rich ontology and a representation scheme that makes explicit all the individuals and abstract objects     involved in the LF interpretation of an utterance.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ontology",
                "makes explicit all the individuals and abstract objects",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "representation scheme",
                "makes explicit all the individuals and abstract objects",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kernels",
                "over parse trees",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "work",
                "focussed on",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Related Work This method is similar to block-orientation modeling   and maximum entropy based phrase reordering model  , in which local orientations   of phrase pairs   are learned via MaxEnt classifiers.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "similar to block-orientation modeling and maximum entropy based phrase reordering model",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "local orientations of phrase pairs",
                "learned via MaxEnt classifiers",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Carletta   also states that in the behavioral sciences, K > .8 signals good replicability, and .67 < K < .8 allows tentative conclusions to be drawn.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "K",
                "signals good replicability",
                "PERFORMANCE",
                "positive",
                0.7
            ],
            [
                "K",
                "allows tentative conclusions to be drawn",
                "PERFORMANCE",
                "positive",
                0.6
            ]
        ]
    },
    {
        "text": "We see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms   do not attempt to parse an entire sentence and operate only in the local window of two to three tokens.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagging paradigms",
                "do not attempt to parse an entire sentence and operate only in the local window",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "text spans",
                "should necessarily be sentences",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We extract a phrase table using the Moses pipeline, based on Model 4 word alignments generated from GIZA++  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Moses pipeline",
                "based on Model 4 word alignments generated from GIZA++",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "GIZA++",
                "generated from",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There are many research directions, e.g., sentiment classification    , subjectivity classification    , feature/topic-based sentiment analysis   (Hu and Liu 2004; Popescu and Etzioni, 2005; Carenini et al., 2005; Ku et al., 2006; Kobayashi, Inui and Matsumoto, 2007; Titov and McDonald.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research directions",
                "e.g., sentiment classification, subjectivity classification, feature/topic-based sentiment analysis",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "sentiment classification",
                "Hu and Liu 2004",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques, e.g.,  , constraint-based techniques   and transformation-based techniques  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques",
                "statistical, constraint-based, transformation-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "various",
                "using various techniques",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "OHara and Wiebe   also make use of high level features, in their case the Penn Treebank   and FrameNet   to classify prepositions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "high level features",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "FrameNet",
                "high level features",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The features used by the POS tagger, some of which are different to those from Collins   and are specific to Chinese, are shown in Table 2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tagger",
                "features",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "features",
                "are different to those from Collins",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "One of the first large scale hand tagging efforts is reported in  , where a subset of the Brown corpus was tagged with WordNet July 2002, pp.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brown corpus",
                "subset of",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "WordNet",
                "July 2002",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "While recent proposals for evaluation of MT systems have involved multi-parallel corpora  , statistical MT algorithms typically only use one-parallel data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "multi-parallel",
                "INNOVATION",
                "positive",
                0.7
            ],
            [
                "algorithms",
                "use one-parallel data",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Lexical relationships under the standard IBM models   do not account for many-to-many mappings, and phrase extraction relies heavily on the accuracy of the IBM word-toword alignment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "standard IBM models",
                "do not account for many-to-many mappings",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase extraction",
                "relies heavily on the accuracy of the IBM word-to-word alignment",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 15  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Models 15",
                "is reminiscent of",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "our approach",
                "let anything align to anything",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 Hebrew Simple NP Chunks The standard definition of English base-NPs is any noun phrase that does not contain another noun phrase, with possessives treated as a special case, viewing the possessive marker as the first word of a new base-NP  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "definition",
                "does not contain another noun phrase",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "possessive marker",
                "as the first word of a new base-NP",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "e used the Ramshaw and Marcus   representation as well  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ramshaw and Marcus representation",
                "representation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Ramshaw and Marcus representation",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this paper we apply perceptron trained HMMs originally proposed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMMs",
                "originally proposed",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "perceptron trained",
                "trained",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This was done for supervised parsing in different ways by Collins  , Klein and Manning  , and McDonald et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins",
                "supervised parsing",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "McDonald et al",
                "supervised parsing",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Taking SIGHAN Bakeoff 2006   as an example, the recall is lower about 5% than the precision for each submitted system on MSRA and CityU closed track.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "lower about 5% than the precision",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "recall",
                "lower",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The set of such ITG alignments,AITG, are a strict subset of A1-1  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG alignments",
                "are a strict subset of A1-1",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "set of ITG alignments",
                "are a strict subset of",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Recently, in the area of parsers based oll a. stochastic context-fi:ee grammar  , some researchers have pointed out the importance of t.he lexicon and proposed lexiealized models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsers",
                "based on a stochastic context-free grammar",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "lexicon",
                "proposed lexiealized models",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "More details on these standard criteria can be found for instance in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "standard criteria",
                "found in ",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "criteria",
                "standard",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Collins   adapted the perceptron learning algorithm to tagging tasks, via sentence-based global feedback.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron learning algorithm",
                "adapted to tagging tasks",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentence-based global feedback",
                "used for",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Such techniques include Gibbs sampling  , a general-purpose Monte Carlo method, and integer linear programming  ,  , a general-purpose exact framework for NP-complete problems.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Gibbs sampling",
                "general-purpose Monte Carlo method",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "integer linear programming",
                "general-purpose exact framework for NP-complete problems",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Translation Models A translation model can be constructed automatically from texts that exist in two languages    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "can be constructed automatically",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "texts that exist in two languages",
                "exist",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 Conclusions and Future Work The results of the evaluation are exlremely encouraging, especially considering that disambiguating word senses to the level of fine-grainedness found in WordNet is quite a bit more difficult than disambiguation to the level of homographs  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "results",
                "exremely encouraging",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "disambiguating word senses",
                "quite a bit more difficult",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.3 Methods We parsed the English side of each bilingual bitext and both sides of each English/English bitext using an off-the-shelf syntactic parser  , which was trained on sections 02-21 of the Penn English Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "off-the-shelf syntactic parser",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn English Treebank",
                "sections 02-21",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "7An alternative framework that formally describes some dependency parsers is that of transition systems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transition systems",
                "formally describes",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "dependency parsers",
                "some",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Weischedel's group   examines unknown words in the context of part-of-speech tagging.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Weischedel's group",
                "examines unknown words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "part-of-speech tagging",
                "in the context of",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Wu   demonstrates the case of binary SCFG parsing, where six string boundary variables, three for each language as in monolingual CFG parsing, interact with each other, yielding an O  dynamic programming algorithm, where N is the string length, assuming the two paired strings are comparable in length.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "string boundary variables",
                "interact with each other",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "O dynamic programming",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In our approach, equation   is further normalized so that the probability for different lengths of F is comparable at the word level: m m j n i ijm eft l EFP /1 10 )| 1 |  The alignment models described in   are all based on the notion that an alignment aligns each source word to exactly one target word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probability for different lengths of F",
                "comparable at the word level",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "alignment models",
                "based on the notion that an alignment aligns each source word to exactly one target word",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Let us now compare our results to those obtained using shallow parsing, as previously done by Grefenstette  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "shallow parsing",
                "previously done",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "results",
                "obtained using shallow parsing",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "a176 Base NP standard data set   This data set was first introduced by  , and taken as the standard data set for baseNP identification task2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data set",
                "standard data set",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "standard data set",
                "taken as the standard data set",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We also tested other automatic methods: content-based evaluation, BLEU   and ROUGE-1  , and compared their results with that of evaluation by revision as reference.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "automatic methods",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "evaluation",
                "compared with that of evaluation by revision as reference",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For our POS tagging experiments, we used the Wall Street Journal in PTB III   with the same data split as used in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal",
                "in PTB III",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "data split",
                "same as used in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We measure this association using pointwise Mutual Information    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pointwise Mutual Information",
                "using pointwise Mutual Information",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "It is worth noting, however, that even in Turney   the choice of seed words is explicitly motivated by domain properties of movie reviews.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "seed words",
                "explicitly motivated by domain properties of movie reviews",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "choice of seed words",
                "motivated by",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "On the other end of the spectrum, character-based bitext mapping algorithms   are limited to language pairs where cognates are common; in addition, they may easily be misled by superficial differences in formatting and page layout and must sacrifice precision to be computationally tractable.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "character-based bitext mapping algorithms",
                "limited to language pairs where cognates are common",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "they",
                "may easily be misled by superficial differences in formatting and page layout",
                "METHODOLOGY",
                "negative",
                0.75
            ]
        ]
    },
    {
        "text": "1 Introduction In the part-of-speech hterature, whether taggers are based on a rule-based approach  ,  ,  , or on a statistical one  ,  ,  ,  ,  ,  , there is a debate as to whether more attention should be paid to lexical probabilities rather than contextual ones.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "taggers",
                "based on a rule-based approach or on a statistical one",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "attention",
                "paid to lexical probabilities rather than contextual ones",
                "APPLICABILITY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "As in  , confusion networks built around all skeletons are joined into a lattice which is expanded and rescored with language models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "language models",
                "rescored with",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "skeletons",
                "built around",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As a measure of association, we use the loglikelihood-ratio statistic recommended by Dunning  , which is the same statistic used by Melamed to initialize his models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "loglikelihood-ratio statistic",
                "recommended by Dunning",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "statistic used by Melamed",
                "initialize his models",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The underlying translation model is Model 2 from  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 2",
                "from",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Model 2",
                "from",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "unning 1993) or else   eschew significance testing in favor of a generic information-theoretic approach",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "significance testing",
                "eschew",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "approach",
                "information-theoretic",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In NLP, vector space models have featured most prominently in information retrieval  , but have also been used for ontology learning   and word sense-related tasks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "vector space models",
                "featured most prominently",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word sense-related tasks",
                "used for",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "6 Coding reliability The reliability of the annotation was evaluated using the kappa statistic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotation",
                "evaluated using the kappa statistic",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "kappa statistic",
                "used",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Automatically creating or extending taxonomies for specific domains is then a very interesting area of research  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "very interesting",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "area of research",
                "interesting",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "SMT has evolved from the original word-based approach   into phrase-based approaches   and syntax-based approaches  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "word-based",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approach",
                "phrase-based",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "294 Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation 2.2 Measuring Translation Performance Changes Caused By Alignment In phrased-based SMT   the knowledge sources which vary with the word alignment are the phrase translation lexicon   and some of the word level translation parameters  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase translation lexicon",
                "phrase translation lexicon",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word level translation parameters",
                "some of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Previous Work Other researchers have investigated the topic of automatic generation of abstracts, but the focus has been different, e.g., sentence extraction  , processing of structured templates  , sentence compression  , and generation of abstracts from multiple sources  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "researchers",
                "investigated",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "generation of abstracts",
                "different",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this vein, the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies   ) and semantic dependencies   c2008.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CoNLL 2008 shared task",
                "sets the challenge",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "jointly learning syntactic and semantic dependencies",
                "the challenge",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "is a WordNet based relatedness measure  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet",
                "based",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "relatedness measure",
                "is",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Under certain precise conditions, as described in  , we can analyze Algorithm 1 as minimizing the entropy of the distribution over translations of U. However, this is true only when the functions Estimate, Score and Select have very prescribed definitions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Algorithm 1",
                "minimizing the entropy of the distribution over translations of U",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "functions Estimate, Score and Select",
                "have very prescribed definitions",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The parameters for each phrase table were tuned separately using minimum error rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase table",
                "minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parameters",
                "tuned separately",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As resolving direct anaphoric descriptions   is a much simpler problem with high performance rates as shown in previous results  , these heuristics should be applied first in a system that resolves definite descriptions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "heuristics",
                "should be applied first",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "performance rates",
                "high",
                "PERFORMANCE",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "Only recently the issue has drawn attention:   present an initial analysis of the factors that influence system performance in content selection.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "issue",
                "has drawn attention",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "factors",
                "influence system performance",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Skipchain CRF model is applied for entity extraction and meeting summarization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Skipchain CRF model",
                "is applied",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "entity extraction and meeting summarization",
                "for",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "To have a fair comparison, for PR, we estimate the conditional probability of a relation given the evidence P , as in  , by using the same set of features as in ME. Table 3 shows precision, recall, and F1measure of each system for WordNet hypernyms  , WordNet meronyms   and ODP hypernyms  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "P",
                "conditional probability",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "features",
                "same set of features",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "There are other approaches in which the generation grammars are extracted semiautomatically   or automatically  , LFG   and CCG  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generation grammars",
                "extracted semiautomatically or automatically",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "LFG and CCG",
                "approaches",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In a test set containing 26 repairs Dowding et al. 1993, they obtained a detection recall rate of 42% with a precision of 85%, and a correction recall rate of 31% with a precision of 62%.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "detection recall rate",
                "42%",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "precision",
                "85%",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "During training each example is broken into elementary trees using head rules and argument/adjunct rules similar to those of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "example",
                "broken into elementary trees using head rules and argument/adjunct rules",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "rules",
                "similar to those of ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "By core phrases, we mean the kind of nonrecursive simplifications of the NP and VP that in the literature go by names such as noun/verb groups   or chunks, and base NPs  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrases",
                "nonrecursive simplifications of the NP and VP",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "NP and VP",
                "noun/verb groups or chunks",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For determining whether an opinion sentence is positive or negative, we have used seed words similar to those produced by   and extended them to construct a much larger set of semantically oriented words with a method similar to that proposed by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "seed words",
                "similar to those produced",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "method",
                "similar to that proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For English, we used the Penn Treebank   in our experiments and the tool Penn2Malt7 to convert the data into dependency structures using a standard set of head rules  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "used in our experiments",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "standard set of head rules",
                "used to convert data",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  and   claim that fine-grained semantic distinctions are unlikely to be of practical value for many applications.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic distinctions",
                "unlikely to be of practical value",
                "INNOVATION",
                "negative",
                0.75
            ],
            [
                "practical value",
                "for many applications",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Many methods have been proposed to deal with this problem, including supervised learning algorithms  , semi-supervised learning algorithms  , and unsupervised learning algorithms  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "supervised learning algorithms",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "methods",
                "semi-supervised learning algorithms",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "methods",
                "unsupervised learning algorithms",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Recently Bean and Riloff   have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic patterns",
                "used as contextual information",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "techniques adapted from information extraction",
                "adapted",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We measure translation performance by the BLEU score   and Translation Error Rate     with one reference for each hypothesis.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "Translation performance",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "Translation Error Rate",
                "Translation performance",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Given a wordq, its set of featuresFq and feature weightswq  for f Fq, a common symmetric similarity measure is Lin similarity  : Lin  = summationtext fFuFv .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "wordq",
                "set of featuresFq and feature weightswq for f Fq",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Lin similarity",
                "summation text fFuFv",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Dredze et al. also indicated that unlabeled dependency parsing is not robust to domain adaptation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "domain adaptation",
                "is not robust",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "dependency parsing",
                "is not robust",
                "METHODOLOGY",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "ecent advances in parsing technology are due to the explicit stochastic modeling of dependency information  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing technology",
                "explicit stochastic modeling of dependency information",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "advances",
                "due to",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The class based disambiguation operator is the Mutual Conditioned Plausibility    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Mutual Conditioned Plausibility",
                "the class based disambiguation operator",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Conditioned Plausibility",
                "the Mutual",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The kappa statistic   for identifying question segments is 0.68, and for linking question and answer segments given a question segment is 0.81.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa statistic",
                "for identifying question segments",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "kappa statistic",
                "for linking question and answer segments given a question segment",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Currently, the scheme supports PhraseChunks with subtypes such as NP, VP, PP, or ADJP  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PhraseChunks",
                "subtypes such as NP, VP, PP, or ADJP",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Doing joint inference instead of taking a pipeline approach has also been shown useful for other problems  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "problems",
                "useful",
                "APPLICABILITY",
                "positive",
                0.7
            ],
            [
                "inference",
                "joint inference",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "SRILM   can produce classes to maximize the mutual information between the classes I ;C ), as described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SRILM",
                "maximize the mutual information between the classes",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "mutual information",
                "between the classes",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The models in the comparative study by Klein and Manning   did not include such features, and so, again for consistency of comparison, we experimentally verified that our maximum entropy model   consistently yielded higher scores than when the features were not used, and   consistently yielded higher scores than nave Bayes using the same features, in agreement with Klein and Manning  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "consistently yielded higher scores",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "features",
                "not used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4 Data Collection We evaluated out method by running RASP over Brown Corpus and Wall Street Journal, as contained in the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "RASP",
                "running over Brown Corpus and Wall Street Journal",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank",
                "as contained in",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In order to resolve all Chinese NLDs represented in the CTB, we modify and substantially extend the     algorithm as follows: Given the set of subcat frames s for the word w, and a set of paths p for the trace t, the algorithm traverses the f-structure f to: predict a dislocated argument t at a sub-fstructure h by comparing the local PRED:w to ws subcat frames s t can be inserted at h if h together with t is complete and coherent relative to subcat frame s traverse f starting from t along the path p link t to its antecedent a if ps ending GF a exists in a sub-f-structure within f; or leave t without an antecedent if an empty path for t exists In the modified algorithm, we condition the probability of NLD path p   on the GF associated of the trace t rather than the antecedent a as in C04.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "modify and substantially extend",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "probability of NLD path",
                "conditioned on the GF associated",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "HMMs have been used many times for POS tagging and chunking, in supervised, semisupervised, and in unsupervised settings  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMMs",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "settings",
                "supervised, semisupervised, and unsupervised",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Here, we use the hidden Markov model   alignment model   and Model 4 of Brown et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 4",
                "of Brown et al.",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "hidden Markov model alignment model",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The comparison phrasal system was const ructed using the same GIZA++ alignments and the heuristic combination described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ alignments",
                "same",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "heuristic combination",
                "described in ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Regression has also been used to order sentences in extractive summarization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Regression",
                "order sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentences",
                "in extractive summarization",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "There is a vast literature on language modeling; see, e.g.,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "literature",
                "on language modeling",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "e.g.",
                "see",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Most of the previous work on statistical machine translation, as exemplified in  , employs word-alignment algorithm  ) that provides local associations between source and target words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "provides local associations",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "previous work",
                "employs word-alignment",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "First, the graph-based models have better precision than the transition-based models when predicting long arcs, which is compatible with the results of McDonald and Nivre  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "graph-based models",
                "better precision",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "transition-based models",
                "predicting long arcs",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ut it is close to the paradigm described by Yarowsky   and Turney   as it also employs self-training based on a relatively small seed data set which is incrementally enlarged with unlabelled samples",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "seed data set",
                "relatively small",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "self-training",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We use 3500 sentences from CoNLL   as the NER data and section 20-23 of the WSJ   as the POS/chunk data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CoNLL",
                "as the NER data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "WSJ",
                "as the POS/chunk data",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We can mentionhere only part of this work:   for monolingualextraction, and (Kupiec, 1993; Wu,1994;Smadjaetal.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kupiec, 1993; Wu, 1994; Smadja et al.",
                "mentioned",
                "INNOVATION",
                "neutral",
                1.0
            ],
            [
                "monolingual extraction",
                "part of",
                "APPLICABILITY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "All 8,907 articles were tagged by the Xerox Part-ofSpeech Tagger   4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Xerox Part-ofSpeech Tagger",
                "tagged by",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "articles",
                "all 8,907",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The first one makes use of the advances in the parsing technology or on the availability of large parsed corpora  ) to produce algorithms inspired by Hobbs' baseline method  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing technology",
                "advances",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithms",
                "inspired by Hobbs' baseline method",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The features used in NLG2 are described in the next section, and the feature weights aj, obtained from the Improved Iterative Scaling algorithm  , are set to maximize the likelihood of the training data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights aj",
                "obtained from the Improved Iterative Scaling algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "feature weights aj",
                "set to maximize the likelihood of the training data",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This approach to term clustering is closely related to others from the literature  .2 Recall that the mutual information between random variables a0 and a1 can be written: a2a4a3a6a5a8a7a10a9a11a13a12a15a14a17a16a19a18a21a20a23a22a25a24a27a26a29a28 a14a17a16a19a18a21a20a23a22a25a24 a14a17a16a19a18a30a24a31a14a17a16a19a22a32a24   Here, a0 and a1 correspond to term and context clusters, respectively, each event a18 and a22 the observation of some term and contextual term in the corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information",
                "can be written",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "term and context clusters",
                "correspond to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Background Default unification has been investigated by many researchers   in the context of developing lexical semantics.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unification",
                "has been investigated",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "lexical semantics",
                "context of developing",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In particular, ROUGE-2 is the recall in bigrams with a set of human-written abstractive summaries  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE-2",
                "recall in bigrams with a set of human-written abstractive summaries",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "human-written abstractive summaries",
                "set of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We develop this intuition into a technique called synchronous binarization   which binarizes a synchronous production or treetranduction rule on both source and target sides simultaneously.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "synchronous binarization",
                "binarizes a synchronous production or treetranduction rule on both source and target sides simultaneously",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "synchronous production or treetranduction rule",
                "on both source and target sides simultaneously",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In the Link Grammar framework  , strictly local contexts are naturally combined with long-distance information coming from long-range trigrams.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Link Grammar framework",
                "strictly local contexts are naturally combined with long-distance information",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Link Grammar framework",
                "naturally combined with long-distance information",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Most of the early work in this area was based on postulating generative probability models of language that included parse structure  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parse structure",
                "included",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "generative probability models",
                "of language",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "This source is very important for repairs that do not have initial retracing, and is the mainstay of the \"\"parser-first\"\" approach  --keep trying alternative corrections until one of them parses.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser-first approach",
                "mainstay",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "alternative corrections",
                "keep trying",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The translation quality on the TransType2 task in terms of WER, PER, BLEU score  , and NIST score   is given in Table 4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TransType2 task",
                "in terms of WER, PER, BLEU score, and NIST score",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "Table 4",
                "translation quality",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In Englishto-German, this result produces results very comparable to a phrasal SMT system   trained on the same data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "phrasal SMT system",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "results",
                "very comparable",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Self-training   is a form of semi-supervised learning.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Self-training",
                "form of semi-supervised learning",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Thus, given a hyponym definition   and a set of candidate hypernym definitions, this method selects the candidate hypernym definition   which returns the maximum score given by formula  : SC  : E cw    'wIEOAwj6E The cooccurrence weight   between two words can be given by Cooccurrence Frequency, Mutual Information   or Association Ratio  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "cooccurrence weight",
                "given by Cooccurrence Frequency, Mutual Information or Association Ratio",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "candidate hypernym definition",
                "returns the maximum score",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " , we are interested in applying alternative metrics such a Meteor  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "alternative",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Meteor",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "WordNet has been criticized for being overly finegrained  , we are using it here because it is the sense inventory used by Erk et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet",
                "being overly finegrained",
                "METHODOLOGY",
                "negative",
                0.75
            ],
            [
                "WordNet",
                "is the sense inventory used by Erk et al.",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Parsing has been also used after extraction   for filtering out invalid results.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Parsing",
                "filtering out invalid results",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "results",
                "invalid",
                "PERFORMANCE",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "Most work has looked to model non-local dependencies only within a document  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "looked to model non-local dependencies",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "dependencies",
                "within a document",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We present a new implication of Wus   Inversion Transduction Grammar   Hypothesis, on the problem of retrieving truly parallel sentence translations from large collections of highly non-parallel documents.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wus Inversion Transduction Grammar Hypothesis",
                "new implication",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "large collections of highly non-parallel documents",
                "problem of retrieving truly parallel sentence translations",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We base our work partly on previous work done by Bagga and Baldwin  , which has also been used in later work  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "has also been used in later work",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "previous work",
                "done by Bagga and Baldwin",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Automated evaluation will utilize the standard DUC evaluation metric ROUGE   which representsrecallovervariousn-gramsstatisticsfrom asystem-generatedsummaryagainstasetofhumangenerated peer summaries.5 We compute ROUGE scores with and without stop words removed from peer and proposed summaries.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE",
                "standard evaluation metric",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ROUGE scores",
                "with and without stop words removed",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use the following features for our rules:  sourceand target-conditioned neg-log lexical weights as described in    neg-log relative frequencies: left-handside-conditioned, target-phrase-conditioned, source-phrase-conditioned  Counters: n.o. rule applications, n.o. target words  Flags: IsPurelyLexical  , IsPurelyAbstract  , IsXRule  , IsGlueRule 139  Penalties: rareness penalty exp ; unbalancedness penalty |MeanTargetSourceRatio  n.o. source words n.o. target words| 4 Parsing Our SynCFG rules are equivalent to a probabilistic context-free grammar and decoding is therefore an application of chart parsing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "neg-log lexical weights",
                "as described in neg-log relative frequencies",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Parsing",
                "is an application of chart parsing",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The novel algorithm differs computationally from earlier work in discriminative training algorithms for SMT   as follows: a90 No computationally expensive a57 -best lists are generated during training: for each input sentence a single block sequence is generated on each iteration over the training data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "does not generate computationally expensive lists",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "training process",
                "generates a single block sequence on each iteration",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use discourse-level feature predicates in a maximum entropy classifier   with binary and n-class classification to select referring expressions from a list.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifier",
                "maximum entropy",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "referring expressions",
                "from a list",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We show that the method of  , which was presented as a simple preprocessing step, is actually equivalent, except our representation explicitly separates hyperparameters which were tied in his work.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "is actually equivalent",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "hyperparameters",
                "explicitly separates",
                "METHODOLOGY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "They are most commonly used for parsing and linguistic analysis  , but are now commonly seen in applications like machine translation   and question answering  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing and linguistic analysis",
                "most commonly used for",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "machine translation and question answering",
                "are now commonly seen in applications",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "When the value of Ilw, r,w'll is unknown, we assume that A and C are conditionally independent given B. The probability of A, B and C cooccurring is estimated by PMLE  PMLE  PMLE , where PMLE is the maximum likelihood estimation of a probability distribution and P.LE  = II*,*,*ll' P. ,~E  = II*,~,*ll ' P, LE  = When the value of Hw, r, w~H is known, we can obtain PMLE  directly: PMLE  = \\ =c. Its value can be corn769 simgindZe  = ~'~ eTCwl)NTCw2)Aresubj.of.obj-of} min , I  ) simHindte,   = ~, eT nT  min , I ) \\]T NT I simcosine  = x/IZ llZ l 2x IT nZ l simDice  = iT l+lT  I simJacard   = T OT l T  + T l-IT rlT l Figure 1: Other Similarity Measures puted as follows: I  = _ Iog PMLE PMLE ) -- ) log IIw,r,wflll*,r,*ll -IIw,r,*ll xll*,r,w'll It is worth noting that I  is equal to the mutual information between w and w'  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PMLE",
                "maximum likelihood estimation of a probability distribution",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "I",
                "equal to the mutual information between w and w'",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Stochastic taggers use both contextual and morphological information, and the model parameters are usually defined or updated automatically from tagged texts  , University of Patras, 265 00 Patras, Greece.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model parameters",
                "defined or updated automatically from tagged texts",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "taggers",
                "use contextual and morphological information",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This con rms Liu and Gildea  s nding that in sentence level evaluation, long n-grams in BLEU are not bene cial.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "not bene cial",
                "PERFORMANCE",
                "negative",
                0.75
            ],
            [
                "n-grams",
                "are not bene cial",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "arowsky   dealt with this problem largely by producing an unsupervised learning algorithm that generates probabilistic decision list models of word senses from seed collocates",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "generates probabilistic decision list models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "seed collocates",
                "unsupervised learning",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For the WMT 2009 Workshop, we selected a linear combination of BLEU   and TER   as optimization criterion,  := argmax{ TER}, based on previous experience  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "optimization criterion",
                "linear combination of BLEU and TER",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "previous experience",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We have adopted the evaluation method of Snow et al  : compare the generated hypernyms with hypernyms present in a lexical resource, in our case the Dutch part of EuroWordNet  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "EuroWordNet",
                "Dutch part of EuroWordNet",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "evaluation method of Snow et al",
                "compare generated hypernyms with hypernyms present",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "e.g. BLEU   for machine translation, ROUGE   for summarization.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "machine translation",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "ROUGE",
                "summarization",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "We are already using the extracted semantic forms in parsing new text with robust, wide-coverage PCFG-based LFG grammar approximations automatically acquired from the f-structure annotated Penn-II treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PCFG-based LFG grammar",
                "wide-coverage",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "Penn-II treebank",
                "annotated",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In Table 6 we report our results, together with the state-of-the-art from the ACL wiki5 and the scores of Turney     and from Amac Herdagdelens PairSpace system, that was trained on ukWaC.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "state-of-the-art",
                "from the ACL wiki5",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Turney and Amac Herdagdelens PairSpace system",
                "was trained on ukWaC",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "That is a significant shortcoming, because in many domains, hard or soft global constraints on the label sequence are motivated by common sense:  For named entity recognition, a phrase that appears multiple times should tend to get the same label each time  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "label sequence",
                "motivated by common sense",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "phrase that appears multiple times",
                "get the same label each time",
                "PERFORMANCE",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "ch and Ney   state that AER is derived from F-Measure",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "AER",
                "derived from F-Measure",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "F-Measure",
                "used in derivation of AER",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For each word pair from the antonym set, we calculated the distributional distance between each of their senses using Mohammad and Hirsts   method of concept distance along with the modified form of Lins   distributional measure  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Mohammad and Hirsts method",
                "concept distance",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Lins distributional measure",
                "modified form",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation , large amount of human effort and time has been invested in collecting parallel corpora of translated texts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "parallel corpora of translated texts",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "effort and time",
                "large amount of",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "ROUGE   is an evaluation metric designed to evaluate automatically generated summaries.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE",
                "evaluation metric",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "evaluation metric",
                "designed to evaluate",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Hindle, D. ,   \"\"Noun Classification from Predicate-Argument Structures,\"\" Proceedings of the 28th Annual Meeting of the ACL, pp.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Noun Classification",
                "from Predicate-Argument Structures",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "Proceedings of the 28th Annual Meeting of the ACL",
                "pp.",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "9 The definition of BLEU used in this training was the original IBM definition  , which defines the effective reference length as the reference length that is closest to the test sentence length.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "definition of BLEU",
                "original IBM definition",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "reference length",
                "closest to the test sentence length",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5.1 Baseline System We trained Moses on all Spanish-English Europarl sentences up to length 20   using GIZA++ Model 4 word alignments and the growdiag-final-and combination heuristic  , which performed better than any alternative combination heuristic.13 The baseline estimates   come fromextractingphrasesuptolength7fromtheword alignment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Moses",
                "using GIZA++ Model 4 word alignments and the growdiag-final-and combination heuristic",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "baseline estimates",
                "come from extracting phrases up to length 7 from the word alignment",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.5 Domain adaptation in Machine Translation Within MT there has been a variety of approaches dealing with domain adaption  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Domain adaptation",
                "dealing with domain adaptation",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "approaches",
                "within MT",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Both training and testing sentences were processed using Collins parser   to generate parse-tree automatically.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins parser",
                "generate parse-tree automatically",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "training and testing sentences",
                "processed using Collins parser",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The first SMT systems were developed in the early nineties  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT systems",
                "developed",
                "INNOVATION",
                "neutral",
                0.5
            ],
            [
                "nineties",
                "early",
                "PERFORMANCE",
                "neutral",
                0.6
            ]
        ]
    },
    {
        "text": "Our approach to STC uses a thesaurus based on corpus statistics   for real-valued similarity calculation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "thesaurus",
                "based on corpus statistics",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "real-valued similarity calculation",
                "real-valued",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "They propose two modifications to f-measure: varying the precision/recall tradeoff, and fully-connecting the alignment links before computing f-measure.11 Weighted Fully-Connected F-Measure Given a hypothesized set of alignment links H and a goldstandard set of alignment links G, we define H+ = fullyConnect  and G+ = fullyConnect , and then compute: f-measure  = 1 precision  + 1 recall  For phrase-based Chinese-English and ArabicEnglish translation tasks,   obtain the closest correlation between weighted fully-connected alignment f-measure and BLEU score using =0.5 and =0.1, respectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "H",
                "fullyConnect",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "f-measure",
                "1 precision + 1 recall",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ing and McKeown   found that human summarization can be traced back to six cut-andpaste operations of a text and proposed a revision method consisting of sentence reduction and combination modules with a sentence extraction part",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "human summarization",
                "can be traced back to six cut-and-paste operations",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "revision method",
                "consisting of sentence reduction and combination modules",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For example,   suggested two different methods: using only the alignment with the maximum probability, the so-called Viterbi alignment, or generating a set of alignments by starting from the Viterbi alignment and making changes, which keep the alignment probability high.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "Viterbi alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "alignment",
                "probability high",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "They have been employed in word sense disambiguation  , automatic construction of bilingual dictionaries  , and inducing statistical machine translation models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word sense disambiguation",
                "employed",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "statistical machine translation models",
                "inducing",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, the lexicalized grammars of Collins   and Charniak   and the statesplit grammars of Petrov et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicalized grammars",
                "of Collins and Charniak",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "statesplit grammars",
                "of Petrov et al.",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "PMI++ is an extended version of  s method for finding the SO label of a phrase  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PMI++",
                "extended version of s method",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "SO label",
                "finding",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These results were achieved using the statistical alignments provided by model 5   and smoothed 11-grams and 6-grams, respectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model 5",
                "statistical alignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "smoothed 11-grams and 6-grams",
                "respectively",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Its applications range from sentence boundary disambiguation   to part-of-speech tagging  , parsing   and machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentence boundary disambiguation",
                "to",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "part-of-speech tagging",
                "and",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "240 2 Motivation Many approaches to identifying base noun phrases have been explored as part of chunking  , but determining sub-NP structure is rarely addressed.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "Many",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "structure",
                "rarely addressed",
                "LIMITATION",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "Discriminative, context-specific training seems to yield a better set of similar predicates, e.g. the highest-ranked contexts for DSPcooc on the verb join,3 lead 1.42, rejoin 1.39, form 1.34, belong to 1.31, found 1.31, quit 1.29, guide 1.19, induct 1.19, launch   1.18, work at 1.14 give a better SIMS  for Equation   than the top similarities returned by  : participate 0.164, lead 0.150, return to 0.148, say 0.143, rejoin 0.142, sign 0.142, meet 0.142, include 0.141, leave 0.140, work 0.137 Other features are also weighted intuitively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "similar predicates",
                "yield a better set",
                "METHODOLOGY",
                "positive",
                0.85
            ],
            [
                "features",
                "weighted intuitively",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A number of systems for automatically learning semantic parsers have been proposed  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "systems",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "semantic parsers",
                "automatic learning",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "We train our feature weights using max-BLEU   and decode with a CKY-based decoder that supports language model scoring directly integrated into the search.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "trained using max-BLEU",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "decoder",
                "supports language model scoring directly integrated into the search",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4.1 Experimental Setup We use the whole Penn Treebank corpus   as our data set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank corpus",
                "as our data set",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "whole",
                "whole",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The parameters, j, were trained using minimum error rate training   to maximise the BLEU score   on a 150 sentence development set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "trained using minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU score",
                "maximise the",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Unfortunately, determining the optimal segmentation is challenging, typically requiring extensive experimentation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "segmentation",
                "extensive experimentation",
                "METHODOLOGY",
                "negative",
                0.6
            ],
            [
                "determining",
                "optimal segmentation",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text   or at the sentence levels  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "at the text or at the sentence levels",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "level of analysis",
                "usually conducted",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Sometimes, due to data sparseness and/or limitations in the machine learning paradigm used, we need to extract features from the available representation in a manner that profoundly changes the representation  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning paradigm",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "representation",
                "profoundly changes",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "aghighi and Klein   propose constraining the mapping from hidden states to POS tags so that at most one hidden state maps to any POS tag",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mapping",
                "constraining so that at most one hidden state maps to any POS tag",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "hidden states",
                "to POS tags",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "both relevant and non-redundant  , some recent work focuses on improved search  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "relevant and non-redundant",
                "some recent work focuses on improved search",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Accordingly, in this section we describe a set of experiments which extends the work of   by evaluating the Marker-based EBMT system of   against a phrase-based SMT system built using the following components:  Giza++, to extract the word-level correspondences;  The Giza++ word alignments are then refined and used to extract phrasal alignments  ; or   for a more recent implementation);  Probabilities of the extracted phrases are calculated from relative frequencies;  The resulting phrase translation table is passed to the Pharaoh phrase-based SMT decoder which along with SRI language modelling toolkit5 performs translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Giza++",
                "to extract the word-level correspondences",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Pharaoh phrase-based SMT decoder",
                "along with SRI language modelling toolkit",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Consequently, considerable effort has gone into devising and improving automatic word alignment algorithms, and into evaluating their performance  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "automatic word alignment algorithms",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "performance",
                "evaluating their performance",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In natural language processing, label propagation has been used for document classification  , word sense disambiguation  , and sentiment categorization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "label propagation",
                "used for document classification, word sense disambiguation, and sentiment categorization",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "label propagation",
                "has been used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use the log-likelihood X ~ statistic, rather than the Pearson's X 2 statistic, as this is thought to be more appropriate when the counts in the contingency table are low  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistic",
                "log-likelihood X ~",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "statistic",
                "Pearson's X 2",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "To examine the effects of including some known AMs on the performance, the following AMs had a 50% chance of being included in the initial population: pointwise mutual information  , the Dice coefficient, and the heuristic measure defined in  : H  =    2log f f f  if POS  = X, log f f f f  otherwise.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "AMs",
                "pointwise mutual information, the Dice coefficient, and the heuristic measure defined in : H  =    2log f f f  if POS  = X, log f f f f  otherwise",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "performance",
                "had a 50% chance of being included in the initial population",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "When the data has distinct sub-structures, models that exploit hidden state variables are advantageous in learning  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "exploit hidden state variables",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "models",
                "are advantageous",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "We also use minimum error-rate training   to tune our feature weights.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "minimum error-rate training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "minimum error-rate training",
                "to tune",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We also compared the cluster gazetteers with the Wikipedia gazetteer constructed by following the method of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "gazetteers",
                "constructed by following the method of",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "Wikipedia gazetteer",
                "constructed by following the method of",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "To perform minimum error rate training   to tune the feature weights to maximize the systems BLEU score on development set, we used the script optimizeV5IBMBLEU.m  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "to maximize the systems BLEU score",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "optimizeV5IBMBLEU.m",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In one experiment, it has to be performed on the basis of the gold-standard, assumed-perfect POS taken directly from the training data, the Penn Treebank  , so as to abstract from a particular POS tagger and to provide an upper bound.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "gold-standard",
                "assumed-perfect POS",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Penn Treebank",
                "directly from the training data",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "While choosing an optimum window size for an application is often subject to trial and error, there are some generally recognized trade-offs between small versus large windows, such as the impact of data-sparseness, and the nature of the associations retrieved   Measures based on distance between words in the text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "window size",
                "small versus large",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "measures",
                "based on distance between words in the text",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In fact, many attempts have recently been made to develop semi-supervised SOL methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SOL methods",
                "develop",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "attempts",
                "many",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Paraphrasesofthiskind have been shown to be useful in applications such as machine translation   and as an intermediate step in inventory-based classification of abstract relations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Paraphrases of this kind",
                "have been shown to be useful",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "inventory-based classification of abstract relations",
                "intermediate step",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, in  , the authors investigate minimum translation units   which is a refinement over a similar approach by   to eliminate the overlap issue.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "minimum translation units",
                "a refinement",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "similar approach",
                "to eliminate the overlap issue",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Fortunately, using distributional characteristics of term contexts, it is feasible to induce part-of-speech categories directly from a corpus of suf cient size, as several papers have made clear  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distributional characteristics of term contexts",
                "feasible to induce",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "part-of-speech categories",
                "directly from a corpus of sufficient size",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This improvement is close to that of one sense per discourse    , which seems to be a sensible upper bound of the proposed method.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "proposed method",
                "sensible upper bound",
                "INNOVATION",
                "positive",
                0.75
            ],
            [
                "sense per discourse",
                "upper bound",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This is a particularly exciting area in computational linguistics as evidenced by the large number of contributions in these special issues: Biber  , Brent  , Hindle and Rooth  , Pustejovsky et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "computational linguistics",
                "exciting area",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "contributions",
                "large number",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Since word senses are often associated with domains  , word senses can be consequently distinguished by way of determining the domain of each description.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word senses",
                "can be distinguished",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "domains",
                "are often associated with",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "82 2 Aggregate Markov models In this section we consider how to construct classbased bigram models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Markov models",
                "construct",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "bigram models",
                "classbased",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Word Sense Disambiguation   competitions have focused on general domain texts, as attested in the last Senseval and Semeval competitions  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "competitions",
                "focused on general domain texts",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "last Senseval and Semeval competitions",
                "attested",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For every pair of nouns, where each noun had a total frequency in the triple data of 10 or more, we computed their distributional similarity using the measure given by Lin  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "nouns",
                "distributional similarity",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "measure given by Lin",
                "given by",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The rules are then treated as events in a relative frequency estimate.4 We used Giza++ Model 4 to obtain word alignments  , using the grow-diag-final-and heuristic to symmetrise the two directional predictions  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Giza++ Model 4",
                "to obtain word alignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "grow-diag-final-and heuristic",
                "to symmetrise the two directional predictions",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Examples include Wus   ITG and Chiangs hierarchical models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wus",
                "ITG",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Chiang's",
                "hierarchical models",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Statistical Translation Engine A word-based translation engine is used based on the so-called IBM-4 model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Statistical Translation Engine",
                "word-based translation engine",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IBM-4 model",
                "so-called",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments  , we are not aware of any studies that have shown that corpus-based evaluation metrics of NLG systems are correlated with human judgments; correlation studies have been made of individual components  , but not of systems.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "correlate well with human judgments",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "corpus-based evaluation metrics",
                "are correlated with human judgments",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In the last few years there has been an increasing interest in applying MaxEnt models for NLP applications  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MaxEnt models",
                "applying for NLP applications",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "interest",
                "increasing",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Moses uses standard external tools for some of these tasks, such as GIZA++   for word alignments and SRILM   for language modeling.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Moses",
                "standard external tools",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "GIZA++",
                "for word alignments",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We describe the experiment in greater detail 2The particular verbs selected were looked up in   and the class for each verb in the classification system defined in   was selected with some discussion with linguists.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "verbs",
                "selected",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "classification system",
                "defined in",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In parsing, the most relevant previous work is due to Collins  , who considered three binary features of the intervening material: did it contain   any word tokens at all,   any verbs,   any commas or colons?",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins",
                "previous work",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "three binary features",
                "intervening material",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The original formulation of statistical machine translation   was defined as a word-based operation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical machine translation",
                "word-based operation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "formulation",
                "original",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "These tasks are generally treated as sequential labeling problems incorporating the IOB tagging scheme  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IOB tagging scheme",
                "is incorporated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tasks",
                "are treated as sequential labeling problems",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Table 2: Figures about clustering algorithms Algorithm # Sentences/# Clusters S-HAC 6,23 C-HAC 2,17 QT 2,32 EM 4,16 In fact, table 2 shows that most of the clusters have less than 6 sentences which leads to question the results presented by   who only keep the clusters that contain more than 10 sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "clustering algorithms",
                "Algorithm # Sentences/# Clusters",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "clusters",
                "have less than 6 sentences",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "Wu   investigated the use of concurrent parsing of parallel corpora in a transduction inversion framework, helping to resolve attachment ambiguities in one language by the coupled parsing state in the second language.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "concurrent parsing",
                "in a transduction inversion framework",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "coupled parsing state",
                "helping to resolve attachment ambiguities",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "They constructed word clusters by using HMMs or Browns clustering algorithm  , which utilize only information from neighboring words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMMs or Browns clustering algorithm",
                "utilize only information from neighboring words",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word clusters",
                "constructed by",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Much of the work in sentiment analysis in the computational linguistics domain has focused either on short segments, such as sentences  , or on longer documents with an explicit polarity orientation like movie or product reviews  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentiment analysis",
                "short segments, such as sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentiment analysis",
                "longer documents with an explicit polarity orientation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this article, we used the algorithm of   to initialize the model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "of",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "model",
                "to initialize",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Furthermore, techniques such as iterative minimum errorrate training   as well as web-based MT services require the decoder to translate a large number of source-language sentences per unit time.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "translate a large number of source-language sentences per unit time",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "decoder",
                "require the decoder",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Thus, conventional methods had to introduce some kinds of restrictions such as the limitation of the kind of chains or the length of chains to be extracted  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chains",
                "kind of chains",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "chains",
                "length of chains",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For a full description of the algorithm, see  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "description",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "description",
                "see [reference]",
                "APPLICABILITY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "It is possible to prove that, provided the training set   is separable with margin > 0, the algorithm is assured to converge after a finite number of iterations to a model with zero training errors  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "converge after a finite number of iterations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "model",
                "zero training errors",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Exact Decoding is the original decoding problem as defined in   and Relaxed Decoding is the relaxation of the decoding problem typically used in practice.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoding problem",
                "defined in and Relaxed Decoding",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Relaxed Decoding",
                "typically used in practice",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "For these classications, we calculated a kappa statistic of 0.528  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa statistic",
                "0.528",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "As we remarked earlier, however, the input data required by our method   could be generated automatically from unparsed corpora making use of existing heuristic rules  , although for the experiments we report here we used a parsed corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "input data",
                "generated automatically from unparsed corpora making use of existing heuristic rules",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parsed corpus",
                "used for experiments",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We will be using the similarity metrics shown in Table 1: Cosine, the Dice and Jaccard coefficients, and Hindles   and Lins   mutual information-based metrics.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "similarity metrics",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Hindle and Lins mutual information-based metrics",
                "mutual information-based metrics",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Second, phrase translation pairs are extracted from the word aligned corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase translation pairs",
                "extracted from the word aligned corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "If the bound is too tight to allow the correct parse of some sentence, we would still like to allow an accurate partial parse: a sequence of accurate parse fragments  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bound",
                "too tight",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parse fragments",
                "accurate",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Our method revises and considerably extends the approach of   originally designed for English, and, to the best of our knowledge, is the first NLD recovery algorithm for Chinese.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NLD recovery algorithm",
                "is the first",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "approach",
                "originally designed for English",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2004) and Barzilay and Lee   used comparable news articles to obtain sentence level paraphrases",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "comparable news articles",
                "to obtain sentence level paraphrases",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence level paraphrases",
                "obtained",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Data and Parameters To facilitate comparison with previous work, we trained our models on sections 2-21 of the WSJ section of the Penn tree-bank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sections 2-21",
                "used for training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Most approaches   inherently extract semantic knowledge in the abstracted form of semantic clusters.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "inherently extract semantic knowledge",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "semantic clusters",
                "abstracted form",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Other factors that distinguish us from previous work are the use of all phrases proposed by a phrase-based system, and the use of a dependency language model that also incorporates constituent information   for related approaches).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based system",
                "proposed by",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "dependency language model",
                "incorporates constituent information",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However,   found that it is actually harmful to restrict phrases to constituents in parse trees, because the restriction would cause the system to miss many reliable translations, such as the correspondence between there is in English and es gibt   in German.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "restriction",
                "would cause the system to miss many reliable translations",
                "PERFORMANCE",
                "negative",
                0.9
            ],
            [
                "phrases to constituents in parse trees",
                "is actually harmful",
                "LIMITATION",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "This score measures the precision of unigrams, bigrams, trigrams and fourgrams with respect to a reference translation with a penalty for too short sentences  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unigrams, bigrams, trigrams and fourgrams",
                "with respect to a reference translation",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "too short sentences",
                "with a penalty",
                "LIMITATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "he tag propagation/elimination scheme is adopted from  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tag propagation/elimination scheme",
                "is adopted from",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tag propagation/elimination scheme",
                "is adopted from",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In addition, the calculation cost for estimating parameters of embedded joint PMs   is independent of the number of HMMs, J, that we used  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMMs",
                "the number of HMMs",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "calculation cost",
                "independent of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A perceptron algorithm gives 97.11%  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron algorithm",
                "gives 97.11%",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "97.11%",
                "",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "In the rest of the paper we use the following notation, adapted from Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "notation",
                "adapted from Collins",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "notation",
                "rest of the paper",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Lexical collocation functions, especially those determined statistically, have recently attracted considerable attention in computational linguistics   mainly, though not exclusively, for use in disambiguation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Lexical collocation functions",
                "attracted considerable attention",
                "INNOVATION",
                "positive",
                0.85
            ],
            [
                "statistically determined",
                "mainly, though not exclusively",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Accuracy on sentiment classification in other domains exceeds 80%  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "accuracy",
                "exceeds 80%",
                "PERFORMANCE",
                "positive",
                0.9
            ],
            [
                "sentiment classification",
                "in other domains",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, both Haghighi and Klein   and Mann and McCallum   have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "33 highly discriminative",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "results",
                "better than 66.1%",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "EMD training   combines generative and discriminative elements.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "EMD training",
                "combines generative and discriminative elements",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "generative and discriminative elements",
                "combined",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  88.02   + unlabeled data   88.41   + supplied gazetters 88.90   + add dev.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unlabeled data",
                "supplied gazetters",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "add dev",
                "add dev",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "NeATS computes the likelihood ratio    to identify key concepts in unigrams, bigrams, and trigrams, and clusters these concepts in order to identify major subtopics within the main topic.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NeATS",
                "computes likelihood ratio",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "concepts",
                "identify major subtopics",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2 Head Lexicalization As previously shown  , Collins  , Carroll and Rooth  , etc.), ContextFree Grammars   can be transformed to lexicalized CFGs, provided that a head-marking scheme for rules is given.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Head Lexicalization",
                "can be transformed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "head-marking scheme",
                "is given",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The WordNet::Similarity package provides a flexible implementation of many of these measures  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet::Similarity package",
                "flexible implementation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "measures",
                "many",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We perform term disambiguation on each document using an entity extractor  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "entity extractor",
                "entity extractor",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "entity extractor",
                "using",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "4.2 Word alignment We have used IBM models proposed by Brown   for word aligning the parallel corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "The translation output is measured using BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "is measured using",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "translation output",
                "is measured",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The same Powells method has been used to estimate feature weights of a standard feature-based phrasal MT decoder in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Powells method",
                "has been used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "standard feature-based phrasal MT decoder",
                "in ",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The relationship between Kneser-Ney smoothing to the Bayesian approach have been explored in   using Pitman-Yor processes.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kneser-Ney smoothing",
                "to the Bayesian approach",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "Pitman-Yor processes",
                "have been explored",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This ITG constraint is characterized by the two forbidden structures shown in Figure 1  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG constraint",
                "is characterized by",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "forbidden structures",
                "shown in Figure 1",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example,   developed a system to identify inflammatory texts and   developed methods for classifying reviews as positive or negative.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "to identify inflammatory texts",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "for classifying reviews as positive or negative",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "To analyze our methods on IV and OOV words, we use a detailed evaluation metric than Bakeoff 2006   which includes Foov and Fiv.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation metric",
                "detailed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Bakeoff 2006",
                "includes Foov and Fiv",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "opez   explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hierarchical rules",
                "phrase discontiguity inherent",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase-based systems",
                "improvements over",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Our approach differs in important ways from the use of hidden Markov models   for classbased language modeling  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hidden Markov models",
                "use",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approach",
                "differs in important ways",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, as   do not propose any evaluation of which clustering algorithm should be used, we experiment a set of clustering algorithms and present the comparative results.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "clustering algorithms",
                "set of clustering algorithms",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "results",
                "comparative results",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5.2 Evaluation Metrics The commonly used criteria to evaluate the translation results in the machine translation community are: WER  , PER  , BLEU  , and NIST  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "criteria",
                "commonly used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU",
                "evaluation metric",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "xperimentation The corpus used in shallow parsing is extracted from the PENN TreeBank   of 1 million words   by a program provided by Sabine Buchholz from Tilburg University",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "program provided by Sabine Buchholz",
                "from Tilburg University",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Experimental results are reported in Table 2: here cased BLEU results are reported on MT03 Arabic-English test set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU results",
                "reported",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "MT03 Arabic-English test set",
                "test set",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A constituent-based system using Collins parser  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins parser",
                "using",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Collins parser",
                "Collins parser",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems, either from the target side  , the source side   or both sides  , just to name a few.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT systems",
                "integrate linguistic knowledge",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "various efforts",
                "just to name a few",
                "PERFORMANCE",
                "neutral",
                0.75
            ]
        ]
    },
    {
        "text": "For example, the adjective unpredictable may have a negative orientation in an automotive review, in a phrase such as unpredictable steering, but it could have a positive orientation in a movie review, in a phrase such as unpredictable plot, as mentioned in   in the context of his sentiment word detection.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unpredictable",
                "negative orientation",
                "INNOVATION",
                "positive",
                0.75
            ],
            [
                "orientation",
                "in a phrase",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ITGs translate into simple  -BRCGs in the following way; see Wu   for a definition of ITGs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITGs",
                "definition of ITGs",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "BRCGs",
                "",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The basic phrase-based model is an instance of the noisy-channel approach  ,1 in which the translation of a French sentence f into an 1Throughout this paper, we follow the convention of Brown et al. of designating the source and target languages as French and English, respectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "noisy-channel approach",
                "instance of",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase-based model",
                "basic",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "urran   and Lin   use syntactic features in the vector definition",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic features",
                "in the vector definition",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "vector definition",
                "uses syntactic features",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "n particular we work with dependency paths that can reach beyond direct dependencies as opposed to Lin   but in the line of Pado and Lapata  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency paths",
                "can reach beyond direct dependencies",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Pado and Lapata",
                "line",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Forced decoding arises in online discriminative training, where model updates are made toward the most likely derivation of a gold translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model updates",
                "toward the most likely derivation of a gold translation",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "gold translation",
                "gold",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Given an input sentence x, the correct output segmentation F  satisfies: F  = argmax yGEN  Score  where GEN  denotes the set of possible segmentations for an input sentence x, consistent with notation from Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GEN",
                "set of possible segmentations",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Score",
                "argmax",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Related work includes Wu  , Zens and Ney   and Wellington et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "In particular, we used this method with WordNet   and using the same training data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "training data",
                "same",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4 Semantic Class Induction from Wikipedia Wikipedia has recently been used as a knowledge source for various language processing tasks, including taxonomy construction  , coreference resolution  , and English NER  , Cucerzan  , Kazama and Torisawa  , Watanabe et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wikipedia",
                "has been used as a knowledge source",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Wikipedia",
                "for various language processing tasks",
                "APPLICABILITY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "  2.3 Reliability To evaluate the reliability of the annotation, we use the kappa coe cient    , which measures pairwise agreement between a set of coders making category judgements, correcting for expected chance agreement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Introduction There has been considerable recent interest in the use of statistical methods for grouping words in large on-line corpora into categories which capture some of our intuitions about the reference of the words we use and the relationships between them  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "statistical methods",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "intuitions",
                "capture some of our",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In our experiments, we will use 4 different kinds of feature combinations: a157 Baseline: The 6 baseline features used in  , such as cost of word penalty, cost of aligned template penalty.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Baseline features",
                "6 baseline features used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "cost of word penalty",
                "such as",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For mention detection we use approaches based on Maximum Entropy     and Robust Risk Minimization   1For a description of the ACE program see http://www.nist.gov/speech/tests/ace/.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Risk Minimization",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The 45 stochastic word mapping is trained on a FrenchEnglish parallel corpus containing 700,000 sentence pairs, and, following Liu and Gildea  , we only keep the top 100 most similar words for each English word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "45 stochastic word mapping",
                "trained on FrenchEnglish parallel corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "words",
                "most similar words for each English word",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, the entry about the Microsoft in Wikipedia has the following categories: Companies listed on NASDAQ; Cloud computing vendors; etc. Both   and   used the free-text description of the Wikipedia entity to reason about the entity type.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wikipedia entity",
                "free-text description",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Microsoft",
                "listed on NASDAQ",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "correspondence points associated with frequent token types   or by deleting frequent token types from the bitext altogether  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "frequent token types",
                "correspondence points associated with",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "frequent token types",
                "deleting frequent token types",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There are good reasons for using such a hand-crafted, genre-specific verb lexicon instead of a general resource such as WordNet or Levins   classes: Many verbs used in the domain of scientific argumentation have assumed a specialized meaning, which our lexicon readily encodes.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicon",
                "hand-crafted, genre-specific",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "verbs used in the domain of scientific argumentation",
                "assumed a specialized meaning",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "  do not use a feature selection technique, employing instead an objective function which includes a Table 4 Values of Savings   for various values of a, b. ab Savings   1100,000 2,692.7 110 48.6 11100 83.5 1011,000 280.0 1,00110,000 1,263.9 10,00150,000 2,920.2 50,001100,000 4,229.8 Collins and Koo Discriminative Reranking for NLP Gaussian prior on the parameter values, thereby penalizing parameter values which become too large: a C3  arg min a  LogLossa X k0:::m a 2 k 7 2 k  28 Closed-form updates under iterative scaling are not possible with this objective function; instead, optimization algorithms such as gradient descent or conjugate gradient methods are used to estimate parameter values.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "objective function",
                "includes a Table 4 Values of Savings",
                "PERFORMANCE",
                "neutral",
                0.85
            ],
            [
                "optimization algorithms",
                "used to estimate parameter values",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Specifically, MIMIC uses an n-dimensional call router front-end  , which is a generalization of the vector-based call-routing paradigm of semantic interpretation  ; that is, instead of detecting one concept per utterance, MIMIC's semantic interpretation engine detects multiple   concepts or classes conveyed by a single utterance, by using n call touters in parallel.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "call router front-end",
                "generalization of the vector-based call-routing paradigm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "semantic interpretation engine",
                "detects multiple concepts or classes",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.3 Forest minimum error training To tune the feature weights of our system, we used a variant of the minimum error training algorithm   that computes the error statistics from the target sentences from the translation search space   that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "minimum error training algorithm",
                "computes error statistics from target sentences",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "feature weights",
                "changing the feature weights along a single vector",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "ur approach is related to those of Collins and Roark   and Taskar et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "related to those of Collins and Roark and Taskar et al",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "approach",
                "related",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For example, the topics Sport and Education are important cues for differentiating mentions of Michael Jordan, which may refer to a basketball player, a computer science professor, etc. Second, as noted in the top WePS run  , feature development is important in achieving good coreference performance.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Michael Jordan",
                "important cues for differentiating mentions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "feature development",
                "important in achieving good coreference performance",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In word-based models, such as IBM Model 1-5  , the probability P  is decomposed into statistical parameters involving words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 1-5",
                "statistical parameters involving words",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probability P",
                "decomposed into",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2008a; 2008b) on CTB 5.0 and Zhang and Clark   on CTB 4.0 since they reported the best performances on joint word segmentation and POS tagging using the training materials only derived from the corpora",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "2008a; 2008b",
                "reported the best performances",
                "PERFORMANCE",
                "positive",
                0.75
            ],
            [
                "training materials only derived from the corpora",
                "derived",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To derive the joint counts c  from which p  and p  are estimated, we use the phrase induction algorithm described in  , with symmetrized word alignments generated using IBM model 2  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase induction algorithm",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM model 2",
                "generated using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Algorithms for the computation of first-order associations have been used in lexicography for the extraction of collocations   and in cognitive psychology for the simulation of associative learning  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms for the computation of first-order associations",
                "used in lexicography",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "algorithms for the computation of first-order associations",
                "used in cognitive psychology",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "1.2 Decoding in Statistical Machine Translation   and   have discussed the first two of the three problems in statistical machine translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine translation",
                "statistical",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "problems",
                "first two of the three",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Training Data Our source for syntactically annotated training data was the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "syntactically annotated training data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank",
                "source",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Collocations have been widely used for tasks such as word sense disambiguation    , information extraction    , and named-entity recognition  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocations",
                "widely used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word sense disambiguation",
                "information extraction",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Some adopt a pipeline approach  , which works by first extracting candidate affixes and stems, and then segmenting the words based on the candidates.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pipeline approach",
                "works by first extracting candidate affixes and stems, and then segmenting the words based on the candidates",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "candidate affixes and stems",
                "segmenting words based on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 Statistical Word Alignment According to the IBM models  , the statistical word alignment model can be generally represented as in equation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM models",
                "can be generally represented as in equation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "statistical word alignment model",
                "can be generally represented",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Decoding weights are optimized using Ochs algorithm   to set weights for the four components of the log-linear model: language model, phrase translation model, distortion model, and word-length feature.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Decoding weights",
                "are optimized using Ochs algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "four components of the log-linear model",
                "language model, phrase translation model, distortion model, and word-length feature",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our MT baseline system is based on Moses decoder   with word alignment obtained from GIZA++  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Moses decoder",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "GIZA++",
                "obtained from",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In addition, Wu   used a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "stochastic inversion transduction grammar",
                "simultaneously parse the sentence pairs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence pairs",
                "get the word or phrase alignments",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There have been a number of methods proposed in the literature to address the word clustering problem  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "proposed in the literature",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word clustering problem",
                "address",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " , and the phrase-based approach to Statistical Machine Translation   has led to the development of heuristics for obtaining alignments between phrases of any number of words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based approach",
                "led to the development of heuristics",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "heuristics for obtaining alignments",
                "between phrases of any number of words",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We evaluated performance by measuring WER  , PER  , BLEU   and TER     using multiple references.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WER",
                "measuring",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "references",
                "multiple",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.6 Tuning procedure The Moses-based systems were tuned using the implementation of minimum error rate training     distributed with the Moses decoder, using the development corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Moses-based systems",
                "tuned using the implementation of minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "development corpus",
                "used",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The model can be seen as a bootstrapping learning process tbr disambiguation, where the information gained from one part   is used to improve tile other   and vice versa, reminiscent of the work by Riloff and Jones   and Yarowsky  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "bootstrapping learning process",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "information gained",
                "improve the other",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We augment Collins head-driven model 2   to incorporate a semantic label on each internal node.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins head-driven model",
                "to incorporate a semantic label",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "semantic label",
                "on each internal node",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In fact, when the perceptron update rule of    which modifies the weights of every divergent node along the predicted and true paths  is used in the ranking framework, it becomes virtually identical with the standard, flat, ranking perceptron of Collins  .5 In contrast, our approach shares the idea of   that if a parent class has been predicted wrongly, then errors in the children should not be taken into account. We also view this as one of the key ideas of the incremental perceptron algorithm of  , which searches through a complex decision space step-by-step and is immediately updated at the first wrong move.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron update rule",
                "modifies weights of every divergent node",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "incremental perceptron algorithm",
                "searches through complex decision space step-by-step and is immediately updated at the first wrong move",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This may stem from the differences between the two models' feature templates, thresholds, and approximations of the expected values for the features, as discussed in the beginning of the section, or may just reflect differences in the choice of training and test sets  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature templates",
                "different",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "choice of training and test sets",
                "may just reflect",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Specifically, Kim and Hovy   identify which political candidate is predicted to win by an opinion posted on a message board and aggregate opinions to correctly predict an election result.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "candidate",
                "predicted to win",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "opinions",
                "aggregate to correctly predict",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Method dev test Finkel et al. , 2005   baseline CRF 85.51 + non-local features 86.86 Krishnan and Manning, 2006   baseline CRF 85.29 + non-local features 87.24 Table 5: Summary of performance with POS/chunk tags by TagChunk.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseline CRF",
                "85.51",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "non-local features",
                "86.86",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "On one hand, as   evidence, clusters of paraphrases can lead to better learning of text-totext rewriting rules compared to just pairs of paraphrases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "paraphrases",
                "can lead to better learning",
                "INNOVATION",
                "positive",
                0.75
            ],
            [
                "pairs of paraphrases",
                "compared to",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We use the averaged perceptron algorithm, as presented in Collins  , to train the parser.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron algorithm",
                "as presented in Collins",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parser",
                "train the parser",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "First, we extend the mechanism of adding gap variables for nodes dominating a site of discontinuity  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mechanism",
                "adding gap variables",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "site of discontinuity",
                "dominating",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": " , the BBN parser builds augmented parse trees according to a process similar to that described in Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BBN parser",
                "builds augmented parse trees according to a process",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "process similar to that described in Collins",
                "similar",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In modern lexicalized parsers, POS tagging is often interleaved with parsing proper instead of being a separate preprocessing module  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tagging",
                "interleaved with parsing proper",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "POS tagging",
                "as a separate preprocessing module",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU  , NIST   and METEOR   scores.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT system performance",
                "evaluated using BLEU, NIST, and METEOR scores",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "test set",
                "uncased",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources     and those inducing distributional properties of words from corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques",
                "relying on pre-existing knowledge resources",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "corpora",
                "inducing distributional properties of words",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Dubey et al. proposed an unlexicalized PCFG parser that modied PCFG probabilities to condition the existence of syntactic parallelism  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PCFG parser",
                "unlexicalized",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "PCFG probabilities",
                "condition the existence of syntactic parallelism",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Following  , we describe the original parsing architecture and our modifications to it as a Dynamic Bayesian network.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing architecture",
                "Dynamic Bayesian network",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "modifications",
                "as a Dynamic Bayesian network",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  and Ponzetto and Strube  ), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+1, NPi+2, . . ., NPj1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "anaphoric NP",
                "each anaphoric NP",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "antecedent",
                "closest antecedent",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Much like kappa statistics proposed by Carletta  , existing employments of majority class baselines assume an equal set of identical potential mark-ups, i.e. attributes and their values, for all markables.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mark-ups",
                "equal set of identical potential mark-ups",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "majority class baselines",
                "assume",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Second, we will discuss the work done by   who use clustering of paraphrases to induce rewriting rules.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "who",
                "use clustering of paraphrases",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "rewriting rules",
                "induced by clustering of paraphrases",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The initial vectors to be clustered are adapted with pointwise mutual information  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "initial vectors",
                "adapted with pointwise mutual information",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "pointwise mutual information",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.2 Co-occurrence-based approaches The second class of algorithms uses cooccurrence statistics  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "co-occurrence statistics",
                "uses",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "algorithms",
                "second class of",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "hese tables were computed from a small fragment of the Canadian Hansards that has been used in a number of other studies: Church   and Simard et al  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "fragment of the Canadian Hansards",
                "used in a number of other studies",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "Canadian Hansards",
                "used in a number of other studies",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "This sparse information, however, can be propagated across all data based on distributional similarity  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data",
                "based on distributional similarity",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "sparse information",
                "can be propagated across all",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We used MXPOST  , a maximum entropy based POS tagger.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MXPOST",
                "maximum entropy based POS tagger",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "POS tagger",
                "based",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "More recently, Haffari and Sarkar   have extended the work of Abney   and given a better mathematical understanding of self-training algorithms.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "self-training algorithms",
                "better mathematical understanding",
                "INNOVATION",
                "positive",
                0.85
            ],
            [
                "work of Abney",
                "extended",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Examples of formalisms using this approach include the work of Magerman  , Charniak  , Collins  , and Goodman  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "of Magerman, Charniak, Collins, and Goodman",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "formalisms",
                "using this approach",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Second, several tagging experiments on newspaper language, whether statistical   or rule-based  , report that the tagging accuracy for unknown words is much lower than the overall accuracy.2 Thus, the lower percentage of unknown words in medical texts seems to be a sublanguage feature beneficial to POS taggers, whereas the higher proportion of unknown words in newspaper language seems to be a prominent source of tagging errors.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unknown words",
                "lower percentage",
                "APPLICABILITY",
                "positive",
                0.8
            ],
            [
                "unknown words",
                "higher proportion",
                "APPLICABILITY",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "The model is often further restricted so that each source word is assigned to exactly one target word  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "source word",
                "assigned to exactly one target word",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "model",
                "restricted",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  The syntactic annotation task consists of marking constituent boundaries, inserting empty categories  , showing the relationships between constituents  , and specifying a particular subset of adverbial roles.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic annotation task",
                "marking constituent boundaries, inserting empty categories, showing relationships between constituents, specifying a particular subset of adverbial roles",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "adverbial roles",
                "particular subset",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We are also interested in examining the approach within a standard phrase-based decoder such as Moses   or a hierarchical phrase system  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Moses",
                "standard phrase-based decoder",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "hierarchical phrase system",
                "approach within",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Therefore, the base forms have been introduced manually and the POS tags have been provided partly manually and partly automatically using a statistical maximum-entropy based POS tagger similar to the one described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "base forms",
                "have been introduced manually",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "POS tags",
                "have been provided partly manually and partly automatically",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Some researchers then tried to automatically extract paraphrase rules  , which facilitates the rule-based PG methods.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "paraphrase rules",
                "facilitates",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "PG methods",
                "rule-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.5 Model Training We adapt the Minimum Error Rate Training     algorithm to estimate parameters for each member model in co-decoding.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model Training",
                "Minimum Error Rate Training algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parameters",
                "estimate",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "u   and Alshawi   describe early work on formalisms that make use of transductive grammars; Graehl and Knight   describe methods for training tree transducers",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "formalisms",
                "make use of transductive grammars",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "training tree transducers",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We can then state the following theorem   for a proof): Theorem 1 For any training sequence   that is separable with margin, for any value of T, then for the perceptron algorithm in figure 1 Ne R 2 2 where R is a constant such that 8i;8z 2 GEN  jj    jj R. This theorem implies that if there is a parameter vector U which makes zero errors on the training set, then after a finite number of iterations the training algorithm will converge to parameter values with zero training error.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron algorithm",
                "in figure 1",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parameter vector U",
                "makes zero errors on the training set",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Then the two models and a search module are used to decode the best translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "best",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "search module",
                "",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Other classes, such as the ones below can be extracted using lexico-statistical tools, such as in  , and then checked by a human.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexico-statistical tools",
                "such as in , and then",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "classes",
                "extracted using",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A systematic exploration of a set of such features for proteinprotein interaction extraction was recently provided by Jiang and Zhai  , who also used features derived from the Collins parser.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "derived from the Collins parser",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Jiang and Zhai",
                "provided a systematic exploration",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "216 The Maximum Entropy Principle   is to nd a model p = argmax pC H , which means a probability model p  that maximizes entropy H .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy Principle",
                "maximizes entropy",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "probability model",
                "that maximizes entropy",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2.5 Evaluation Minnen and Carroll   report an evaluation of the accuracy of the morphological generator with respect to the CELEX lexical database  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CELEX lexical database",
                "report an evaluation of the accuracy",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "morphological generator",
                "with respect to the CELEX lexical database",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For the evaluation of translation quality, we used the BLEU metric  , which measures the n-gram overlap between the translated output and one or more reference translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metric",
                "measures the n-gram overlap",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "reference translations",
                "one or more",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "417 structure of semantic networks was proposed in  , with a disambiguation accuracy of 50.9% measured on all the words in the SENSEVAL-2 data set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "structure of semantic networks",
                "proposed",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "disambiguation accuracy",
                "50.9%",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The previous studies, with the exception of Kazama and Torisawa  , used smaller gazetteers than ours.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "gazetteers",
                "smaller than ours",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "Kazama and Torisawa",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "araphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "statistical",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Barzilay and Lee",
                "showed",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Table 1 shows the LP and LR scores obtained with our base line subtree set, and compares these scores with those of previous stochastic parsers tested on the WSJ  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "base line subtree set",
                "obtained with",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "stochastic parsers",
                "tested on WSJ",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Penn Treebank corpus   sections 0-20 were used for training, sections 2124 for testing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sections 0-20",
                "used for training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sections 2124",
                "used for testing",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To make sense tagging more precise, it is advisable to place constraint on the translation counterpart c of w. SWAT considers only those translations c that has been linked with w based the Competitive Linking Algorithm   and logarithmic likelihood ratio  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation counterpart c",
                "placed on the basis of Competitive Linking Algorithm and logarithmic likelihood ratio",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "constraint",
                "advisable to place",
                "APPLICABILITY",
                "neutral",
                0.75
            ]
        ]
    },
    {
        "text": "Using Maximum Entropy   classifiers I built a parser that achieves a throughput of over 200 sentences per second, with a small loss in accuracy of about 23 %.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "achieves a throughput of over 200 sentences per second",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "accuracy",
                "small loss of about 23%",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "From the extracted n-grams, those with a flequc'ncy of 3 or more were kept  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-grams",
                "with a frequency of 3 or more",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "n-grams",
                "kept",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Since then, supervised learning from sense-tagged corpora has since been used by several researchers: Zernik  , Hearst  , Leacock, Towell, and Voorhees  , Gale, Church, and Yarowsky  , Bruce and Wiebe  , Miller et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "sense-tagged",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "researchers",
                "several",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Statistical techniques, both supervised learning from tagged corpora  ,  , and unsupervised learning  ,  , have been investigated.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques",
                "supervised learning from tagged corpora",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "learning",
                "unsupervised",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We have observed in several experiments that the number of SuperARVs does not grow signi cantly as training set size increases; the moderate-sized Resource Management corpus   with 25,168 words produces 328 SuperARVs, compared to 538 SuperARVs for the 1 million word Wall Street Journal   Penn Treebank set  , and 791 for the 37 million word training set of the WSJ continuous speech recognition task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training set size",
                "does not grow significantly",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Resource Management corpus",
                "moderate-sized",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Negation was processed in a similar way as previous works  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "negation",
                "processed in a similar way as previous works",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "previous works",
                "similar way",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "According to this model, when translating a stringf in the source language into the target language, a string e is chosen out of all target language strings e if it has the maximal probability given f  : e = arg maxe {Pr } = arg maxe {Pr Pr } where Pr  is the translation model and Pr  is the target language model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "stringf",
                "has the maximal probability",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Pr ",
                "is the translation model",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  represent chunking as tagging problem and the CoNLL2000 shared task   is now the standard evaluation task for chunking English.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunking",
                "standard evaluation task",
                "APPLICABILITY",
                "positive",
                0.75
            ],
            [
                "CoNLL2000 shared task",
                "now the standard",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This results in two forbidden alignment structures, shown in Figure 1, called inside-out transpositions in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment structures",
                "forbidden",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "transpositions",
                "called inside-out",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Maximum Entropy Modeling     and Support Vector Machine     were used to build the classifiers in our solution.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy Modeling",
                "was used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Support Vector Machine",
                "was used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "All conditions were optimized using BLEU   and evaluated using both BLEU and Translation Edit Rate    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "optimized using BLEU",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU",
                "evaluated using BLEU",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "While the research in statistical machine translation   has made significant progress, most SMT systems   relyonparallel corpora toextract translation entries.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "has made significant progress",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "SMT systems",
                "rely on parallel corpora",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, Hindle   used cooccurrences between verbs and their subjects and objects, and proposed a similarity metric based on mutual information, but no exploration concerning the effectiveness of other kinds of word relationship is provided, although it is extendable to any kinds of contextual information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hindle's method",
                "cooccurrences between verbs and their subjects and objects",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "proposed metric",
                "based on mutual information",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We evaluated the translation quality using case-insensitive BLEU metric  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metric",
                "case-insensitive",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "s a basis mapping function  we used a generalisation of the one used by Grefenstette   and Lin  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generalisation of the one used by Grefenstette and Lin",
                "basis mapping function",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Grefenstette and Lin",
                "used by",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "11 This low agreement ratio is also re ected in a measure called the  statistic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ratio",
                "low",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "statistic",
                "called the statistic",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "robust mforrmatlon extractlon, and readlly-avmlable on-hne NLP resources These techtuques and resources allow us to create a richer indexed source of Imgmstlc and domain knowledge than other frequency approaches Our approach attempts to apprommate text dlscourse structure through these multlple layers of mformatlon, ohtinned from automated methods m contrast to labor-lntenslve, discourse-based approaches Moreover, our planned training methodology will also allow us to explmt thin productlve infrastructure m ways whlch model human performance whde avoidmg hand-crafting domain-dependent rules of the knowledge-based approaches Our ultlmate goal m to make our summarlzatlon system scalable and portable by learning summarization rules from easily extractable text features 2 System Description Our summarization system DlmSum consmts of the Summarization Server and the Summarlzatzon Chent The Server extracts features   from a document using various robust NLP techmques, described In Sectzon 2 1, and combines these features   to basehne multiple combinations of features, as described m Section 2 2 Our work m progress to automattcally tram the Feature Combiner based upon user and apphcatlon needs m presented in Section 2 2 2 The Java-based Chent, which wdl be dmcnssed In Section 4, provides a graphical user interface   for the end user to cnstomlze the summamzatlon preferences and see multiple views of generated sumInarles 2.1 Extracting Stlmmarization Features In this section, we describe how we apply robust NLP technology to extract summarization features Our goal IS to add more mtelhgence to frequencybased approaches, to acqmre domain knowledge In a more automated fashion, and to apprommate text structure by recogmzing sources of dmcourse cohesion and coherence 2.1.1 Going Beyond a Word Frequency-based summarization systems typically use a single word stnng as a umt for counting frequencies Whde such a method IS very robust, it ignores the semantic content of words and their potential membership m multi-word phrases For example, zt does not dmtmgumh between \"\"bill\"\" m \"\"Bdl Table 1 Collocations with \"\"chlps\"\" {potato tortdla corn chocolate b~gle} chips {computer pentmm Intel macroprocessor memory} chips {wood oak plastlc} cchlps bsrgmmng clups blue clups mr chips Clmton\"\" and \"\"bill\"\" in \"\"reform bill\"\" This may introduce noise m frequency counting as the same strmgs are treated umformly no matter how the context may have dmamblguated the sense or regardless of membership in multl-word phrases For DlrnSum, we use term frequency based on tf*Idf   to derive ssgnature words as one of the summarization features If single words were the sole basra of countmg for our summarization application, nome would be introduced both m term frequency and reverse document frequency However, recent advances in statmtlcal NLP and information extraction make it possible to utilize features which go beyond the single word level Our approach is to extract multi-word phrases automatlcally with high accuracy and use them as the basic unit in the summarization process, including frequency calculation Ftrst, just as word association methods have proven effective m lemcal analysis, e g  , we are exploring whether frequently occurring Collocatlonal reformation can improve on simple word-based approaches We have preprocessed about 800 MB of LA tlmes/Wastnngton Post newspaper articles nsmg a POS tagger   and derived two-word noun collocations using mutual information The.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Tagging can also be done using maximum entropy modeling  : a maximum entropy tagger, called MXPOST, was developed by Ratnaparkhi    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MXPOST",
                "was developed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "maximum entropy tagger",
                "called MXPOST",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "6.1 Evaluation of Translation Performance We use the BLEU score   to evaluate our systems.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "to evaluate our systems",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Much work has been performed on learning to identify and classify polarity terms   or a negative sentiment  ) and exploiting them to do polarity classification  , Turney  , Kim and Hovy  , Whitelaw et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "polarity terms",
                "identify and classify",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "polarity classification",
                "exploiting them to do",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "3 Results and Analysis Hall   shows that the oracle parsing accuracy of a k-best edge-factored MST parser is considerably higher than the one-best score of the same parser, even when k is small.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "k-best edge-factored MST parser",
                "considerably higher",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "one-best score of the same parser",
                "small",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Following the perspective of  , a minimal set of phrase blocks with lengths   where either m or n must be greater than zero results in the following types of blocks: 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase blocks",
                "with lengths where either m or n must be greater than zero",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "types of blocks",
                "1.",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Chunks can be represented with bracket structures but alternatively one can use a tagging representation which classifies words as being inside a chunk  , outside a chunk   or at a chunk boundary    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Chunks",
                "bracket structures or tagging representation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tagging representation",
                "classifies words as inside/outside/at a chunk boundary",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4 Structural Correspondence Learning SCL     is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Structural Correspondence Learning",
                "recently proposed",
                "INNOVATION",
                "neutral",
                0.7
            ],
            [
                "uses unlabeled data",
                "from both source and target domain",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Class based models   distinguish between unobserved cooccurrences using classes of \"\"similar\"\" words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "using classes of similar words",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "cooccurrences",
                "distinguish between unobserved",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Evidence have shown that by exploiting the constraint of so-called \"\"one sense per discourse,\"\"   and the strategy of bootstrapping  , it is possible to boost coverage, while maintaining about the same level of precision.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "constraint",
                "one sense per discourse",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "strategy",
                "bootstrapping",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "ch   shows that setting those weights should take into account the evaluation metric by which the MT system will eventually be judged",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights",
                "should take into account the evaluation metric",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "MT system",
                "will eventually be judged",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The 1000-best lists are augmented with IBM Model-1   scores and then rescored with a second set of MET parameters.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MET parameters",
                "second set of",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IBM Model-1 scores",
                "augmented with",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Until now, we have defined BestLossk, a to be the minimum of the loss given that the kth feature is updated an optimal amount: BestLossk, amin d LogLossUpda,k,d In this section we sketch a different approach, based on results from Collins, Schapire, and Singer  , which leads to an algorithm very similar to that for ExpLoss in Figures 3 and 4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BestLossk",
                "defined as the minimum of the loss given that the kth feature is updated an optimal amount",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "very similar to that for ExpLoss",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Collins and Roark   used the averaged perceptron  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron",
                "averaged",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "perceptron",
                "averaged",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "hunks as a separate level have also been used in Collins   and Ratnaparkhi  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hunks",
                "as a separate level",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "hunks",
                "used in Collins and Ratnaparkhi",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he initial phase relies on a parser that draws on the SPECIALIST Lexicon   and the Xerox Part-of-Speech Tagger   to produce an underspecified categorial analysis",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "draws on SPECIALIST Lexicon and Xerox Part-of-Speech Tagger",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "categorial analysis",
                "underspecified",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.3.4 Word Translation Probability Estimation Many methods are used to estimate word translation probabilities from unparallel or parallel bilingual corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "used to estimate",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "many methods",
                "are used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Motivated by the fact that non-syntactic phrases make non-trivial contribution to phrase-based SMT, the tree sequencebased translation model is proposed   that uses tree sequence as the basic translation unit, rather than using single sub-tree as in the STSG.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tree sequencebased translation model",
                "uses tree sequence as the basic translation unit",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "single sub-tree",
                "in the STSG",
                "INNOVATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "The corpus used for training our models was on the order of 100,000 words, whereas that used by   was around 1,000 times this size.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "on the order of 100,000 words",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "corpus",
                "1,000 times this size",
                "METHODOLOGY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "ne conclusion that we can draw is that at present the additional word features used in Ratnaparkhi   looking at words more than one position away from the current do not appear to be helping the overall performance of the models",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word features",
                "do not appear to be helping",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "additional word features",
                "used in Ratnaparkhi looking at words more than one position away from the current",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To make things worse, languages are non-isomorphic, i.e., there is no 1to-1 mapping between tree nodes, thus in practice one has to use more expressive formalisms such as synchronous tree-substitution grammars  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tree nodes",
                "non-isomorphic",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "formalisms",
                "more expressive",
                "METHODOLOGY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "The flow using non-local features in two-stage architecture 2.4 Results We employ BIOE1 label scheme for the NER task because we found it performs better than IOB2 on Bakeoff 2006   NER MSRA and CityU corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BIOE1 label scheme",
                "performs better",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "IOB2",
                "performs worse",
                "PERFORMANCE",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "2.1 Minimum Error Rate Training The predominant approach to reconciling the mismatch between the MAP decision rule and the evaluation metric has been to train the parameters  of the exponential model to correlate the MAP choice with the maximum score as indicated by the evaluation metric on a development set with known references  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "exponential model",
                "parameters to correlate the MAP choice with the maximum score",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "development set with known references",
                "on",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Finally, Zhang and Clark   achieve an SF of 95.90% and a TF of 91.34% by 10-fold cross validation using CTB data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SF",
                "95.90%",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "TF",
                "91.34%",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "With the exception of  , most unsupervised work on PP attachment is based on superficial analysis of the unlabeled corpus without the use of partial parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unsupervised work",
                "based on superficial analysis of the unlabeled corpus",
                "METHODOLOGY",
                "negative",
                0.7
            ],
            [
                "PP attachment",
                "without the use of partial parsing",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "4.1.1 Lexical co-occurrences Lexical co-occurrences have previously been shown to be useful for discourse level learning tasks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexical co-occurrences",
                "useful for discourse level learning tasks",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "co-occurrences",
                "previously been shown",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Despite ME theory and its related training algorithm   do not set restrictions on the range of feature functions1, popular NLP text books   and research papers   seem to limit them to binary features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ME theory",
                "do not set restrictions on the range of feature functions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "feature functions",
                "limit them to binary features",
                "METHODOLOGY",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "tandard data sets for machine learning approaches to this task were put forward by Ramshaw and Marcus  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data sets",
                "put forward",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "data sets",
                "standard",
                "APPLICABILITY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson  , and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tag distribution",
                "uniform",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "tag distribution",
                "skewed",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "We chose to train maximum entropy models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy models",
                "to train",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "maximum entropy models",
                "",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Much previous work on unsupervised grammar induction has used gold-standard partof-speech tags  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "previous work",
                "used gold-standard part-of-speech tags",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "unsupervised grammar induction",
                "has used",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our systems use both corpus-based and knowledge-based approaches: Maximum Entropy    is a corpus-based and supervised method based on linguistic features; ME is the core of a bootstrapping algorithm that we call re-training inspired  This paper has been partially supported by the Spanish Government   under project number TIC-2003-7180 and the Valencia Government   under project number CTIDIB-2002-151 by co-training  ; Relevant Domains     is a resource built from WordNet Domains   that is used in an unsupervised method that assigns domain and sense labels; Specification Marks    exploits the relations between synsets stored in WordNet   and does not need any training corpora; Commutative Test    , based on the Sense Discriminators device derived from EWN  , disambiguates nouns inside their syntactic patterns, with the help of information extracted from raw corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy",
                "corpus-based and supervised method",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Specification Marks",
                "exploits the relations between synsets stored in WordNet",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It is often argued that the ability to translate discontiguous phrases is important to modeling translation  , and it may be that this explains the results.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ability",
                "to translate discontiguous phrases",
                "INNOVATION",
                "neutral",
                0.7
            ],
            [
                "results",
                "may be that this explains",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Wed like to learn the number of paradigm classes from the data, but doing this would probably require extending adaptor grammars to incorporate the kind of adaptive statesplitting found in the iHMM and iPCFG  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "adaptor grammars",
                "extend to incorporate adaptive statesplitting",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "iHMM and iPCFG",
                "found",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In   cooccurrence analyses augmented with syntactic parsing is used for the purpose of word classification.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "cooccurrence analyses",
                "augmented with syntactic parsing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word classification",
                "used for the purpose of",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The Inversion Transduction Grammar The Inversion Transduction Grammar of Wu   can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous context-free grammar productions",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Inversion Transduction Grammar",
                "generative process",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "grammar productions",
                "synchronous context-free",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The Penn Treebank annotation   was chosen to be the first among equals: it is the starting point for the merger and data from other annotations are attached at tree nodes.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank annotation",
                "starting point",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tree nodes",
                "attached at",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Other possibilities for the weighting include assigning constant one or the exponential of the final score etc. One of the advantages of the proposed phrase training algorithm is that it is a parameterized procedure that can be optimized jointly with the trans82 lation engine to minimize the final translation errors measured by automatic metrics such as BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "proposed phrase training algorithm",
                "is a parameterized procedure",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parameterized procedure",
                "can be optimized jointly",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "In a factored translation model other factors than surface form can be used, such as lemma or part-of-speech  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "other factors",
                "such as lemma or part-of-speech",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "surface form",
                "can be used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Selectional preferences are estimated using grammatical collocation information from the British National Corpus  , obtained with the Word Sketch Engine    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "British National Corpus",
                "obtained with the Word Sketch Engine",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "grammatical collocation information",
                "estimated",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The mapping typically is made to try to give the most favorable mapping in terms of accuracy, typically using a greedy assignment  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mapping",
                "try to give the most favorable mapping",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "accuracy",
                "typically using a greedy assignment",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "what does student want to write your Figure 3: A derivation tree of lexicalized parse trees, such as the distinction of arguments/modifiers and unbounded dependencies  , are elegantly represented in derivation trees.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "derivation trees",
                "elegantly represented",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "lexicalized parse trees",
                "are represented",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Examples of such early work include  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "early work",
                "such",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "work",
                "include",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "This is the strategy that is usually adopted in other phrase-based MT approaches  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "strategy",
                "usually adopted",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase-based MT approaches",
                "other",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "The definitions of the phrase and lexical translation probabilities are as follows  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase and lexical translation probabilities",
                "are as follows",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "definitions",
                "are",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Furthermore, they extended WSD to phrase sense disambiguation    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSD",
                "to phrase sense disambiguation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "WSD",
                "extended",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he last line shows the results of Ramshaw and Marcus     with the same train/test data",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ramshaw and Marcus",
                "results",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "train/test data",
                "same",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  considered some location constrains in meeting summarization evaluation, which utilizes speaker information to some extent.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "location constrains",
                "considered",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "speaker information",
                "utilizes to some extent",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The release has implementations for BLEU  , WER and PER error criteria and it has decoding interfaces for Phramer and Pharaoh.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "error criteria",
                "BLEU, WER and PER",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "decoding interfaces",
                "for Phramer and Pharaoh",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Church and Hanks   thus emphasize the importance of human judgment used in conjunction with these tools",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "human judgment",
                "used in conjunction with these tools",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "importance",
                "emphasize",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Based on this theoretical cornerstone, Cahill and van Genabith   presented a PCFG-based chart generator using wide-coverage LFG approximations automatically extracted from the Penn-II treebank.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PCFG-based chart generator",
                "using wide-coverage LFG approximations",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Among various language modeling approaches, ngram modeling has been widely used in many applications, such as speech recognition, machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ngram modeling",
                "has been widely used",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "speech recognition",
                "machine translation",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction The goal of this study has been to automatically extract a large set of hyponymy relations, which play a critical role in many NLP applications, such as Q&A systems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hyponymy relations",
                "play a critical role",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "NLP applications",
                "many",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Model 4 of   is also a first-order alignment model   like the HMM, trot includes also fertilities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 4",
                "is a first-order alignment model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Model 4",
                "includes fertilities",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The lexicalized PCFG that sits behind Model 2 of   has rules of the form P ~ LnLn-I\"\"'\"\" LIHRI\"\"\"\".Rn-IRn   S  NP  VP  NNP I Apple MD VP   VB PRT  NP  I \\[ I buy RP NNP I I out Microsoft Figure 1: A sample sentence with parse tree.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Starting from a word-based alignment for each pair of sentences, the training for the algorithm accepts all contiguous bilingual phrase pairs   whose words are only aligned with each other  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "all contiguous bilingual phrase pairs whose words are only aligned with each other",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignment",
                "word-based",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ur method was applied to 23 million words of the WSJ that were automatically tagged with Ratnaparkhi's maximum entropy tagger   and chunked with the partial parser CASS  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ratnaparkhi's maximum entropy tagger",
                "automatically tagged",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "CASS",
                "partial parser",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Typically, the local context around the 215 word to be sense-tagged is used to disambiguate the sense  , and it is common for linguistic resources such as WordNet  , or bilingual data   to be employed as well as more longrange context.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linguistic resources",
                "WordNet",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "context",
                "longrange",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Because of this, Wu   and Zens and Ney   introduced a normal form ITG which avoids this over-counting.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG",
                "normal form",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ITG",
                "avoids over-counting",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Similarly to classical NLP tasks such as text chunking   and named entity recognition  , we formulate mention detection as a sequence classification problem, by assigning a label to each token in the text, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mention detection",
                "sequence classification problem",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "token",
                "starts a specific mention, is inside a specific mention, or is outside any mentions",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 A cept is defined as the set of target words connected to a source word  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "set",
                "target words connected to a source word",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "target words",
                "connected to a source word",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Like Collins  , the decoder is the same for both the perceptron and the log-linear parsing models; the only change is the method for setting the weights.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "the same for both",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "method for setting the weights",
                "is the only change",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The first, Powells method, was advocated by Och   when MERT was first introduced for statistical machine translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MERT",
                "was introduced",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Powells method",
                "advocated by Och",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "One way of resolving query ambiguities is to use the statistics, such as mutual information  , to measure associations of query terms, on the basis of existing corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistics",
                "to measure associations of query terms",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "corpora",
                "on the basis of existing",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For a comparison, we also include the ROUGE-1 Fscores   of each system output against the human compressed sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE-1 Fscores",
                "of each system output against the human compressed sentences",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "human compressed sentences",
                "comprehensive evaluation",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "summarization  , paraphrasing  , natural language generation  , and language modeling  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "summarization, paraphrasing, natural language generation, and language modeling",
                "techniques",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "To this extent, we cast the supersense tagging problem as a sequence labeling task and train a discriminative Hidden Markov Model  , based on that of Collins  , on the manually annotated Semcor corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hidden Markov Model",
                "discriminative",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Semcor corpus",
                "manually annotated",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The first work in SMT, done at IBM  , developed a noisy-channel model, factoring the translation process into two portions: the translation model and the language model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT",
                "done at IBM",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation model and the language model",
                "factoring the translation process",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "all-Street Journal   Sections 15-18 and 20 were used by Ramshaw and Marcus   as training and test data respectively for evaluating their base-NP chunker",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Sections 15-18 and 20",
                "used as training and test data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "base-NP chunker",
                "evaluating",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The feature weights were tuned on a heldout development set so as to maximize an equally weighted linear combination of BLEU and 1-TER   using the minimum error training algorithm on a packed forest representation of the decoders hypothesis space  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "tuned",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "minimum error training algorithm",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ahill and van Genabith   attain 98.2% coverage and a BLEU score of 0.6652 on the standard WSJ test set  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSJ test set",
                "standard",
                "APPLICABILITY",
                "neutral",
                1.0
            ],
            [
                "BLEU score",
                "0.6652",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Some of these methods make use of prior knowledge in the form of an existing thesaurus  , while others do not rely on any prior knowledge  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "thesaurus",
                "existing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "do not rely on any prior knowledge",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We consider three learning algorithms, namely, the C4.5 decision tree induction system  , the RIPPER rule learning algorithm  , and maximum entropy classification  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "C4.5 decision tree induction system",
                "decision tree induction system",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "RIPPER rule learning algorithm",
                "rule learning algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This method was preferred against other related methods, like the one introduced in  , since it embeds all the available semantic information existing in WordNet, even edges that cross POS, thus offering a richer semantic representation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "embeds all the available semantic information",
                "METHODOLOGY",
                "positive",
                0.85
            ],
            [
                "semantic representation",
                "richer",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To model p , we use a standard loglinear approach: p   exp bracketleftBiggsummationdisplay i ifi  bracketrightBigg where each fi  is a feature function, and weights i are set using Ochs algorithm   to maximize the systems BLEU score   on a development corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights i",
                "are set using Ochs algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "system's BLEU score",
                "maximize on a development corpus",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "McClosky et al   use sections 2-21 of the WSJ PennTreebank as seed data and between 50K to 2,500K unlabeled NANC corpus sentences as self-training data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSJ PennTreebank",
                "sections 2-21",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "NANC corpus",
                "sentences as self-training data",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "indle   classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classified nouns",
                "on the basis of co-occurring patterns of subjectverb and verb-object pairs",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "co-occurring patterns",
                "of subjectverb and verb-object pairs",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This model shares some similarities with the stochastic inversion transduction grammars   presented by Wu in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "stochastic inversion transduction grammars",
                "presented by Wu",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "similarities",
                "shares",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ntroduction Automatic word alignment   is a vital component of all statistical machine translation   approaches",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "component",
                "vital",
                "METHODOLOGY",
                "positive",
                0.75
            ],
            [
                "approaches",
                "statistical machine translation",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Alternatively, order is modelled in terms of movement of automatically induced hierarchical structure of sentences  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "modelled",
                "automatically induced hierarchical structure of sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "movement",
                "of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Collins   gives convergence proofs for the methods; Collins   directly compares the boosting and perceptron approaches on a named entity task; and Collins and Duffy   use a reranking approach with kernels, which allow representations of parse trees or labeled sequences in very-high-dimensional spaces.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "gives convergence proofs for",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "boosting and perceptron approaches",
                "directly compares",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he feature weights for the overall translation models were trained using Och?s   minimum-error-rate training procedure",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Och's minimum-error-rate training procedure",
                "was trained using",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Based on annotation differences in the datasets   and a bug in their system  , their results are inconclusive.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "datasets",
                "annotation differences",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "results",
                "inconclusive",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  1 Introduction Dependency grammar has a long tradition in syntactic theory, dating back to at least Tesni~re's work from the thirties3 Recently, it has gained renewed attention as empirical methods in parsing are discovering the importance of relations between words  ), which is what dependency grammars model explicitly do, but context-free phrasestructure grammars do not.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Dependency grammar",
                "has a long tradition",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "dependency grammars",
                "model explicitly do",
                "METHODOLOGY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "One judge annotated allarticles in four datasets of the Wall Street Journal Treebank corpus     as well as thecorpusofWall Street Journal articles used in    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal Treebank corpus",
                "annotated",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpus of Wall Street Journal articles",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Firstly, we run GIZA++   on the training corpus in both directions and then apply the ogrow-diag-finalprefinement rule   to obtain many-to-many word alignments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "on the training corpus in both directions",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ogrow-diag-finalprefinement rule",
                "to obtain many-to-many word alignments",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Rule-based taggers   use POS-dependent constraints defined by experienced linguists.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS-dependent constraints",
                "defined by experienced linguists",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "linguists",
                "experienced",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "In future work we plan to experiment with richer representations, e.g. including long-range n-grams  , class n-grams  , grammatical features  , etc'.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "representations",
                "richer",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "features",
                "grammatical",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Since a handmade thesaurus is not slfitahle for machine use, and expensive to compile, automatical construction of~a thesaurus has been attempted using corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "thesaurus",
                "not suitable for machine use",
                "METHODOLOGY",
                "negative",
                0.7
            ],
            [
                "compilation",
                "expensive",
                "METHODOLOGY",
                "negative",
                0.75
            ]
        ]
    },
    {
        "text": "Introduction Word sense disambiguation has long been one of the major concerns in natural language processing area  , whose aim is to identify the correct sense of a word in a particular context, among all of its senses defined in a dictionary or a thesaurus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "concerns",
                "major concerns",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "sense",
                "correct sense",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "They developed a simple heuristic function for Model 2 from   which was non admissible.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 2",
                "simple heuristic function",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "non admissible",
                "",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "5 Reliability of Annotations 5.1 The Kappa Statistic To measure the reliability of annotations we used the Kappa statistic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotations",
                "measured using Kappa statistic",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "Kappa statistic",
                "used to measure reliability",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "e also trained an HMM aligner as described in DeNero and Klein   and used the posteriors of this model as features",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMM aligner",
                "as described in DeNero and Klein",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "posteriors of this model",
                "used as features",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The dependency trees induced when each rewrite rule in an i-th order LCFRS distinguish a unique head can similarly be characterized by being of gap-degree i, so that i is the maximum number of gaps that may appear between contiguous substrings of any subtree in the dependency tree  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LCFRS",
                "distinguish a unique head",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dependency trees",
                "being of gap-degree i",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The bigram translation probability t2  specifies the likelihood that target word f is to follow f in a phrase generated by source word e. 170 2.1 Properties of the Model and Prior Work The formulation of the WtoP alignment model was motivated by both the HMM word alignment model   and IBM Model-4 with the goal of building on the strengths of each.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bigram translation probability t2",
                "specifies the likelihood that target word f is to follow f",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "WtoP alignment model",
                "was motivated by both the HMM word alignment model and IBM Model-4",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 is based on several realvalued feature functions fi . Their computation is based on the so-called IBM Model-1  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature functions",
                "based on IBM Model-1",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "feature functions",
                "realvalued",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The component features are weighted to minimize a translation error criterion on a development set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "component features",
                "minimize a translation error criterion",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "development set",
                "on a development set",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "lish nouns first appeared in Hindle  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hindle",
                "appeared in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "lish nouns",
                "first",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "2 The Tagger We used Ratnaparkhi's maximum entropybased POS tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ratnaparkhi's maximum entropy-based POS tagger",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We have used the optimal experiment configurations that we had obtained from the fourth experiment series for processing the complete   data set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "experiment configurations",
                "optimal",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "data set",
                "complete",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Johnson   evaluates both estimation techniques on the Bayesian bitag model; Goldwater and Griffiths   emphasize the advantage in the MCMC approach of integrating out the HMM parameters in a tritag model, yielding a tagging supported by many different parameter settings.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "estimation techniques",
                "evaluates",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "MCMC approach",
                "emphasize advantage",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "Previous studies have shed light on the predictability of the next unix command that a user will enter  , the next keystrokes on a small input device such as a PDA  , and of the translation that a human translator will choose for a given foreign sentence  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unix command",
                "predictability",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "translation",
                "translation that a human translator will choose",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Recentworkconsidersadamagedtagdictionary by assuming that tags are known only for words that occur more than once or twice  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tag dictionary",
                "assuming tags are known only for words that occur more than once or twice",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tags",
                "known only for words that occur more than once or twice",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "K-best suffix arrays have been used in autocomplete applications  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "suffix arrays",
                "used in autocomplete applications",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "suffix arrays",
                "have been used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4 Experimental Set-up For the experiments, we use the WSJ portion of the Penn tree bank  , using the standard train/development/test splits, viz 39,832 sentences from 2-21 sections, 2416 sentences from section 23 for testing and 1,700 sentences from section 22 for development.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSJ portion of the Penn tree bank",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "train/development/test splits",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The agreement on identifying the boundaries of units, using the AK statistic discussed in  , was AK BP BMBL  ; the agreement on features  was follows: Attribute AK Value utype .76 verbed .9 finite .81 subject .86 NPs Our instructions for identifying NP markables derive from those proposed in the MATE project scheme for annotating anaphoric relations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "AK statistic",
                "discussed in ",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "instructions for identifying NP markables",
                "derive from those proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Averaging parameters is a way to reduce overfitting for perceptron training  , and is applied to all our experiments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "reduce overfitting",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "experiments",
                "all",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The PT grammar 2 was extracted from the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PT grammar",
                "extracted from Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank",
                "standard dataset",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ee Weeds and Weir   for an overview of other measures",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Weeds and Weir",
                "overview",
                "INNOVATION",
                "neutral",
                0.5
            ],
            [
                "measures",
                "other",
                "APPLICABILITY",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "2.4 METEOR Given a pair of strings to compare  , METEOR   first creates a word alignment between the two strings.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Unlike a full blown machine translation task  , annotators and systems will not be required to translate the whole context but just the target word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "target word",
                "just the target word",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "annotators and systems",
                "not required",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Many approaches have been proposed for semisupervised learning in the past, including: generative models  , self-learning  , cotraining  , informationtheoretic regularization  , and graphbased transductive methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "many",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "graphbased transductive methods",
                "graphbased",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The upper envelope is a convex hull and can be inscribed with a convex polygon whose edges are the segments of a piecewise linear function in   : EnvD4fD5 AG max eC8C AWa D4e,fD5 A0  A4 bD4e,fD5 :  C8 RB4   726 Score  Error count  0 0 e1 e2 e5 e6 e8 e1e 2 e3 e4 e5e6e 7 e8 Figure 1: The upper envelope   for a set of lines is the convex hull which consists of the topmost line segments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "piecewise linear function",
                "segments of a piecewise linear function",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "upper envelope",
                "convex hull",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "So far research in automatic opinion recognition has primarily addressed learning subjective language  , identifying opinionated documents   and sentences  , and discriminating between positive and negative language  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "primarily addressed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "opinion language",
                "subjective",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5.2 Maximum Entropy Model We use the Maximum Entropy   Model   for our classification task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy Model",
                "for our classification task",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Maximum Entropy Model",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Other techniques have tried to quantify the generalizability of certain features across domains  , or tried to exploit the common structure of related problems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "quantify the generalizability",
                "PERFORMANCE",
                "neutral",
                0.85
            ],
            [
                "structure",
                "common structure of related problems",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Machine Translation using Inversion Transduction Grammar The Inversion Transduction Grammar   of Wu   is a type of context-free grammar   for generating two languages synchronously",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Inversion Transduction Grammar",
                "of Wu",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "context-free grammar",
                "for generating two languages synchronously",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our features were based on those in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "based on those in",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "features",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4An adaptation of the averaged perceptron algorithm   is used to tune the model parameters.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model parameters",
                "tune the model",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "averaged perceptron algorithm",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Although the BLEU   score from Finnish to English is 21.8, the score in the reverse direction is reported as 13.0 which is one of the lowest scores in 11 European languages scores  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "21.8",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "score",
                "one of the lowest scores",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "We then scored each query pair   in this subset using the log-likelihood ratio   between q1 and q2, which measures the mutual dependence within the context of web search queries  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "query pair",
                "measures the mutual dependence",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "log-likelihood ratio",
                "between q1 and q2",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We present results in the form of search error analysis and translation quality as measured by the BLEU score   on the IWSLT 06 text translation task  1, comparing Cube Pruning with our two-pass approach.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "measured by",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Cube Pruning",
                "compared with",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Since the lexical translations and dependency paths are typically not labeled in the English corpus, a given pair must be counted fractionally according to its posterior probability of satisfying these conditions, given models of contextual translation and English parsing.3 3Similarly, Jansche   imputes missing trees by using comparable corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "models of contextual translation and English parsing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Jansche",
                "imputes missing trees",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Consider the lexical model pw , defined following Koehn et al  , with a denoting the most frequent word alignment observed for the rule in the training set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pw",
                "defined following Koehn et al",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word alignment",
                "most frequent",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2.1 The Standard Machine Learning Approach We use maximum entropy   classification   in conjunction with the 33 features described in Ng   to acquire a model, PC, for determining the probability that two mentions, mi and mj, are coreferent.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy",
                "classification",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "33 features",
                "described in Ng",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In phrase-based SMT systems  , foreign sentences are firstly segmented into phrases which consists of adjacent words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based SMT systems",
                "are segmented into phrases",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "phrases",
                "consist of adjacent words",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Then the word alignment is refined by performing growdiag-final method  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word alignment",
                "is refined",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "growdiag-final method",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Furthermore, use of the self-training techniques described in   raise this to 87.8%   again without any use of labeled Brown data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "self-training techniques",
                "described in raise this to 87.8%",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "labeled Brown data",
                "without any use of",
                "LIMITATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In such a process, original phrase-based decoding   does not take advantage of any linguistic analysis, which, however, is broadly used in rule-based approaches.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based decoding",
                "does not take advantage of any linguistic analysis",
                "METHODOLOGY",
                "negative",
                0.7
            ],
            [
                "rule-based approaches",
                "broadly used",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The tagger described in this paper is based on the standard Hidden Markov Model architecture  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "The most common answer is component testing, where the component is compared against a standard of goodness, usually the Penn Treebank for English  , allowing a numerical score of precision and recall  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "component",
                "compared against a standard of goodness",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank for English",
                "usually the",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "In order to calculate a global score or probability for a transition sequence, two systems used a Markov chain approach  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Markov chain approach",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "transition sequence",
                "calculate a global score or probability",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A similar observation was made in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "observation",
                "made",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "similar",
                "similar",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "6 Penn Discourse Treebank   The Penn Discourse TreeBank   annotates discourse relations over the Wall Street Journal corpus  , in terms of discourse connectives and their arguments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Discourse TreeBank",
                "annotates discourse relations",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Wall Street Journal corpus",
                "over",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example,   paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "automatic evaluation metrics",
                "like BLEU",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "paraphrase references",
                "to make them closer to the system translation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "On the other hand, purely statistical systems   extract discriminating MWUs from text corpora by means of association measure regularities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MWUs",
                "by means of association measure regularities",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical systems",
                "extract discriminating",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "These include scripts for creating alignments from a parallel corpus, creating phrase tables and language models, binarizing phrase tables, scripts for weight optimization using MERT  , and testing scripts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "scripts",
                "for creating alignments from a parallel corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "scripts",
                "for weight optimization using MERT",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our strategy for choosing heads is similar to the one in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "strategy",
                "similar to the one in [1]",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "heads",
                "choosing",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The result in Wu   implies that for the special case of Bracketing ITGs, the time complexity of the algorithm is parenleftbigT3V 3parenrightbig where T and V are the lengths of the two sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "time complexity parenleftbigT3V 3parenrightbig",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "time complexity",
                "is parenleftbigT3V 3parenrightbig",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "It is a natural extension of the Viteri>i algorithm for those languages that do not have delimiters between words, and it can generate N-best morphological analysis hypotheses, like tree trellis search.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Viteri>i algorithm",
                "natural extension",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "N-best morphological analysis hypotheses",
                "can generate",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model   or edit distance alignments allowing shifts  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pair-wise alignment algorithms",
                "based on symmetric alignments from a HMM alignment model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "edit distance alignments",
                "allowing shifts",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We measured the accuracy of the POS tagger trained in three settings: Original: The tagger is trained with the union of Wall Street Journal   section of Penn Treebank  , GENIA, and Penn BioIE.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger",
                "trained with the union of Wall Street Journal section of Penn Treebank, GENIA, and Penn BioIE",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tagger",
                "trained in three settings",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In each case the input to the network is a sequence of tag-word pairs.2 We report results for two different vocabulary sizes, varying in the frequency with which tag-word pairs must 2We used a publicly available tagger   to provide the tags.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tag-word pairs",
                "sequence of tag-word pairs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tagger",
                "publicly available",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "The number of weights wi is 3 plus the number of source languages, and they are trained using minimum error-rate training   to maximize the BLEU score   on a development set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights",
                "trained using minimum error-rate training",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "BLEU score",
                "maximize on a development set",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "After that, we used three types of methods for performing a symmetrization of IBM models: intersection, union, and refined methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "for performing a symmetrization of IBM models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "refined methods",
                "refined methods",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the first, a separate language model is trained on each column of the database and these models are then used to segment and label a given text sequence  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "language model",
                "trained on each column of the database",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "models",
                "used to segment and label a given text sequence",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In Section 3 we then describe the probabilistic taxonomy learning model introduced by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilistic taxonomy learning model",
                "introduced by",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "taxonomy learning model",
                "described",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Secondly, we explore the possibility of designing complementary similarity metrics that exploit linguistic information at levels further than lexical. Inspired in the work by Liu and Gildea  , who introduced a series of metrics based on constituent/dependency syntactic matching, we have designed three subgroups of syntactic similarity metrics.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "complementary similarity metrics",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "syntactic similarity metrics",
                "designed",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In the thriving area of research on automatic analysis and processing of product reviews  , little attention has been paid to the important task studied here  assessing review helpfulness.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "thriving",
                "INNOVATION",
                "positive",
                0.7
            ],
            [
                "review helpfulness",
                "important task",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Given a manually compiled lexicon containing words and their relative frequencies Ps , the best segmentationfJ1 is the one that maximizes the joint probability of all words in the sentence, with the assumption that words are independent of each other1: fJ1 = argmax fprimeJprime1 Pr   argmax fprimeJprime1 Jprimeproductdisplay j=1 Ps , where the maximization is taken over Chinese word sequences whose character sequence is cK1 . 2.2 Translation system Once we have segmented the Chinese sentences into words, we train standard alignment models in both directions with GIZA++   using models of IBM-1  , HMM   and IBM-4  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ps",
                "containing words and their relative frequencies",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM-1",
                "standard alignment models",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  introduced a polynomial-time solution for the alignment problem based on synchronous binary trees.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "solution",
                "polynomial-time",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignment problem",
                "synchronous binary trees",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "4.3 Corpora The evaluations of the different models were carried out on the Penn Wall Street Journal corpus   for English, and the Tiger treebank   for German.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Wall Street Journal corpus",
                "for English",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Tiger treebank",
                "for German",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We used the implementation of MaxEnt classifier described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "implementation of MaxEnt classifier",
                "described in  ",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "implementation of MaxEnt classifier",
                "implementation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Here, we extract part-of-speech tags from the Collins parsers output   for section 23 instead of reinventing a tagger.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins parsers",
                "output",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tagger",
                "reinventing",
                "METHODOLOGY",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "Additionally, our approach makes it possible to do inference in just about twice the inference time with a single sequential CRF; in contrast, approaches like Gibbs Sampling that model the dependencies directly can increase inference time by a factor of 30  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "inference time",
                "just about twice",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "approaches like Gibbs Sampling",
                "can increase inference time by a factor of 30",
                "PERFORMANCE",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "This approach attempts to improve translation quality by optimizing an automatic translation evaluation metric, such as the BLEU score  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "automatic translation evaluation metric",
                "BLEU score",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "translation quality",
                "improve",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "4 Sub Translation Combining For sub translation combining, we mainly use the best-first expansion idea from cube pruning   to combine subtranslations and generate the whole k-best translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Sub Translation Combining",
                "best-first expansion idea from cube pruning",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "whole k-best translations",
                "generate",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In particular, since we treat each individual speech within a debate as a single document, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "speech within a debate",
                "as a single document",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "positive and negative documents",
                "distinguishing between",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "notable exception is the work of Kim and Hovy  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work of Kim and Hovy",
                "notable exception",
                "INNOVATION",
                "negative",
                0.8
            ],
            [
                "work of Kim and Hovy",
                "notable exception",
                "APPLICABILITY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "A tree sequence to string rule  174 A tree-sequence to string translation rule in a forest is a triple <L, R, A>, where L is the tree sequence in source language, R is the string containing words and variables in target language, and A is the alignment between the leaf nodes of L and R. This definition is similar to that of   except our treesequence is defined in forest.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tree-sequence to string rule",
                "in a forest",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "definition",
                "similar to that of",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In computational linguistics, our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs   inter alia).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pattern discovery procedure",
                "extends over previous approaches",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "previous approaches",
                "use surface patterns as indicators",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Note that the minimum error rate training   uses only the target sentence with the maximum posterior probability whereas, here, the whole probability distribution is taken into account.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "minimum error rate training",
                "uses only the target sentence with the maximum posterior probability",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "probability distribution",
                "taken into account",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The results we obtained on the CoNLL03 test set were consistent with what was reported in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CoNLL03 test set",
                "consistent with",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "reported results",
                "what was reported",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Full discriminative parser training faces signi cant algorithmic challenges in the relationship between parsing alternatives and feature values   and in computing feature expectations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing alternatives",
                "relationship between",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "feature expectations",
                "computing",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Research have also been made into alternatives to the current log-linear scoring model such as discriminative models with millions of features  , or kernel based models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-linear scoring model",
                "alternatives to",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "discriminative models",
                "with millions of features",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Table 1 shows the percentage of agreement in classifying words as compounds or non-compounds   for each language and the Kappa score   obtained from it, and the percentage of words for which also the decomposition provided was identical  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "classifying as compounds or non-compounds",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Kappa score",
                "obtained from it",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  MEDLINE DT JJ VBN NNS IN DT NN NNS VBP The oncogenic mutated forms of the ras proteins are RB JJ CC VBP IN JJ NN NN . constitutively active and interfere with normal signal transduction . Figure 1: Part of speech-tagged sentences from both corpora we investigate its use in part of speech   tagging  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ras proteins",
                "oncogenic mutated forms",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "normal signal transduction",
                "interfere with",
                "PERFORMANCE",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "Resources specifying the relations among lexical items such as WordNet   and HowNet     have inspired the work of many researchers in NLP  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet",
                "have inspired",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "HowNet",
                "have inspired",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "MTTK provides implementations of various alignment, models including IBM Model-1, Model-2  , HMM-based word-to-word alignment model   and HMM-based word-to-phrase alignment model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment models",
                "IBM Model-1, Model-2, HMM-based word-to-word alignment model, HMM-based word-to-phrase alignment model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignment models",
                "various",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The third baseline, COMP is the document compression system developed by Daume III and Marcu  , which compresses documents by cutting out constituents in a combined syntax and discourse tree.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "COMP",
                "document compression system",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "cutting out constituents",
                "in a combined syntax and discourse tree",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Finally, it would be nice to merge some of the approaches by   and   with the ideas of semi-supervised learning introduced here, since they seem orthogonal in at least some aspects  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "nice to merge",
                "INNOVATION",
                "positive",
                0.75
            ],
            [
                "ideas",
                "orthogonal in at least some aspects",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Previous work on POS tagging of unknown words has proposed a number of features based on prefixes and suffixes and spelling cues like capitalization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "based on prefixes and suffixes and spelling cues",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "spelling cues",
                "like capitalization",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Finally, the loglikelihood ratios test     is applied on each set of pairs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "loglikelihood ratios test",
                "is applied",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "set of pairs",
                "each",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction In statistical machine translation, output translations are evaluated by their similarity to human reference translations, where similarity is most often measured by BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "similarity",
                "is most often measured by BLEU",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "translations",
                "are evaluated by their",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "An alternative to tercom, considered in this paper, is to use the Inversion Transduction Grammar   formalism   which allows one to view the problem of alignment as a problem of bilingual parsing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Inversion Transduction Grammar",
                "allows one to view the problem of alignment as a problem of bilingual parsing",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tercom",
                "alternative to",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This algorithm is proved to converge   in the separable case  .1 Thatis,ifthereexistweightvectorU  ,   , and R   that satisfy: i,y  Y|xi|  U  U  , i,y  Y|xi| ||  ||  R, the number of updates is at most R2/2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "converge",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "number of updates",
                "at most R2/2",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The elementary trees were extracted from the parse trees in sections 02-21 of the Wall Street Journal in Penn Treebank  , which is transformed by using parent-child annotation and left factoring  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parse trees",
                "in sections 02-21 of the Wall Street Journal in Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parent-child annotation and left factoring",
                "used by Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It was also included in the DUC 2004 evaluation plan where summary quality was automatically judged using a set of n-gram word overlap metrics called ROUGE  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation plan",
                "DUC 2004",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "summary quality",
                "automatically judged using n-gram word overlap metrics",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "With the help of the kappa coefficient   proposes to represent the dialog success independently from the task intrinsic complexity, thus opening the way to task generic comparative evaluation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa coefficient",
                "represent the dialog success independently from the task intrinsic complexity",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "task generic comparative evaluation",
                "opening the way to",
                "INNOVATION",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "upertags Part-of-speech disambiguation techniques     are often used prior to parsing to eliminate   the part-of-speech ambiguity",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Part-of-speech disambiguation techniques",
                "are often used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "part-of-speech ambiguity",
                "eliminate",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Some researchers apply shallow or partial parsers   to acquiring specific patterns from texts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsers",
                "shallow or partial",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "patterns",
                "acquiring specific",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "veraged Perceptron Algorithm 5 Experiments We evaluate our method on both Chinese and English syntactic parsing task with the standard division on Chinese Penn Treebank Version 5.0 and WSJ English Treebank 3.0   as shown in Table 1",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Perceptron Algorithm 5",
                "standard division",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank Version 5.0",
                "WSJ English Treebank 3.0",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Some examples of POS taggers that perform reasonably well on monolingual text of each language can be found in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS taggers",
                "perform reasonably well",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "monolingual text of each language",
                "can be found in",
                "LIMITATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Otherwise they are generated along with the words using the same approach as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "using the same approach",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "words",
                "generated along with",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The data set is same as in Section 5.1, except that we also parsed the English-side using a variant of the Collins   parser, and then extracted 24.7M tree-to-string rules using the algorithm of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data set",
                "same as in Section 5.1",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "of ",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In fact, a limitation of the experiments described in this paper is that the loglinear weights for the glass-box techniques were optimized for BLEU using Ochs algorithm  , while the linear weights for 55 black-box techniques were set heuristically.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "loglinear weights",
                "optimized for BLEU using Ochs algorithm",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "linear weights",
                "set heuristically",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "2 IBM Model 4 In this paper we focus on the translation model defined by IBM Model 4  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 4",
                "defined",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translation model",
                "by IBM Model 4",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The PropBank corpus adds a semantic layer to parse trees from the Wall Street Journal section of the Penn Treebank II corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PropBank corpus",
                "adds a semantic layer",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank II corpus",
                "section of",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The algorithm is exactly the same as the one described in   to find the most probable part-of-speech sequence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "the same as the one described",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "part-of-speech sequence",
                "most probable",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We found that the deletion of lead parts did not occur very often in our summary, unlike the case of Jing and McKeown  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "deletion of lead parts",
                "did not occur very often",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "case of Jing and McKeown",
                "unlike",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Labeled data for one domain might be used to train a initial classifier for another   domain, and then bootstrapping can be employed to learn new knowledge from the new domain  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifier",
                "initial classifier",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "bootstrapping",
                "learn new knowledge",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Features identified using distributional similarity have previously been used for syntactic and semantic disambiguation   and to develop lexical resources from corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Features",
                "used for syntactic and semantic disambiguation",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "corpora",
                "develop lexical resources",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There bas recently been work in the detection of semantically related nouns via, for example, shared argument structures  , and shared dictionary definition context  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "shared argument structures",
                "via shared argument structures",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dictionary definition context",
                "shared dictionary definition context",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Following the guidelines of the workshop we built baseline systems, using the lower-cased Europarl parallel corpus  , GIZA++  , Moses  , and the SRI LM toolkit   to build 5-gram LMs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SRI LM toolkit",
                "build 5-gram LMs",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Europarl parallel corpus",
                "lower-cased",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For the factored language models, a feature-based word representation was obtained by tagging the text with Rathnaparkis maximum-entropy tagger   and by stemming words using the Porter stemmer  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Rathnaparkis maximum-entropy tagger",
                "tagging the text",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Porter stemmer",
                "stemming words",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Using a vector-based topic identification process  , these keywords are used to determine a set of likely values   for that attribute.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "vector-based topic identification process",
                "used to determine",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "keywords",
                "likely values for that attribute",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The training data for the French/English data set is taken from the LDC Canadian Hansard data set, from which the word aligned data   was also taken.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LDC Canadian Hansard data set",
                "taken from",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "word aligned data",
                "was also taken",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The weights of feature functions are optimized to maximize the scoring measure  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature functions",
                "optimized to maximize",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "scoring measure",
                "",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts  , as well as in similar predicateargument structure contexts  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "large",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "strategies",
                "based on statistical analyses",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  The heuristics in Section 6 are designed specifically to find the interesting features in that featureless desert.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "heuristics",
                "designed specifically to find interesting features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "featureless desert",
                "featureless",
                "APPLICABILITY",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "corpora and corpus query tools has been particularly significant in the area of compiling and developing lexicographic materials   and in the area of creating various kinds of lexical resources, such as WordNet   and FrameNet  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora and corpus query tools",
                "significant",
                "INNOVATION",
                "positive",
                0.7
            ],
            [
                "WordNet and FrameNet",
                "lexical resources",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our question here is not only what this relation looks like  ), but also how it compares to the reliability of other metrics.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "reliability of other metrics",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "relation",
                "looks like",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This technique is called system combination  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system combination",
                "is called",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "For the efficiency of minimum-errorrate training  , we built our development set   using sentences not exceeding 50 characters from the NIST MT-02 evaluation test data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NIST MT-02 evaluation test data",
                "not exceeding 50 characters",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "minimum-errorrate training",
                "efficient",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Unlike Yarowsky  , we use automatic collection of seeds.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "seeds",
                "automatic collection of seeds",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Yarowsky",
                "",
                "INNOVATION",
                "negative",
                0.75
            ]
        ]
    },
    {
        "text": "4 The Experiment For our experiment, we used a tree-bank grammar induced from sections 2-21 of the Penn Wall Street Journal text  , with section 22 reserved for testing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tree-bank grammar",
                "induced from sections 2-21 of the Penn Wall Street Journal text",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "section 22",
                "reserved for testing",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "With this constraint, each of these binary trees is unique and equivalent to a parse tree of the canonical-form grammar in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "binary trees",
                "unique and equivalent to a parse tree of the canonical-form grammar",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "canonical-form grammar",
                "in ",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "his second expression is similar to that used in  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "expression",
                "similar to that used in",
                "METHODOLOGY",
                "neutral",
                0.5
            ],
            [
                "expression",
                "second",
                "PERFORMANCE",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "Finally, another soft-constraint approach that can also be viewed as coming from the data-driven side, adding syntax, is taken by Riezler and Maxwell  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Riezler and Maxwell",
                "soft-constraint approach",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "syntax",
                "adding",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "6 Related Work A description of the IBM models for statistical machine translation can be found in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM models",
                "for statistical machine translation",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "description",
                "found in ",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "RIDF is like MI, but different References Church, K. and P. Hanks  Word association norms, mutual information, and lexicography Computational Linguistics, 16:1, pp.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "RIDF",
                "like MI but different",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "References Church and P Hanks",
                "Word association norms, mutual information, and lexicography",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This can be done automatically with unparsed corpora  , from parsed corpora such as Marcus et al.'s   Treebank   or manually as was done for COMLEX  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "unparsed corpora",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "COMLEX",
                "was done manually",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Thus, our generative model is a quasi-synchronous grammar, exactly as in  .3 When training on target sentences w, therefore, we tune the model parameters to maximize notsummationtextt p  as in ordinary EM, but rather 3Our task here is new; they used it for alignment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "quasi-synchronous grammar",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "task",
                "new",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "e also tested the flat syntactic feature set proposed in Luo and Zitouni  s work",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "flat syntactic feature set",
                "proposed in Luo and Zitouni's work",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "feature set",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Polarity orientation identification has many useful applications, including opinion summarization   and sentiment retrieval  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "identification",
                "many useful applications",
                "APPLICABILITY",
                "positive",
                0.8
            ],
            [
                "identification",
                "opinion summarization and sentiment retrieval",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This can either be semi-supervised parsing, using both annotated and unannotated data   or unsupervised parsing, training entirely on unannotated text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data",
                "annotated and unannotated",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "training",
                "entirely on unannotated text",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4.1 Experimental Setup Like several previous work  , Pang and Lee  , Whitelaw et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Experimental Setup",
                "Like several previous work",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Pang and Lee",
                "several previous work",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "One obvious first approach would be to run a simpler model for the first iteration  , which tends to be very recall oriented) and use this to see subsequent iterations of the more complex model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "simpler model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "iterations",
                "more complex model",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The closest work is that of Jing and McKeown   and Daume III and Marcu  , in which multiple sentences are processed, with fragments within them being recycled to generate the novel generated text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "processed multiple sentences with fragments recycled",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "generated text",
                "novel generated text",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We then train IBM models   using the GIZA++ package  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM models",
                "using the GIZA++ package",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We used the NP data prepared by Ramshaw and Marcus  , hereafter RM95.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NP data",
                "prepared by Ramshaw and Marcus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Ramshaw and Marcus",
                "RM95",
                "LIMITATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4 Related Work The automatic extraction of English subcategorization frames has been considered in  , where a procedure is presented that takes untamed text as input and generates a list of verbal subcategorization frames.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "procedure",
                "takes untamed text as input and generates a list of verbal subcategorization frames",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "verbal subcategorization frames",
                "list of",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Hindle   uses a mutual-information based metric derived from the distribution of subject, verb and object in a large corpus to classify nouns.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metric",
                "mutual-information based",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "large corpus",
                "large",
                "APPLICABILITY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Proceedings of the 40th Annual Meeting of the Association for cently, semantic resources have also been used in collocation discovery  , smoothing and model estimation   and text classi cation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic resources",
                "used in collocation discovery",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "semantic resources",
                "used in smoothing and model estimation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Model weights were trained separately for all 3 systems using minimum error rate training to maximize BLEU   on the development set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model weights",
                "trained separately for all 3 systems",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "training",
                "minimize error rate",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Ratnaparkhi, 1996), a single inconsistency in a test set tree will very likely yield a zero percent parse accuracy for the particular test set sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "test set",
                "will very likely yield a zero percent parse accuracy",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "inconsistency",
                "will very likely yield a zero percent parse accuracy",
                "LIMITATION",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "Evaluation 6.1 Evaluation at the Token Level This section compares translation model estimation methods A, B, and C to each other and to Brown et al.'s   Model 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model estimation methods",
                "A, B, and C",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Brown et al.'s Model 1",
                "compared to",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "A new automatic metric METEOR   uses stems and synonyms of the words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "METEOR",
                "uses stems and synonyms of the words",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "METEOR",
                "new automatic metric",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, the Penn Treebank   was annotated with skeletal syntactic structure, and many syntactic parsers were evaluated and compared on the corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "annotated with skeletal syntactic structure",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "syntactic parsers",
                "evaluated and compared",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Using the components of the row-vector bm as feature function values for the candidate translation em  , the system prior weights  can easily be trained using the Minimum Error Rate Training described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system prior weights",
                "can easily be trained",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Minimum Error Rate Training",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Finally, we compare against the mapping from WordNet to the Oxford English Dictionary constructed in  , equivalent to clustering based solely on the OED feature.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet",
                "constructed in",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Oxford English Dictionary",
                "equivalent to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The bracketed portions of Figure 1, for example, show the base NPs in one sentence from the Penn Treebank Wall Street Journal   corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank Wall Street Journal corpus",
                "is a corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "base NPs",
                "are shown",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In the present work, the approach taken by Turney   is used to derive such values for selected phrases in the text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "taken by Turney",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "values",
                "derived for selected phrases",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Indeed, as Sinopalnikova and Pavel   note, Deese   was the first to conduct linguistic analyses of word association norms, such as measurements of semantic similarity based on his convictions that similar words evoke similar word association responsesan approach that is somewhat reminiscent of Church and Hanks   notion of mutual information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Deese",
                "approach",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Church and Hanks",
                "notion of mutual information",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "To generate phrase pairs from a parallel corpus, we use the \"\"diag-and\"\" phrase induction algorithm described in  , with symmetrized word alignments generated using IBM model 2  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase induction algorithm",
                "diag-and",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "symmetrized word alignments",
                "generated using IBM model 2",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Metrics based on syntactic similarities such as the head-word chain metric    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "head-word chain metric",
                "based on syntactic similarities",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "metrics",
                "such as",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We examine Structural Correspondence Learning     for this task, and compare it to several variants of Self-training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Structural Correspondence Learning",
                "for this task",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Self-training",
                "variants",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Another way of doing the parameter estimation for this matching task would have been to use an averaged perceptron method, as in Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameter estimation",
                "averaged perceptron method",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "method",
                "as in Collins",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As was demonstrated in  , even a minimal set of local explicit features achieves results which are non-significantly different from a carefully chosen set of explicit features, given the language independent definition of locality described in section 2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "local explicit features",
                "achieves results which are non-significantly different",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "definition of locality",
                "described in section 2",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Parametertuningwasdonewithminimum error rate training  , which was used to maximize BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Parametertuning",
                "minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU",
                "maximize",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The WordNet::Similarity package   implements this distance measure and was used by the authors.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet::Similarity package",
                "implements this distance measure",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "WordNet::Similarity package",
                "was used by the authors",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "It can be applied to complicated models such IBM Model-4  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model-4",
                "complicated",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "models",
                "can be applied to",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3.2 Automatic ROUGE Evaluation ROUGE measuresthen-grammatchbetween system generated summaries and human summaries.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE measure",
                "then-gram match",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "human summaries",
                "human",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "4 Global Transliteration Modeling In global transliteration modeling, we directly model the agreement function between f and e. We follow   and consider the global feature representation : F * E *  R d . 613 Each global feature corresponds to a condition on the pair of strings.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "global feature representation",
                "F * E * R d",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "pair of strings",
                "corresponds to a condition",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We parsed the TimeEval data using MSTParser v0.2  , which is trained with all Penn Treebank   without dependency label.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MSTParser v0.2",
                "trained with all Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dependency label",
                "without",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In most recent parsing work the history consists of a small number of manually selected features  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "manually selected",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "history",
                "small number of",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The measures vary from simple edge-counting to attempt to factor in peculiarities of the network structure by considering link direction, relative path, and density, such as vector, lesk, hso, lch, wup, path, res, lin and jcn  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "network structure",
                "considering link direction, relative path, and density",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "measures",
                "simple edge-counting",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "It was first cast as a classification problem by Ramshaw and Marcus  , as a problem of NP chunking.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunking",
                "problem of NP",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Ramshaw and Marcus",
                "first cast as a classification problem",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Indeed, in the II scenario,   reported no improvement of the base parser for small   and large   seed datasets respectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "base parser",
                "no improvement",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "seed datasets",
                "small and large",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "There are also attempts at a more fine-grained analysis of accuracy, targeting specific linguistic constructions or grammatical functions  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linguistic constructions",
                "targeting specific",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "accuracy",
                "fine-grained analysis",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "6 Evaluation 6.1 Data The data used for our comparison experiments were developed as part of the OntoNotes project  , which uses the WSJ part of the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "OntoNotes project",
                "uses",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank",
                "part of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Ramshaw and Marcus  used transformation-based learning, an error-driven learning technique introduced by Eric Bn11 , to locate chunks in the tagged corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transformation-based learning",
                "error-driven learning technique",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tagged corpus",
                "locating chunks",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The baseline system is based on the synchronous binarization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseline system",
                "based on synchronous binarization",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "synchronous binarization",
                "the synchronous binarization",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Moreover, in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain, we have performed the study on two parsing models: one based on a context-free variant of tree-adjoining grammars  , the Probabilistic Lexicalized Tree Insertion Grammar   formalism  , and Collinss Model 2 parser  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Probabilistic Lexicalized Tree Insertion Grammar",
                "formalism",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Collinss Model 2 parser",
                "parser",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We use the beam search technique of   to search the space of all hypotheses.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "beam search technique",
                "search the space of all hypotheses",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "hypotheses",
                "all",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "154 2 Translation Models 2.1 Standard Phrase-based Model Most phrase-based translation models   rely on a pre-existing set of word-based alignments from which they induce their parameters.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "They are not used in LN, but they are known to be useful for WSD  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "they",
                "useful",
                "PERFORMANCE",
                "positive",
                0.7
            ],
            [
                "WSD",
                "not used in LN",
                "APPLICABILITY",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "5 Datasets For evaluation we selected two domain adaptation datasets: spam   and sentiment  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "datasets",
                "two domain adaptation datasets",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "domain adaptation datasets",
                "spam and sentiment",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "a65 The rest of the factors denote distorsion probabilities  , which capture the probability that words change their position when translated from one language into another; the probability of some French words being generated from an invisible English NULL element  , etc. See   or   for a detailed discussion of this translation model and a description of its parameters.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "probability that words change their position",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parameters",
                "description of its parameters",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As a learning algorithm for our classification model, we used Maximum Entropy  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy",
                "learning algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "classification model",
                "used",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ROUGE-L   This measure evaluates summaries by longest common subsequence   defined by Equation 4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE-L",
                "evaluates summaries by longest common subsequence",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "longest common subsequence",
                "defined by Equation 4",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "First, we show how one can use an existing statistical translation model   in order to automatically derive a statistical TMEM.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical translation model",
                "existing",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "TMEM",
                "derive",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "im and Hovy   predict the results of an election by analyzing forums discussing the elections",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "elections",
                "discussing the elections",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "results",
                "predict",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "However, after several advances in tasks such as automatic tagging of text with high level semantics such as parts-of-speech  , named-entities  , sentence-parsing  , etc. , there is increasing hope that one could leverage this information into IR techniques.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tasks",
                "automatic tagging of text with high level semantics",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IR techniques",
                "leverage this information",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Syntactic context information is used   to compute term similarities, based on which similar words to a particular word can directly be returned.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Syntactic context information",
                "used to compute term similarities",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "similar words",
                "can directly be returned",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Typically, frequency information for rare words in the training data is used to estimate parameters for unknown words  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "frequency information",
                "is used to estimate parameters",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parameters",
                "for unknown words",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  also worked on one of our data sets.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data sets",
                "one of our",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "data sets",
                "worked on",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Research in the first category aims to identify specific types of nonanaphoric phrases, with some identifying pleonastic it  , Lappin and Leass  , Kennedy and Boguraev  ], supervised approaches [e.g., Evans  , Muller  , Versley et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pleonastic phrases",
                "identify specific types",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "supervised approaches",
                "e.g., Evans, Muller, Versley et al.",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We first determine lexical heads of nonterminal nodes by using Bikels implementation of Collins head detection algorithm9  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bikels implementation of Collins head detection algorithm",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Collins head detection algorithm",
                "is implemented by Bikels",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ur study is also different from these previous ones in that measuring the agreement among annotators became an issue  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotators",
                "measuring agreement among annotators became an issue",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "study",
                "is different from previous ones",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It seems nevertheless that all 2Church and Hanks  , Smadja   use statistics in their algorithms to extract collocations from texts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Church and Hanks",
                "use statistics",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "algorithms",
                "extract collocations from texts",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Estimating the degree of semantic relatedness between words in a text is deemed important in numerous applications: word-sense disambiguation  , story segmentation  , error correction  , summarization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic relatedness",
                "estimating the degree of",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word-sense disambiguation",
                "important in numerous applications",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Such techniques are currently being applied in many areas, including language identification, authorship attribution  , text genre classification  , topic identification  , and subjective sentiment classification  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques",
                "being applied in many areas",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "subjective sentiment classification",
                "classification",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this paper we will describe extensions to tile Hidden-Markov alignment model froln   and compare tlmse to Models 1 4 of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hidden-Markov alignment model",
                "froln",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Models 1 4",
                "Models 1 4 of  ",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Also, we used Adwait Ratnaparkhis part-of-speech tagger   to tag unknown words in the test data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Adwait Ratnaparkhi's part-of-speech tagger",
                "tag unknown words in the test data",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "unknown words",
                "tagged",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In our experiments, we used the Hidden Markov Model   tagging method described in \\ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hidden Markov Model tagging method",
                "described in \\.",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tagging method",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Given this, the mutual information ratio   is expressed by Formula 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information ratio",
                "is expressed by Formula 1",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "3 Implementation 3.1 Pronoun resolution model We built a machine learning based pronoun resolution engine using a Maximum Entropy ranker model  , similar with Denis and Baldridges model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy ranker model",
                "similar with Denis and Baldridges model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "pronoun resolution engine",
                "machine learning based",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "First, splitting and merging of sentences  , which seems related to content planning and aggregation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "splitting and merging of sentences",
                "related to content planning and aggregation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "splitting and merging of sentences",
                "seems",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We will briefly review the perceptron algorithm, and its convergence properties  see Collins   for a full description.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron algorithm",
                "see Collins for a full description",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "convergence properties",
                "see Collins for a full description",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "That is, phrases are heuristically extracted from word-level alignments produced by doing GIZA++ training on the corresponding parallel corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "produced by",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word-level alignments",
                "heuristically extracted",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Liang   uses the discriminative perceptron algorithm   to score whole character tag sequences, finding the best candidate by the global score.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "discriminative perceptron algorithm",
                "to score whole character tag sequences",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "global score",
                "best candidate",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Collins et al.  proposed two algorithms for NER by modifying Yarowskys method   and the framework suggested by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Yarowskys method",
                "proposed two algorithms for NER",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "framework suggested by",
                "modifying",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our observation is that this situation is ideal for so-called bootstrapping, co-training, or minimally supervised learning methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "so-called bootstrapping, co-training, or minimally supervised learning",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "situation",
                "ideal",
                "PERFORMANCE",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Predicate argument structures, which consist of complements   and verbs, have also been used in the task of noun classification  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "predicate argument structures",
                "have been used in the task of noun classification",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "complements and verbs",
                "consist of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The principle of our approach is more similar to  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "more similar to",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "principle",
                ".",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We enrich the semantic information available to the classifier by using semantic similarity measures based on the WordNet taxonomy  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic similarity measures",
                "based on WordNet taxonomy",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "classifier",
                "enriched with semantic information",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "And again, we see this insight informing statistical machine translation systems, for instance, in the phrase-based approaches of Och   and Koehn et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based approaches",
                "of Och and Koehn et al",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "statistical machine translation systems",
                "informing",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "3.5 The Experiments We have ran LexTract on the one-millionword English Penn Treebank   and got two Treebank grammars.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LexTract",
                "ran on the one-millionword English Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Treebank grammars",
                "got",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Many probabilistic evaluation models have been published inspired by one or more of these feature types  , but discrepancies between training sets, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature types",
                "make it difficult to compare",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "models",
                "have been published",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One attempt to implement this idea is lexicalization: increasing the information in the POS tag by adding the lemma to it  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicalization",
                "increasing the information in the POS tag by adding the lemma to it",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "POS tag",
                "by adding the lemma",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Such linguistic-preprocessing techniques could 1Various models have been constructed by the IBM team  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linguistic-preprocessing techniques",
                "could be constructed",
                "METHODOLOGY",
                "neutral",
                0.7
            ],
            [
                "IBM team",
                "Various models have been constructed",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "In the early statistical translation model work at IBM, these representations were called cepts, short for concepts  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "representations",
                "cepts",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "cepts",
                "short for concepts",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Then the alignments are symmetrized using a refined heuristic as described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignments",
                "symmetrized using a refined heuristic",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "heuristic",
                "refined",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Inversion Transduction Grammar",
                "of Wu",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "binary-branching",
                "binary-branching",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2 Related Work The Yarowsky algorithm  , originally proposed for word sense disambiguation, makes the assumption that it is very unlikely for two occurrences of a word in the same discourse to have different senses.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Yarowsky algorithm",
                "makes the assumption that it is very unlikely for two occurrences of a word in the same discourse to have different senses",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word sense disambiguation",
                "originally proposed for",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics, conducted three prior word segmentation bakeoffs, in 2003, 2005 and 2006 , which established benchmarks for word segmentation and named entity recognition.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SIGHAN",
                "conducted",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "benchmarks",
                "established",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In  , automatically extracted collocations are judged by a lexicographer.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocations",
                "judged by a lexicographer",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "lexicographer",
                "judged",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3.2 Maximum Entropy ME models implement the intuition that the best model will be the one that is consistent with the set of constrains imposed by the evidence, but otherwise is as uniform as possible  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "as uniform as possible",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "model",
                "consistent with the set of constraints imposed by the evidence",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We then tagged the search queries using a maximum entropy part-of-speech tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "part-of-speech tagger",
                "maximum entropy",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "search queries",
                "using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Some existing resources contain lists of subjective words  ), and some empirical methods in NLP have automatically identified adjectives, verbs, and N-grams that are statistically associated with subjective language  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "resources",
                "contain lists of subjective words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "automatically identified adjectives, verbs, and N-grams",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Linear weights are assigned to each of the transducers features using an averaged perceptron for structure prediction  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transducers features",
                "assigned to",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "perceptron",
                "used for structure prediction",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Finally, we plan to apply the model to other paraphrasing tasks including fully abstractive document summarisation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "to other paraphrasing tasks",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "document summarisation",
                "fully abstractive",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "atnaparkhi   estimates a POS tagging error rate of 3% in the Treebank",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "atnaparkhi",
                "estimates a POS tagging error rate of 3%",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "Treebank",
                "used for evaluation",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Clark   reports results on a corpus containing 12 million terms, Schcurrency1utze   on one containing 25 million terms, and Brown, et al,   on one containing 365 million terms.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "containing 12 million terms",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpus",
                "containing 25 million terms",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "If POS denotes the POS of the English word, we can define the word-to-word distance measure   as POS POS   Ratnaparkhis POS tagger   was used to obtain POS tags for each word in the English sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS",
                "tagger",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Ratnaparkhis POS tagger",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Maximum Entropy models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence but otherwise is as uniform as possible  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "as uniform as possible",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "models",
                "consistent with the set of constraints imposed by the evidence",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The last row shows the results for the feature augmentation algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature augmentation algorithm",
                "results for",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "feature augmentation algorithm",
                "feature augmentation algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Words surrounding the current word have been occasionally used in taggers, such as  , Brills transformation based tagger  , and the HMM model of Lee et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "taggers",
                "occasionally used",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "HMM model of Lee et al.",
                "widely used in taggers",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": " , Pedersen  , Yarowsky and Florian  ) as well as maximum entropy models  , Klein and Manning  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "maximum entropy",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Pedersen, Yarowsky, and Florian",
                "as well as",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Both data were extracted from the Penn Treebank Wall Street Journal   Corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank Wall Street Journal Corpus",
                "extracted from",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "data",
                "extracted",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As our work is based on the first paradigm, we will focus on the works proposed by   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "proposed by and ",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "paradigm",
                "first",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Perceptron Training We optimize feature weights using a modification of averaged perceptron learning as described by Collins  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Perceptron Training",
                "using a modification of averaged perceptron learning",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "feature weights",
                "optimize",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In both cases there 1Alternatively, decisions from the sentence classifier can guide which input is seen by the document level classifier  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentence classifier",
                "guide",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "document level classifier",
                "seen",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We parse the data using the Collins Parser  , and then tag person, location and organization names using the Stanford Named Entity Recognizer  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins Parser",
                "using the Collins Parser",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Stanford Named Entity Recognizer",
                "tagging person, location and organization names",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Such a lexicon can be used, e.g., to classify individual sentences or phrases as subjective or not, and as bearing positive or negative sentiments  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicon",
                "subjective or not",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "sentiments",
                "positive or negative",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4 Building Noun Similarity Lists A lot of work has been done in the NLP community on clustering words according to their meaning in text  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NLP community",
                "a lot of work has been done",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "clustering words according to their meaning",
                "according to their meaning",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The model presented above is based on our previous work  , which bears the same spirit of some other recent work on multitask learning  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "based on our previous work",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "work",
                "same spirit as some other recent work",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "The statistical machine translation approach is based on the noisy channel paradigm and the Maximum-A-Posteriori decoding algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical machine translation approach",
                "based on the noisy channel paradigm and the Maximum-A-Posteriori decoding algorithm",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "Maximum-A-Posteriori decoding algorithm",
                "used",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "2 Related Work Two different approaches have been proposed for Sentence Compression: purely statistical methodologies   and hybrid linguistic/statistic methodologies  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "statistical methodologies",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approaches",
                "linguistic/statistic methodologies",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The X 2 statistic is performing at least as well as G 2, and the results show that the average level of generalization is slightly higher for G 2 than X 2 . This suggests a possible explanation for the results presented here and those in Dunning  : that the X 2 statistic provides a less conservative test when counts in the contingency table are low.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "X 2 statistic",
                "performing at least as well as G 2",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "X 2 statistic",
                "provides a less conservative test when counts in the contingency table are low",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We apply a maximum entropy   model   to this task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "maximum entropy",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "model",
                "apply to this task",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  produced a corpus of 4,000 questions annotated with syntactic trees, and obtained an improvement in parsing accuracy for Bikels reimplementation of the Collins parser   by training a new parser model with a combination of newspaper and question data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser model",
                "combination of newspaper and question data",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Bikels reimplementation of the Collins parser",
                "improvement in parsing accuracy",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The corresponding unlabeled figures are 73.3 and 33.4.3 This confirms the results of previous studies showing that the pseudo-projective parsing technique used by MaltParser tends to give high precision  given that non-projective dependencies are among the most difficult to parse correctly  but rather low recall  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MaltParser",
                "gives high precision",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "MaltParser",
                "tends to give low recall",
                "PERFORMANCE",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "he only difference is that we 5See also work on partial parsing as a task in its own right: Hindle   inter alia",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "task",
                "as a task in its own right",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "work on partial parsing",
                "See also",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We used pointwise mutual information   to obtain these distances.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pointwise mutual information",
                "to obtain these distances",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "3.2 The parsers The parsers that we chose to evaluate are the C&C CCG parser  , the Enju HPSG parser  , the RASP parser  , the Stanford parser  , and the DCU postprocessor of PTB parsers  , based on LFG and applied to the output of the Charniak and Johnson reranking parser.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsers",
                "C&C CCG, Enju HPSG, RASP, Stanford, DCU postprocessor of PTB",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parsers",
                "based on LFG and applied to the output of the Charniak and Johnson reranking parser",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Relative frequencies of word-forms have been used in previous work to detect incorrect affix attachments in Bengali and English  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word-forms",
                "have been used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "previous work",
                "detect incorrect affix attachments",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "On the other hand, works done by   have proposed methodologies to automatically acquire these patterns mostly based on supervised learning to leverage manual work.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methodologies",
                "mostly based on supervised learning",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "manual work",
                "manual work",
                "APPLICABILITY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Many methods have been proposed to measure the co-occurrence relation between two words such as  2   , mutual information  , t-test  , and loglikelihood  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "many have been proposed",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "loglikelihood",
                "such as",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Unlike probabilistic parsing, proposed by  , *also a staff member of Matsushita Electric Industrial Co. ,Ltd., Shinagawa, Tokyo, JAPAN.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilistic parsing",
                "proposed by",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Shinagawa, Tokyo, JAPAN",
                "staff member of Matsushita Electric Industrial Co., Ltd.",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this paper, we modify the method in Albrecht and Hwa   to only prepare human reference translations for the training examples, and then evaluate the translations produced by the subject systems against the references using BLEU score  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "only prepare human reference translations for the training examples",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "translations",
                "produced by the subject systems",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " , and in some cases, to factor the translation problem so that the baseline MT system can take advantage of the reduction in sparsity by being able to work on word stems.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT system",
                "take advantage of the reduction in sparsity",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "word stems",
                "being able to work on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": " .5 6.2 Results We performed experiments using three training algorithms: the averaged perceptron  , log-linear training  , and max-margin training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training algorithms",
                "three",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "perceptron",
                "training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "istory-based models for predicting the next parser action   3",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "history-based models",
                "predicting the next parser action",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parser action",
                "next",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The normalization is visualized as a translation problem where messages in the SMS language are to be translated to normal English using a similar phrase-based statistical MT method  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMS language",
                "translated to normal English",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase-based statistical MT method",
                "similar",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Next, using our feature vector, we applied five different linear classifiers to extract PPI from AIMed: L2-SVM, 1-norm soft-margin SVM  , logistic regression    , averaged perceptron    , and confidence weighted linear classification    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linear classifiers",
                "five different",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "L2-SVM",
                "1-norm soft-margin SVM",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Initial estimates of lexical translation probabilities came from the IBM Model 4 translation tables produced by GIZA++  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 4",
                "translation tables",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "GIZA++",
                "produced",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A few researchers have focused on other aspects of summarization, including single sentence  , paragraph or short document  , query-focused  , or speech  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "summarization",
                "single sentence",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "summarization",
                "query-focused",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The feature weights are tuned using minimum error rate training   to optimize BLEU score on a held-out development set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "tuned using minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU score",
                "optimize on held-out development set",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "n application of the idea of alternative targets can be seen in Kim and Hovys   work on election prediction",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "idea",
                "alternative targets",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "work",
                "on election prediction",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "ero derivation Dolan   pointed out that it is helpful to identify zero-derived noun/verb pairs for such tasks as normalization of the semantics of expressions that are only superficially different",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "zero-derived noun/verb pairs",
                "helpful for normalization of semantics",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "expressions that are only superficially different",
                "semantic normalization",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use the n-best generation scheme interleaved with  optimization as described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-best generation scheme",
                "interleaved with optimization",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "optimization",
                "as described in ",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Performance also degrades when the domain of the test set differs from the domain of the training set, in part because the test set includes more OOV words and words that appear only a few times in the training set    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "test set",
                "includes more OOV words and words that appear only a few times in the training set",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "domain of the training set",
                "differs from the domain of the test set",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A more fine-grained distinction is made by Bean and Riloff   and Vieira and Poesio   to distinguish restrictive from non-restrictive postmodification by ommitting those modifiers that occur between commas, which should not be classified as chain starting.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "modifiers",
                "occur between commas",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "postmodification",
                "distinguish restrictive from non-restrictive",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "e set our space usage to match the 3.08 bytes per n-gram reported in Talbot and Brants   and held out just over 1M unseen n-grams to test the error rates of our models",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "space usage",
                "match the 3.08 bytes per n-gram",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "n-grams",
                "test the error rates of our models",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  and   also report results for semi-supervised learning for these domains.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semi-supervised learning",
                "for these domains",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "results",
                "for semi-supervised learning",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We performed a comparison between the existing CFG filtering techniques for LTAG   and HPSG  , using strongly equivalent grammars obtained by converting LTAGs extracted from the Penn Treebank   into HPSG-style.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CFG filtering techniques",
                "existing",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "LTAGs extracted from the Penn Treebank",
                "into HPSG-style",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In particular, we use a feature augmentation technique recently introduced by Daume III  , and active learning   to perform domain adaptation of WSD systems.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature augmentation technique",
                "recently introduced by Daume III",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "WSD systems",
                "domain adaptation",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This probability is computed using IBMs Model 1  : P  = productdisplay qQ P    P  =  Pml +Pml    Pml  = summationdisplay aA  Pml )   where the probability that the question term q is generated from answer A, P , is smoothed using the prior probability that the term q is generated from the entire collection of answers C, Pml .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBMs Model 1",
                "productdisplay",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "prior probability",
                "smoothed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, both papers propose minimum-risk decoding, and McDonald and Satta   discuss unsupervised learning and language modeling, while Smith and Smith   define hiddenvariable models based on spanning trees.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "minimum-risk decoding",
                "proposes",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "hiddenvariable models",
                "based on spanning trees",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Note that there are some similarities between our two-stage semi-supervised learning approach and the semi-supervised learning method introduced by  , which is an extension of the method described by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semi-supervised learning approach",
                "two-stage",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "semi-supervised learning method",
                "introduced by",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In the BB.N model, as with Model 2 of  , modifying nonterminals are generated conditioning both on the parent P and its head child H. Unlike Model 2 of  , they are also generated conditioning on the previously generated modifying nonterminal, L/-1 or Pq-1, and there is no subcat frame or distance feature.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 2",
                "unlike Model 2 of  ",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "modifying nonterminals",
                "generated conditioning on previously generated modifying nonterminal",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Parsers have been developed for a variety of grammar formalisms, for example HPSG  , LFG  , TAG  , CCG  , and variants of phrase-structure grammar  , including the phrase-structure grammar implicit in the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar formalisms",
                "various",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase-structure grammar",
                "implicit in Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 Experiments and Results All experiments were conducted on the treebanks provided in the shared task  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treebanks",
                "provided in the shared task",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "experiments",
                "conducted",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Word alignment is an important step of most modern approaches to statistical machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "most modern",
                "METHODOLOGY",
                "positive",
                0.7
            ],
            [
                "step",
                "important",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  observe that their predominant sense method is not performing as well for 3We use the Lesk   similarity as implemented by the WordNet::similarity package  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Lesk similarity",
                "as implemented by the WordNet::similarity package",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "predominant sense method",
                "not performing as well",
                "PERFORMANCE",
                "negative",
                0.6
            ]
        ]
    },
    {
        "text": "Agreement is sometimes measured as percentage of the cases on which the annotators agree, but more often expected agreement is taken into account in using the kappa statistic  , which is given by:  = po  pe1  p e   where po is the observed proportion of agreement and pe is the proportion of agreement expected by chance.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa statistic",
                "is given by",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "agreement",
                "expected agreement",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "However, most of the existing models have been developed for English and trained on the Penn Treebank  , which raises the question whether these models generalize to other languages, and to annotation schemes that differ from the Penn Treebank markup.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "For comparison purposes, we consider two different algorithms for our AnswerExtraction module: one that does not bridge the lexical chasm, based on N-gram cooccurrences between the question terms and the answer terms; and one that attempts to bridge the lexical chasm using Statistical Machine Translation inspired techniques   in order to find the best answer for a given question.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "AnswerExtraction module",
                "based on N-gram cooccurrences between question terms and answer terms",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Statistical Machine Translation inspired techniques",
                "find the best answer for a given question",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "ProAlign models P  directly, using a different decomposition of terms than the model used by IBM  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ProAlign models",
                "directly",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "model used by IBM",
                "different decomposition of terms",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "1 Introduction Statistical Machine Translation is a data driven machine translation technique which uses probabilistic models of natural language for automatic translation  ,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Machine Translation",
                "data driven",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probabilistic models",
                "of natural language",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Therefore, other machine learning techniques such as perceptron   could also be applied for this problem.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron",
                "could also be applied",
                "METHODOLOGY",
                "neutral",
                0.5
            ],
            [
                "machine learning techniques",
                "other",
                "METHODOLOGY",
                "neutral",
                0.6
            ]
        ]
    },
    {
        "text": "It combines online Peceptron learning   with a parsing model based on the Eisner algorithm  , extended so as to jointly assign syntactic and semantic labels.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Peceptron learning",
                "with",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Eisner algorithm",
                "extended so as to jointly assign syntactic and semantic labels",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In this paper, we present Phramer, an open-source system that embeds a phrase-based decoder, a minimum error rate training   module and various tools related to Machine Translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based decoder",
                "embeds",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "minimum error rate training module",
                "various tools related to Machine Translation",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "We used the averaged perceptron algorithm   to train the parameters of the model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters of the model",
                "trained using the averaged perceptron algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parameters of the model",
                "trained",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  and   use syntactic markers to increase the significance of the data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data",
                "increase the significance of",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "syntactic markers",
                "use to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.2 Adaptation to Chinese  s algorithm   only resolves certain NLDs with known types of antecedents   at fstructures.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "only resolves certain NLDs with known types of antecedents",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "at fstructures",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Some researchers   classify terms by similarities based on their distributional syntactic patterns.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "terms",
                "distributional syntactic patterns",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "researchers",
                "classify based on",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As a model learning method, we adopt the maximum entropy model learning method  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model learning method",
                "maximum entropy model learning method",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Word alignments traditionally are based on IBM Models 1-5   or on HMMs  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Models 1-5",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "HMMs",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "INTRODUCTION Word associations have been studied for some time in the fields of psycholinguistics  , linguistics  , and more recently, by researchers in natural language processing   using statistical measures to identify sets of associated words for use in various natural language processing tasks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "fields",
                "psycholinguistics, linguistics, and more recently, by researchers in natural language processing",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "statistical measures",
                "identify sets of associated words",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Note that although the source of the data is the same as in Section 5, as Yarowsky   did.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "source of the data",
                "same as in Section 5",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Yarowsky",
                "did",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The problem of choosing an appropria.te level in the h.ierarchy at which to represent a particular noun sense   has been investigated by Resnik  , Li and Abe   and ll,iba,s  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Resnik, Li and Abe",
                "investigated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "hierarchy",
                "at which to represent",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "These categories were automatically generated using the labeled parses in Penn Treebank   and the labeled semantic roles of PropBank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "labeled parses",
                "in Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "labeled semantic roles",
                "of PropBank",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Without removing them, extracted rules cannot be triggered until when completely the same strings appear in a text.4 6 Performance Evaluation We measured the performance of our robust parsing algorithm by measuring coverage and degree of overgeneration for the Wall Street Journal in the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rules",
                "cannot be triggered until when completely the same strings appear",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "performance",
                "degree of overgeneration",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  used iterative scaling algorithms for CRF training, following earlier work on maximumentropy models for natural language  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "iterative scaling",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "models",
                "maximumentropy",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  have introduced a convenient data representation for chunking by converting it to a tagging task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data representation",
                "convenient",
                "METHODOLOGY",
                "positive",
                0.75
            ],
            [
                "tagging task",
                "neutral",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In our Machine %'anslation system, transfer rules are generated automatically from parsed parallel text along the lines of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transfer rules",
                "generated automatically from parsed parallel text",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parallel text",
                "along the lines of  ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.2 Weight optimization A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score   on a development set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "coefficients of the log-linear combination of feature functions",
                "maximize the BLEU score",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "development set",
                "used for optimization",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Statistical parsers trained on the Penn Treebank     produce trees annotated with bare phrase structure labels  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Statistical parsers",
                "trained on the Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase structure labels",
                "bare",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.3 Perceptron learning of feature weights As we saw above, our model is a linear model with the global weight vector w acting as the coefficient vector, and hence various existing techniques can be exploited to optimize w. In this paper, we use the averaged perceptron learning   to optimize w on a training corpus, so that the system assigns the highest score to the correct coordination tree among all possible trees for each training sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "w",
                "as the coefficient vector",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "perceptron learning",
                "to optimize w",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In contrast, the C&C tagger, which is based on that of Ratnaparkhi  , utilizes a wide range of features and a larger contextual window including the previous two tags and the two previous and two following words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "C&C tagger",
                "based on that of Ratnaparkhi",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "features",
                "wide range",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Binarizing the grammars   further increases the size of these sets, due to the introduction of virtual nonterminals.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammars",
                "further increases the size",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "virtual nonterminals",
                "introduction of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Breidt  alsopointedouta coupleof problemsthatmakes extractionfor Germanmoredifficultthanfor English: the stronginflectionfor verbs,the variable word-order,andthepositionalambiguityofthearguments.Sheshowsthatevendistinguishingsubjectsfromobjectsisverydifficultwithoutparsing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "inflection for verbs",
                "strong",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word-order",
                "variable",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Disambiguation of a limited number of words is not hard, and necessary context information can be carefully collected and hand-crafted to achieve high disambiguation accuracy as shown in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "limited number of words",
                "not hard",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "necessary context information",
                "can be carefully collected and hand-crafted",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "7.1.3 Similarity via pagerank Pagerank   is the celebrated citation ranking algorithm that has been applied to several natural language problems from summarization   to opinion mining   to our task of lexical relatedness  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pagerank",
                "celebrated citation ranking algorithm",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "Pagerank",
                "applied to several natural language problems",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The suffixes C* and V* denote the models using incomplete skip-chain edges and vertical sequential edges proposed in  , as shown in Figures 2  and 2 .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "using incomplete skip-chain edges and vertical sequential edges",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "edges",
                "proposed in ",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "More recently, Haghighi and Klein   use the distinction between pronouns, nominals and proper nouns 660 in their unsupervised, generative model for coreference resolution; for their model, this is absolutely critical for achieving better accuracy.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "unsupervised, generative model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "accuracy",
                "better accuracy",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "This is in contrast to work by researchers such as Schiitze and Pedersen  , Brown et al   and Futrelle and Gauch  , where it is often the most frequent words in the lexicon which are clustered, predominantly with the purpose of determining their grammatical classes.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Schiitze and Pedersen",
                "researchers",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Futrelle and Gauch",
                "researchers",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "This approach allows to combine strengths of generality of context attributes as in n-gram models   with their specificity as for binary features in MaxEnt taggers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "context attributes",
                "generality and specificity",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "MaxEnt taggers",
                "binary features",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Abney   notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations, motivating the use of log-linear models   for parse ranking that Johnson and colleagues further developed  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "important problems with the soundness",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "log-linear models",
                "for parse ranking",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "SCL for Discriminative Parse Selection So far, pivot features on the word level were used  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SCL",
                "pivot features on the word level",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "pivot features",
                "were used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Only one word is labeled with the concept; the syntactic head word   is preferred.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic head word",
                "is preferred",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "concept",
                "is labeled with the",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "GIZA++ refined alignments have been used in state-of-the-art phrase-based statistical MT systems such as  ; variations on the refined heuristic have been used by     and by the phrase-based system Moses    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ refined alignments",
                "used in state-of-the-art",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase-based system Moses",
                "used variations on the refined heuristic",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "MAXENT, Zhang Les C++ implementation 8 of maximum entropy modelling  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MAXENT",
                "C++ implementation",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "maximum entropy modelling",
                "implementation",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Evaluation We evaluate translation output using three automatic evaluation measures: BLEU  , NIST  , and METEOR  .5 All measures used were the case-sensitive, corpuslevel versions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation measures",
                "BLEU, NIST, and METEOR",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "measures used",
                "case-sensitive, corpus-level versions",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical representations that are finer-grained than those that can be read off the parsed sentences in treebanks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "stand in sharp contrast",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "partially supervised techniques",
                "have been proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Note that apart from previous work   we use complete skip-chain   edges in hc .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hc",
                "complete skip-chain edges",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "previous work",
                "apart from",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Bean and Riloff   and Uryupina   have already employed a definite probability measure in a similar way, although the way the ratio is computed is slightly different.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probability measure",
                "in a similar way",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "way the ratio is computed",
                "is slightly different",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4 Experiments We evaluated our classifier-based best-first parser on the Wall Street Journal corpus of the Penn Treebank   using the standard split: sections 2-21 were used for training, section 22 was used for development and tuning of parameters and features, and section 23 was used for testing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal corpus",
                "Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sections 2-21",
                "used for training",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The mutual information of a cooccurrence pair, which measures the degree of association between the two words  , is defined as  : P  I  -log 2 P  _ log 2   P P  P  = log 2 P  P  where P  and P  are the probabilities of the events x and y   and P  is the probability of the joint event  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "cooccurrence pair",
                "measures the degree of association",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probability",
                "defined as",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There are accurate parsers available such as Chaniak parser  , Stanford parser   and Berkeley parser  , among which we use the Berkeley parser 2 to help identify the head word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Berkeley parser",
                "help identify the head word",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Chaniak parser",
                "Stanford parser",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "1999) O:98.1% C:98.2% 92.4% 93.1% Ramshaw and Marcus   IOB1:97.37% 91.80% 92.27% Argamon et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ramshaw and Marcus",
                "IOB1",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "Argamon et al",
                "IOB1",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1087 Model 3 of   is a zero-order alignment model like Model 2 including in addition fertility paranmters.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 3",
                "zero-order alignment model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Model 3",
                "including fertility parameters",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Many-to-many alignments can be created by combining two GIZA++ alignments, one where English generates Foreign and another with those roles reversed  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ alignments",
                "one where English generates Foreign and another with those roles reversed",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "GIZA++ alignments",
                "can be created by combining two",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques, including TRP   and Gibbs sampling  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CRFs",
                "containing nonlocal dependencies",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approximate probabilistic inference techniques",
                "including TRP and Gibbs sampling",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The second one is heuristic and tries to use a wordaligned corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "wordaligned corpus",
                "tries to use",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "heuristic",
                "is heuristic",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "hese include the bootstrapping approach   and the context clustering approach  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bootstrapping approach",
                "approach",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "context clustering approach",
                "approach",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For evaluation we have selected a set of 8 metric variants corresponding to seven different families: BLEU    , NIST    , GTM F1-measure    , 1-WER  , 1-PER  , ROUGE     and METEOR3  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metric variants",
                "corresponding to seven different families",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "families",
                "BLEU, NIST, GTM F1-measure, 1-WER, 1-PER, ROUGE, METEOR",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Alignment quality can be further improved when the chunking procedure is based on translation lexicons from IBM Model-1 alignment model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunking procedure",
                "based on translation lexicons from IBM Model-1 alignment model",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "alignment quality",
                "can be further improved",
                "PERFORMANCE",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "We evaluate our results with case-sensitive BLEU-4 metric  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU-4 metric",
                "case-sensitive",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "results",
                "evaluated with",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "2  also presents the method using the averaged perceptron   3For re-ranking problems, Shen and Joshi   proposed a perceptron algorithm that also uses margins.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron",
                "using margins",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "perceptron algorithm",
                "also uses margins",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Interestingly, similar conclusions were also reached in the area of Machine Translation evaluation; in their experiments, Zhang and Vogel   show that adding an additional reference translation compensates the effects of removing 1015% of the testing data, and state that, therefore, it seems more cost effective to have more test sentences but fewer reference translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "testing data",
                "removing 1015%",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "reference translations",
                "fewer",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Feature function scaling factors m are optimized based on a maximum likelihood approach   or on a direct error minimization approach  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature function scaling factors m",
                "optimized",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "approach",
                "maximum likelihood or direct error minimization",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Data Sets for the Experiments 2.1 Coordination Annotation in the PENN TREEBANK For our experiments, we used the WSJ part of the PENN TREEBANK  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSJ part of PENN TREEBANK",
                "used for experiments",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Statistical machine translation   was originally focused on word to word translation and was based on the noisy channel approach  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine translation",
                "based on the noisy channel approach",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical machine translation",
                "originally focused on word to word translation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To support a more rigorous analysis, however, wc have followed Carletta's suggestion   of using the K coettMcnt   as a measure of coder agreement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "K coefficient",
                "as a measure of coder agreement",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "Carletta's suggestion",
                "of using the K coefficient",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Movie-domainSubjectivityDataSet : Pang and Lee   used a collection of labeled subjective and objective sentences in their work on review classification.5 The data set contains 5000 subjective sentences, extracted from movie reviews collected from the Rotten Tomatoes web formed best.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Movie-domainSubjectivityDataSet",
                "contains 5000 subjective sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Pang and Lee",
                "used a collection of labeled subjective and objective sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "7However, the algorithms shares many common points with iterative algorithm that are known to converge and that have been proposed to find maximum entropy probability distributions under a set of constraints  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "many common points with iterative algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithms",
                "converge",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The algorithm is essentially the same as the one introduced in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "essentially the same",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "introduced in",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The two annotators agreed on the annotations of 385/453 turns, achieving 84.99% agreement  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotators",
                "agreed on the annotations",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "agreement",
                "84.99%",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Supervision for simple features has been explored in the literature  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "has been explored in the literature",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "features",
                "simple",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Thus, some research has been focused on deriving different word-sense groupings to overcome the finegrained distinctions of WN  ,  ,  ,  ,   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word-sense groupings",
                "different",
                "INNOVATION",
                "neutral",
                0.7
            ],
            [
                "finegrained distinctions",
                "of WN",
                "PERFORMANCE",
                "negative",
                0.6
            ]
        ]
    },
    {
        "text": "1 Introduction Conditional Maximum Entropy   modeling has received a great amount of attention within natural language processing community for the past decade  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Conditional Maximum Entropy",
                "has received a great amount of attention",
                "INNOVATION",
                "positive",
                0.75
            ],
            [
                "natural language processing community",
                "for the past decade",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Early work employed a diverse range of features in a linear classifier  , including lexical features, syntactic parse features, dependency features and semantic features  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "diverse range",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "linear classifier",
                "employed",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "arowsky   presented an approach that significantly reduces the amount of labeled data needed for word sense disambiguation",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "labeled data",
                "reduces the amount",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "approach",
                "significantly reduces",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Above the phrase level, some models perform no reordering  , some have a simple distortion model that reorders phrases independently of their content  , and some, for example, the Alignment Template System  , hereafter ATS, and the IBM phrase-based system  , have phrase-reordering models that add some lexical sensitivity.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-reordering models",
                "add some lexical sensitivity",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase-based system",
                "IBM",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "2 Architecture of the system The goal of statistical machine translation   is to produce a target sentence e from a source sentence f. It is today common practice to use phrases as translation units   and a log linear framework in order to introduce several models explaining the translation process: e??= argmaxp  = argmaxe {exp )}   The feature functions hi are the system models and the i weights are typically optimized to maximize a scoring function on a development set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system models",
                "are the system models",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "scoring function",
                "on a development set",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "translation lexicon entries were scored according to the log likelihood ratio   (cf.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation lexicon entries",
                "scored according to the log likelihood ratio",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "log likelihood ratio",
                "used for scoring",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We performed experiments with two statistical classifiers: the decision tree induction system C4.5   and the Tilburg Memory-Based Learner    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "C4.5",
                "decision tree induction system",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Tilburg Memory-Based Learner",
                "statistical classifier",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Marcu and Echihabi   lter training instances based on Part-of-Speech   tags, and Soricut and Marcu   use syntactic features to identify sentence-internal RST structure.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training instances",
                "based on Part-of-Speech tags",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentence-internal RST structure",
                "identify using syntactic features",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We generate POS tags using the MXPOST tagger   for English and Chinese, and Connexor for Spanish.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MXPOST tagger",
                "for English and Chinese",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Connexor",
                "for Spanish",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Word alignment is also a required first step in other algorithms such as for learning sub-sentential phrase pairs   or the generation of parallel treebanks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "required first step",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase pairs",
                "generation of parallel treebanks",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "4 POS Tagger and Named Entity Recognizer For the POS tagging task, the tagger is built based on the work of Ratnaparkhi   which was applied for English POS tagging.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ratnaparkhi",
                "was applied for English POS tagging",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "POS Tagger",
                "built based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  follow   in using the noisy channel model, by decomposing the translation decisions modeled by the translation model into different types, and inducing probability distributions via maximum likelihood estimation over each decision type.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "modeled by",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "probability distributions",
                "induced via maximum likelihood estimation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This is referred to as an IOB representation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IOB representation",
                "is referred to as",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "5.3 Evaluation Metric This paper focuses on the BLEU metric as presented in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metric",
                "as presented in  ",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "BLEU metric",
                "as presented",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "forest concept",
                "used in machine translation decoding",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "search space",
                "characterize with integrated language models",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Pereira  , Curran   and Lin   use syntactic features in the vector definition.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic features",
                "in the vector definition",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "vector definition",
                "use syntactic features",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The sequence Ws is thought as a noisy version of WT and the best guess I)d~ is then computed as ^ W~ = argmax P  wT = argmax P P    wT In   they propose a method for maximizing P  by estimating P  and P  and solving the problem in equation 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ws",
                "thought as a noisy version of WT",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "P  and P ",
                "estimating and solving the problem",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We ran GIZA++   on the training corpus in both directions with IBM model 4, and then applied the refinement rule described in   to obtain a many-to-many word alignment for each sentence pair.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "on the training corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IBM model 4",
                "applied",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "While   does not discuss distinguishing more than 2 senses of a word, there is no immediate reason to doubt that the \"\"one sense per collocation\"\" rule   would still hold for a larger number of senses.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "senses of a word",
                "no immediate reason to doubt",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "one sense per collocation rule",
                "would still hold",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Based on the word alignment results, if the aligned target words of any two adjacent foreign linguistic phrases can also be formed into two valid adjacent phrase according to constraints proposed in the phrase extraction algorithm by Och  , they will be extracted as a reordering training sample.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase extraction algorithm by Och",
                "proposed in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "reordering training sample",
                "will be extracted as",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The syntactic parser is the version that is selftrained using 2,500,000 sentences from NANC, and where the starting version is trained only on WSJ data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic parser",
                "self-trained using 2,500,000 sentences from NANC",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "starting version",
                "trained only on WSJ data",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Techniques that analyze n-gram precision such as BLEU score   have been developed with the goal of comparing candidate translations against references provided by human experts in order to determine accuracy; although in our application the candidate translator is a student and not a machine, the principle is the same, and we wish to adapt their technique to our context.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "have been developed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "candidate translator",
                "is a student and not a machine",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Collins   reports 88% labeled precision and recall on individual parse constituents on data from the Penn Treebank, roughly consistent with our finding of at least 13% error.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parse constituents",
                "roughly consistent with our finding of at least 13% error",
                "PERFORMANCE",
                "neutral",
                0.7
            ],
            [
                "precision and recall",
                "88%",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Even for many unsupervised situations, this is available from a lexicon  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicon",
                "is available",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "unsupervised situations",
                "many",
                "APPLICABILITY",
                "neutral",
                0.6
            ]
        ]
    },
    {
        "text": "Others have introduced alternative discriminative training methods  , in which a recurring challenge is scalability: to train many features, we need many train218 ing examples, and to train discriminatively, we need to search through all possible translations of each training example.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "many",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "training methods",
                "scalability",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This preprocessing step can be accomplished by applying the GIZA++ toolkit   that provides Viterbi alignments based on IBM Model-4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ toolkit",
                "provides Viterbi alignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IBM Model-4",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The corpus is aligned in the word level using IBM Model4  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "aligned in the word level using IBM Model4",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IBM Model4",
                "using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The PCFG is a Markov grammar  , i.e. the production probabilities are estimated by decomposing the joint probability of the categories on the right-hand side into a product of conditionals via the chain rule, and making a Markov assumption.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PCFG",
                "estimated by decomposing the joint probability",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "production probabilities",
                "via the chain rule",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ohnson   and Gao & Johnson   assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hidden Markov model",
                "resulting states strongly correlate",
                "PERFORMANCE",
                "neutral",
                0.85
            ],
            [
                "POS tags",
                "are generated",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Most importantly, whereas the one-sense-per-discourse assumption   also applies to discriminating images, there is no guarantee of a local collocational or co-occurrence context around the target image.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "one-sense-per-discourse assumption",
                "applies to discriminating images",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "guarantee of local collocational or co-occurrence context",
                "no guarantee",
                "PERFORMANCE",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "Data-based Methods Data-based approaches extract their information directly from texts and are divided into supervised and unsupervised methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Data-based Methods",
                "extract their information directly from texts",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "supervised and unsupervised",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "It is believed that improvement can be achieved by training the generative model based on a discriminative optimization criteria   in which the training procedure is designed to maximize the conditional probability of the parses given the sentences in the training corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generative model",
                "based on discriminative optimization criteria",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "training procedure",
                "designed to maximize the conditional probability of parses given sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he novel idea presented in Strube & Ponzetto   was to induce a semantic network from the Wikipedia categorization graph to compute measures of semantic relatedness",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wikipedia categorization graph",
                "compute measures of semantic relatedness",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "semantic network",
                "induce from",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In Kanayamas method, the co-occurrence is considered as the appearance in intraor inter-sentential context  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "co-occurrence",
                "considered as the appearance in intraor inter-sentential context",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Kanayama's method",
                "intraor inter-sentential context",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Given the motivations for performing a linguistically-informedextraction whichwere also put forth, among others, by Church and Hanks , Smadja  and Heid    and given the recent developmentof linguisticanalysistools,itseemsplausiblethatthe linguisticstructurewill be more and more taken intoaccountbycollocationextractionsystems.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linguistic structure",
                "will be more and more taken into account",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "linguistic analyst tools",
                "recent development",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2Note that sentence extraction does not solve the problem of selecting and ordering summary sentences to form a coherent There are several approaches to modeling document content: simple word frequency-based methods  , graph-based approaches  , as well as more linguistically motivated techniques  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word frequency-based methods",
                "simple",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "linguistically motivated techniques",
                "more",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm   applied to the   dependency-parsing data structures   for projective dependency structures, or the matrix-tree theorem   for nonprojective dependency structures.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency-parsing data structures",
                "projective dependency structures",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "matrix-tree theorem",
                "nonprojective dependency structures",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parses of bitext",
                "higher quality",
                "METHODOLOGY",
                "positive",
                0.75
            ],
            [
                "self-training",
                "similar to",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "indle   grouped nouns into thesaurus-like lists based on the similarity of their syntactic contexts",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grouped nouns",
                "thesaurus-like lists",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "syntactic contexts",
                "similarity",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5.1 Baseline The baseline system we used for comparison was Pharaoh  , as publicly distributed.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pharaoh",
                "publicly distributed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "baseline system",
                "used for comparison",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Computing the phrase translation probability is trivial in the training corpora, but lexical weighting   needs lexical-level alignment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation probability",
                "trivial",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "lexical weighting",
                "needs lexical-level alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In addition, we developed a word clustering procedure  ) that optimizes conditional word clusters.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word clustering procedure",
                "optimizes conditional word clusters",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word clustering procedure",
                "developed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Many grammars, such as finite-state grammars  , bracket/inversion transduction grammars    , context-free grammar  , tree substitution grammar     and their synchronous versions, have been explored in SMT.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammars",
                "finite-state grammars, bracket/inversion transduction grammars, context-free grammar, tree substitution grammar, and their synchronous versions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "grammars",
                "have been explored in SMT",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This is in contrast to purely statistical systems  , which are difficult to inspect and modify.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical systems",
                "difficult to inspect and modify",
                "METHODOLOGY",
                "negative",
                0.75
            ],
            [
                "statistical systems",
                "difficult to",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "of ACL 1990  , F. Smadja, Retrieving collocations fi'cma text: XTRACT,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "XTRACT",
                "retrieving collocations from text",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "collocations",
                "from text",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ur intuition is that we cannot apply our binarization to Collins  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "binarization",
                "cannot apply",
                "METHODOLOGY",
                "negative",
                0.75
            ],
            [
                "Collins",
                "our",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The probabilities from these back-off levels are interpolated using the techniques in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "back-off levels",
                "are interpolated using",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "techniques",
                "in",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The state of a left-corner parser does capture some linguistic generalizations  , but one might still expect sparse-data problems.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "capture some linguistic generalizations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parser",
                "sparse-data problems",
                "LIMITATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Other methods that have been proposed are one based on using the gain   and an approximate method for selecting informative features  , and several criteria for feature selection were proposed and compared with other criteria  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "criteria for feature selection",
                "compared with other criteria",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Some approaches have used syntax at the core   while others have integrated syntax into existing phrase-based frameworks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "have used syntax at the core",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "phrase-based frameworks",
                "existing",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2 Background 2.1 Phrase Table Extraction Phrasal decoders require a phrase table  , which contains bilingual phrase pairs and 17 scores indicating their utility.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase table",
                "contains bilingual phrase pairs and 17 scores indicating their utility",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase table",
                "requires",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For the named entity features, we used a fairly standard feature set, similar to those described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "named entity features",
                "fairly standard feature set",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "feature set",
                "similar to those described",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There have been many statistical measures which estimate co-occurrence and the degree of association in previous researches, such as mutual information  , t-score  , dice matrix  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information",
                "estimate co-occurrence",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "dice matrix",
                "estimate degree of association",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Specifically, we explore the statistical term weighting features of the word generation model with Support Vector machine  , faithfully reproducing previous work as closely as possible  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word generation model",
                "statistical term weighting features",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "previous work",
                "faithfully reproducing as closely as possible",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": " ), in which translation and language models are trainable separately too.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation and language models",
                "are trainable separately",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation and language models",
                "are trainable",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "At the present time, given the key role of window size in determining the selection and apparent strength of associations under the conventional co-occurrence model highlighted here and in the works of Church et al  , Rapp  , Wang  , and Schulte im Walde & Melinger   we would urge that this is an issue which window-driven studies continue to conscientiously address; at the very least, scale is a parameter which findings dependent on distributional phenomena must be qualified in light of.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "window size",
                "determining the selection and apparent strength of associations",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "scale",
                "dependent on distributional phenomena must be qualified",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our conception of the task is inspired by Ramshaw and Marcus representation of text chunking as a tagging problem   . The information that can be used to train the system appears in columns 1 to 8 of Table 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "task",
                "inspired by Ramshaw and Marcus representation",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Table 1",
                "columns 1 to 8",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "SMT Team   also used minimum error training as in Och  , but used a large number of feature functions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature functions",
                "large number of",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "minimum error training",
                "used as in Och",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "All evaluation is in terms of the BLEU score on our test set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "on our test set",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Table 4 shows the linguistic features of the resulting model compared to the models of Carroll and Rooth  , Collins  , and Charniak  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "compared to the models of Carroll and Rooth, Collins, and Charniak",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "linguistic features",
                "resulting model",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " , Yarowsky  , and Karol & Edelman   where strong reliance on statistical techniques for the calculation of word and context similarity commands large source corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical techniques",
                "commands large source corpora",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpora",
                "strong reliance",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "a  showed that this representation tends to provide better results than the representation used in   where each word is tagged with a tag I , O , or B .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "representation",
                "provides better results",
                "PERFORMANCE",
                "positive",
                0.7
            ],
            [
                "representation",
                "used in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues    , sense preference  , acquisition of selectional restrictions  , lexical preference in generation  , word clustering  , etc. In the majority of these papers, even though the   statistical processing reduces the number of accidental associations, very large corpora   are necessary to obtain reliable data on a \"\"large enough\"\" number of words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocational analysis",
                "for lexical acquisition",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "statistical processing",
                "reduces the number of accidental associations",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": " , these models have non-uniform linguistically motivated structure, at present coded by hand.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "non-uniform linguistically motivated structure",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "structure",
                "coded by hand",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Translation accuracy is measured in terms of the BLEU score  , which is computed here for translations generated by using the tuple n-gram model alone, in the case of Table 2, and by using the tuple n-gram model along with the additional four feature functions described in Section 3.2, in the case of Table 3.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "computed here",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "feature functions",
                "described in Section 3.2",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "All topic models utilize Gibbs sampling for inference  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "topic models",
                "utilize Gibbs sampling for inference",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "A major issue in MaxEnt training is how to select proper features and determine the feature targets  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "proper",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "feature targets",
                "determine",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Finally, since non-projective constructions often involve long-distance dependencies, the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "non-projective constructions",
                "involving long-distance dependencies",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "constituency-based parsing",
                "recovery of empty categories and non-local dependencies",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "ps  is increased by 1110 1/  if the hypothesis ranking k in the system s contains the arc  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hypothesis ranking k",
                "contains the arc",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ps",
                "is increased by 1110 1/ if",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Experimental System and Data HMIHY is a spoken dialogue system based on the notion of call routing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMIHY",
                "spoken dialogue system",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "call routing",
                "notion of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To avoid this problem, we adopt cross-validation training as used in Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training",
                "as used in Collins",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "cross-validation",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ollins   falls back to the POS tagging of Ratnaparkhi   for words seen fewer than 5 times in the training corpus",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tagging",
                "falls back to the POS tagging of Ratnaparkhi",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Ratnaparkhi",
                "for words seen fewer than 5 times in the training corpus",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We also plan to explore other types of reranking features, such as the features used in semantic role labeling    , like the path between a target predicate and its argument, and kernel methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reranking features",
                "used in semantic role labeling",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "kernel methods",
                "features used in",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  compare two tagging frameworks for tagging French, one that is statistical, built upon the Xerox tagger  , and another based on linguistic constraints only.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Xerox tagger",
                "built upon",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "linguistic constraints",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.1 Definition The following set-up, adapted from Collins  , was used for all three discriminative training methods: 266  Training data is a set of input-output pairs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "set-up",
                "adapted from Collins",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "training data",
                "set of input-output pairs",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.2 Automatic Evaluation We first present our soft cohesion constraints effect on BLEU score   for both our dev-test and test sets.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "for both our dev-test and test sets",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "soft cohesion constraints",
                "effect on BLEU score",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "More suitable ways could be bilingual chunk parsing, and refining the bracketing grammar as described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bracketing grammar",
                "as described in  ",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "bilingual chunk parsing",
                "More suitable ways",
                "METHODOLOGY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "The approach presented here has some resemblance to the bracketing transduction grammars   of  , which have been applied to a phrase-based machine translation system in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bracketing transduction grammars",
                "of which have been applied to a phrase-based machine translation system",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "approach",
                "has some resemblance",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this paper, we use IBM model 1   in order to get the probability P  as follows.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM model 1",
                "in order to get the probability P",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probability P",
                "as follows",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "At the same time, grammar theoreticians have proposed various generative synchronous grammar formalisms for MT, such as Synchronous Context Free Grammars     or Synchronous Tree Adjoining Grammars    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar theoreticians",
                "proposed",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Synchronous Context Free Grammars",
                "or Synchronous Tree Adjoining Grammars",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In his analysis of Yarowsky  , Abney   formulates several variants of bootstrapping.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Yarowsky",
                "several variants of bootstrapping",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Abney",
                "formulates",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The decoder uses a binarized representation of the rules, which is obtained via a syncronous binarization procedure  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rules",
                "binarized representation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syncronous binarization procedure",
                "obtained via",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "t is a variant of the batch-based Bloomier filter LM of Talbot and Brants   which we refer to as the TB-LM henceforth",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TB-LM",
                "variant of the batch-based Bloomier filter LM",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "TB-LM",
                "refer to as TB-LM henceforth",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "oehn and Hoang   present Factored Translation Models as an extension to phrase-based statistical machine translation models",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Factored Translation Models",
                "extension to phrase-based statistical machine translation models",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "phrase-based statistical machine translation models",
                "statistical machine translation models",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "By using 8-bit floating point quantization 1 , N-gram language models are compressed into 10 GB, which is comparable to a lossy representation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "8-bit floating point quantization",
                "compressed into 10 GB",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "lossy representation",
                "comparable to",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "However, few papers in the field of computational linguistics have focused on this approach  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "field of computational linguistics",
                "have focused on this approach",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "papers",
                "few",
                "PERFORMANCE",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": ":~ The difl'erent kinds of noun chunks covered by our grmnmar are listed below and illustrated with exmnples:  a combination of a non-obligatory deternfiner, optional adjectives or cardinals and the noun 1Other types of lexicalised PCFGs have been  ,  ,  ,  a and .lelinek, 1998) mid  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "noun chunks",
                "covered by our grammar",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "lexicalised PCFGs",
                "mid",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "This approach is also used in base-NP chunking   and named entity recognition   as well as word segmentation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "used in base-NP chunking and named entity recognition and word segmentation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "named entity recognition",
                "used in",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this work we will use structured linear classifiers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "structured linear classifiers",
                "structured linear classifiers",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Slightly differently from  , we use possible alignments in computing recall.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "possible alignments",
                "in computing recall",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "differently from",
                "slightly",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Both the global models   use fairly small training sets, and there is no evidence that their techniques will scale to larger data sets.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training sets",
                "fairly small",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "techniques",
                "will scale to larger data sets",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For this paper, we train the parameter vector  using the perceptron algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameter vector",
                "using the perceptron algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "perceptron algorithm",
                "algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Other languagesfor which this is the case include English  , the Susanne Corpus  , and the British section of the ICE Corpus  ) and Italian   and TUT  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Susanne Corpus",
                "British section of the ICE Corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "TUT",
                "Italian",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Our corpora were automatically aligned with Giza++   in both directions between source and target and symmetrised using the intersection heuristic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Giza++",
                "symmetrised using the intersection heuristic",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpora",
                "aligned with Giza++ in both directions",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It is interesting to note that, while the study of how the granularity of context-free grammars   affects the performance of a parser  = = =f1  = =f2  = =f3  =f4 Figure 1: Cand f-structures with  links for the sentence GSC4ESESDOAIC1D3D2 of grammar transforms   and lexicalisation  ) has attracted substantial attention, to our knowledge, there has been a lot less research on this subject for surface realisation, a process that is generally regarded as the reverse process of parsing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "granularity of context-free grammars",
                "affects the performance of a parser",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "surface realisation",
                "reverse process of parsing",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Other metrics assess the impact of alignments externally, e.g., different alignments are tested by comparing the corresponding MT outputs using automated evaluation metrics   or METEOR  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignments",
                "are tested",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "automated evaluation metrics",
                "or METEOR",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Due to advances in statistical syntactic parsing techniques  , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical syntactic parsing techniques",
                "recent advances",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "natural language sentences",
                "analyzing the meaning",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Instead of computing all intersections, Och   only computes critical intersections where highest-score translations will change.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Och",
                "computes critical intersections",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "highest-score translations",
                "will change",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he Tagger Support cutoff Accuracy Collins   0 96.60% 5 96.72% Model 3W+TAGS variant 1 96.97% 5 96.93% Table 6: Effect of changing common word feature cutoffs  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Tagger Support",
                "cutoff Accuracy Collins",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "Model 3W+TAGS variant",
                "96.97%",
                "PERFORMANCE",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "There have been a lot of prol)OS~fls for statistical analysis, in ninny languages, in particular in English and Japanese        arkhi, 1997)       .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prolifs",
                "for statistical analysis",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "English and Japanese",
                "in particular",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "For example, since the Collins parser depends on a prior part-of-speech tagger  , we included the time for POS tagging in our Collins measurements.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "depends on a prior part-of-speech tagger",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "measurements",
                "includes time for POS tagging",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A major difference between our approach and most other models tested on the WSJ is that the DOP model uses frontier lexicalization while most other models use constituent lexicalization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "uses constituent lexicalization",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "DOP model",
                "uses frontier lexicalization",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The hierarchical phrase translation pairs are extracted in a standard way  : First, the bilingual data are word alignment annotated by running GIZA++   in two directions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bilingual data",
                "word alignment annotated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "GIZA++",
                "standard way",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use Viterbi training   but neighborhood estimation   or pegging   could also be used.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Viterbi training",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "neighborhood estimation or pegging",
                "could also be used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The weights of the different knowledge sources in the log-linear model used by our system are trained using Maximum BLEU  , which we run for 25 iterations individually for each system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-linear model",
                "used by our system",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Maximum BLEU",
                "run for 25 iterations",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ROUGE  , a recall-oriented evaluation package for automatic summarization.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE",
                "evaluation package",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "automatic summarization",
                "recall-oriented",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++   for word alignments and SRILM for language modeling.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "standard external tools",
                "for some of the tasks to avoid duplication",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "GIZA++",
                "for word alignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "SRILM",
                "for language modeling",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Many stochastic parsing models use linguistic intuitions to find this minimal set, for example by restricting the statistical dependencies to the locality of headwords of constituents  , leaving it as an open question whether there exist important statistical dependencies that go beyond linguistically motivated dependencies.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "2 Evaluation All of the experiments described below have the same basic structure: an estimator is used to infer a bitag HMM from the unsupervised training corpus   Wall Street Journal corpus  ), and then the resulting model is used to label each word of that corpus with one of the HMMs hidden states.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "estimator",
                "infer a bitag HMM from the unsupervised training corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "HMM",
                "label each word with one of the HMM's hidden states",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Inspired by  , we have implemented an f-structure annotation algorithm to automatically obtain f-structures from CFG-trees in the CTB5.1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "f-structure annotation algorithm",
                "to automatically obtain f-structures",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "CFG-trees in the CTB5.1",
                "described as a specific dataset",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "4.2 Base Model II Using the translation model II  , where alignments are dependent on word/entity positions and word/entity sequence lengths, we have p  = mproductdisplay j=1 lsummationdisplay i=0 p p    where aj = i means that wj is aligned with ei.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model II",
                "dependent on word/entity positions and word/entity sequence lengths",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "alignments",
                "means that wj is aligned with ei",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We just assign these rules a constant score trained using our implementation of Minimum Error Rate Training  , which is 0.7 in our system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "implementation",
                "Minimum Error Rate Training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "score",
                "0.7",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In Experiment 1, we applied three standard parsing models from the literature to Negra: an unlexicalized PCFG model  , Carroll and Rooths   head-lexicalized model, and Collinss   model based on head-head dependencies.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing models",
                "standard parsing models from the literature",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Carroll and Rooths' model",
                "head-lexicalized model",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In previous work  , we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual  ), using charalign  , a method that looks for character sequences that are the same in both the source and target.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "charalign",
                "looks for character sequences that are the same in both the source and target",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "AWK manual",
                "English and Japanese versions",
                "APPLICABILITY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "ohnson   considers conversion to a number of different representations and discusses how this influences accuracy for nonlexicalized PCFGs",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "representations",
                "different",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "accuracy",
                "for nonlexicalized PCFGs",
                "PERFORMANCE",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "  also used re-decoding to do system combination by extracting sentence-specific phrase translation tables from the outputs of different MT systems and running a phrase-based decoding with this new translation table.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT systems",
                "outputs of different",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase translation tables",
                "sentence-specific",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "e compared our system with the concepts in WordNet and Fleischman et al.s instance/concept relations  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "concepts",
                "in WordNet",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "instance/concept relations",
                "in Fleischman et al.",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "They can be used for discriminative training of reordering models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reordering models",
                "can be used for discriminative training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "reordering models",
                "can be used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he basic phrase-based model is an instance of the noisy-channel approach  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "basic phrase-based model",
                "an instance of the noisy-channel approach",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "noisy-channel approach",
                "basic phrase-based model",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The target set is built using the 88-89 Wall Street Journal Corpus   tagged using the   tagger and the   SuperTagger; the feedback sets are built using WSJ sentences con330 Algorithm 1 KE-train:   algorithm adapted to literal/nonliteral classification Require: S: the set of sentences containing the target word Require: L: the set of literal seed sentences Require: N: the set of nonliteral seed sentences Require: W: the set of words/features, w  s means w is in sentence s, s owner w means s contains w Require: epsilon1: threshold that determines the stopping condition 1: w-sim0  := 1 if wx = wy,0 otherwise 2: s-simI0  := 1, for all sx,sy  S S where sx = sy, 0 otherwise 3: i := 0 4: while   do 5: s-simLi+1  := summationtextwxsx p maxwysy w-simi , for all sx,sy  S L 6: s-simNi+1  := summationtextwxsx p maxwysy w-simi , for all sx,sy  S N 7: for wx,wy  W W do 8: w-simi+1  := braceleftBigg i = 0 summationtextsxownerwx p maxsyownerwy s-simIi  else summationtextsxownerwx p maxsyownerwys-simLi  ,s-simNi  } 9: end for 10: if wx,maxwyw-simi+1 w-simi }  epsilon1 then 11: break # algorithm converges in 1epsilon1 steps.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "KE-train",
                "algorithm adapted to literal/nonliteral classification",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "SuperTagger",
                "tagged using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Examples are Andersen  , Okanohara and Tsujii  , Sun et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Andersen et al.",
                "Examples",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Tsujii et al.",
                "Examples",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Firstly, they classify all the GHKM2 rules   into two categories: lexical rules and non-lexical rules.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GHKM2 rules",
                "into two categories: lexical rules and non-lexical rules",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "GHKM2 rules",
                "classified",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In contrast, the latter computes four definite probabilities  which are included as features within a machine-learning classifier  from the Web in an attempt to overcome Bean and Riloffs   data sparseness problem.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifier",
                "machine-learning classifier",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "data sparseness problem",
                "data sparseness problem",
                "LIMITATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "At this point, one can imagine estimating a linear matching model in multiple ways, including using conditional likelihood estimation, an averaged perceptron update  ), or in large-margin fashion.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linear matching model",
                "in multiple ways",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "large-margin fashion",
                "in large-margin fashion",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our baseline uses Giza++ alignments   symmetrized with the grow-diag-final-and heuristic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Giza++ alignments",
                "symmetrized with the grow-diag-final-and heuristic",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "grow-diag-final-and heuristic",
                "used in Giza++ alignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Though our motivation is similar to that of Koehn and Hoang  , we chose to build an independent component for inflection prediction in isolation rather than folding morphological information into the main translation model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "main translation model",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "component",
                "independent component for inflection prediction in isolation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Dunning 1993), make use of both positive and negative instances of performing a task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "instances of performing a task",
                "positive and negative",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "positive instances",
                "performing a task",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Many traditional clustering techniques   attempt to maximize the average mutual information of adjacent clusters  = 21, 2 12 2121 ) |  ,  where the same clusters are used for both predicted and conditional words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "clustering techniques",
                "maximize the average mutual information of adjacent clusters",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "same clusters",
                "used for both predicted and conditional words",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "3 Surface Realisation from f-Structures Cahill and van Genabith   present a probabilistic surface generation model for LFG  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Surface Realisation from f-Structures",
                "probabilistic surface generation model",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "LFG",
                "probabilistic surface generation model",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "We adopt the approach of Marcu and Echihabi  , using a small set of patterns to build relation models, and extend their work by re ning the training and classi cation process using parameter optimization, topic segmentation and syntactic parsing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "using a small set of patterns",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "relation models",
                "re ning the training and classi cation process",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "By building the entire system on the derivation level, we side-step issues that can occur when perceptron training with hidden derivations  , but we also introduce the need to transform our training source-target pairs into training derivations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "on the derivation level",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "training source-target pairs",
                "transform into training derivations",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Grammar rules were induced with the syntaxbased SMT system SAMT described in  , which requires initial phrase alignments that we generated with GIZA++  , and syntactic parse trees of the target training sentences, generated by the Stanford Parser   pre-trained on the Penn Treebank.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SAMT",
                "syntax-based SMT system",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Stanford Parser",
                "pre-trained on Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The thesaurus was produced using the metric described by Lin   with input from the grammatical relation data extracted using the 90 million words of written English from the British National Corpus     using the RASP parser  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "RASP parser",
                "used for extracting grammatical relation data",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "90 million words of written English from the British National Corpus",
                "used for extracting grammatical relation data",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This metric tests the hypothesis that the probability of phrase  is the same whether phrase  has been seen or not by calculating the likelihood of the observed data under a binomial distribution using probabilities derived using each hypothesis  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hypothesis",
                "the same",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "binomial distribution",
                "using probabilities derived",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The frequency counts of dependency relationships are filtered with the loglikelihood ratio  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "frequency counts of dependency relationships",
                "filtered with the loglikelihood ratio",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "loglikelihood ratio",
                "used for filtering",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The data used for all our experiments is extracted from the PENN\"\" WSJ Treebank   by the program provided by Sabine Buchholz from Tilbug University.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "program",
                "by the program",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "More recently, the integration of information sources, and the modeling of more complex language processing tasks in the statistical framework has increased the interest in smoothing methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "integration of information sources",
                "increased the interest",
                "INNOVATION",
                "positive",
                0.85
            ],
            [
                "smoothing methods",
                "has increased interest",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "The approach made use of a maximum entropy model   formulated from frequency information for various combinations of the observed features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "formulated from frequency information",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "features",
                "various combinations of the observed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bilingual word alignment",
                "is first introduced",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "statistical machine translation",
                "intermediate result",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For experiment on English, we used the English Penn Treebank     and the constituency structures were converted to dependency trees using the same rules as  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "English Penn Treebank",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "constituency structures",
                "converted to dependency trees",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction The research presented in this paper forms part of an ongoing effort to develop methods to induce wide-coverage multilingual LexicalFunctional Grammar     resources from treebanks by means of automatically associating LFG f-structure information with constituency trees produced by probabilistic parsers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "automatically associating LFG f-structure information with constituency trees",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probabilistic parsers",
                "produced by",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There has been a sizable amount of research on structure induction ranging fromlinearsegmentation tocontent modeling  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "structure induction",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "amount",
                "sizable",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In none of these cases did we repeat minimum-error-rate training; all these systems were trained using max-B. The metrics we tested were:  METEOR  , version 0.6,usingtheexact,Porter-stemmer,andWordNet synonmy stages, and the optimized parameters  = 0.81,  = 0.83,  = 0.28 as reported in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "METEOR",
                "version 0.6",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "parameters",
                "optimized",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he typical problems like doctor-nurse   could be avoided by using such information",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "doctor-nurse",
                "could be avoided",
                "LIMITATION",
                "negative",
                0.8
            ],
            [
                "such information",
                "using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "production rules are typically learned from alignment structures   or from alignment structures and derivation trees for the source string  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "production rules",
                "are learned from alignment structures",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "alignment structures",
                "and derivation trees for the source string",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he work most similar in spirit to ours that of Turney  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Turney",
                "work most similar in spirit",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "Turney",
                "work",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use three different kinds of metrics: DR-STM Semantic Tree Matching, a la Liu and Gildea  , but over DRS instead of over constituency trees.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "DR-STM Semantic Tree Matching",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Semantic Tree Matching",
                "over DRS",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3.3 Language Model   As a second baseline we use the classification based on the language model using overlapping ngram sequences   as suggested by Pang & Lee   for the English language.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "language model",
                "using overlapping ngram sequences",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "classification based on the language model",
                "suggested by Pang & Lee",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A large corpus is vahmble as a source of such nouns  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "is vahmble",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "source",
                "of nouns",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Artificial ungrammaticalities have been used in various NLP tasks   The idea of an automatically generated ungrammatical treebank was proposed by Foster  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ungrammaticalities",
                "used in various NLP tasks",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "treebank",
                "automatically generated",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Instead of directly minimizing error as in earlier work  , we decompose the decoding process into a sequence of local decision steps based on Eq.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoding process",
                "sequence of local decision steps",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Eq",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Using the log-linear form to model p  gives us the flexibility to introduce overlapping features that can represent global context while decoding   and rescoring  , albeit at the cost of the traditional source-channel generative model of translation proposed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "introduce overlapping features",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "traditional source-channel generative model",
                "proposed",
                "INNOVATION",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "In this work, we use the GIZA++ implementation   of IBM Model 5  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ implementation",
                "of IBM Model 5",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM Model 5",
                "implementation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 Data Sets and Supervised Tagger 5.1 Source Domain: WSJ We used sections 02-21 of the Penn Treebank   for training.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "sections 02-21",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "WSJ",
                "Source Domain",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "The two systems we use are ENGCG   and the Xerox Tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ENGCG",
                "the two systems",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Xerox Tagger",
                "the two systems",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 Bilingual Task: An Application for Word Alignment 3.1 Sentence and word alignment Bilingual alignment methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bilingual alignment methods",
                "bilingual alignment",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Bilingual Task",
                "application",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Another application of hard clustering methods   is that they can also produce a binary tree, which can be used for decision-tree based systems such as the SPATTER parser   or the ATR Decision-Tree Part-OfSpeech Tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hard clustering methods",
                "produce a binary tree",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Decision-Tree Part-OfSpeech Tagger",
                "ATR",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "High quality word alignments can yield more accurate phrase-pairs which improve quality of a phrase-based SMT system  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-pairs",
                "improve quality",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "word alignments",
                "high quality",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Machine learning methods should be interchangeable: Transformation-based learning     and Memory-based learning     have been applied to many different problems, so a single interchangeable component should be used to represent each method.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "interchangeable",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "component",
                "single",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "a larger number of labeled documents, its performance on this corpus is comparable to that of Support Vector Machines and Maximum Entropy models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "comparable to that of Support Vector Machines and Maximum Entropy models",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "performance",
                "comparable",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For process  , machine-learning methods are usually used to classify subjective descriptions into bipolar categories   or multipoint scale categories  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine-learning methods",
                "usually used",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "subjective descriptions",
                "into bipolar categories or multipoint scale categories",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " ), a binarizable segmentation/permutation can be recognized by a binarized Synchronous Context-Free Grammar  , i.e., an SCFG in which the right hand sides of all non-lexical rules constitute binarizable permutations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SCFG",
                "binarized Synchronous Context-Free Grammar",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "non-lexical rules",
                "constitute binarizable permutations",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Recently, a number of machine learning approaches have been proposed  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning approaches",
                "have been proposed",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "number",
                "of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The reader is referred to   and   for details of MI clustering, but we will first briefly summarize the MI clustering and then describe our hierarchical clustering algorithm.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MI clustering",
                "briefly summarize",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "hierarchical clustering algorithm",
                "describe",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "On the other hand, structural annotation such as that used in syntactic treebanks   assigns a syntactic category to a contiguous sequence of corpus positions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic treebanks",
                "assigns a syntactic category",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpus positions",
                "contiguous sequence",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The problem is that with such a definition of collocations, even when improved, one identifies not only collocations but freecombining pairs frequently appearing together such as lawyer-client; doctor-hospital, as pointed out by Smadja  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "definition of collocations",
                "even when improved",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "collocations",
                "freecombining pairs frequently appearing together",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We also have an additional held-out translation set, the development set, which is employed by the MT system to train the weights of its log-linear model to maximize BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT system",
                "train the weights of its log-linear model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU",
                "maximize",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In order to determine interannotator agreement for step 2 of the coding procedure for the database of annotated texts, we calculated kappa statistics  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa statistics",
                "calculated",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "coding procedure",
                "for step 2",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "First, we use the standard approach of greedily assigning each of the learned classes to the POS tag with which it has the greatest overlap, and then computing tagging accuracy  .8 Additionally, we compute the mutual information of the learned clusters with the gold tags, and we compute the cluster F-score  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "learned classes",
                "greatest overlap",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "cluster F-score",
                "",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4 Corpus Annotation For our corpus, we selected 1,000 sentences containing at least one comma from the Penn Treebank   WSJ section 00, and manually annotated them with comma information3.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Corpus",
                "1,000 sentences containing at least one comma",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "annotation",
                "manually annotated with comma information",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The labeling agreement was 84%  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "labeling agreement",
                "was 84%",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "labeling agreement",
                "was",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  who employ clusters of related words constructed by the Brown clustering algorithm   for syntactic processing of texts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brown clustering algorithm",
                "constructed by",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "clusters of related words",
                "for syntactic processing of texts",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We computed precision, recall and error rate on the entire set of sentence pairs for each data set.5 To evaluate NeurAlign, we used GIZA++ in both directions   or Spanish  ) as input and a refined alignment approach   that uses a heuristic combination method called grow-diagfinal   for comparison.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data set",
                "entire set of sentence pairs",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "GIZA++",
                "used in both directions",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Following   and other work on general-purpose generators, we adopt BLEU score  , average simple string accuracy   and percentage of exactly matched sentences for accuracy evaluation.6 For coverage evaluation, we measure the percentage of input fstructures that generate a sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "for accuracy evaluation",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "percentage of exactly matched sentences",
                "for accuracy evaluation",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5 Related work The methodology which is closest to our framework is ORANGE  , which evaluates a similarity metric using the average ranks obtained by reference items within a baseline set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methodology",
                "closest to our framework",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "similarity metric",
                "using average ranks",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Several non-linear objective functions, such as F-score for text classification  , and BLEU-score and some other evaluation measures for statistical machine translation  , have been introduced with reference to the framework of MCE criterion training.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "F-score",
                "for text classification",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "BLEU-score",
                "for statistical machine translation",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Inspired by  s methodology which was originally designed for English and Penn-II treebank, our approach to Chinese non-local dependency recovery is based on Lexical-Functional Grammar  , a formalism that involves both phrase structure trees and predicate-argument structures.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methodology",
                "originally designed for English and Penn-II treebank",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "formalism",
                "involves both phrase structure trees and predicate-argument structures",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The perceptron has been used in many NLP tasks, such as POS tagging  , Chinese word segmentation   and so on.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron",
                "used in many NLP tasks",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "tasks",
                "such as POS tagging, Chinese word segmentation and so on",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "More recently, Ramshaw & Marcus   apply transformation-based learning   to the problem.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ramshaw & Marcus",
                "apply transformation-based learning",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "transformation-based learning",
                "to the problem",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5.3 Baseline System We conducted experiments using different segmenters with a standard log-linear PB-SMT model: GIZA++ implementation of IBM word alignment model 4  , the refinement and phrase-extraction heuristics described in  , minimum-errorrate training  , a 5-gram language model with Kneser-Ney smoothing trained with SRILM   on the English side of the training data, and Moses   to translate both single best segmentation and word lattices.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ implementation of IBM word alignment model 4",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase-extraction heuristics described in ",
                "refinement",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The model was trained using minimum error rate training for Arabic   and MIRA for Chinese  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training",
                "minimum error rate",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "MIRA",
                "for Chinese",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he cohesion between two words is measured as in Church and Hanks   by an estimation of the mutual information based on their collocation frequency",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information",
                "based on their collocation frequency",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "collocation frequency",
                "is used for estimation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As shown in  , using this representation, a linear classifier cannot distinguish sentences sampled from a trigram and real sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "representation",
                "cannot distinguish",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "sentences",
                "sampled from a trigram and real",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In acknowledgment of this fact, a series of conferences like Text Retrieval Conferences    , Message Understanding Conferences    , TIPSTER SUMMAC Text Summarization Evaluation  , Document Understanding Conference    , and Text Summar</context> </contexts> <marker>Voorhees, Harman, 1999</marker> <rawString>Voorhees, E. M. and Harman, D. K., 1999.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "conferences",
                "like Text Retrieval Conferences, Message Understanding Conferences, TIPSTER SUMMAC Text Summarization Evaluation, Document Understanding Conference, and Text Summarization",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "Voorhees and Harman's work",
                "1999",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "For example,   collected reviews from a movie database and rated them as positive, negative, or neutral based on the rating   given by the reviewer.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reviews",
                "rated as positive, negative, or neutral",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "rating",
                "given by the reviewer",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Within this class would fall the Lexical Implication Rules   of Ostler and Atkins  , the lexical rules of Copestake and Briscoe  , the Generative Lexicon of Pustejovsky  , and the ellipsis recovery procedUres of Viegas and Nirenburg  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Lexical Implication Rules",
                "of Ostler and Atkins",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ellipsis recovery procedures",
                "of Viegas and Nirenburg",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Most probabilistic parsing research  including, for example, work by by Collins  , and Charniak    is based on branching process models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilistic parsing research",
                "based on branching process models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "work by Collins and Charniak",
                "is included",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Since most phrases appear only a few times in training data, a phrase pair translation is also evaluated by lexical weights   or term weighting   as additional features to avoid overestimation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase pair translation",
                "additional features to avoid overestimation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "lexical weights or term weighting",
                "additional features",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In our experiments we use the same definition of structural locality as was proposed for the ISBN dependency parser in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "definition of structural locality",
                "proposed for the ISBN dependency parser",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ISBN dependency parser",
                "proposed",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The ongoing evaluation literature is perhaps most obvious in the machine translation communitys efforts to better BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation literature",
                "perhaps most obvious",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "machine translation community's efforts",
                "to better BLEU",
                "PERFORMANCE",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "This can also be interpreted as a generalization of standard class-based models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "standard class-based models",
                "generalization of",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "standard class-based models",
                "standard",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "The central question in learning is how to set the parameters a, given the training examples Logistic regression and boosting involve different algorithms and criteria for training the parameters a, but recent work   has shown that the methods have strong similarities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters a",
                "given the training examples",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "methods",
                "have strong similarities",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Word alignments were produced by GIZA++   with a standard training regimen of five iterations of Model 1, five iterations of the HMM Model, and five iterations of Model 4, in both directions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "produced word alignments",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "training regimen",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Good partof-speech results can be obtained using only the preceding category  , which is what we will be using.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "part-of-speech results",
                "good",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "preceding category",
                "what we will be using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However, to be more expressive and flexible, it is often easier to start with a general SCFG or tree-transducer  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SCFG or tree-transducer",
                "easier to start with",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "SCFG or tree-transducer",
                "more expressive and flexible",
                "INNOVATION",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "  apply the theory of   and   to emotion classification and propose a method based on the co-occurrence distribution over content words and six emotion words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "theory of   and   ",
                "to emotion classification",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "method",
                "based on co-occurrence distribution over content words and six emotion words",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "8Interestingly, in work on the automated classification of nouns,   also noted problems with \"\"empty\"\" words that depend on their complements for meaning.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "empty",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "complements",
                "for meaning",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Rapp   calls this trade-off specificity; equivalent observations were made by Church & Hanks   and Church et al  , who refer to the tendency for large windows to wash out, smear or defocus those associations exhibited at smaller scales.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "trade-off",
                "specificity",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "large windows",
                "wash out, smear or defocus",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A Greek model was trained on 440,082 aligned sentences of Europarl v.3, tuned with Minimum Error Training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "Minimum Error Training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentences",
                "aligned",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Tile full description of Model 4   is rather complica.ted as there have to be considered tile cases that English words have fertility larger than one and that English words have fertility zero.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 4",
                "complicated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "English words",
                "have fertility larger than one and have fertility zero",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Given a set of features and a training corpus, the MaxEnt estimation process produces a model in which every feature fi has a weight i. We can compute the conditional probability as  : p  = 1Z  productdisplay i ifi    Z  = summationdisplay o productdisplay i ifi    The conditional probability of the outcome is the product of the weights of all active features, normalized over the products of all the features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "has a weight",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "conditional probability",
                "the product of the weights of all active features",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use evaluations similar to those used before  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluations",
                "similar to those used before",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In  , the definition words were used as initial sense indicators, automatically tagging the target word examples containing them.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "definition words",
                "used as initial sense indicators",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "target word examples",
                "containing them",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Test and training materials were derived from the Brown corpus of American English, all of which has been parsed and manually verified by the Penn T~eebank project   and parts of which have been manually sense-tagged by the WordNet group  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brown corpus",
                "parsed and manually verified",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn T~eebank project",
                "parsed and manually verified",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Numerous experiments have shown parallel bilingual corpora to provide a rich source of constraints for statistical analysis  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parallel bilingual corpora",
                "provide a rich source of constraints",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "statistical analysis",
                "numerous experiments have shown",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The output of a contextfree parser, such as that of Collins   or Charniak  , can be transformed into a sequence of shallow constituents for comparison with the output of a shallow parser.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "output of a context-free parser",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "shallow parser",
                "output of a shallow parser",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our named entity recognizer used a maximum entropy model, built with Adwait Ratnaparkhi's tools   to label word sequences as either person, place, company or none of the above based on local cues including the surrounding words and whether honorifics  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "named entity recognizer",
                "built with Adwait Ratnaparkhi's tools",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "local cues",
                "including the surrounding words and whether honorifics",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2 Related Work Starting with the IBM models  , researchers have developed various statistical word alignment systems based on different models, such as hidden Markov models    , log-linear models  , and similarity-based heuristic methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM models",
                "developed",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "hidden Markov models",
                "different models",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For instance, in Pang and Lee  , yd would be the polarity of the document and ysi would indicate whether sentence si is subjective or objective.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pang and Lee",
                "document",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence si",
                "subjective or objective",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We obtain weights for the combinations of the features by performing minimum error rate training   on held-out data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "combinations of features",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "training",
                "minimum error rate",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Cucerzan  , by contrast to the above, used Wikipedia primarily for Named Entity Disambiguation, following the path of Bunescu and Paca  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wikipedia",
                "primarily for Named Entity Disambiguation",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "Bunescu and Paca",
                "following the path of",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "On a separate note, previous research has explicitly studied sentiment analysis as an application of transfer learning  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transfer learning",
                "as an application",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentiment analysis",
                "as an application",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, the words test and exam are similar because both of them can follow verbs such as administer, cancel, cheat on, conduct, etc. Many methods have been proposed to compute distributional similarity between words, e.g.,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "can follow verbs such as administer, cancel, cheat on, conduct, etc.",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "methods",
                "compute distributional similarity between words",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Estimated clues are derived from the parallel data using, for example, measures of co-occurrence  ), statistical alignment models  ), or string similarity measures  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measures of co-occurrence",
                "statistical alignment models, or string similarity measures",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parallel data",
                "using",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Because of this, it is generally accepted that some kind of postprocessing should be performed to improve the final result, by shortening, fusing, or otherwise revising the material  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "postprocessing",
                "improve the final result",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "material",
                "shortening, fusing, or otherwise revising",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We evaluate the summaries using the automatic evaluation tool ROUGE     and the ROUGE value works as the feedback to our learning loop.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE",
                "works as the feedback",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "ROUGE value",
                "as the feedback",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We have also used ROUGE evaluation approach   which is based on n-gram co-occurrences between machine summaries and ideal human summaries.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE evaluation approach",
                "based on n-gram co-occurrences",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ROUGE evaluation approach",
                "is based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "everal artificial techniques have been used so that classifiers can be developed and tested without having to invest in manually tagging the data: Yarowsky   and Sch/itze   have acquired training and testing materials by creating pseudowords from existing nonhomographic forms",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifiers",
                "developed and tested",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "training and testing materials",
                "creating pseudowords from existing nonhomographic forms",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Of particular relevance are class-based language models  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "class-based language models",
                "Of particular relevance",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "class-based language models",
                "are",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Intercoder reliability was assessed using Cohen's Kappa statistic    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Cohen's Kappa statistic",
                "was assessed using",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "There has been a growing interest in corpus-based approaches which retrieve collocations from large corpora  ,    ,  ,  ,  ,  ,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "large corpora",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approaches",
                "corpus-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Classes can be induced directly from the corpus using distributional clustering   or taken from a manually crafted taxonomy  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "directly using distributional clustering",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "taxonomy",
                "manually crafted",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "inimum-error-rate training was done using Koehns implementation of Ochs   minimum-error-rate model",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Koehns implementation",
                "minimum-error-rate model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "minimum-error-rate model",
                "minimum-error-rate",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Previous approaches, e.g.,   and  , have all used the Brown algorithm for clustering  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brown algorithm",
                "have all used",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "clustering",
                "used by previous approaches",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The trees may be learned directly from parallel corpora  , or provided by a parser trained on hand-annotated treebanks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "trees",
                "learned directly",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parser",
                "trained on hand-annotated treebanks",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "e adopt the similarity score proposed by Lin   as the distributional similarity score and use 50 nearest neighbours in line with McCarthy et al. For the random baseline we select one word sense at random for each word token and average the precision over 100 trials",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "similarity score",
                "proposed by Lin",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "50 nearest neighbours",
                "in line with McCarthy et al.",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2 Three New Features for MT Evaluation Since our source-sentence constrained n-gram precision and discriminative unigram precision are both derived from the normal n-gram precision, it is worth describing the original n-gram precision metric, BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-gram precision metric",
                "BLEU",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "original n-gram precision metric",
                "worth describing",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Once this is accomplished, a variant of Powells algorithm is used to find weights that optimize BLEU score   over these hypotheses, compared to reference translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Powells algorithm",
                "optimize BLEU score",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "hypotheses",
                "compared to reference translations",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To obtain these distances, Ratnaparkhis partof-speech   tagger   and Collins parser   were used to obtain parse trees for the English side of the test corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ratnaparkhis part-of-speech tagger",
                "were used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Collins parser",
                "were used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This is similar to  s and Charniak97s definition of a separate category for auxiliary verbs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "auxiliary verbs",
                "definition of a separate category",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Charniak97s",
                "definition",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Due to the lack of a good Arabic parser compatible with the Sakhr tokenization that we used on the source side, we did not test the source dependency LM for Arabic-to-English MT. When extracting rules with source dependency structures, we applied the same well-formedness constraint on the source side as we did on the target side, using a procedure described by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Sakhr tokenization",
                "compatible",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "source dependency LM",
                "did not test",
                "PERFORMANCE",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "The block set is generated using a phrase-pair selection algorithm similar to  , which includes some heuristic filtering to mal statement here.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-pair selection algorithm",
                "similar to",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "heuristic filtering",
                "to mal statement",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he most direct comparison is between our system and those presented in Cahill and van Genabith   and Hogan et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "presented in Cahill and van Genabith and Hogan et al",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "comparison",
                "most direct",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Following Lin  , we use syntactic dependencies between words to model their semantic properties.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic dependencies",
                "model their semantic properties",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "model",
                "their semantic properties",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " , in which we translate a source-language sentence f into the target-language sentence e that maximizes a linear combination of features and weights:1 e,a = argmax e,a score    = argmax e,a Msummationdisplay m=1 mhm    where a represents the segmentation of e and f into phrases and a correspondence between phrases, and each hm is a R-valued feature with learned weight m. The translation is typically found using beam search  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features and weights",
                "linear combination",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "translation",
                "typically found using beam search",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4.1.3 Alternative Paraphrasing Techniques To investigate the effect of paraphrase quality on automatic evaluation, we consider two alternative paraphrasing resources: Latent Semantic Analysis  , and Brown clustering  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "paraphrasing resources",
                "Latent Semantic Analysis and Brown clustering",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "evaluation",
                "automatic evaluation",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "More specifically, a statistical word alignment model   is used to acquire a bilingual lexicon consisting of NL substrings coupled with their translations in the target MRL.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical word alignment model",
                "used to acquire bilingual lexicon",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "bilingual lexicon",
                "consisting of NL substrings coupled with their translations",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In all experiments that follow, each system configuration was independently optimized on the NIST 2003 Chinese-English test set   using minimum error rate training   and tested on the NIST 2005 Chinese-English task  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system configuration",
                "independently optimized",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "NIST 2003 Chinese-English test set",
                "using minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The measures2  Mutual Information    , the log-likelihood ratio test  , two statistical tests: t-test and a3a5a4 -test, and co-occurrence frequency  are applied to two sets of data: adjective-noun   pairs and preposition-noun-verb   triples, where the AMs are applied to   pairs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Mutual Information",
                "statistical tests",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "AMs",
                "applied to pairs",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Word alignmentdetection of corresponding words between two sentences that are translations of each otheris usually an intermediate step of statistical machine translation    , but also has been shown useful for other applications such as construction of bilingual lexicons, word-sense disambiguation, projection of resources, and crosslanguage information retrieval.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Word alignment",
                "detection of corresponding words between two sentences that are translations of each other",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word-sense disambiguation",
                "projection of resources, and cross-language information retrieval",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "To measure the translation quality, we use the BLEU score   and the NIST score  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "is used",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "NIST score",
                "is used",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "82 Chen and Chang Topical Clustering Dolan   maintains the position that intersense relations are mostly idiosyncratical, thereby making it difficult to characterize them in a general way so as to identify them.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "intersense relations",
                "idiosyncratical",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "characterize them",
                "difficult",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Any encoding scheme, such as the packed representation of Talbot and Brants  , is viable here.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "encoding scheme",
                "viable",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "representation",
                "packed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "17 Citation Observed data Hidden data Collins   Treebank tree with head child annotated on each nonterminal No hidden data",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Treebank tree",
                "with head child annotated on each nonterminal",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "hidden data",
                "No hidden data",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Hence, either the best translation hypothesis is directly extracted from the word graph and output, or an N-best list of translations is computed  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation hypothesis",
                "directly extracted from the word graph and output",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "N-best list of translations",
                "computed",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "They may rely only on this information  ), or they may combine it with additional information as well  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "information",
                "may rely only on this",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "information",
                "may combine it with additional",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This algorithm adjusts the log-linear weights so that BLEU   is maximized over a given development set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-linear weights",
                "is adjusted",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU",
                "is maximized",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, given that each semantic class exhibits a particular syntactic behaviour, information on the semantic class should improve POStagging for adjective-noun and adjective-participle ambiguities, probably the most difficult distinctions both for humans and computers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic class",
                "exhibits a particular syntactic behaviour",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "POStagging",
                "improve for adjective-noun and adjective-participle ambiguities",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Re-decoding   based regeneration re-decodes the source sentence using original LM as well as new trans105 lation and reordering models that are trained on the source-to-target N-best translations generated in the first pass.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "re-decoding",
                "based regeneration re-decodes",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "re-decoding models",
                "trained on source-to-target N-best translations",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In  ,   the significance of an association   is measured by the mutual information I , i.e. the probability of observing x and y together, compared with the probability of observing x and y independently.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information I",
                "measured by",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probability of observing x and y together",
                "compared with",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The true segmentation can now be compared with the N-best list in order to train an averaged perceptron algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "N-best list",
                "can be compared",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "perceptron algorithm",
                "train an",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As an example of it s application, N-gram co-occurrence is used for evaluating machine translations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "N-gram co-occurrence",
                "is used for evaluating machine translations",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "machine translations",
                "evaluating",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Correspondences between MALTUS and other tagsets   were also provided  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MALTUS",
                "tagsets",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "correspondences",
                "provided",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Previous approaches include supervised learning  ,  , vectorial similarity computed between an initial abstract and sentences in the given document, intradocument similarities  , or graph algorithms  ,  ,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "supervised learning",
                "previous approaches",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "vectorial similarity",
                "computed between an initial abstract and sentences in the given document",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Similarly,   and   learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "stemming from different news source",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence level paraphrase templates",
                "learn from",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "These texts were not seen at the training phase which means that neither the 6Since Brill's tagger was trained on the Penn tag-set   we provided an additional mapping.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brill's tagger",
                "was trained on the Penn tag-set",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "additional mapping",
                "was provided",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Then, the method of Smith and Smith   can be used to compute the probability of every possible edge conditioned on the presence of ki, p , using K1ki. Multiplying this probability by p  yields the desired two edge marginal.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "of Smith and Smith",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "probability",
                "using K1ki",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Evaluation Metrics We evaluated the generated translations using three different evaluation metrics: BLEU score  , mWER  , and mPER    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation metrics",
                "BLEU score, mWER, and mPER",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "generated translations",
                "using three different evaluation metrics",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "53 2 Bilexicalization of Inversion Transduction Grammar The Inversion Transduction Grammar of Wu   models word alignment between a translation pair of sentences by assuming a binary synchronous tree on top of both sides",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Inversion Transduction Grammar",
                "models word alignment",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "binary synchronous tree",
                "on top of both sides",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This dependency graph is partitioned into treelets; like  , we assume a uniform probability distribution over all partitions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "partitioned",
                "uniform probability distribution",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "partitions",
                "all",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Only recently have robust knowledge-based methods for some of these tasks begun to appear, and their performance is still not very good, as seen above in our discussion of using WordNet as a semantic network; 33 as for checking the plausibility of a hypothesis on the basis of causal knowledge about the world, we now have a much better theoretical grasp of how such inferences could be made  , but we are still quite a long way from a general inference engine.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "knowledge-based methods",
                "not very good",
                "PERFORMANCE",
                "negative",
                0.85
            ],
            [
                "theoretical grasp",
                "much better",
                "INNOVATION",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "For natural language engineers, the problem bears on information management systems like abstractive summarizers that must measure semantic overlap between sentences  , question answering modules   and machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "information management systems",
                "abstractive summarizers, question answering modules, machine translation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "semantic overlap",
                "must measure",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5 Experimental Evaluation To perform empirical evaluations of the proposed methods, we considered the task of parsing the Penn Treebank Wall Street Journal corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank Wall Street Journal corpus",
                "was considered",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "proposed methods",
                "empirical evaluations",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As a side product, we find empirical evidence to suggest that the effectiveness of rule lexicalization techniques   and parent annotation techniques   is due to the fact that both lead to a reduction in perplexity in the automata induced from training corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rule lexicalization techniques",
                "lead to a reduction in perplexity",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "parent annotation techniques",
                "lead to a reduction in perplexity",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The main application of these techniques to written input has been in the robust, lexical tagging of corpora with part-of-speech labels  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "with part-of-speech labels",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "techniques",
                "robust, lexical tagging",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The sentences in the training and testing sets were already   POS-tagged and noun chunked, and that in a real-life situation additional preprocessing by a POS-tagger   and noun chunker  ) which will introduce additional errors.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentences",
                "already POS-tagged and noun chunked",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "preprocessing",
                "will introduce additional errors",
                "METHODOLOGY",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "ollins and Roark   present an incremental perceptron algorithm for parsing that uses early update to update the parameters when an error is encountered",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron algorithm",
                "incremental",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "update parameters",
                "when an error is encountered",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As with many domain adaptation problems, it is quite helpful to have some annotated target data, especially when annotation styles vary  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "domain adaptation problems",
                "it is quite helpful",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "annotation styles vary",
                "especially",
                "LIMITATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We use GIZA++   to do m-to-n word-alignment and adopt heuristic grow-diag-final-and to do refinement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "m-to-n word-alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "heuristic grow-diag-final-and",
                "refinement",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Each token is labelled with a class using the IOB type of segmentation coding as introduced by Ramshaw and Marcus  , marking whether the middle word is inside  , outside  , or at the beginning   of a chunk, or named entity.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IOB type of segmentation coding",
                "introduced by Ramshaw and Marcus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "chunk",
                "marking whether the middle word is inside, outside, or at the beginning of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Although Phramer provides decoding functionality equivalent to Pharaohs, we preferred to use Pharaoh for this task because it is much faster than Phramer  between 2 and 15 times faster, depending on the configuration  and preliminary tests showed that there is no noticeable difference between the output of these two in terms of BLEU   score.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Phramer",
                "is much slower",
                "PERFORMANCE",
                "negative",
                0.85
            ],
            [
                "Pharaoh",
                "is faster",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "For this reason, paraphrase poses a great challenge for many Natural Language Processing   tasks, just as ambiguity does, notably in text summarization and NL generation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Natural Language Processing tasks",
                "pose a great challenge",
                "INNOVATION",
                "positive",
                0.75
            ],
            [
                "ambiguity",
                "does",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "So unlike some other studies  , we used manually annotated alignments instead of automatically generated ones.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignments",
                "manually annotated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "studies",
                "some other",
                "INNOVATION",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "The published F score for voted perceptron is 93.53% with a different feature set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "F score",
                "93.53%",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "feature set",
                "different",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "2 Automatic Annotation Schemes Using ROUGE Similarity Measures ROUGE   is an automatic tool to determine the quality of a summary using a collection of measures ROUGE-N  , ROUGE-L, ROUGE-W and ROUGE-S which count the number of overlapping units such as n-gram, word-sequences, and word-pairs between the extract and the abstract summaries  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE",
                "ROUGE-N, ROUGE-L, ROUGE-W and ROUGE-S",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ROUGE",
                "count the number of overlapping units",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "McArthur 1992; Mei et al. 1993) Classification allows a word to align with a target word using the collective translation tendency of words in the same class",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "collective translation tendency of words in the same class",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "words",
                "align with a target word",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We also employ the voted perceptron algorithm   and the early update technique as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron algorithm",
                "in ",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "early update technique",
                "as in  ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The results so far mainly come from studies where a parser originally developed for English,such as the Collins parser  , is applied to a new language,which often leads to a signicant decrease in the measured accuracy  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "applied to a new language",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "accuracy",
                "decrease in the measured",
                "PERFORMANCE",
                "negative",
                0.75
            ]
        ]
    },
    {
        "text": "Instead of using the NP bracketing information present in the tagged Treebank data, Ramshaw and Marcus modified the data so as to include bracketing information related only to the non-recursive, base NPs present in each sentence while the subject verb phrases were taken as is. The data sets include POS tag information generated by Ramshaw and Marcus using Brill's transformational part-of-speech tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NP bracketing information",
                "related only to non-recursive, base NPs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "POS tag information",
                "generated by Brill's transformational part-of-speech tagger",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "First, we trained a finitestate shallow parser on base phrases extracted from the Penn Wall St. Journal   Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "shallow",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Wall St. Journal Treebank",
                "base phrases extracted from",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In general, they can be divided into two major categories, namely lexicalized models   and un-lexicalized models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "lexicalized and un-lexicalized",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "models",
                "divided into two major categories",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Och   introduced minimum error rate training  , a technique for optimizing log-linear modelparametersrelativetoameasureoftranslation quality.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "minimum error rate training",
                "technique for optimizing log-linear model parameters relative to a measure of translation quality",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "log-linear model parameters",
                "relative to a measure of translation quality",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However, they can be usefully employed during system development, for example, for quickly assessing modeling ideas or for comparing across different system configurations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system development",
                "usefully employed",
                "APPLICABILITY",
                "positive",
                0.8
            ],
            [
                "system configurations",
                "comparing across",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In  , various aspects of phrase-based systems are compared, e.g. the phrase extraction method, the underlying word alignment model, or the maximum phrase length.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase extraction method",
                "phrase extraction method",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "phrase extraction method",
                "various aspects compared",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he distortion probabilities are class-based: They depend on the word class F  of a covered source word f as well as on the word class E  of the previously generated target word e. The classes are automatically trained  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distortion probabilities",
                "class-based",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "classes",
                "automatically trained",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The lexical scores are computed as the   log probability of the Viterbi alignment for a phrase pair under IBM word-translation Model 1  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Viterbi alignment",
                "log probability",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "IBM word-translation Model 1",
                "Model 1",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "While in this paper we evaluated our framework on the discovery of concepts, we have recently proposed fully unsupervised frameworks for the discovery of different relationship types  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "framework",
                "fully unsupervised",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "relationship types",
                "different",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For this work, an off-the-shelf maximum entropy tagger 10   was used.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "max entropy tagger",
                "off-the-shelf",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "max entropy tagger",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There is also work on grouping senses of other inventories using information in the inventory   along with information retrieval techniques  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "information in the inventory",
                "using",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "information retrieval techniques",
                "using",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2.1 Relationship Types There is a large body of related work that deals with discovery of basic relationship types represented in useful resources such as WordNet, including hypernymy  , synonymy   and meronymy  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "related work",
                "deals with discovery of basic relationship types",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "WordNet",
                "useful resources",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Bilexical CFG is at the heart of most modern statistical parsers  , because the statistics associated with word-specific rules are more informative for disambiguation purposes.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bilexical CFG",
                "more informative for disambiguation purposes",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "statistics associated with word-specific rules",
                "more informative",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  demonstrates a discriminatively trained system for machine translation that has the following characteristics: 1) requires a varying update strategy   depending on whether the reference sentence is reachable or not, 2) uses sentence level BLEU as a criterion for selecting which output to update towards, and 3) only trains on limited length   sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "requires a varying update strategy",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "system",
                "uses sentence level BLEU as a criterion",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Thenthewordalignment is refined by performing grow-diag-final method  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "wordalignment",
                "is refined by",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "grow-diag-final method",
                "is performed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "85 Recently some alignment evaluation metrics have been proposed which are more informative when the alignments are used to extract translation units  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment evaluation metrics",
                "are more informative",
                "PERFORMANCE",
                "positive",
                0.75
            ],
            [
                "alignments",
                "used to extract translation units",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 Stochastic Inversion Transduction Grammars Stochastic Inversion Transduction Grammars     can be viewed as a restricted subset of Stochastic Syntax-Directed Transduction Grammars.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stochastic Inversion Transduction Grammars",
                "restricted subset of Stochastic Syntax-Directed Transduction Grammars",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Stochastic Inversion Transduction Grammars",
                "can be viewed as",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This model is trained on approximately 5 million sentence pairs of Hansard   and UN proceedings which have been aligned on a sentence-by-sentence basis by the methods of  , and then further aligned on a word-by-word basis by methods similar to  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods of ,",
                "methods similar to  ",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence pairs of Hansard and UN proceedings",
                "aligned on a sentence-by-sentence basis",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Sentiment analysis of text documents has received considerable attention recently  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "attention",
                "considerable",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "recently",
                "neutral",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In the present work, we decided to use WSR instead of Key Stroke Ratio  , which is used in other works on IMT such as  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSR",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Key Stroke Ratio",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Jiang & Zhai   gave a systematic examination of the efficacy of unigram, bigram and trigram features drawn from different representations  surface text, constituency parse tree and dependency parse tree.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "drawn from different representations",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "different representations",
                "surface text, constituency parse tree and dependency parse tree",
                "APPLICABILITY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "In our experiments, we used a dependency parser only in English   that has been adapted for building dependencies) but not in the other language.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency parser",
                "adapted for building dependencies",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "other language",
                "not used",
                "APPLICABILITY",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "A contrasting approach   relies only upon documents whose labels are unknown.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "documents",
                "whose labels are unknown",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approach",
                "relies only upon",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The forest representation was obtained by adopting chart generation   where ambiguous candidates are packed into an equivalence class and mapping a chart into a forest in the same way as parsing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chart generation",
                "in the same way as parsing",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "forest representation",
                "obtained",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 The Experimental Results We used the Penn Treebank WSJ corpus   to perform empirical experiments on the proposed parsing models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank WSJ corpus",
                "used to perform empirical experiments",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "proposed parsing models",
                "on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In fact, in   it was shown that this neural network can be viewed as a coarse approximation to the corresponding ISBN model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "neural network",
                "coarse approximation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ISBN model",
                "corresponding",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One solution would be to apply the maximum entropy estimation technique  ) to all of the three components of the SLM, or at least to the CONSTRUCTOR.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy estimation technique",
                "apply to all of the three components of the SLM",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "CONSTRUCTOR",
                "at least to",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We set the feature weights by optimizing the Bleu score directly using minimum error rate training   on the development set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bleu score",
                "directly using minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "development set",
                "optimized for",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Training via the voted perceptron algorithm   or using a max-margin criterion also correspond to the first option  , Finley and Joachims  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "voted perceptron algorithm",
                "correspond to the first option",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "max-margin criterion",
                "correspond to the first option",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "This makes it suitable for discriminative SMT training, which is still a challenge for large parameter sets  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameter sets",
                "still a challenge",
                "LIMITATION",
                "negative",
                0.8
            ],
            [
                "SMT training",
                "suitable",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our approach differs from the corpus-based surface generation approaches of   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus-based surface generation approaches",
                "of  and  ",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "approach",
                "differs from",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The model weights are trained using the improved iterative scaling algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model weights",
                "trained using improved iterative scaling algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "improved iterative scaling algorithm",
                "",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "It achieves 90.1% average precision/recall for sentences with maximum length 40 and 89.5% for sentences with maximum length 100 when trained and tested on the standard sections of the Wall Street Journal Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "trained and tested",
                "achieves 90.1% average precision/recall",
                "PERFORMANCE",
                "positive",
                0.95
            ]
        ]
    },
    {
        "text": "In order to objectively evaluate our representation, we derived it from two different sources: constituency parse trees  ) and dependency parse trees  )1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "representation",
                "derived from two different sources",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parse trees",
                "constituency and dependency",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "nline discriminative training has already been studied by Tillmann and Zhang   and Liang et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Tillmann and Zhang",
                "has already been studied",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "Liang et al",
                "has already been studied",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Due to limited variations in the N-Best list, the nature of ranking, and more importantly, the non-differentiable objective functions used for MT  ), one often found only local optimal solutions to , with no clue to walk out of the riddles.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "objective functions",
                "non-differentiable",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "solutions",
                "local optimal",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "We implemented this model within an ME modeling framework  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "implemented within an ME modeling framework",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "ME modeling framework",
                "modeling framework",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Also, attribute classi cation is a hard problem and there is no existing classi cation scheme that can be used for open domains like newswire; for example, WordNet   organises adjectives as concepts that are related by the non-hierarchical relations of synonymy and antonymy  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classi cation",
                "no existing classi cation scheme",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "WordNet",
                "organises adjectives as concepts",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "One sees this clear trend in the supervised NLP literature examples include the Perceptron algorithm for tagging  , MIRA for dependency parsing  , exponentiated gradient algorithms  , stochastic gradient for constituency parsing  , just to name a few.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Perceptron algorithm",
                "for tagging",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "exponentiated gradient algorithms",
                "for constituency parsing",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The training methods of LRM-F and SVM-F were useful to improve the F M -scores of LRM and SVM, as reported in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training methods",
                "useful to improve",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "F M -scores",
                "reported",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "urney   and Turney and Littman   exploit the first two generalizations for unsupervised sentiment classification of movie reviews",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generalizations",
                "for unsupervised sentiment classification of movie reviews",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Turney and Turney and Littman",
                "exploit",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Secondly, while all taggers use lexical information, and, indeed, it is well-known that lexical probabilities are much more revealing than tag sequence probabilities  , most taggers make quite limited use of lexical probabilities  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "taggers",
                "make limited use",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "lexical probabilities",
                "are much more revealing",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Whereas until recently the focus of research had been on sense disambiguation, papers like Pantel & Lin  , Neill  , and Rapp   give evidence that sense induction now also attracts attention.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "attracts attention",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "papers",
                "give evidence",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Most current SMT systems   use a generative model for word alignment such as the freely available GIZA++  , an implementation of the IBM alignment models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "  measure annotation quality in terms of precision and recall against manually constructed, gold-standard f-structures for 105 randomly selected trees from section 23 of the WSJ section of Penn-II.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "f-structures",
                "gold-standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "precision and recall",
                "against manually constructed",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "type system F1% D Collins   89.7 Henderson   90.1 Charniak and Johnson   91.0 updated   91.4 this work 91.7 G Bod   90.7Petrov and Klein   90.1 S McClosky et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "F1",
                "89.7",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "Charniak and Johnson",
                "91.0",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Standard CI Model 1 training, initialised with a uniform translation table so that t  is constant for all source/target word pairs  , was run on untagged data for 10 iterations in each direction  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Standard CI Model 1",
                "initialised with a uniform translation table",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "training",
                "was run on untagged data for 10 iterations",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For detailed descriptions of SMT models see for example  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT models",
                "detailed descriptions",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "SMT models",
                "for example",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We use minimum error rate training   to tune the feature weights for the log-linear model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "for the log-linear model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "minimum error rate training",
                "to tune",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We dealt with this by either limiting the translation probability from the null word   at the hypothetical 0-position  over a threshold during the EM training, or setting SHo   to a small probability 7r instead of 0 for the initial null hypothesis H0.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation probability",
                "over a threshold",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "null hypothesis",
                "initial",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This task evaluated parsing performance on 10 languages: Arabic, Basque, Catalan, Chinese, Czech, English, Greek, Hungarian, Italian, and Turkish using data originating from a wide variety of dependency treebanks, and transformations of constituency-based treebanks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "languages",
                "10 languages",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "dependency treebanks",
                "wide variety of",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1As do constraint relaxation   and forest reranking  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "constraint relaxation",
                "and forest reranking",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "constraint relaxation",
                "METHODOLOGY",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Sentiment analysis includes a variety of different problems, including: sentiment classification techniques to classify reviews as positive or negative, based on bag of words   or positive and negative words  ; classifying sentences in a document as either subjective or objective  ; identifying or classifying appraisal targets  ; identifying the source of an opinion in a text  , whether the author is expressing the opinion, or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentiment classification techniques",
                "based on bag of words or positive and negative words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "opinion targets",
                "identifying or classifying",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "While bound compositions are not predictable, i.e., their reasonableness cannot be derived from the syntactic and semantic properties of the words in them .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "compositions",
                "cannot be derived",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntactic and semantic properties",
                "cannot be derived",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Each element in vectorw gives a weight to its corresponding element in  , which is the count of a particular feature over the whole sentence y. We calculate the vectorw value by supervised learning, using the averaged perceptron algorithm  , given in Figure 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "vectorw",
                "gives a weight to its corresponding element",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "perceptron algorithm",
                "given in Figure 1",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We used an implementation of McDonald  forcomparisonofresults .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "implementation",
                "McDonald's",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "results",
                "for comparison",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For more information on these models, please refer to Brown et al.  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "refer to Brown et al.",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "utual infornaation involves a problem in that it is overestimated for low-frequency terms  unning 1993)",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "low-frequency terms",
                "is overestimated",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "problem",
                "involves",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Previous work aligns a group of sentences into a compact word lattice  , a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lattice",
                "compact word lattice",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentences",
                "into a compact word lattice",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Previous work on building hybrid systems includes, among others, approaches using reranking, regeneration with an SMT decoder  , and confusion networks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "using reranking, regeneration with an SMT decoder, and confusion networks",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "SMT decoder",
                "used in regeneration",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The classical Bayes relation is used to introduce a target language model  : e = argmaxe Pr  = argmaxe Pr Pr  where Pr  is the translation model and Pr  is the target language model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "target language model",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation model",
                "is the",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Abduction has been applied to the solution of local pragmatics problems   and to story understanding  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "abduction",
                "solution of local pragmatics problems",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "abduction",
                "story understanding",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In contrast, generative models are trained to maximize the joint probability of the training data, which is 1Ramshaw and Marcus   used transformation-based learning  , which for the present purposes can be tought of as a classi cation-based method.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "joint probability",
                "maximize",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "transformation-based learning",
                "can be thought of as a classification-based method",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Following  , we only include features which occur 5 times or more in training data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "occur 5 times or more in training data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "features",
                "occur",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Perceptron Algorithm for Sequence Labeling Collins   proposed an extension of the perceptron algorithm   to sequence labeling.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Perceptron Algorithm",
                "extension of the perceptron algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sequence labeling",
                "proposed",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "2 Hierarchical Clustering of Words Several algorithms have been proposed for automatically clustering words based on a large corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpus",
                "large",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Fortunately, there is a straightforward parallel between our object recognition formulation and the statistical machine translation problem of building a lexicon from an aligned bitext  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "formulation",
                "straightforward parallel",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "bitext",
                "aligned",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "6 Related Work The most relevant previous works include word sense translation and translation disambiguation  , frame semantic induction  , and bilingual semantic mapping  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word sense translation and translation disambiguation",
                "most relevant",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "frame semantic induction",
                "most relevant",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "These measures have, in fact, been used previously in measuring term recognition  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measures",
                "have been used previously",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "term recognition",
                "measuring",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Hockenmaier et al  , although to some extent following the approach of Xia   where LTAGs are extracted, have pursued an alternative by extracting Combinatory Categorial Grammar     lexicons from the Penn Treebank.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LTAGs",
                "are extracted",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Combinatory Categorial Grammar lexicons",
                "are extracted",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3 The M&E Framework We model two RSRs, Cause and Contrast, adopting the de nitions of Marcu and Echihabi     for their Cause-ExplanationEvidence and Contrast relations, respectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "M&E Framework",
                "adopting the definitions of Marcu and Echihabi",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Cause and Contrast",
                "modeling",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To accommodate multiple overlapping features on observations, some other approaches view the sequence labeling problem as a sequence of classification problems, including support vector machines     and a variety of other classifiers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "support vector machines",
                "view as sequence of classification problems",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "classifiers",
                "include variety of other",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The same probabilities are also included using 50 hard word classes derived from the parallel corpus using the GIZA++ mkcls utility  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ mkcls utility",
                "derived from the parallel corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word classes",
                "50 hard",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Here, we compare two similarity measures: the familiar BLEU score   and a score based on string kernels.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "familiar",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "score based on string kernels",
                "novel",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We trained the parser on the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "trained on Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank",
                "standard corpus",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.1 Translation Modeling We can test our models utility for translation by transforming its parameters into a phrase table for the phrasal decoder Pharaoh  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "parameters",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase table for the phrasal decoder Pharaoh",
                "transforming parameters",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "These wordbased models are used to find the latent wordalignments between bilingual sentence pairs, from which a weighted string transducer can be induced   or synchronous context free grammar  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "wordbased models",
                "used to find latent wordalignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "synchronous context free grammar",
                "induced",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he preliminary labeling by keyword matching used in this paper is similar to the seed collocations used by Yarowsky  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "keyword matching",
                "similar to",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Yarowsky's seed collocations",
                "",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "A Viterbi alignment computed from an IBM model 4   was computed for each translation direction.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Viterbi alignment",
                "computed from an IBM model 4",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translation direction",
                "each",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Following the suggestions in  , Core et al. consider kappa scores above 0.67 to indicate significant agreement and scores above 0.8 reliable agreement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa scores",
                "above 0.67 to indicate significant agreement",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "kappa scores",
                "above 0.8 reliable agreement",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "There are several distance measures suitable for this purpose, such as the mutual information , the dice coefficient , the phi coefficient , the cosine measure  and the confidence .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distance measures",
                "such as the mutual information, the dice coefficient, the phi coefficient, the cosine measure and the confidence",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "confidence",
                "the confidence",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We then built separate English-to-Spanish and Spanish-to-English directed word alignments using IBM model 4  , combined them using the intersect+grow heuristic  , and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM model 4",
                "using IBM model 4",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignment template approach",
                "using the alignment template approach",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As a unified approach, we augment the SDIG by adding all the possible word pairs   ji fe as a parallel ET pair and using the IBM Model 1   word to word translation probability as the ET translation probability.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SDIG",
                "adding all the possible word pairs",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IBM Model 1 word to word translation probability",
                "as the ET translation probability",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he word sense disambiguation method proposed in Yarowsky   can also be viewed as a kind of co-training",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "The ME Tagger The ME tagger is based on Ratnaparkhi  s POS tagger and is described in Curran and Clark  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ME Tagger",
                "based on Ratnaparkhi's POS tagger",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ME tagger",
                "described in Curran and Clark",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Another consequence of not generating posthead conjunctions and punctuation as first-class words is that they 19 In fact, if punctuation occurs before the head, it is not generated at alla deficiency in the parsing model that appears to be a holdover from the deficient punctuation handling in the model of Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing model",
                "deficient punctuation handling",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "punctuation handling",
                "deficiency",
                "LIMITATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our evaluation metric is case-insensitive BLEU-4  , as defined by NIST, that is, using the shortest   reference sentence length for the brevity penalty.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU-4",
                "defined by NIST",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "breivty penalty",
                "using the shortest reference sentence length",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "To train the model, we use the averaged perceptron algorithm described by Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "described by Collins",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "perceptron algorithm",
                "averged",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These tags are drawn from a tagset which is constructed by 363 extending each argument label by three additional symbols a80a44a81a83a82a84a81a86a85, following  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagset",
                "constructed by extending each argument label by three additional symbols",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "argument label",
                "by three additional symbols",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These tools are important in that the strongest collocational associations often represent different word senses, and thus 'they provide a powerful set of suggestions to the lexicographer for what needs to be accounted for in choosing a set of semantic tags'  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic tags",
                "set of suggestions to the lexicographer for what needs to be accounted for",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "collocational associations",
                "represent different word senses",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "??search engines: Turney   uses the Altavista web browser, while we consider and combine the frequency information acquired from three web search engines.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "search engines",
                "uses the Altavista web browser",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "web search engines",
                "acquired from",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "2 Related Work Previous studies on entailment, inference rules, and paraphrase acquisition are roughly classified into those that require comparable corpora   and those that do not  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "comparable",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "studies",
                "require",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The recent emphasis on improving these components of a translation system   is likely due in part to the widespread availability of NLP tools for the language that is most frequently the target: English.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NLP tools",
                "widespread availability",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "English",
                "most frequently the target",
                "APPLICABILITY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "3 Algorithm As in previous work  , our computations are based on a partially lemmatized version of the British National Corpus   which has the function words removed.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "British National Corpus",
                "partially lemmatized version",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "function words",
                "removed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As suggested in  , we use the averaged perceptron when applying the model to held-out or test data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "averaged perceptron",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "model",
                "applying to held-out or test data",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "e adopted the stop condition suggested in Berger et al. 1996 the maximization of the likelihood on a cross-validation set of samples which is unseen at the parameter esti~_tion",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "stop condition",
                "suggested",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parameter estimation",
                "unseen",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Hidden Markov models   are one of the earliest structured learning algorithms, which have recently been followedbydiscriminativelearningapproachessuch as conditional random fields    , the structured perceptron   and its large-margin variants  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hidden Markov models",
                "one of the earliest",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "conditional random fields",
                "discriminative learning approaches",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For process  , existing methods aim to distinguish between subjective and objective descriptions in texts  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "process",
                "existing methods aim to distinguish between subjective and objective descriptions",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "methods",
                "aim to distinguish",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Phrase pairs are extracted up to a fixed maximum length, since very long phrases rarely have a tangible impact during translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrases",
                "rarely have a tangible impact",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "long phrases",
                "very long",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Analogous techniques for tree-structured translation models involve either allowing each nonterminal to generate both terminals and other nonterminals  , or, given a constraining parse tree, to flatten it  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "nonterminal",
                "generate both terminals and other nonterminals",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parse tree",
                "flatten it",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "One of the simplest models in the context of lexical triggers is the IBM model 1   which captures lexical dependencies between source and target words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM model 1",
                "captures lexical dependencies",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "lexical triggers",
                "in the context of",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "he outcomes of CW resemble those of MinCut  : Dense regions in the graph are grouped into one cluster while sparsely connected regions are separated",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CW",
                "resemble those of MinCut",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Dense regions",
                "grouped into one cluster",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2.1 Baseline: IBM Model-1 The translation process can be viewed as operations of word substitutions, permutations, and insertions/deletions   in noisychannel modeling scheme at parallel sentence-pair level.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation process",
                "operations of word substitutions, permutations, and insertions/deletions",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "noisychannel modeling scheme",
                "at parallel sentence-pair level",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our MT experiments use a re-implementation of Moses   called Phrasal, which provides an easier API for adding features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Phrasal",
                "easier API for adding features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Moses",
                "re-implementation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.2 Questions and Corpus To get a clear picture of the impact of using different information extraction methods for the offline construction of knowledge bases, similarly to  , we focused only on questions about persons, taken from the TREC8 through TREC 2003 question sets.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TREC8 through TREC 2003 question sets",
                "taken from",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "information extraction methods",
                "for the offline construction of knowledge bases",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he features that define the constraints on the model are obtained by instantiation of feature templates as in Ratnaparkhi  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature templates",
                "instantiation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "constraints on the model",
                "define",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "previous attempts",
                "characterize text",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "domain-independent rhetorical elements",
                "characterize text",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "2 Related Work Question Answering has attracted much attention from the areas of Natural Language Processing, Information Retrieval and Data Mining  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "areas",
                "Natural Language Processing, Information Retrieval and Data Mining",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "attention",
                "has attracted",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "In the English all-words task of the previous SENSEVAL evaluations  , the best performing English all-words task systems with the highest WSD accuracy were trained on SEMCOR  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "English all-words task systems",
                "with the highest WSD accuracy",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "SEMCOR",
                "trained on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Subjective phrases are used by   and others in order to classify reviews or sentences as positive or negative.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Subjective phrases",
                "are used by and others",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "reviews or sentences",
                "as positive or negative",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Decoding Conditions For tuning of the decoder's parameters, minimum error training   with respect to the BLEU score using was conducted using the respective development corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder's parameters",
                "minimum error training with respect to the BLEU score using",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "development corpus",
                "was conducted using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Formally, transformational rules ri presented in   are equivalent to 1-state xRs transducers mapping a given pattern   to a right hand side string.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transformational rules",
                "are equivalent to",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "1-state xRs transducers",
                "mapping a given pattern to a right hand side string",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This system was worse than the baseline on Bleu  , but an error analysis showed some improvements.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseline",
                "worse",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "improvements",
                "some",
                "PERFORMANCE",
                "positive",
                0.6
            ]
        ]
    },
    {
        "text": "The performance of tl,e presented tagger is measured and compared to that of two other taggers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger",
                "presented",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "tagger",
                "compared to that of two other taggers",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In prior research, ILP was used as a postprocessing step to remove redundancy and make other global decisions about parameters  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ILP",
                "remove redundancy and make other global decisions about parameters",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parameters",
                "global decisions",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Summary of approaches Given a source language sentence f, statistical machine translation defines the translation task as selecting the most likely target translation e under a model P , i.e.: e  = argmax e P  = argmax e msummationdisplay i=1 hi i where the argmax operation denotes a search through a structured space of translation ouputs in the target language, hi  are bilingual features of e and f and monolingual features of e, and weights i are trained discriminitively to maximize translation quality   on held out data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "P",
                "trained discriminitively to maximize translation quality",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "hi",
                "bilingual features of e and f and monolingual features of e",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "t also has close links with theoretical work in situation semantics  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "situation semantics",
                "theoretical work in situation semantics",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "links",
                "has close links",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "According to current tagger comparisons  , and according to a comparsion of the results presented here with those in  , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy framework",
                "yielding comparable results",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "approach",
                "comparable results",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, many statistical part-of-speech   taggers have been developed and they use corpora as the training data to obtain statistical information or rules  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "use corpora as training data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "statistical part-of-speech taggers",
                "have been developed",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "High correlation is reported between the BLEU score and human evaluations for translations from Arabic, Chinese, French, and Spanish to English  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "reported",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "human evaluations",
                "reported",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "n this data set the 4-tuples of the test and training sets were extracted from Penn Treebank Wall Street Journal \\ ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank Wall Street Journal",
                "test and training sets were extracted from",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "4-tuples",
                "of the test and training sets",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  propose using a statistical word alignment algorithm as a more robust way of aligning   outputs into a confusion network for system com2Barzilay and Lee   construct lattices over paraphrases using an iterative pairwise multiple sequence alignment   algorithm",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "statistical word alignment algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "iterative pairwise multiple sequence alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To make feature ranking computationally tractable in Della Pietra et al. 1995 and Berger et al. 1996 a simplified process proposed: at the feature ranking stage when adding a new feature to the model all previously computed parameters are kept fixed and, thus, we have to fit only one new constraint imposed by a candidate feature.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature ranking",
                "simplified process",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "candidate feature",
                "imposed constraint",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Turney   applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "internet-based technique",
                "originally developed for word sentiment classification",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "semantic orientation classification of phrases",
                "had originally been developed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Table 2 summarizes the characteristics of the training corpus used for training the parameters of Model 4 proposed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 4",
                "parameters",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "training corpus",
                "used for training",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To set the weights, m, we performed minimum error rate training   on the development set using Bleu   as the objective function.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "m",
                "minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Bleu",
                "objective function",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Traditionally, such unsupervised EM-trained HMM taggers are thought to be inaccurate, but   showed that by feeding the EM process with sufficiently good initial probabilities, accurate taggers   can be learned for both English and Hebrew, based on a   lexicon and large amount of raw text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "EM-trained HMM taggers",
                "inaccurate",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "initial probabilities",
                "sufficiently good",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "This is exactly the standard lexicon probability a27a28a18a26a4 a20a12 a22 employed in the translation model described in   and in Section 2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicon probability",
                "standard",
                "METHODOLOGY",
                "positive",
                0.75
            ],
            [
                "translation model",
                "described in Section 2",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Other research has been conducted in analysing sentiment at a sentence level using bootstrapping techniques  , finding strength of opinions  , summing up orientations of opinion words in a sentence  , and identifying opinion holders  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bootstrapping techniques",
                "finding strength of opinions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "opinion words",
                "summing up orientations",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction In a classical statistical machine translation, a foreign language sentence f J1 = f1, f2, fJ is translated into another language, i.e. English, eI1 = e1, e2,, eI by seeking a maximum likely solution of: eI1 = argmax eI1 Pr    = argmax eI1 Pr Pr    The source channel approach in Equation 2 independently decomposes translation knowledge into a translation model and a language model, respectively  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "source channel approach",
                "independently decomposes translation knowledge",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "translation model and a language model",
                "respectively",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "More complete discussions of M.E. as applied to computational linguistics, including a description of the M.E. estimation procedure can be found in   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "M.E.",
                "applied to computational linguistics",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "M.E. estimation procedure",
                "description",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he translation probability can also be discriminatively trained such as in Tillmann and Zhang  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation probability",
                "discriminatively trained",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Tillmann and Zhang",
                "discriminatively trained",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "he algorithm is similar to the perceptron algorithm described in Collins  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "similar to the perceptron algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "described in Collins",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Thelistsmaybeused withannotation and a tuning process, such as in  , to iteratively alter feature weights and improve results.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "alter and improve results",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "tuning process",
                "iteratively",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The traditional framework presented in   assumes a generative process where the source sentence is passed through a noisy stochastic process to produce the target sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "framework",
                "traditional",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "noisy stochastic process",
                "produce the target sentence",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Examples of this work include a system by Liu et al  , and experiments by Hindle and Rooth  , and Resnik and Hearst  .2 These efforts had mixed success, suggesting that while multi-level preference scores are problematic, integrating some corpus data does not solve the problems.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system by Liu et al",
                "had mixed success",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "integrating some corpus data",
                "does not solve the problems",
                "INNOVATION",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "One possible use for this technique is for parser adaptation  initially training the parser on one type of data for which hand-labeled trees are available  ) and then self-training on a second type of data in order to adapt the parser to the second domain.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "initially training on one type of data",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parser",
                "adapting to the second domain",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "It has been observed that words close to each other in the source language tend to remain close to each other in the translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "remain close to each other",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation",
                "tend to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "But Koehn, Och, and Marcu   find that phrases longer than three words improve performance little for training corpora of up to 20 million words, suggesting that the data may be too sparse to learn longer phrases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "of up to 20 million words",
                "PERFORMANCE",
                "neutral",
                0.85
            ],
            [
                "phrases longer than three words",
                "improve performance little",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The experiment used all 578 sentences in the ATIS corpus with a parse tree, in the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ATIS corpus",
                "with a parse tree",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank",
                "None",
                "LIMITATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The translation system is a factored phrasebased translation system that uses the Moses toolkit   for decoding and training, GIZA++ for word alignment  , and SRILM   for language models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Moses toolkit",
                "for decoding and training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "SRILM",
                "for language models",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Recently used machine learning methods including maximum entropy models   and support vector machines   provide grounds for this type of modeling, because it allows various dependent features to be incorporated into the model without the independence assumption.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning methods",
                "provide grounds for this type of modeling",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "independence assumption",
                "without",
                "LIMITATION",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "he idea of synchronous SSMT can be traced back to Wu  s Stochastic Inversion Transduction Grammars",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stochastic Inversion Transduction Grammars",
                "Wu's",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "idea",
                "can be traced back to",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Following the setup in Johnson  , we initialize the transition and emission distributions to be uniform with a small amount of noise, and run EM and VB for 1000 iterations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transition and emission distributions",
                "uniform with a small amount of noise",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "EM and VB",
                "run for 1000 iterations",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Most of the current work in statistical machine translation builds on word replacement models developed at IBM in the early 1990s  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word replacement models",
                "developed at IBM in the early 1990s",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "current work",
                "builds on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For our experiments we used the following features, analogous to Pharaohs default feature set:  P  and P , the latter of which is not found in the noisy-channel model, but has been previously found to be a helpful feature  ;  the lexical weights Pw  and Pw   , which estimate how well the words in  translate the words in ;2  a phrase penalty exp , which allows the model to learn a preference for longer or shorter derivations, analogous to Koehns phrase penalty  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "P and P",
                "analogous to Pharaohs default feature set",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "phrase penalty",
                "allows the model to learn a preference",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "We then describe the two main paradigms for learning and inference, in this years shared task as well as in last years, which we call transition-based parsers   and graph-based parsers  , adopting the terminology of McDonald and Nivre  .5 Finally, we give an overview of the domain adaptation methods that were used  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "paradigms",
                "transition-based parsers and graph-based parsers",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "domain adaptation methods",
                "were used",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "5.5 Applying F-score Optimization Technique In addition, we can simply apply the F-score optimization technique for the sequence labeling tasks proposed in   to boost the HySOL performance since the base discriminative models pD  and discriminative combination, namely Equation  , in our hybrid model basically uses the same optimization procedure as CRFs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "F-score optimization technique",
                "boost the HySOL performance",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "discriminative models pD and discriminative combination",
                "uses the same optimization procedure as CRFs",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The other is the self-training   which first parses and reranks the NANC corpus, and then use them as additional training data to retrain the model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "retrain the model",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "NANC corpus",
                "additional training data",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We tokenized sentences using the standard treebank tokenization script, and then we performed part-of-speech tagging using MXPOST tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tokenization script",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "MXPOST tagger",
                "tagger",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The corpus consists of sections 15-18 and section 20 of the Penn Treebank  , and is pre-divided into a 8936-sentence   training set and a 2012-sentence   test set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "sections 15-18 and section 20",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "training set and test set",
                "8936-sentence and 2012-sentence",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Also, even the two-category version of the rating-inference problem for movie reviews has proven quite challenging for many automated classi cation techniques  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rating-inference problem",
                "quite challenging",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "automated classification techniques",
                "many",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Other commonly used measures include kappa   and relative utility  , both of which take into account the performance of a summarizer that randomly picks passages from the original document to produce an extract.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "summarizer",
                "randomly picks passages from the original document",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "performance of a summarizer",
                "take into account",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For example, a statistical machine translation system such as ISIs AlTemp SMT system   can generate a list of n-best alternative translations given a source sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ISIs AlTemp SMT system",
                "can generate a list of n-best alternative translations",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "source sentence",
                "given",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These linguistically-motivated trimming rules   iteratively remove constituents until a desired sentence compression rate is reached.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linguistically-motivated trimming rules",
                "iteratively remove constituents",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "desired sentence compression rate",
                "reached",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our proposal is a first order linear model that relies on an online averaged Perceptron for learning   and an extended Eisner algorithm for the joint parsing inference.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "proposal",
                "first order linear model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Eisner algorithm",
                "extended",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Context extraction begins with a Maximum Entropy POS tagger and chunker  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy POS tagger and chunker",
                "begins with",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "POS tagger and chunker",
                "Maximum Entropy",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Yarowsky  , Mihalcea and Moldovan  , and Mihalcea   have made further research to obtain large corpus of higher quality from an initial seed corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "higher quality",
                "PERFORMANCE",
                "positive",
                0.7
            ],
            [
                "seed corpus",
                "initial",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Parameter tuning is done with Minimum Error Rate Training    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Parameter tuning",
                "with Minimum Error Rate Training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Minimum Error Rate Training",
                "is used for parameter tuning",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Most current transliteration systems use a generative model for transliteration such as freely available GIZA++1  ,an implementation of the IBM alignment models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "freely available",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "IBM alignment models",
                "implementation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The learning methods using in discriminative parsing are Perceptron   and online large-margin learning    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Perceptron",
                "and online large-margin learning",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Perceptron",
                "and online large-margin learning",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "That is obtained using the Viterbi alignment provided by a translation model as described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Roughly in keeping with  , we hereby regard paradigmatic assocations as those based largely on word similarity  , whereas syntagmatic associations are all those words which strongly invoke one another yet which cannot readily be said to be similar.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "paradigmatic associations",
                "based largely on word similarity",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntagmatic associations",
                "cannot readily be said to be similar",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The window size may vary, Church and Hanks   used windows of size 2 and 5.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "window size",
                "used windows of size 2 and 5",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Its rule binarization is described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rule binarization",
                "is described in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "",
                "",
                "LIMITATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "uch tasks will require an extension of the current framework of Turney   beyond evidence from the direct cooccurrence of target word pairs",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Turney",
                "current framework",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "direct cooccurrence of target word pairs",
                "evidence from",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Official DUC scoring utilizes the jackknife procedure and assesses significance using bootstrapping resampling  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "jackknife procedure",
                "uses",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "bootstrapping resampling",
                "assesses significance",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "e follow   in using a tree kernel to represent structural information using the subtree that covers a pronoun and its antecedent candidate",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tree kernel",
                "represent structural information",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "subtree that covers a pronoun and its antecedent candidate",
                "covers",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  for English, but not identical to strictly anaphoric ones5  , since a non-anaphoric NP can corefer with a previous mention.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NP",
                "anaphoric",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corefer",
                "previous mention",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.2.2 Alignment Error Rate Since MT systems are usually built on the union of the two sets of alignments  , we consider the union of alignments in the two directions as well as those in each direction.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT systems",
                "built on the union of the two sets of alignments",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "alignments",
                "in the two directions as well as those in each direction",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Initial results show the potential benefit of factors for statistical machine translation,   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "factors",
                "potential benefit",
                "INNOVATION",
                "positive",
                0.75
            ],
            [
                "machine translation",
                "statistical",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5 Discussion and Future Work The work in this paper substantially differs from previous work in SMT based on the noisy channel approach presented in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "substantially differs",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "noisy channel approach",
                "presented in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The small differences from their work are:   We used characters as the unit as we described above,   While Kazama and Torisawa   checked only the word sequences that start with a capitalized word and thus exploitedthecharacteristicsofEnglishlanguage, we checked the matching at every character,   We used a TRIE to make the look-up efcient.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kazama and Torisawa",
                "checked only the word sequences that start with a capitalized word",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "we",
                "checked the matching at every character",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Following previous work on using global features of candidate structures to learn a ranking model  , the global   features we consider here are simple functions of the local features that capture the relationship between NP pairs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "global features",
                "simple functions of the local features",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "NP pairs",
                "capture the relationship",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This involves running GIZA++   on the corpus in both directions, and applying renement rules   to obtain a single many-tomany word alignment for each sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "running on the corpus in both directions",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "renement rules",
                "to obtain a single many-to-many word alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "his therefore suggests that better parameters are likely to be learned in the 2Haghighi and Kleins   generative coreference model mirrors this in the posterior distribution which it assigns to mention types given their salience  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "better",
                "METHODOLOGY",
                "positive",
                0.75
            ],
            [
                "posterior distribution",
                "assigns to mention types given their salience",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Open-domain opinion extraction is another trend of research on opinion extraction, which aims to extract a wider range of opinions from such texts as newspaper articles  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "on opinion extraction",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "opinions",
                "wider range",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "As expected, Malt and MST have very similar accuracy for short sentences but Malt degrades more rapidly with increasing sentence length because of error propagation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Malt",
                "degrades more rapidly",
                "METHODOLOGY",
                "negative",
                0.7
            ],
            [
                "Malt",
                "accuracy",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The models are based on a maximum entropy framework  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "based on maximum entropy framework",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "maximum entropy framework",
                "used for",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We referred to the studies of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "studies",
                "of ",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "studies",
                "of ",
                "APPLICABILITY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "ROUGE-L and ROUGE-1 are supposed to be appropriate for the headline gener853 ation task  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE-L and ROUGE-1",
                "supposed to be appropriate",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "headline generation task",
                "headline generation task",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Given a sentence-pair  , the most likely   word alignment is found as  : a = argmaxa P .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "a",
                "argmaxa P",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word alignment",
                "most likely",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use the maximum entropy tagging method described in   for the experiments, which is a variant of   modified to use HMM state features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy tagging method",
                "is a variant",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "HMM state features",
                "uses",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Recently, severalmethods  have been proposed with similar motivation to ours.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "similar motivation to ours",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Then, we run GIZA++   on the corpus to obtain word alignments in both directions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "word alignments in both directions",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpus",
                "to obtain",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We therefore ran the dependency model on a test corpus tagged with the POS-tagger of Ratnaparkhi  , which is trained on the original Penn Treebank   in Table 3).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ratnaparkhi",
                "POS-tagger",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank",
                "original",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our evaluation metric was BLEU  , as calculated by the NIST script   with its default settings, which is to perform case-insensitive matching of n-grams up to n = 4, and to use the shortest   reference sentence for the brevity penalty.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "default settings",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "n-grams",
                "up to n = 4",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction In latent variable approaches to parsing  , one models an observed treebank of coarse parse trees using a grammar over more refined, but unobserved, derivation trees.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar",
                "over more refined, but unobserved, derivation trees",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "treebank",
                "coarse parse trees",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This amounts to performing binary text categorization under categories Objective and Subjective  ; 2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "text categorization",
                "under categories",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "categorization",
                "under categories",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the nal step, we score our translations with 4-gram BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translations",
                "with 4-gram BLEU",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "BLEU",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We envision the use of a clever datastructure would reduce the complexity, but leave this to future work, as the experiments   show that 5Our definition implies that we only consider faithful spans to be contiguous  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "datastructure",
                "reduce complexity",
                "METHODOLOGY",
                "positive",
                0.75
            ],
            [
                "definition",
                "only consider faithful spans to be contiguous",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2.1 Scale-dependence It has been shown that varying the size of the context considered for a word can impact upon the performance of applications  , there being no ideal window size for all applications.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "context size",
                "impact upon performance",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "window size",
                "no ideal for all applications",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2007) and Smith and Smith   showed that the MatrixTree Theorem can be used to train edge-factored log-linearmodelsofdependencyparsing",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MatrixTree Theorem",
                "can be used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "log-linear models",
                "of dependence parsing",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "73 ment and phrase-extraction heuristics described in  , minimum-error-rate training  , a trigram language model with KneserNey smoothing trained with SRILM   on the English side of the training data, and Moses   to decode.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "trigram language model with KneserNey smoothing",
                "trained with SRILM",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Moses",
                "decode",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Translation quality is automatically evaluated by the IBM-BLEU metric     on the following publicly 1148 Ch.-En.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM-BLEU metric",
                "automatically evaluated",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "publicly 1148 Ch.-En",
                "following",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2.2 Evaluation of Acquisition Algorithms Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "suggested",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "methods",
                "ranging from distributional similarity to finding shared contexts",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "OS tag the text using the tagger of Ratnaparkhi  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ratnaparkhi",
                "tagger",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "tagger",
                "of Ratnaparkhi",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Within the generative model, the Bayes reformulation is used to estimate where is considered the language model, and is the translation model; the IBM   models being the de facto standard.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM models",
                "de facto standard",
                "INNOVATION",
                "positive",
                0.85
            ],
            [
                "Bayes reformulation",
                "used to estimate",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm   to take the evaluation metric into account.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "using a maximum a posteriori estimator",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "MERT algorithm",
                "take the evaluation metric into account",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This is based on the idea from   that rare words in the training set are similar to unknown words in the test set, and can be used to learn how to tag the unknown words that will be encountered during testing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rare words",
                "similar to unknown words",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "unknown words",
                "can be used to learn",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The model is defined mathematically   as following: p  = 1Zexp nsummationdisplay i=1 ihi    where i is a vector of weights determined during a tuning process, and hi is the feature function.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "defined mathematically as following",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "weights",
                "determined during a tuning process",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This is due to the reason that Telugu     is comparitively a high entropy language than English  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Telugu",
                "comparitively a high entropy language",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "English",
                "comparitively a low entropy language",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "For a detailed description for Model 4 the reader is referred to  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 4",
                "detailed description",
                "INNOVATION",
                "neutral",
                1.0
            ],
            [
                "reader",
                "referred to",
                "APPLICABILITY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The table also shows the -score, which is another commonly used measure for inter-annotator agreement  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measure",
                "another commonly used",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "inter-annotator agreement",
                "commonly used",
                "APPLICABILITY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "In  , target trees were employed to improve the scoring of translation theories.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "target trees",
                "improve the scoring of translation theories",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation theories",
                "employed to",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2 We illustrate the rule extraction with an example from the tree-to-tree translation model based on tree sequence alignment   without losing of generality to most syntactic tree based models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tree-to-tree translation model",
                "based on tree sequence alignment without losing of generality",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "most syntactic tree based models",
                "to most syntactic tree based models",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Instead, we follow a simplified form of previous work on biography creation, where a classifier is trained to distinguish biographical text  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifier",
                "trained to distinguish biographical text",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "biographical text",
                "distinguish",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The second uses the decoder to search for the highest-B translation  , which Arun and Koehn   call max-B updating.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "max-B updating",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "max-B updating",
                "called by Arun and Koehn",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Recent work has explored two-stage decoding, which explicitly decouples decoding into a source parsing stage and a target language model integration stage  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "two-stage decoding",
                "decouples decoding into a source parsing stage and a target language model integration stage",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "decoding",
                "decouples into a source parsing stage and a target language model integration stage",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "They are based on the sourcechannel approach to statistical machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sourcechannel approach",
                "statistical machine translation",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Pattern-based approaches are known for their high accuracy in recognizing instances of relations if the patterns are carefully chosen, either manually   or via automatic bootstrapping  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pattern-based approaches",
                "high accuracy",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "patterns",
                "carefully chosen",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "here have been many approaches to compute the similarity between words based on their distribution in a corpus  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "compute the similarity between words",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "distribution in a corpus",
                "description of word distribution",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Reference-based metrics such as BLEU   have rephrased this subjective task as a somewhat more objective question: how closely does the translation resemble sentences that are known to be good translations for the same source?",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "rephrased this subjective task as a somewhat more objective question",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation",
                "resemble sentences that are known to be good translations",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We used the Wall Street Journal   part of the Penn Treebank  , where extraction is represented by co-indexing an empty terminal element   to its antecedent.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "part of",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "extraction",
                "represented by co-indexing an empty terminal element to its antecedent",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Monotone Nonmonotone Target B A Positions C D Source Positions Figure 1: Two Types of Alignment The IBM model 1     assumes that all alignments have the same probability by using a uniform distribution: p  = 1IJ  Jproductdisplay j=1 Isummationdisplay i=1 p    We use the IBM-1 to train the lexicon parameters p , the training software is GIZA++  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM model 1",
                "uses a uniform distribution",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "GIZA++",
                "is the training software",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Markov model approach",
                "equivalent to",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Maximum Entropy approach",
                "equivalent to",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed  , and the most probable parse is found by PCFG parsing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CFG backbone",
                "Probabilistic models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "PCFG parsing",
                "finds the most probable parse",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Others proposed distributional similarity measures between words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "distributional similarity measures between",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "distributional similarity measures",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We compared a baseline system, the state-of-the-art phrase-based system Pharaoh  , against our system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "state-of-the-art",
                "INNOVATION",
                "positive",
                0.7
            ],
            [
                "system",
                "baseline",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the first of our methods we align manual transcripts and ASR sentences using the IBM translation model   to obtain a probabilistic dictionary.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM translation model",
                "used to obtain",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "probabilistic dictionary",
                "obtained",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ethod Number of frames Number of verbs Linguistic resources F-Score   Coverage on a corpus C. Manning   19 200 POS tagger + simple finite state parser 58 T. Briscoe & J. Carroll   161 14 Full parser 55 A. Sarkar & D. Zeman   137 914 Annotated treebank 88 D. Kawahara et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tagger + simple finite state parser",
                "58",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Full parser",
                "55",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The resolution of alignment can vat3, from low to high: section, paragraph, sentence, phrase, and word  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "resolution",
                "from low to high",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "alignment",
                "section, paragraph, sentence, phrase, and word",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We hence chose transformation-based learning to create this   segmentation grammar, converting the segmentation task into a tagging task  , inter alia).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transformation-based learning",
                "create segmentation grammar",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "segmentation task",
                "tagging task",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One example is the algorithm for word sense disambiguation in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "word sense disambiguation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "in ",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4.1 Data We used Penn-Treebank   data, presented in Table 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn-Treebank",
                "data",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Table 1",
                "presented",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Haghighi and Klein   do the reverse: for each class label y, they ask the annotators to propose a few prototypical featuresf such thatp  is as high as possible.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prototypical features",
                "propose a few prototypical features",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "class label y",
                "as high as possible",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "There are several basic methods for evaluating associations between words: based on frequency counts  , information theoretic   and statistical significance  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "based on frequency counts",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "information theoretic",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The classifier uses mutual information   scores rather than the raw frequences of the occurring patterns  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information scores",
                "rather than the raw frequences of the occurring patterns",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "raw frequences of the occurring patterns",
                "raw",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Many-to-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++  , in both directions and by combining the results based on a heuristic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "one-to-many word alignment model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "results",
                "based on a heuristic",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this paper a discriminative parser is proposed to implement maximum entropy   models   to address the learning task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "implement maximum entropy models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "learning task",
                "address",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2 Previous Work We briefly outline the most important existing methods and cite error rates on a standard English data set, sections 03-06 of the Wall Street Journal   corpus  , containing nearly 27,000 examples.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data set",
                "standard English data set",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "most important existing methods",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As modern systems move toward integrating many features  , resources such as this will become increasingly important in improving translation quality.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "resources",
                "will become increasingly important",
                "APPLICABILITY",
                "positive",
                0.8
            ],
            [
                "translation quality",
                "improving",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The tree-to-string model   views the translation as a structure mapping process, which first breaks the source syntax tree into many tree fragments and then maps each tree fragment into its corresponding target translation using translation rules, finally combines these target translations into a complete sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tree-to-string model",
                "structure mapping process",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translation rules",
                "using translation rules",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "night and Marcu   treat reduction as a translation process using a noisychannel model  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reduction",
                "as a translation process",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "noisy-channel model",
                "no opinion term",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "F-Struct Feats Grammar Rules {PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP-nom   she {PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP-acc   her Table 5: Lexical item rules with case markings 4 A History-Based Generation Model The automatic generation grammar transform presented in   provides a solution to coarse-grained and   inappropriate independence assumptions in the basic generation model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "F-Struct Feats Grammar Rules",
                "PRP-nom   she   PRP-acc   her",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "automatic generation grammar transform",
                "provides a solution",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "We have used the Improved Iterative Scaling algorithm    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Improved Iterative Scaling algorithm",
                "used",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The largest corpus that Goldwater and Griffiths   studied contained 96,000 words, while Johnson   used all of the 1,173,766 words in the full Penn WSJ treebank.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "96,000 words",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpus",
                "1,173,766 words",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The first model, referred to as Maxent1 below, is a loglinear combination of a trigram language model with a maximum entropy translation component that is an analog of the IBM translation model 2  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maxent1",
                "loglinear combination of a trigram language model with a maximum entropy translation component",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM translation model",
                "analog",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The first solution might also introduce errors elsewhere As Ramshaw and Marcus   already noted: \"\"While this automatic derivation process introduced a small percentage of errors on its own, it was the only practical way both to provide the amount of training data required and to allow for fully-automatic testing\"\".",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "automatic derivation process",
                "introduced a small percentage of errors",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "automatic testing",
                "fully-automatic",
                "APPLICABILITY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "The system was trained in a standard manner, using a minimum error-rate training   procedure   with respect to the BLEU score   on held-out development data to optimize the loglinear model weights.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "loglinear model weights",
                "to optimize",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "BLEU score",
                "on held-out development data",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Using this model, we can assign consistent probabilities to parsing results with complex structures, such as ones represented with feature structures  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "assign consistent probabilities",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parsing results",
                "represented with feature structures",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "405 PRF 1 proposed .383 .437 .408 multinomial mixture .360 .374 .367 Newman   .318 .353 .334 cosine .603 .114 .192 -skew divergence   .730 .155 .255 Lins similarity   .691 .096 .169 CBC   .981 .060 .114 Table 3: Precision, recall, and F-measure.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PRF 1",
                ".383.437.408 multinomial mixture",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Newman",
                ".318.353.334 cosine",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Self-training is a commonly used technique for semi-supervised learning that has been ap532 plied to several natural language processing tasks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "technique",
                "commonly used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "natural language processing tasks",
                "has been applied to",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Previous studies   defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilistic model",
                "log-linear model or maximum entropy model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "unification-based grammars",
                "defined as",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As  , we adopted an evaluation of mutual information as a cohesion measure of each cooccurrence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation",
                "mutual information as a cohesion measure",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "cohesion measure",
                "of each cooccurrence",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Many reordering constraints have been used for word reorderings, such as ITG constraints  , IBM constraints   and local constraints  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reordering constraints",
                "ITG constraints, IBM constraints, local constraints",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "reordering constraints",
                "various",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4 Information Base 4.1 Text Corpus Text corpora are essential to statistical modeling, in developing formal theories of the grammars, investigating prosodic phenomena in speech, and evaluating or comparing the adequacy of parsing models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Text corpora",
                "are essential to statistical modeling, in developing formal theories of the grammars, investigating prosodic phenomena in speech, and evaluating or comparing the adequacy of parsing models",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "Text corpora",
                "are essential",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": ".1 The gender/animaticity statistics After we have identified the correct antecedents it is a simple counting procedure to compute P  where wa is in the correct antecedent for the pronoun p  : \\[ wain the antecedent for p \\[ P  = When there are multiple relevant words in the antecedent we apply the likelihood test designed by Dunning   on all the words in the candidate NP",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "antecedents",
                "correct",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "likelihood test",
                "designed by Dunning",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  and   utilized bootstrapping for word sense disambiguation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bootstrapping",
                "for word sense disambiguation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "bootstrapping",
                "utilized",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  calculated the scores by matching the unigrams on the surface forms, stemmed forms and senses.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unigrams",
                "surface forms, stemmed forms and senses",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "scores",
                "calculated by matching",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Since manual word alignment is an ambiguous task, we also explicitly allow for ambiguous alignments, i.e. the links are marked as sure   or possible    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "links",
                "marked as sure or possible",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ambiguous alignments",
                "explicitly allow",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In order to estimate the conditional distributions shown in Table 1, we use the general technique of choosing the MaxEnt distribution that properly estimates the average of each feature over the training data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MaxEnt distribution",
                "properly estimates the average of each feature over the training data",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "general technique",
                "choosing",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The other approach is to estimate a single score or likelihood of a translation with rich features, for example, with the maximum entropy   method as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "maximum entropy method",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation",
                "with rich features",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "However, in yet unpublished work we found that at least for the computation of synonyms and related words neither syntactical analysis nor singular value decomposition lead to significantly better results than the approach described here when applied to the monolingual case  , so we did not try to include these methods in our system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "described here",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "singular value decomposition",
                "do not lead to significantly better results",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "As a result, the string translation probability can be decomposed into a lexicon probability and an alignment probability  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "string translation probability",
                "can be decomposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "lexicon probability and an alignment probability",
                "string translation probability",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "5.1 Evaluation of Translation Translations are evaluated on two automatic metrics: Bleu   and PER, position independent error-rate  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translations",
                "evaluated on two automatic metrics",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "position independent error-rate",
                "position independent",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this work, we focus on learning bilingual word phrases by using Stochastic Inversion Transduction Grammars    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stochastic Inversion Transduction Grammars",
                "using",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "bilingual word phrases",
                "learning",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "By increasing the size of the basic unit of translation, phrase-based machine translation does away with many of the problems associated with the original word-based formulation of statistical machine translation  , in particular:  The Brown et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based machine translation",
                "does away with many of the problems",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word-based formulation",
                "associated with problems",
                "METHODOLOGY",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "5To test the reliability of the annotation scheme, we had a subset of the data annotated by two annotators and found a satisfactory -agreement   of  = 0.81.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotation scheme",
                "satisfactory",
                "PERFORMANCE",
                "positive",
                0.7
            ],
            [
                "agreement",
                "0.81",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We still use complex structures to represent the partial analyses, so as to employ both top-down and bottom-up information as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "complex structures",
                "to represent partial analyses",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "top-down and bottom-up information",
                "as in ",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4.1 Judging Rule Correctness Following the spirit of the fine-grained human evaluation in  , we randomly sampled 800 rules from our rule-base and presented them to an annotator who judged them for correctness, according to the lexical reference notion specified above.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rule-base",
                "sampled 800 rules",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "annotator",
                "judged for correctness",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "he line search is an extension of that described in (Och 2003; Quirk et al. 2005",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "line search",
                "an extension of that described",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "search",
                "described in (Och 2003; Quirk et al. 2005",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Among the chunk types, NP chunking is the first to receive the attention  , than other chunk types, such as VP and PP chunking  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunk types",
                "first to receive the attention",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "VP and PP chunking",
                "other chunk types",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In  , these forbidden subsequences are called inside-out transpositions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "subsequences",
                "forbidden",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "transpositions",
                "inside-out",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "7 This discussion could also be cast in an information theoretic framework using the notion of \"\"mutual information\"\"  , estimating the variance of the degree of match in order to find a frequency-threshold  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information",
                "notion of mutual information",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "frequency-threshold",
                "find a frequency-threshold",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "However, following the work of Yarowsky  , Yarowsky  , many supervised WSD systems use minimal information about syntactic structures, for the most part restricting the notion of context to topical and local features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSD systems",
                "use minimal information about syntactic structures",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "context",
                "restricting the notion of context to topical and local features",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In Table 10, Baseline gives the results of the generation algorithm of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generation algorithm",
                "results",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "Baseline",
                "",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To solve this problem, we will adapt the idea of null generated words from machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine translation",
                "null generated words from",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "idea",
                "adapt",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "More rare words rather than common words are found even in standard dictionaries  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "are found",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "standard dictionaries",
                "standard",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "As a result, the empirical approach has been adopted by almost all contemporary part-of-speech programs: Bahl and Mercer  , Leech, Garside, and Atwell  , Jelinek  , Deroualt and Merialdo  , Garside, Leech, and Sampson  , Church  , DeRose  , Hindle  , Kupiec  , Ayuso et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "has been adopted by almost all contemporary part-of-speech programs",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "programs",
                "contemporary part-of-speech",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "N-best results for phrasal alignment and ordering models in the decoder were optimized by lambda training via Maximum Bleu, along the lines described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lambda training",
                "optimized by",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Maximum Bleu",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "text-oriented ranking methods",
                "can be applied to tasks",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "keyphrases",
                "automated extraction",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ang and Lee   frame the problem of detecting subjective sentences as finding the minimum cut in a graph representation of the sentences",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "graph representation",
                "minimum cut",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentences",
                "finding",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For such cases, unsupervised approaches have been developed for predicting relations, by using sentences containing discourse connectives as training data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unsupervised approaches",
                "predicting relations",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "sentences containing discourse connectives",
                "as training data",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Following Church & Hanks  , Rapp  , and Wettler et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Church & Hanks",
                "Rapp",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Wettler et al.",
                "their work",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "While transfer learning was proposed more than a decade ago  , its application in natural language processing is still a relatively new territory  , and its application in relation extraction is still unexplored.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transfer learning",
                "proposed more than a decade ago",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "application in relation extraction",
                "unexplored",
                "APPLICABILITY",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "Similar ideas were explored in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ideas",
                "explored in",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "ideas",
                "similar",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Therefore,   defined the translation candidate with the minimum word-error rate as pseudo reference translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation candidate",
                "minimum word-error rate",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "pseudo reference translation",
                "defined as",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We proposed a Perceptron like learning algorithm   for guided learning.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Perceptron like learning algorithm",
                "for guided learning",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "Perceptron like",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A related method is multi-category perceptron, which explicitly finds a weight vector that separates correct labels from the incorrect ones in a mistake driven fashion  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weight vector",
                "separates correct labels from incorrect ones",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "method",
                "mistake driven fashion",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "One such relational reasoning task is the problem of compound noun interpretation, which has received a great deal of attention in recent years  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "compound noun interpretation",
                "problem of compound noun interpretation",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "attention in recent years",
                "received a great deal of attention",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  all use multiple context words as discriminating features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "context words",
                "as discriminating features",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "discriminating features",
                "are used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In  , different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment models",
                "by considering all hypothesis pairs as a parallel corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "GIZA++",
                "taken into account",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Head word   of the constituent  After POS tagging, a syntactic parser   was then used to obtain the parse tree for the sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic parser",
                "was used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parse tree",
                "obtained",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5 Combining In-Domain and Out-of-Domain Data for Training In this section, we will first introduce the AUGMENT technique of Daume III  , before showing the performance of our WSD system with and without using this technique.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "AUGMENT technique",
                "of Daume III",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "WSD system",
                "with and without using this technique",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In the training phase, bilingual parallel sentences are preprocessed and aligned using alignment algorithms or tools such as GIZA++  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment algorithms or tools",
                "GIZA++",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "bilingual parallel sentences",
                "preprocessed and aligned",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The translation quality is evaluated by BLEU metric  , as calculated by mteval-v11b.pl with case-insensitive matching of n-grams, where n =4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metric",
                "as calculated",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "mteval-v11b.pl",
                "with case-insensitive matching of n-grams",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In Hirschberg and Nakatani  , average reliability   of segmentinitial labels among 3 coders on 9 monologues produced by the same speaker, labeled using text and speech, is.8 or above for both read and spontaneous speech; values of at least .8 are typically viewed as representing high reliability  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reliability",
                "at least.8",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "segmentinitial labels",
                "high reliability",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In the first approach, heuristic rules are used to find the dependencies   or penalties for label inconsistency are required to handset ad-hoc  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "heuristic rules",
                "are used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "label inconsistency",
                "are required",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Previous Work There have been several approaches to automatically discovering lexico-semantic information from text  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "automatically discovering lexico-semantic information from text",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "lexico-semantic information",
                "from text",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The annotation scheme   is modeled to a certain extent on that of the Penn Treebank  , with crucial differences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotation scheme",
                "modeled on Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "certain extent",
                "crucial differences",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "When evaluated against the state-of-the-art, phrase-based decoder Pharaoh  , using the same experimental conditions  translation table trained on the FBIS corpus  , trigram language model trained on 155M words of English newswire, interpolation weights a65   trained using discriminative training    , probabilistic beam a90 set to 0.01, histogram beam a58 set to 10  and BLEU   as our metric, the WIDL-NGLM-Aa86 a129 algorithm produces translations that have a BLEU score of 0.2570, while Pharaoh translations have a BLEU score of 0.2635.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WIDL-NGLM-Aa86 algorithm",
                "produces translations with a BLEU score of 0.2570",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Pharaoh",
                "has a BLEU score of 0.2635",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "272 Similarity-based estimation was first used for language modeling in the cooccurrence smoothing method of Essen and Steinbiss  , derived from work on acoustic model smoothing by Sugawara et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "cooccurrence smoothing method",
                "derived from work on acoustic model smoothing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Essen and Steinbiss",
                "first used for language modeling",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Most of this work has so far focused either on post-processing to recover non-local dependencies from context-free parse trees  , or on incorporating nonlocal dependency information in nonterminal categories in constituency representations   or in the categories used to label arcs in dependency representations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parse trees",
                "context-free",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "categories",
                "nonlocal dependency information",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "While Kazama and Torisawa used a chunker, we parsed the definition sentence using Minipar  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunker",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Minipar",
                "",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The Pearson correlation is calculated over these ten pairs  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pearson correlation",
                "calculated over these ten pairs",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "Pearson correlation",
                "calculated",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "part-of-speech language model  We use factored translation models   to also output part-of-speech tags with each word in a single phrase mapping and run a second n-gram model over them.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "factored translation models",
                "output part-of-speech tags",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "n-gram model",
                "run over part-of-speech tags",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognizedthe role of syntactic units is well attested in recent systematic studies of translation  , and their absence in phrase-based models is quite evident when looking at MT system output.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic behavior",
                "tendency of constituents to move together as a unit",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "syntactic units",
                "well attested in recent systematic studies of translation",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The model scaling factors  1 ,, 5 and the word and phrase penalties are optimized with respect to some evaluation criterion   such as BLEU score.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model scaling factors",
                "optimized with respect to BLEU score",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word and phrase penalties",
                "optimized with respect to some evaluation criterion",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Similarity measures can be based on any level of linguistic analysis: semantic similarity relies on context vectors , whilesyntacticsimilarityisbased on the alignment of parallel corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linguistic analysis",
                "any level",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "similarity measures",
                "based on context vectors",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Proceedings of EACL '99 Determinants of Adjective-Noun Plausibility Maria Lapata and Scott McDonald and Frank Keller School of Cognitive Science Division of Informatics, University of Edinburgh 2 Buccleuch Place, Edinburgh EH8 9LW, UK {mlap, scottm, keller} @cogsci.ed.ac.uk Abstract This paper explores the determinants of adjective-noun plausibility by using correlation analysis to compare judgements elicited from human subjects with five corpus-based variables: co-occurrence frequency of the adjective-noun pair, noun frequency, conditional probability of the noun given the adjective, the log-likelihood ratio, and Resnik's   selectional association measure.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "correlation analysis",
                "to compare judgements",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "selectional association measure",
                "Resnik's",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, the HMM aligner achieves an AER of 20.7 when using the competitive thresholding heuristic of DeNero and Klein  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMM aligner",
                "achieves an AER of 20.7",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "competitive thresholding heuristic",
                "of DeNero and Klein",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Semantic classification programs   use statistical information based on cooccurrence with appropriate marker words to partition a set of words into semantic groups or classes.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "programs",
                "use statistical information",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "semantic groups or classes",
                "partition a set of words",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The information content of this set is defined as mutual information I )  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "information content",
                "defined as mutual information",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "mutual information",
                "I",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he optimal bilingual parsing tree for a given sentence-pair can be computed using dynamic programming   algorithm ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "dynamic programming",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parsing tree",
                "computed using dynamic programming algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Uses Maximum Entropy   classification, trained on JNLPBA    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy",
                "classification",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": " ), less prior work exists for bilingual acquisition of domain-specific translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prior work",
                "exists for bilingual acquisition of domain-specific translations",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "prior work",
                "less",
                "INNOVATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "1 Introduction In this paper, we present an approach for extracting the named entities   of natural language inputs which uses the maximum entropy   framework  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "named entities",
                "maximum entropy framework",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "named entities",
                "extracting",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4 Experiments 4.1 Experiment Settings A series of experiments were run to compare the performance of the three SWD models against the baseline, which is the standard phrase-based approach to SMT as elaborated in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SWD models",
                "against the baseline",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "phrase-based approach to SMT",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To reduce the time complexity, we adapted the lazy update proposed in  , which was also used in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lazy update",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "lazy update",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To combine the many differently-conditioned features into a single model, we provide them as features to the linear model   and use minimum error-rate training   to obtain interpolation weights m. This is similar to an interpolation of backed-off estimates, if we imagine that all of the different contextsaredifferently-backedoffestimatesofthe complete context.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "as features to the linear model",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "interpolation weights m",
                "obtained using minimum error-rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 Method 3.1 Standard text classication approach We take our starting point from topic-based text classication   and sentiment classication  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classication approach",
                "standard text classication approach",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "text classication and sentiment classication",
                "starting point",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We used a maximummatching algorithm and a dictionary compiled from the CTB   to do segmentation, and trained a maximum entropy part-ofspeech tagger   and TAG-based parser   on the CTB to do tagging and parsing.4 Then the same feature extraction and model-training was done for the PDN corpus as for the CTB.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximummatching algorithm",
                "used",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "They train from the Penn Treebank  ; a collection of 40,000 sentences that are labeled with corrected parse trees  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "collection of 40,000 sentences",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "parse trees",
                "labeled with corrected",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "However, we believe this passage is in error: such an estimate is ineffective in the iterative scaling algorithm.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "estimate",
                "is ineffective",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "algorithm",
                "iterative scaling",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  and  , the specific technique we used by means of a context language model is rather different.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "technique",
                "rather different",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "language model",
                "specific technique used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As a first step, SemEval2007 Task 4 offered many useful insights into the performance of different approaches to semantic relation classification; it has also motivated followup research  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SemEval2007 Task 4",
                "many useful insights",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "different approaches",
                "performance",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The model scaling factors are optimized using minimum error rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model scaling factors",
                "optimized using minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "minimum error rate training",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.1 Generation using PHARAOH PHARAOH   is an SMT system that uses phrases as basic translation units.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PHARAOH PHARAOH",
                "uses phrases as basic translation units",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "PHARAOH PHARAOH",
                "is an SMT system",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Following the phrase extraction phase in PHARAOH, we eliminate word gaps by incorporating unaligned words as part of the extracted NL phrases  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word gaps",
                "are eliminated by incorporating unaligned words",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "NL phrases",
                "are extracted",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ichman and Schone   used a method similar to Nothman et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "similar to Nothman et al",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Nothman et al",
                "et al",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Ever since its introduction in general   and in computational linguistics  , many researchers have pointed out that there are quite some problems in using  (e.g.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "using",
                "problems",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "many researchers",
                "pointed out",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Classi er Training Set Precision Recall F-Measure Linear 10K pairs 0.837 0.774 0.804 Maximum Entropy 10K pairs 0.881 0.851 0.866 Maximum Entropy 450K pairs 0.902 0.944 0.922 Table 4: Performance of Alignment Classi er 3.2 Paraphrase Acquisition Much recent work on automatic paraphrasing   has used relatively simple statistical techniques to identify text passages that contain the same information from parallel corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Linear 10K pairs",
                "0.837",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "Maximum Entropy 10K pairs",
                "0.881",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The word alignment models implemented in GIZA++, the so-called IBM   and HMM alignment models   are typical implementation of the EM algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "implementation of the EM algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IBM and HMM alignment models",
                "typical implementation",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "  proposed a summarization system based on the draft and revision. Jing and McKeown   proposed a system based on extraction and cut-and-paste generation. Our abstractors performed the same cut-and-paste operations that Jing and McKeown noted in their work, and we think that our two-step model will be a reasonable starting point for our subsequent research.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "two-step model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "system",
                "reasonable starting point",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "For extrinsic evaluation of machine translation, we use the BLEU metric  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metric",
                "use",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "BLEU metric",
                "extrinsic evaluation of machine translation",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "ome research into factored machine translation has been published by  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "factored machine translation",
                "has been published by",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "research",
                "into",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our framework makes use of the log-frequency Bloom filter presented in  , and described briefly below, to compute smoothed conditional n-gram probabilities on the fly.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-frequency Bloom filter",
                "presented in ",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "smoothed conditional n-gram probabilities",
                "compute on the fly",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "At the sentence level,   employed an unsupervised learning approach to cluster sentences and extract lattice pairs from comparable monolingual corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unsupervised learning approach",
                "unsupervised learning approach",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "cluster sentences and extract lattice pairs",
                "extract lattice pairs",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In the context of headline generation, simple statistical models are used for aligning documents and headlines  , based on IBM Model 1  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "simple statistical models",
                "used for aligning documents and headlines",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM Model 1",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our approach to statistical machine translation differs from the model proposed in   in that:  We compute the joint model P  from the bilanguage corpus to account for the direct mapping of the source sentence Ws into the target sentence I?VT that is ordered according to the  source language word order.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "joint model P",
                "is computed from the bilanguage corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "source language word order",
                "is ordered according to",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In the refined model 2   alignment probabilities a  are included to model the effect that the position of a word influences the position of its translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment probabilities",
                "model the effect that the position of a word influences the position of its translation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "position of a word",
                "influences the position of its translation",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Table 2 shows the total space and number of bytes required per n-gram to encode the model under different schemes: LDC gzipd is the size of the files as delivered by LDC; Trie uses a compact trie representation  ) with 3 byte word ids, 1 byte values, and 3 byte indices; Block encoding is the encoding used in  ; and randomized uses our novel randomized scheme with 12 error bits.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LDC gzipd",
                "the size of the files as delivered by LDC",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Block encoding",
                "the encoding used in",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We prepare the corpus by passing it through Adwait Ratnaparkhis part-of-speech tagger     and then running Steve Abneys chunker   over the entire text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Adwait Ratnaparkhis part-of-speech tagger",
                "part-of-speech tagger",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Steve Abneys chunker",
                "chunker",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "By reranking a 1000-best list generated by the baseline MT system from Och  , the BLEU   score on the test dataset was improved from 31.6% to 32.9%.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseline MT system",
                "Och",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "BLEU score",
                "was improved",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "To our knowledge no systems directly address Problem 1, instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list  , or else making local independence assumptions which side-step the issue  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reference derivations",
                "in an n-best list",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "local independence assumptions",
                "side-step the issue",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The system used for baseline experiments is two runs of IBM Model 4   in the GIZA++   implementation, which includes smoothing extensions to Model 4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 4",
                "in the GIZA++ implementation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "smoothing extensions",
                "to Model 4",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": ".2 Inversion Transduction Grammar Wu  s inversion transduction grammar   is a synchronous grammar formalism in which derivations of sentence pairs correspond to alignments",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wu's inversion transduction grammar",
                "synchronous grammar formalism",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "derivations of sentence pairs",
                "correspond to alignments",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The training set is extracted from TreeBank   section 1518, the development set, used in tuning parameters of the system, from section 20, and the test set from section 21.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "section 1518",
                "extracted from",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "section 20",
                "used in tuning parameters",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "In recent years, many researchers have employed statistical models   or association measures   to build alignment links.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "statistical",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "alignment links",
                "build",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "For the Penn Treebank, our research and the work of others   have shown that such a correspondence exists in most cases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "exists in most cases",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "work of others",
                "exists in most cases",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4 Filtering with the CFG Rule Dictionary We use an idea that is similar to the method proposed by Ratnaparkhi   for partof-speech tagging.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CFG Rule Dictionary",
                "similar to the method proposed by Ratnaparkhi",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "method",
                "proposed by Ratnaparkhi",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction Bilingual data   are critical resources for building many applications, such as machine translation   and cross language information retrieval  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bilingual data",
                "critical resources",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "machine translation and cross language information retrieval",
                "many applications",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In order to build models that perform well in new   domains we usually find two settings  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "settings",
                "two settings",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "perform well",
                "perform well",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Given a source sentence f, the preferred translation output is determined by computing the lowest-cost derivation   yielding f as its source side, where the cost of a derivation R1 Rn with respective feature vectors v1,,vn  Rm is given by msummationdisplay i=1 i nsummationdisplay j=1  i. Here, 1,,m are the parameters of the loglinear model, which we optimize on a held-out portion of the training set   using minimum-error-rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "loglinear model",
                "parameters",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "minimum-error-rate training",
                "parameters",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Various methods are based on Mutual Information between classes, see  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "based on Mutual Information between classes",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Mutual Information",
                "between classes",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he fertility for the null word is treated specially  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "null word",
                "is treated specially",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "null word",
                "is treated specially",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Starting from an initial point M1 , computing the most probable sentence hypothesis out of a set of K candidate translations Cs AG D8e1,,eKD9 along the line M1 A0  A4 dM1 results in the following optimization problem  : e D4fs;D5 AG argmax eC8Cs AX D4 M 1 A0  A4 d M 1 D5 C2 A4 hM1 D4e,fsD5 B5 AG argmax eC8Cs AY F4 m mhmD4e,fsD5 D0D3D3D3D3D3D3D3D3D1D3D3D3D3D3D3D3D3D2 AGaD4e,fsD5 A0 A4 F4 m dmhmD4e,fsD5 D0D3D3D3D3D3D3D3D3D1D3D3D3D3D3D3D3D3D2 AGbD4e,fsD5 B6 AG argmax eC8Cs AWa D4e,fsD5 A0  A4 bD4e,fsD5 D0D3D3D3D3D3D3D3D3D3D3D3D1D3D3D3D3D3D3D3D3D3D3D3D2 D4A6D5 B4   Hence, the total score D4A6D5 for any candidate translation corresponds to a line in the plane with  as the independent variable.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "candidate translations",
                "set of K candidate translations Cs",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "total score",
                "corresponds to a line in the plane",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This set of words   corresponds to the   Characterize  , Declare  , Admire  , and Judgment verbs   and hence may have particular syntactic and semantic patterning.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "verbs",
                "Characterize, Declare, Admire, Judgment",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "verbs",
                "may have particular syntactic and semantic patterning",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Word alignments are provided by GIZA++   with grow-diag-final combination, with infrastructure for alignment combination and phrase extraction provided by the shared task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "grow-diag-final combination",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "shared task",
                "infrastructure for alignment combination and phrase extraction",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The features are similar to the ones used in phrasal systems, and their weights are trained using max-BLEU training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "similar to the ones used in phrasal systems",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "weights",
                "trained using max-BLEU training",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Obviously, all these semantic resources have been acquiredusing a very differentset of processes  , tools and corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "resources",
                "using a very different set of processes, tools and corpora",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "processes, tools and corpora",
                "very different",
                "METHODOLOGY",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "The tools used are the Moses toolkit   for decoding and training, GIZA++ for word alignment  , and SRILM   for language models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Moses toolkit",
                "for decoding and training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "SRILM",
                "for language models",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.2 Classifier and Features For our AL framework we decided to employ a Maximum Entropy   classifier  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Classifier",
                "Maximum Entropy",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Features",
                "for AL framework",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "html 162 3.1.1 Penn Treebank 3 The Penn Treebank 3 corpus   consists of hand-coded parses of the Wall Street Journal   and a small subset of the Brown corpus    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank 3 corpus",
                "hand-coded parses",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Wall Street Journal",
                "small subset of the Brown corpus",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Recently, many works combined a MRD and a corpus for word sense disambiguation .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MRD",
                "combined with a corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpus",
                "for word sense disambiguation",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Instead, and as suggested by Och  , we chose to maximize directly the quality of the translations produced by the system, as measured with a machine translation evaluation metric.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "produced by the system",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "machine translation evaluation metric",
                "as measured with",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As in much recent empirical work in discourse processing  , we performed an intercoder reliability study investigating agreement in annotating the times.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "intercoder reliability study",
                "investigating agreement in annotating the times",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "recent empirical work",
                "in discourse processing",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction Several recent syntax-based models for machine translation   can be seen as instances of the general framework of synchronous grammars and tree transducers.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntax-based models",
                "instances of the general framework",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "synchronous grammars and tree transducers",
                "general framework",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Word alignments were generated using GIZA++   over a stemmed version of the parallel text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "over a stemmed version of the parallel text",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parallel text",
                "stemmed version",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The f are trained using a held-out corpus using maximum BLEU training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "held-out corpus",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "training",
                "maximum BLEU",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The first constraints are based on inversion transduction grammars    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "constraints",
                "based on inversion transduction grammars",
                "LIMITATION",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "However, to what extent that assumption holds is tested only on a small number of language pairs using hand aligned data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "language pairs",
                "small number",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "hand aligned data",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The evaluation results also confirm the argument of Dunning  , who suggested G2 as a more robust alternative to X2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "G2",
                "as a more robust alternative",
                "INNOVATION",
                "positive",
                0.75
            ],
            [
                "X2",
                "as a more robust alternative",
                "INNOVATION",
                "negative",
                0.75
            ]
        ]
    },
    {
        "text": "Recently, various approaches   to word sense division have been used in WSD research.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "used in WSD research",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "WSD research",
                "various approaches used",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "3 Data The data consists of six sections of the Wall Street Journal part of the Penn Treebank  , and follows the setting of past editions of the CoNLL shared task: training set  , development set   and test set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "part of",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "training set",
                "setting of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.4 Feature Representation Ranking Models Following previous work on sentiment classi cation  , we represent each review as a vector of lexical features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "review",
                "vector of lexical features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "previous work",
                "on sentiment classification",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "7 For the most frequent 184 expressions, on the average, the agreement rate between two human annotators is 0.93 and the Kappa value is 0.73, which means allowing tentative conclusions to be drawn  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "expressions",
                "average agreement rate between two human annotators is 0.93",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "Kappa value",
                "0.73",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In addition to the manual alignment supplied with these data, we create an automatic word alignment for them using GIZA++   and the grow-diagfinal   symmetrization algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "using",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "grow-diagfinal symmetrization algorithm",
                "symmetrization algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 The task: Base NP chunking The task is base NP chunking on section 20 of the Wall Street Journal corpus, using sections 15 to 18 of the corpus as training data as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "Wall Street Journal corpus",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "training data",
                "sections 15 to 18",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "This wrong translation of content words is similar to the incorrect omission reported in  , which both hurt translation adequacy.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation",
                "wrong",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "omission",
                "incorrect",
                "METHODOLOGY",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "  established that it is important to tune    to maximize performance.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "to tune",
                "maximize performance",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "it is important",
                "established",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Determining the sense of an ambiguous word, using bootstrapping and texts from a different language was done by Yarowsky  , Hearst  , Diab  , and Li and Li  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Yarowsky, Hearst, Diab, and Li and Li",
                "done by",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "texts from a different language",
                "using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ur system assumes POS tags as input and uses the tagger of Ratnaparkhi   to provide tags for the development and evaluation sets",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger of Ratnaparkhi",
                "to provide tags",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "POS tags",
                "as input",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The grow-diag-final   combination heuristic   adds links so that each new link connects a previously unlinked token.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "combination heuristic",
                "adds links",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "new link",
                "connects a previously unlinked token",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4.2 Support Vector Machines We chose to adopt a tagging perspective for the Simple NP chunking task, in which each word is to be tagged as either B, I or O depending on wether it is in the Beginning, Inside, or Outside of the given chunk, an approach first taken by Ramshaw and Marcus  , and which has become the de-facto standard for this task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagging perspective",
                "adopted",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "de-facto standard",
                "has become",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "The statistical significance often evaluate whether two words are independant using hypothesis tests such as t-score  , the X2, the log-likelihood   and Fishers exact test  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hypothesis tests",
                "such as t-score, the X2, the log-likelihood and Fishers exact test",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical significance",
                "evaluate whether two words are independant",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "While Galley   describes extracting treeto-string rules from 1-best trees, Mi and Huang et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Galley",
                "describes extracting treeto-string rules from 1-best trees",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Mi and Huang et al.",
                "Overall Sentiment: neutral",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Various methods   have been proposed for synonym acquisition.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "have been proposed",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "As with many dependency parsers  , we handle non-projective   arcs by transforming them into noncrossing arcs with augmented labels.1 Because our syntactic derivations are equivalent to those of  , we use their HEAD methods to projectivise the syntactic dependencies.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "arcs",
                "noncrossing arcs with augmented labels",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "HEAD methods",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in Wu  , but the specific bracketing of the parse tree provided.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reorderings",
                "match not just a possible bracketing but the specific bracketing of the parse tree provided",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "translation model",
                "more information about the structure of the source language",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For this we used two resources: CELEX a linguistically annotated dictionary of English, Dutch and German  , and the Dutch snowball stemmer implementing a suf x stripping algorithm based on the Porter stemmer.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CELEX",
                "linguistically annotated dictionary",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Dutch snowball stemmer",
                "implementing suf x stripping algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Many methods exist for clustering, e.g.,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "exist",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "e.g.",
                "methods",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "is just the probability of the disjunction of the concepts in C; that is, = Zp  cEC In order to see how p  relates to the input data, note that given a concept c, verb v and argument position r, a noun can be generated according to the distribution p , where p  = 1 nEsyn  Now we have a model for the input data: p  = p p  = p  p  cecn  Note that for c  cn , p  = O. The association norm   have been criticised   because these scores can be greatly over-estimated when frequency counts are low.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "p",
                "is the probability of the disjunction of the concepts in C",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "association norm",
                "have been criticised",
                "LIMITATION",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "Another alternative for future work is to compare the dynamic programming approach taken here with the beam-search approach of Collins and Roark  , which allows more global features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dynamic programming approach",
                "taken here",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "beam-search approach",
                "allows more global features",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "The word alignment is computed using GIZA++2 for the selected 73,597 sentence pairs in the FBIS corpus in both directions and then combined using union and heuristic diagonal growing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++2",
                "computed using",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "union and heuristic diagonal growing",
                "combined using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  extends the dictionarybased approach to sequential labeling tasks by propagating the information given in the seeds with contextual word similarity.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dictionarybased approach",
                "extends",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word similarity",
                "given in the seeds with",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This concept of alignment has been also used for tasks like authomatic vocabulary derivation and corpus alignment  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment",
                "used for tasks",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "tasks",
                "like authomatic vocabulary derivation and corpus alignment",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Our story makes use of a weighted formalism known as quasi-synchronous grammar  , originally developed by D. Smith and Eisner   for machine translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "quasi-synchronous grammar",
                "originally developed by D. Smith and Eisner",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "formalism",
                "weighted",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For extracting simple noun phrases we first used Ramshaw and Marcuss base NP chunker  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ramshaw and Marcuss base NP chunker",
                "used",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "base NP chunker",
                "base",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he Penn Wall Street Journal treebank   was used as training and test data",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Wall Street Journal treebank",
                "was used",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "training and test data",
                "was used",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Automatically determining the degree of antonymy between words has many uses including detecting and generating paraphrases   and detecting contradictions    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "antonymy",
                "detecting and generating paraphrases and detecting contradictions",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "uses",
                "many",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "While we do not have a direct comparison, we note that Turney   performs worse on movie reviews than on his other datasets, the same type of data as the polarity dataset.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Turney",
                "performs worse",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "movie reviews",
                "same type of data as the polarity dataset",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We then piped the text through a maximum entropy sentence boundary detector   and performed text normalization using NSW tools  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy sentence boundary detector",
                "performed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "NSW tools",
                "used for text normalization",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "These instances can be retagged with their countability by using the proposed method and some kind of bootstrapping  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "proposed method",
                "bootstrapping",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "countability",
                "can be retagged",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The first is to align the words using a standard word alignement technique, such as the Refined Method described in     and then generate bi-phrases by combining together individual alignments that co-occur in the same pair of sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word alignement technique",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "alignments",
                "co-occur in the same pair of sentences",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We optimized separately for both the NIST   and the BLEU metrics  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "NIST and BLEU",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "evaluation",
                "optimized separately for",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This might prove beneficial for various discriminative training methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "discriminative training methods",
                "might prove beneficial",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "to the pair-wise TER alignment described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pair-wise TER alignment",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "TER alignment",
                "pair-wise",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The kappa value   was used to evaluate the agreement among the judges and to estimate how difficult the evaluation task was.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa value",
                "evaluate the agreement among the judges and to estimate how difficult the evaluation task was",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "evaluation task",
                "was difficult",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Following our previous work  , we extract features from a sequence representation and a parse tree representation of each relation instance.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "relation instance",
                "sequence representation and a parse tree representation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "features",
                "extracted",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "in   and Lin and Och   proposed an LCS-based automatic evaluation measure called ROUGE-L",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE-L",
                "automatic evaluation measure",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "LCS-based",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "We concatenate the lists and we learn a new combination of weights that maximizes the Bleu score of the combined nbest list using the same development corpus we used for tuning the individual systems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights",
                "maximizes the Bleu score",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "development corpus",
                "used for tuning individual systems",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The concept of mutual information, taken from information theory, was proposed as a measure of word association  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "concept of mutual information",
                "taken from information theory",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "measure of word association",
                "proposed as",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 Automatic Evaluation of MT Quality We utilize BLEU   for the automatic evaluation of MT quality in this paper.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "automatic evaluation of MT quality",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "MT quality",
                "utilize for automatic evaluation",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Proceedings of the 22nd International Conference on Computational Linguistics  , pages 585592 Manchester, August 2008 Random Restarts in Minimum Error Rate Training for Statistical Machine Translation Robert C. Moore and Chris Quirk Microsoft Research Redmond, WA 98052, USA bobmoore@microsoft.com, chrisq@microsoft.com Abstract Ochs   minimum error rate training   procedure is the most commonly used method for training feature weights in statistical machine translation   models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "minimum error rate training",
                "most commonly used method",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "minimum error rate training",
                "procedure",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "If human-aligned data is available, the EMD algorithm provides higher baseline alignments than GIZA++ that have led to better MT performance  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "EMD algorithm",
                "higher baseline alignments",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "GIZA++",
                "worse MT performance",
                "PERFORMANCE",
                "negative",
                0.75
            ]
        ]
    },
    {
        "text": "For this reason, name classification has been studied in solving the named entity extraction task in the NLP and information extraction communities   and various approaches reported in the MUC conferences  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "name classification",
                "has been studied",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "approaches",
                "reported in MUC conferences",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "have been explored",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "entailment rules",
                "many",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It is interesting to constrast this method with the \"\"parse-parse-match\"\" approaches that have been reported recently for producing parallel bracketed corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "parse-parse-match approaches",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approaches",
                "reported recently",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "This procedure uses the head finding rules of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "head finding rules",
                "of ",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "head finding rules",
                "of ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "amshaw and Marcus   first introduced the machine learning techniques to chunking problem",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning techniques",
                "to chunking problem",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Marcus",
                "introduced",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "We obtained 47,025 50-dimensional reduced vectors from the SVD and clustered them into 200 classes using the fast clustering algorithm Buckshot    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "vectors",
                "50-dimensional reduced",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "fast clustering",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For word alignment accuracy, F-measure is reported, i.e., the harmonic mean of precision and recall against a gold-standard reference set; for translation quality, Bleu   and its variation of NIST scores are reported.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "F-measure",
                "harmonic mean of precision and recall",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "NIST scores",
                "reporting variation",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Aware of this problem, Resnik and Yarowsky suggest creating the sense distance matrix based on results in experimental psychology such as Miller and Charles   or Resnik  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sense distance matrix",
                "based on results in experimental psychology",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "experimental psychology",
                "such as Miller and Charles or Resnik",
                "APPLICABILITY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Language modeling  , noun-clustering  , constructing syntactic rules for SMT  , and finding analogies   are examples of some of the problems where we need to compute relative frequencies.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "noun-clustering",
                "constructing syntactic rules for SMT",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "relative frequencies",
                "need to compute",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": ".3 Model Construction The head transducer model was trained and evaluated on English-to-Mandarin Chinese translation of transcribed utterances from the ATIS corpus  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "head transducer model",
                "trained and evaluated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ATIS corpus",
                "transcribed utterances",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  proposed a method to retrieve collocations by combining bigrams whose cooccurrences are greater than a given threshold 3.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bigrams",
                "whose cooccurrences are greater than a given threshold",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "collocations",
                "by combining",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The corpus was automatically derived from the Penn Treebank II corpus  , by means of the script chunklink.pl   that we modified to fit our purposes.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank II corpus",
                "modified to fit our purposes",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "chunklink.pl",
                "modified to fit our purposes",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We can, however, produce a useful surrogate: a pair of monolingual WCFGs with structures projected by G and weights that, when combined, underestimate the costs of G. Parsing optimally relative to a synchronous grammar using a dynamic program requires time O  in the length of the sentence  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WCFGs",
                "structures projected by G and weights",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "grammar",
                "synchronous",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Lacking an automatic method, recent WSD works   still resort to human intervention to identify and group closely related senses in an MRD.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSD works",
                "lack an automatic method",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "human intervention",
                "identify and group closely related senses",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3 Experiments We built baseline systems using GIZA++  , Moses phrase extraction with grow-diag-finalend heuristic  , a standard phrasebased decoder  , the SRI LM toolkit  , the suffix-array language model  , a distance-based word reordering model Algorithm 5 Rich Interruption Constraints   Input: Source tree T, previous phrase fh, current phrase fh+1, coverage vector HC 1: Interruption  False 2: ICount,VerbCount,NounCount  0 3: F  the left and right-most tokens of fh 4: for each of f  F do 5: Climb the dependency tree from f until you reach the highest node n such that fh+1 / T .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase-based decoder",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This conclusion is supported by the fact that true IMT is not, to our knowledge, used in most modern translator's support environments, eg  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IMT",
                "not used in most modern translator's support environments",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "true IMT",
                "to our knowledge",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": " , mention about substrings of collocations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "substrings of collocations",
                "mention about",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "collocations",
                "no opinion term",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As argued in Carletta  , Kappa values of 0.8 or higher are desirable for detecting associations between several coded variables; we were thus satisfied with the level of agreement achieved.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa values",
                "0.8 or higher",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "level of agreement",
                "satisfied with",
                "PERFORMANCE",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "Therefore, domain adaptation methods have recently been proposed in several NLP areas, e.g., word sense disambiguation  , statistical parsing  , and lexicalized-grammar parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "domain adaptation methods",
                "have been proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word sense disambiguation",
                "statistical parsing",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The supertagger uses a log-linear model to define a distribution over the lexical category set for each word and the previous two categories   and the forward backward algorithm efficiently sums over all histories to give a distribution for each word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "supertagger",
                "uses log-linear model",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "forward backward algorithm",
                "efficiently sums over all histories",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction Many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical tests",
                "many different",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word similarity or word association",
                "measure the strength of",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To enable such techniques, we bring the cohesion constraint inside the ITG framework  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG framework",
                "inside the ITG framework",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "cohesion constraint",
                "bring the cohesion constraint",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The translation and reference files are analyzed by a treebank-based, probabilistic LFG parser  , which produces a set of dependency triples for each input.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LFG parser",
                "probabilistic",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dependency triples",
                "produces",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "On the contrary, a string-to-tree decoder  ) is a parser that applies string-to-tree rules to obtain a target parse for the source string.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "applies string-to-tree rules",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "target parse",
                "obtain a target parse",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Performance is also measured by the BLEU score  , which measures similarity to the reference translation taken from the English side of the parallel corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "measures similarity to the reference translation",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "parallel corpus",
                "taken from the English side",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We evaluate accuracy performance using two automatic metrics: an identity metric, ID, which measures the percent of sentences recreated exactly, and BLEU  , which gives the geometric average of the number of uni-, bi-, tri-, and four-grams recreated exactly.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "an identity metric, ID, and BLEU",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "BLEU",
                "gives the geometric average of the number of uni-, bi-, tri-, and four-grams recreated exactly",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2.2 Maximum Entropy Model The maximum entropy model   estimates a probability distribution from training data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy Model",
                "estimates a probability distribution",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "training data",
                "used for estimation",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Extensive research concerning the integration of semantic knowledge into NLP for the English language has been arguably fostered by the emergence of WordNet::Similarity package  .1 In its turn, the development of the WordNet based semantic similarity software has been facilitated by the availability of tools to easily retrieve 1http://www.d.umn.edu/a0 tpederse/similarity.html data from WordNet, e.g. WordNet::QueryData,2 jwnl.3 Research integrating semantic knowledge into NLP for languages other than English is scarce.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet::Similarity package",
                "has been arguably fostered",
                "INNOVATION",
                "positive",
                0.7
            ],
            [
                "tools to easily retrieve data from WordNet",
                "availability",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "We directly model the conditional probability of the alignment a, given x and y, using the maximum entropy framework  , P  = exp{F }summationdisplay aC  exp{F } .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "F",
                "maximum entropy framework",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "P",
                "directly modeled",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This segmentation task can be achieved by assigning words in a sentence to one of three tokens: B for Begin-NP, I for Inside-NP, or O for OutsideNP  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tokens",
                "one of three",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "words in a sentence",
                "assigning to one of three",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Separating the scoring from the source language reordering also has the advantage that the approach in essence is compatible with other approaches such as a traditional PSMT system   or a hierarchical phrase system  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "is compatible with other approaches",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "system",
                "traditional PSMT or hierarchical phrase system",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The smoothing methods proposed in the literature   and Lee  ) can be generally divided into three types: discounting  , class-based smoothing  , and distance-weighted averaging  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "smoothing methods",
                "proposed in the literature",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "distance-weighted averaging",
                "none",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  can be used to motivate a novel class-based language model and a regularized version of minimum discrimination information   models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "language model",
                "novel class-based",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "minimum discrimination information models",
                "regularized version",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A similar use of the term phrase exists in machine translation, where phrases are often pairs of word sequences consistent with word-based alignments  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase",
                "pairs of word sequences consistent with word-based alignments",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "term phrase",
                "exists in machine translation",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "579 The MaxEnt algorithm associates a set of weights  i=1nj=1m with the features, which are estimated during the training phase to maximize the likelihood of the data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MaxEnt algorithm",
                "associates a set of weights",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "features",
                "are estimated during the training phase to maximize the likelihood of the data",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Since parsing is just an initial stage of natural language understanding, the project was focused not just on obtaining syntactic trees alone   or Tiger  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing",
                "just an initial stage of natural language understanding",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Tiger",
                "alone",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Johnson   compared two Bayesian inference algorithms, Variational Bayes and what we call here a point-wise collapsed Gibbs sampler, and found that Variational Bayes produced the best solution, and that the Gibbs sampler was extremely slow to converge and produced a worse solution than EM.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Variational Bayes",
                "produced the best solution",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "Gibbs sampler",
                "extremely slow to converge and produced a worse solution",
                "PERFORMANCE",
                "negative",
                0.95
            ]
        ]
    },
    {
        "text": "4.3 Experiments results Our evaluation metric is BLEU  , which are to perform case-insensitive matching of n-grams up to n = 4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "to perform case-insensitive matching of n-grams up to n = 4",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "BLEU",
                "are",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The main reason behind this lies in the difference between the two corpora used: Penn Treebank   and EDR corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "Penn Treebank and EDR corpus",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "corpora",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  applies this approach to the so-called IBM Candide system to build context dependent models, compute automatic sentence splitting and to improve word reordering in translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Candide system",
                "to build context dependent models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "automatic sentence splitting and to improve word reordering in translation",
                "to improve",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "We used a publicly available tagger   to provide the part-of-speech tags for each word in the sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger",
                "publicly available",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "part-of-speech tags",
                "for each word in the sentence",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "t is worth noting that we observed the same relation between subjectivity detection and polarity classification accuracy as described by Pang and Lee   and Eriksson  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pang and Lee's description",
                "same relation",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "Eriksson's work",
                "same relation",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "So far, most previous work on domain adaptation for parsing has focused on data-driven systems  , i.e. systems employing   treebank grammars  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "previous work",
                "data-driven systems",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "treebank grammars",
                "employing",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, the sets of tags and rule labels have been clustered by our team gr~:mm~trian, while a vocabulary of about 60,000 words has been clustered by machine  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "team",
                "gr~:mm~trian",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "vocabulary",
                "clustered by machine",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "oth Agichtein and Ganti   and Canisius and Sporleder   train a language model for each database column",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "language model",
                "for each database column",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "train",
                "a language model",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Turney   reported that the NEAR operator outperformed simple page co-occurrence for his purposes; our early experiments informally showed the same for this work.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NEAR operator",
                "outperformed",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "early experiments",
                "informally showed",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Recently there have been some studies addressing domain adaptation from different perspectives  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "studies",
                "addressing domain adaptation",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "perspectives",
                "different",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Domain adaptation deals with these feature distribution changes  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature distribution changes",
                "deals with",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "In this case, we use the log-likelihood measure as described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-likelihood measure",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "measure",
                "described",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Networks   97.24 SVM   97.05 ME based a bidirectional inference   97.15 Guided learning for bidirectional sequence classification   97.33 AdaBoost.SDF with candidate features   97.32 AdaBoost.SDF with candidate features   97.32 SVM with candidate features   97.32 Text Chunking F=1 Regularized Winnow + full parser output   94.17 SVM-voting   93.91 ASO + unlabeled data   94.39 CRF+Reranking  94.12 ME based a bidirectional inference   93.70 LaSo     94.4 HySOL   94.36 AdaBoost.SDF with candidate featuers   94.32 AdaBoost.SDF with candidate featuers   94.30 SVM with candidate features   94.31 One of the reasons that boosting-based classifiers realize faster classification speed is sparseness of rules.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "boosting-based classifiers",
                "realize faster classification speed",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "rules",
                "sparseness of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The translation models were pharse-based   created using the GIZA++ toolkit  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ toolkit",
                "created using",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translation models",
                "phrase-based",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " , 1We are overloading the word state to mean Arabic word position.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word state",
                "mean Arabic word position",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word state",
                "overloading",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, extractive text summarization generates a summary by selecting a few good sentences from one or more articles on the same topic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "extractive text summarization",
                "selecting a few good sentences",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "good sentences",
                "from one or more articles",
                "PERFORMANCE",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Charniak   gives a thorough explanation of the equations for an HMM model, and Kupiec   describes an HMM tagging system in detail.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "equations",
                "thorough explanation",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "HMM model",
                "describes in detail",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  and Magerman   used the clustering algorithm of Brown et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "of Brown et al",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "clustering algorithm",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The parameters of the MT system were optimized on MTEval02 data using minimum error rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT system",
                "optimized",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "MTEval02 data",
                "using",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he use of structured prediction to SMT is also investigated by  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "structured prediction",
                "to SMT",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "SMT",
                "is also investigated",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Similarly, Smadja   uses a six content word window to extract significant collocations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "content word window",
                "six content word",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "collocations",
                "significant",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The translation models they presented in various papers between 1988 and 1993   are commonly referred to as IBM models 15, based on the numbering in Brown, Della Pietra, Della Pietra, and Mercer  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM models",
                "commonly referred to",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "Brown, Della Pietra, Della Pietra, and Mercer",
                "numbering in",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ord association norms based on co-occurrence information have been proposed by  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "norms",
                "based on co-occurrence information",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "proposed",
                "by",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This problem has been considered for instance in   for his inversion transduction grammars and has applications in the support of several tasks of automatic annotation of parallel corpora, as for instance segmentation, bracketing, phrasal and word alignment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "The results have demonstrated the existence of priming effects in corpus data: they occur for specific syntactic constructions  , consistent with the experimental literature, but also generalize to syntactic rules across the board, which repeated more often than expected by chance  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "priming effects",
                "exist in corpus data",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "syntactic constructions",
                "occur more often than expected by chance",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": " , is to translate dependency parses into neo-Davidsonian-style quasilogical forms, and to perform weighted abductive theorem proving in the tradition of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency parses",
                "into neo-Davidsonian-style quasilogical forms",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "weighted abductive theorem proving",
                "in the tradition of",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "odel weights were also trained following Och  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model weights",
                "trained following Och",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Och",
                "standard",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "We rerank derivations with cube growing, a lazy beam search algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "derivation",
                "with cube growing, a lazy beam search algorithm",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "algorithm",
                "lazy",
                "METHODOLOGY",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "5 Synchronous DIG 5.1 Definition   introduced synchronous binary trees and   introduced synchronous tree adjoining grammars, both of which view the translation process as a synchronous derivation process of parallel trees.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "DIG 5.1 Definition",
                "introduced synchronous binary trees and introduced synchronous tree adjoining grammars",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "translation process",
                "as a synchronous derivation process of parallel trees",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Most work in machine learning literature on utilizing labeled features has focused on using them to generate weakly labeled examples that are then used for standard supervised learning:   propose one such framework for boosting logistic regression;   build a modi ed SVM and   use a combination of clustering and EM based methods to instantiate similar frameworks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning literature",
                "focusing on generating weakly labeled examples",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "frameworks",
                "instantiating similar frameworks",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "To identify these, we use a word-aligned corpus annotated with parse trees generated by statistical syntactic parsers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parse trees",
                "generated by statistical syntactic parsers",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpus",
                "annotated with parse trees",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The second uses Lin dependency similarity, a syntacticdependency based distributional word similarity resource described in  9.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Lin dependency similarity",
                "syntactic dependency based distributional word similarity resource",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "dependency similarity",
                "syntactic dependency based",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "GIZA++   and the heuristics grow-diag-final-and are used to generate m-ton word alignments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "used to generate m-ton word alignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "heuristics grow-diag-final-and",
                "used to generate m-ton word alignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, in the context of syntactic disambiguation, Black   and Magerman   proposed statistical parsing models based-on decisiontree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decisiontree learning techniques",
                "statistical parsing models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "decision-trees",
                "incorporated not only syntactic but also lexical/semantic information",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he adaptive approach is somehow similar to their idea of incremental learning and to the bootstrap approach proposed by Yarowsky  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Finally, we are investigating several avenues for using this system output for Machine Translation   including:   aiding word alignment for other MT system  ; and   aiding the creation various MT models involving analyzed text, e.g.,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT system",
                "aiding word alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "MT models",
                "involving analyzed text",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "An alternative is to create an automatic system that uses a set of training question-answer pairs to learn the appropriate question-answer matching algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "question-answer pairs",
                "to learn the appropriate question-answer matching algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "question-answer matching algorithm",
                "appropriate",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "PairClass is most similar to the algorithm of Turney  , but it differs in the following ways:  PairClass does not use a lexicon to find synonyms for the input word pairs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "PairClass does not use a lexicon",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "PairClass",
                "differ in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In order to extract the linguistic features necessary for the model, all sentences were first automatically part-of-speech-tagged using a maximum entropy tagger   and parsed using the Collins parser  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy tagger",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Collins parser",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5.2 Pseudo-Disambiguation Task Pseudo-disambiguation tasks have become a standard evaluation technique   and, in the current setting, we may use a nouns neighbors to decide which of two co-occurrences is the most likely.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pseudo-disambiguation tasks",
                "have become a standard evaluation technique",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "nouns neighbors",
                "to decide which of two co-occurrences is the most likely",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For BPM, we run 100 averaged perceptrons   with 10 iterations for each.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptrons",
                "with 10 iterations",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "iterations",
                "for each",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There are five different IBM translation models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM translation models",
                "five different",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Equation   reads If the target noun appears, then it is distinguished by the majority . The log-likelihood ratio   decides in which order rules are applied to the target noun in novel context.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rules",
                "applied to the target noun in novel context",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "log-likelihood ratio",
                "decides",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": " , bilingual sentences are trained by GIZA++   in two directions  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "trained by",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "bilingual sentences",
                "in two directions",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The parser expresses distinctions that are especially important for a predicate-argument based deep syntactic representation, as far as they are expressed in the training data generated from the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "especially important",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "training data",
                "generated from Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There are multiple studies   showing that the agreement between two   native speakers is about upper a15 a12a14a7 to lower a0a4a12a14a7.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "agreement",
                "about upper a15 a12a14a7 to lower a0a4a12a14a7",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "studies",
                "showing",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.3 Using Unlabeled Data for Parsing Recent studies on parsing indicate that the use of unlabeled data by self-training can help parsing on the WSJ data, even when labeled data is relatively large  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unlabeled data",
                "can help parsing",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "labeled data",
                "is relatively large",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Oncetraininghastakenplace,minimumerrorrate training   is used to tune the parameters i. Finally, decoding in Hiero takes place using a CKY synchronous parser with beam search, augmented to permit efficient incorporation of language model scores  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "tuning using minimum error rate",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parser",
                "efficient incorporation of language model scores",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "During the SRC stage, a Maximum entropy   classifier is used to predict the probabilities of a word in the sentence Language No-duplicated-roles Catalan arg0-agt, arg0-cau, arg1-pat, arg2-atr, arg2-loc Chinese A0, A1, A2, A3, A4, A5, Czech ACT, ADDR, CRIT, LOC, PAT, DIR3, COND English A0, A1, A2, A3, A4, A5, German A0, A1, A2, A3, A4, A5, Japanese DE, GA, TMP, WO Spanish arg0-agt, arg0-cau, arg1-pat, arg1-tem, arg2-atr, arg2-loc, arg2-null, arg4-des, argL-null, argMcau, argM-ext, argM-fin Table 1: No-duplicated-roles for different languages to be each semantic role.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum entropy classifier",
                "used to predict the probabilities of a word",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "No-duplicated-roles",
                "for different languages to be each semantic role",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The reliability for the two annotation tasks  ) was of 0.94 and 0.90 respectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotation tasks",
                "was of 0.94 and 0.90 respectively",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "reliability",
                "0.94 and 0.90 respectively",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "First, a non-anaphoric NP classifier identifies definite noun phrases that are existential, using both syntactic rules and our learned existential NP recognizer  , and removes them from the resolution process.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifier",
                "using both syntactic rules and our learned existential NP recognizer",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "resolution process",
                "removes definite noun phrases",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The syntactic and part-of-speech informations were obtained from the part of the corpus processed in the Penn Treebank project  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank project",
                "processed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpus",
                "part of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Numbers in the table correspond to the percentage of experiments in which the condition at the head of the column was true   proach  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "condition at the head of the column",
                "was true",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "experiments",
                "percentage of",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "6 Conclusions and Future Directions In previous work, statistical NLP computation over large corpora has been a slow, of ine process, as in KNOWITALL   and also in PMI-IR applications such as sentiment classi cation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NLP computation",
                "slow process",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "PMI-IR applications",
                "sentiment classification",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To this end we follow the method introduced by  , i.e. by sliding a window of a given size over some texts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "by sliding a window of a given size over some texts",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "window size",
                "given size",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There are cases, though, where the labels consist of several related, but not entirely correlated, properties; examples include mention detectionthe task we are interested in, syntactic parsing with functional tag assignment  ), and, to a lesser extent, part-of-speech tagging in highly inflected languages.4 The particular type of mention detection that we are examining in this paper follows the ACE general definition: each mention in the text   is assigned three types of information:5  An entity type, describing the type of the entity it points to    An entity subtype, further detailing the type    A mention type, specifying the way the entity is realized  a mention can be named  , nominal  , or pronominal  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mention detection",
                "task we are interested in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ACE general definition",
                "assigns three types of information",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, at the short term, the incorporation of these type of features will force us to either build a new decoder or extend an existing one, or to move to a new MT architecture, for instance, in the fashion of the architectures suggested by Tillmann and Zhang   or Liang et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT architecture",
                "new",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "architectures suggested by Tillmann and Zhang or Liang et al.",
                "fashion of",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The class-based approaches   calculate co-occurrence data of words belonging to different classes,~ rather than individual words, to enhance the co-occurrence data collected and to cover words which have low occurrence frequencies.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "co-occurrence data",
                "calculate co-occurrence data of words belonging to different classes",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "words with low occurrence frequencies",
                "cover words which have low occurrence frequencies",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Language models, such as N-gram class models   and Ergodic Hidden Markov Models   were proposed and used in applications such as syntactic class   tagging for English  , clustering and scoring of recognizer sentence hypotheses.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "N-gram class models",
                "proposed",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "N-gram class models",
                "used in applications",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This permits us to make exact comparisons with the parser of Yamada and Matsumoto  , but also the parsers of Collins   and Charniak  , which are evaluated on the same data set in Yamada and Matsumoto  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "of Yamada and Matsumoto",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parsers of Collins and Charniak",
                "evaluated on the same data set in Yamada and Matsumoto",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Corpora in various languages, such as the English Penn Treebank corpus  , the Swedish Stockholm-Ume corpus  , and the Icelandic Frequency Dictionary   corpus  , have been used to train   and develop   different taggers, and to evaluate their accuracy, e.g.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "such as the English Penn Treebank corpus, the Swedish Stockholm-Ume corpus, and the Icelandic Frequency Dictionary corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "taggers",
                "evaluate their accuracy",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We use the IBM Model 1     and the Hidden Markov Model  ) to estimate the alignment model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 1",
                "to estimate the alignment model",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Hidden Markov Model",
                "to estimate the alignment model",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A statistical language model  a lexicalized PCFG    is derived from the analysis grammar by processing a corpus using the same grammar with no statistical model and recording frequencies of substructures built by each rule.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical language model",
                "derived from the analysis grammar",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "corpus",
                "using the same grammar",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Much research is also being directed at acquiring affect lexica automatically  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "affect lexica",
                "acquiring automatically",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "research",
                "is directed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Second, McDonald and Satta   propose an O  algorithm for computing the marginals, as opposed to the O  matrix-inversion approach used by Smith and Smith   and ourselves.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "O algorithm for computing the marginals",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approach",
                "matrix-inversion approach",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "  presented results suggesting that the additional parameters required to ensure that a model is not deficient result in inferior performance, but we plan to study whether this is the case for our generative model in future work.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "required to ensure a model is not deficient",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "performance",
                "inferior",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "Iterating between these two 1 Note that these problems are associated with corpus-based approaches in general, and have been identified by a number of researchers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus-based approaches",
                "associated with problems",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "researchers",
                "identified by",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Ontologies are formal specifications of a conceptualization   so that it seems straightforward to formalize annotation schemes as ontologies and make use of semantic annotation tools such as OntoMat   for the purpose of linguistic annotation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ontologies",
                "formal specifications of a conceptualization",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "semantic annotation tools",
                "such as OntoMat",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In order to extract the linguistic features necessary for the models, all sentences containing the target word were automatically part-of-speech-tagged using a maximum entropy tagger   and parsed using the Collins parser  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy tagger",
                "automatically part-of-speech-tagged",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Collins parser",
                "parsed using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "his is the scenario considered by Haghighi and Klein   for POS tagging: how to construct an accurate tagger given a set of tags and a few example words for each of those tags",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tags",
                "accurate tagger",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "example words",
                "given a set of tags",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "arletta   deserves the credit for bringing  to the attention of computational linguists",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "arletta",
                "bringing to the attention of computational linguists",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "credit",
                "deserves",
                "APPLICABILITY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "or placing the head the center function center    is used: the average position of the source words with which the target word e i1 is aligned",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "head",
                "center function center",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "average position",
                "of the source words with which the target word e i1 is aligned",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The typical practice of preprocessing distributional data is to remove rare word co-occurrences, thus aiming to reduce noise from idiosyncratic word uses and linguistic processing errors and at the same time form more compact word representations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rare word co-occurrences",
                "remove",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word representations",
                "form more compact",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "To compute the degree of interaction between two proteins D4 BD and D4 BE, we use the information-theoretic measure of pointwise mutual information  , which is computed based on the following quantities: 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pointwise mutual information",
                "is computed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "quantities",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For instance, word alignment models are often trained using the GIZA++ toolkit  ; error minimizing training criteria such as the Minimum Error Rate Training   are employed in order to learn feature function weights for log-linear models; and translation candidates are produced using phrase-based decoders   in combination with n-gram language models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ toolkit",
                "often trained using",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "translation candidates",
                "produced using phrase-based decoders in combination with n-gram language models",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Previous work on linguistic annotation pipelines   has enforced consistency from one stage to the next.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linguistic annotation pipelines",
                "enforced consistency from one stage to the next",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "consistency",
                "enforced",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Manual processes, such as lexicon development could be automated in the future using standard contextbased, word distribution methods  , or other corpus-based techniques.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicon development",
                "could be automated",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "context-based, word distribution methods",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Uses for k-best lists include minimum Bayes risk decoding  , discriminative reranking  , and discriminative training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "k-best lists",
                "minimum Bayes risk decoding",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "k-best lists",
                "discriminative reranking and discriminative training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Empty categories however seem different, in that, for the most part, their location and existence is determined, not by observable data, but by explicitly constructed linguistic principles, which 1 Both Collins   and Higgins   are explicit about this predisposition.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linguistic principles",
                "explicitly constructed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "location and existence",
                "determined",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "These probabilities are estimated with IBM model 1   on parallel corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM model 1",
                "on parallel corpora",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "probabilities",
                "estimated with",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "1 Introduction Mainstream approaches in statistical parsing are based on nondeterministic parsing techniques, usually employing some kind of dynamic programming, in combination with generative probabilistic models that provide an n-best ranking of the set of candidate analyses derived by the parser  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "nondeterministic parsing techniques",
                "usually employing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "generative probabilistic models",
                "provide an n-best ranking",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "2 Problem Setting In the multi-class setting, instances from an input spaceX take labels from a finite setY,|Y| = K. 496 We use a standard approach   for generalizing binary classification and assume a feature function f  Rd mapping instances xX and labels yY into a common space.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature function",
                "mapping instances xX and labels yY into a common space",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "standard approach",
                "for generalizing binary classification",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Setting the gradient to zero yields the usual maximum entropy constraints  , except that in this case the empirical values are themselves expectations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "gradient",
                "setting to zero",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "empirical values",
                "themselves expectations",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We also compare our algorithm to Structural Correspondence Learning    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "to Structural Correspondence Learning",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Structural Correspondence Learning",
                "no opinion term",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the supervised condition, we used just 2 additional task instances, plant and tank, each with 4000 handannotated instances drawn from a large balanced corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "task instances",
                "4000 handannotated instances drawn from a large balanced corpus",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "corpus",
                "large balanced",
                "INNOVATION",
                "positive",
                0.6
            ]
        ]
    },
    {
        "text": "the syntax-based system, we ran a reimplementation of the Collins parser   on the English half of the bitext to produce parse trees, then restructured and relabeled them as described in Section 3.2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins parser",
                "reimplementation",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parse trees",
                "restructured and relabeled",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ur POS tagger is essentially the maximum entropy tagger by Ratnaparkhi   retrained on the CTB-I data",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tagger",
                "retrained on the CTB-I data",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "tagger",
                "by Ratnaparkhi",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "  describe how to learn hundreds of millions of treetransformation rules from a parsed, aligned Chinese/English corpus, and Galley et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treetransformation rules",
                "learn hundreds of millions",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parsed, aligned Chinese/English corpus",
                "used to learn",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A few exceptions are the hierarchical   transduction models   and the string transduction models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transduction models",
                "hierarchical",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "transduction models",
                "string",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We then proceed to split the data into smaller sentences and tag them using Ratnaparkhis Maximum Entropy Tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ratnaparkhis Maximum Entropy Tagger",
                "tagging sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Maximum Entropy Tagger",
                "widely used tool",
                "METHODOLOGY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "Systems based on word-to-word lexicons, such as the IBM systems  , incorporate further devices that allow reordering of words   and ranking of alternatives  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word-to-word lexicons",
                "allow reordering of words",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word-to-word lexicons",
                "ranking of alternatives",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4.2 Smoothing: Gaussian Priors Since NLP maximum entropy models usually have lots of features and lots of sparseness  , smoothing is essential as a way to optimize the feature weights  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "lots",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "smoothing",
                "essential",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For the evaluation of translation quality, we applied standard automatic evaluation metrics, i.e., BLEU   and METEOR  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "automatic evaluation metrics",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU and METEOR",
                "evaluation quality",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Yarowsky   proposes a method for word sense disambiguation, which is based on Monolingual Bootstrapping.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "based on Monolingual Bootstrapping",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word sense disambiguation",
                "proposed",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our approach is to use finite-state approximations of long-distance dependencies, as they are described in   for Dependency Grammar   and   for Lexical Functional Grammar  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "finite-state approximations",
                "as they are described in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Dependency Grammar",
                "described in",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "  used the Base-NP tag set as presented in  : I for inside a Base-NP, O for outside a Base-NP, and B for the first word in a Base-NP following another Base-NP.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Base-NP tag set",
                "as presented in",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "first word in a Base-NP",
                "following another Base-NP",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There are many method proposed to extract rigid expressions from corpora such as a method of focusing on the binding strength of two words  ; the distance between words  ; and the number of combined words and frequency of appearance  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "binding strength of two words",
                "of focusing on",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We evaluate translation output using case-insensitive BLEU  , as provided by NIST, and METEOR  , version 0.6, with Porter stemming and WordNet synonym matching.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "as provided by NIST",
                "PERFORMANCE",
                "neutral",
                0.85
            ],
            [
                "METEOR version 0.6",
                "with Porter stemming and WordNet synonym matching",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Most recently, Yarowsky used an unsupervised learning procedure to perform WSD  , although this is only tested on disambiguating words into binary, coarse sense distinction.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unsupervised learning procedure",
                "to perform WSD",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "WSD",
                "binary, coarse sense distinction",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In order to capture the dependency relationship between lexcial heads Collins   breaks down the rules from head outwards, which prevents us from factorizing them in other ways.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rules",
                "breaks down the rules from head outwards",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "factorizing",
                "in other ways",
                "LIMITATION",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "Then we use both Moses decoder and its suppo We run the decoder with its d then use Moses' implementation of minimum error rate training   to tune the feature weights on the development set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Moses decoder",
                "use",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "feature weights",
                "tune on the development set",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  show that exploiting all contiguous word blocks in phrase-based alignment is better than focusing on syntactic constituents only.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based alignment",
                "better",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "syntactic constituents",
                "only",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Systems were optimized on the WMT08 French-English development data   using minimum error rate training   and tested on the WMT08 test data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WMT08 French-English development data",
                "minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "WMT08 test data",
                "tested on",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This source of overcounting is considered and fixed by Wu   and Zens and Ney  , which we briefly review here.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wu and Zens and Ney",
                "fixed by",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "source of overcounting",
                "considered",
                "LIMITATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The elements of this set are pairs   where y is a possible translation for x. 4 IBMs model 1 IBMs model 1 is the simplest of a hierarchy of five statistical models introduced in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model 1",
                "is the simplest of a hierarchy of five statistical models",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "statistical models",
                "introduced in ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This is an instance of the ITG alignment algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG alignment algorithm",
                "instance of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Och   introduced minimum error rate training   as an alternative training regime to the conditional likelihood objective previously used with log-linear translation models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "minimum error rate training",
                "alternative training regime",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "conditional likelihood objective",
                "previously used",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In fact, the largest source of English dependency trees is automatically generated from the Penn Treebank   and is by convention exclusively projective.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "automatically generated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dependency trees",
                "exclusively projective",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The underlying formalisms used has been quite broad and include simple formalisms such as ITGs  , hierarchicalsynchronousrules , string to tree models by   and  , synchronous CFG models such    , synchronous Lexical Functional Grammar inspired approaches   and others.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITGs",
                "simple formalisms",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "synchronous CFG models",
                "synchronous",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There have been considerable amount of efforts to improve the reordering model in SMT systems, ranging from the fundamental distance-based distortion model  , flat reordering model  , to lexicalized reordering model  , hierarchical phrase-based model  , and maximum entropy-based phrase reordering model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reordering model",
                "fundamental distance-based distortion model",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "reordering model",
                "lexicalized reordering model",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The model we use is similar to that of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "similar to that of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These lists are rescored with the different models described above, a character penalty, and three different features based on IBM Models 1 and 2   calculated in both translation directions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "described above",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "features",
                "based on IBM Models 1 and 2",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction The statistical machine translation framework   formulates the problem of translating a sentence from a source language S into a target language T as the maximization problem of the conditional probability: TM LM = argmaxT p  p ,   where p  is called a translation model  , representing the generation probability from T into S, p  is called a language model   and represents the likelihood of the target language  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical machine translation framework",
                "formulates the problem",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "language model",
                "represents the likelihood",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Other systems   also look at Web product reviews but they do not extract 345 opinions about particular product features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "systems",
                "do not extract 345 opinions about particular product features",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "systems",
                "look at Web product reviews",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We would expect the opposite effect with hand-aligned data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hand-aligned data",
                "opposite effect",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "hand-aligned data",
                "opposite effect",
                "INNOVATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": ".1 Reader Judgments There is a growing concern surrounding issues of intercoder reliability when using human judgments to evaluate discourse-processing algorithms  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "We are encoding the knowledge as axioms in what is for the most part a first-order logic, described by Hobbs  , although quantification over predicates is sometimes convenient.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "axioms",
                "in first-order logic",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "quantification over predicates",
                "is convenient",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "3 Phrase-Based SMT According to the translation model presented in  , given a source sentence f, the best target translation best e can be obtained according to the following model )  | |  Where the translation model )| ,|  | |  Where )| ( 1  ii bad denote phrase translation probability and distortion probability, respectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "best target translation",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "phrase translation probability",
                "denote",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Two major research topics in this field are Named Entity Recognition     and Word Sense Disambiguation    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Named Entity Recognition",
                "research topic",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "Word Sense Disambiguation",
                "research topic",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This criticism leads us to automatic approaches for building thesauri from large corpora \\ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "thesauri",
                "building from large corpora",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "automatic approaches",
                "for building",
                "METHODOLOGY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Much of the work in subjectivity analysis has been applied to English data, though work on other languages is growing: e.g., Japanese data are used in  , Chinese data are used in  , and German data are used in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data",
                "used in",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "languages",
                "growing",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "he formally syntax-based model for SMT was first advocated by Wu  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wu",
                "formally syntax-based model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "SMT",
                "advocated",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": ".1 Likelihood Ratios in the Type-based Stage The log-likelihood ratio by Dunning   tests whether the probability of a word is dependent on the occurrence of the preceding word type",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-likelihood ratio",
                "tests whether the probability of a word is dependent on the occurrence of the preceding word type",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Dunning",
                "by",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Turney   argues that many NLP tasks can be formulated in terms of analogical reasoning, and he applies his PairClass algorithm to a number of problems including SAT verbal analogy tests, synonym/antonym classification and distinction between semantically similar and semantically associated words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PairClass algorithm",
                "applies to a number of problems",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "NLP tasks",
                "can be formulated in terms of analogical reasoning",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5.2 Experimental Results Following   and other work on general-purpose generators, BLEU score  , average NIST simple string accuracy   and percentage of exactly matched sentences are adopted as evaluation metrics.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation metrics",
                "BLEU score, average NIST simple string accuracy, percentage of exactly matched sentences",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "evaluation metrics",
                "adopted",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The publicly available Moses4 decoder is used for training and decoding  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Moses4 decoder",
                "used for training and decoding",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "Moses4 decoder",
                "publicly available",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Turney   applied an internet-based technique to the semantic orientation classification of phrases, whichhadoriginallybeendevelopedforwordsentiment classification.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "technique",
                "internet-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "orientation classification",
                "of phrases",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " , a robust risk minimization classifier, based on a regularized winnow method     and a maximum entropy classifier    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifier",
                "based on a regularized winnow method and a maximum entropy classifier",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "risk minimization",
                "robust",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "7Following Carletta  , we measure agreement in Kappa, which follows the formula K = P P 1P  where P  is observed, and P  expected agreement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa",
                "follows the formula K = P P 1P  where P  is observed, and P  expected agreement",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "agreement",
                "measured in Kappa",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To determine headwords of the semantic roles, the corpus was parsed using the Collins   parser.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins parser",
                "was parsed using",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "corpus",
                "parsed using Collins parser",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Lexical cues of differing complexities have been used, including single words and Ngrams  ), as well as phrases and lexico-syntactic patterns  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Lexical cues",
                "of differing complexities",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Ngrams",
                "as well as phrases and lexico-syntactic patterns",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We also plan to employ this evaluation metric as feedback in building dialogue coherence models as is done in machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation metric",
                "as feedback",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "machine translation",
                "as done",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "model reranking has also been established, both for synchronous binarization   and for target-only binarization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model reranking",
                "established",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "binarization",
                "both for synchronous and target-only",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In another line of research,   and   have shown that it is possible to reduce the need for supervision with the help of large amounts of unannotated data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unannotated data",
                "large amounts",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "supervision",
                "reduce the need for",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "For instance, some approaches coarsely discriminate between biographical and non-biographical information  ,whileothersgobeyondbinary distinction by identifying atomic events  e.g., occupation and marital status  that are typically included in a biography  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "biographical and non-biographical information",
                "coarsely discriminate",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "atomic events",
                "are typically included",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "1 Full Morphological Tagging English Part of Speech   tagging has been widely described in the recent past, starting with the   paper, followed by numerous others using various methods: neural networks  , HMM tagging  , decision trees  , transformation-based error-driven learning  , and maximum entropy  , to select just a few.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "various methods",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "neural networks, HMM tagging, decision trees, transformation-based error-driven learning, maximum entropy",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "6 Training Similar to most state-of-the-art phrase-based SMT systems, we use the SRI toolkit   for language model training and Giza++ toolkit   for word alignment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SRI toolkit",
                "for language model training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Giza++ toolkit",
                "for word alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There has been considerable skepticism over whether WSD will actually improve performance of applications, but we are now starting to see improvement in performance due to WSD in cross-lingual information retrieval   and machine translation   and we hope that other applications such as question-answering, text simplication and summarisation might also benet as WSD methods improve.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSD",
                "improve performance",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "WSD methods",
                "might also benefit",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It is often straightforward to obtain large amounts of unlabeled data, making semi-supervised approaches appealing; previous work on semisupervised methods for dependency parsing includes  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "large amounts of unlabeled data",
                "straightforward to obtain",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "semisupervised approaches",
                "appealing",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "For the English experiments, we use the now-standard training and test sets that were introduced in  2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training and test sets",
                "now-standard",
                "METHODOLOGY",
                "positive",
                0.7
            ],
            [
                "training and test sets",
                "introduced in 2",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Note that this early discarding is related to ideas behind cube pruning  , which generates the top n most promising hypotheses, but in our method the decision not to generate hypotheses is guided by the quality of hypotheses on the result stack.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hypotheses",
                "guided by the quality of hypotheses on the result stack",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "quality of hypotheses",
                "on the result stack",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "With our best performing features, we get ROUGE-2   scores of 0.11 and 0.0925 on 2007 and 2006 5This threshold was derived experimentally with previous data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE-2",
                "scores of 0.11 and 0.0925",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "threshold",
                "derived experimentally with previous data",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  are appealing, as they have rather simple structure, modeling only NP, VP and LCP via one-level sub-tree structure with two children, in the source parse-tree  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parse-tree",
                "one-level sub-tree structure with two children",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "NP, VP, and LCP",
                "simple structure",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The performance of the related work   is listed in Table4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "related work",
                "listed in Table4",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Table4",
                "no opinion term",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Parsers that attempt to disambiguate the input completely  full parsing  typically first employ some kind of dynamic programming algorithm to derive a packed parse forest and then applies a probabilistic top-down model in order to select the most probable analysis  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsers",
                "dynamic programming algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "probabilistic top-down model",
                "select the most probable analysis",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Performance of Alternative Models 157 5 Related Work Previous parsing models   maximize the joint probability P  of a sentence S and its parse tree T. We maximize the conditional probability P .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Previous parsing models",
                "maximize the joint probability P of a sentence S and its parse tree T",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "conditional probability P",
                "maximize",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "he local dependencies between sentiment labels on sentences is similar to the work of Pang and Lee   where soft local consistency constraints were created between every sentence in adocument and inference wassolved using a min-cut algorithm",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "local dependencies",
                "similar to the work of Pang and Lee",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "min-cut algorithm",
                "used for inference",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "n alternative representation for baseNPs has been put tbrward by Ramshaw and Marcus  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ramshaw and Marcus",
                "alternative representation for baseNPs",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "alternative representation",
                "put forward",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Wordalignment, however, isalmost exclusively done using statistics  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wordalignment",
                "almost exclusively done using statistics",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "using statistics",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ughes and Ramage   present a lexical similarity model based on random walks on graphs derived from WordNet; Rao et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexical similarity model",
                "based on random walks on graphs derived from WordNet",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "graphs derived from WordNet",
                "derived from",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The k-best list is very important for the minimum error rate training   which is used for tuning the weights  for our model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "k-best list",
                "very important",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "weights for our model",
                "tuning",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "During training, the early update strategy of Collins and Roark   is used: when the correct state item falls out of the beam at any stage, parsing is stopped immediately, and the model is updated using the current best partial item.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "early update strategy",
                "used by Collins and Roark",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parsing is stopped immediately",
                "when the correct state item falls out of the beam",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The details of the algorithm can be found in the literature for statistical translation models, such as  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "found in the literature for statistical translation models",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "algorithm",
                "such as ",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "A synchronous 363 binarization method is proposed in   whose basic idea is to build a left-heavy binary synchronous tree   with a left-to-right shift-reduce algorithm.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "In addition, uniform conditioning on mother grammatical function is more general than the case-phenomena specific generation grammar transform of  , in that it applies to each and every sub-part of a recursive input f-structure driving generation, making available relevant generation history   to guide local generation decisions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "uniform conditioning",
                "more general",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "generation grammar transform",
                "case-phenomena specific",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Recent computational work either focuses on sentence subjectivity  , concentrates just on explicit statements of evaluation, such as of films  , or focuses on just one aspect of opinion, e.g.,   on adjectives.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "computational work",
                "focuses on",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "explicit statements of evaluation",
                "concentrates on",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Portage is a statistical phrase-based SMT system similar to Pharaoh  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Portage",
                "statistical phrase-based SMT system",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Portage",
                "similar to Pharaoh",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Features For each frame element, features are extracted from the surface text of the sentence and from an automatically generated syntactic parse tree  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "extracted from the surface text of the sentence and from an automatically generated syntactic parse tree",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "syntactic parse tree",
                "automatically generated",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Extracting paraphrases Much previous work on extracting paraphrases   has focused on finding identifying contexts within aligned monolingual sentences from which divergent text can be extracted, and treated as paraphrases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "paraphrases",
                "extracting paraphrases",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "previous work",
                "focused on finding identifying contexts within aligned monolingual sentences",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "It is difficult to compare these with previous work, but Haghighi and Klein   report that in a completely unsupervised setting, their MRF model, which uses a large set of additional features and a more complex estimation procedure, achieves an average 1-to-1 accuracy of 41.3%.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MRF model",
                "uses a large set of additional features and a more complex estimation procedure",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "average 1-to-1 accuracy of 41.3%",
                "41.3%",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In all experiments, word alignment was obtained using the grow-diag-final heuristic for symmetrizing GIZA++   alignments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ alignments",
                "symmetrizing",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "grow-diag-final heuristic",
                "obtained word alignment",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For an alignment model, most of these use the Aachen HMM approach  , the implementation of IBM Model 4 in GIZA++   or, more recently, the semi-supervised EMD algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Aachen HMM approach",
                "use",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "EMD algorithm",
                "semi-supervised",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Yarowsky   uses a conceptually similar technique for WSD that learns from a small set of seed examples and then increases recall by bootstrapping, evaluated on 12 idiosyncratically polysemous words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Yarowsky",
                "conceptually similar technique",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "bootstrapping",
                "increases recall",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "he first is a novel stochastic search strategy that appears to make better use of Och  s algorithm for finding the global minimum along any given search direction than either coordinate descent or Powells method",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Och's algorithm",
                "make better use",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "Powell's method",
                "than",
                "METHODOLOGY",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "Sang used the IOB tagging method proposed by Ramshow  and memory-based learning for each level of chunking and achieved an f-score of 80.49 on the Penn Treebank corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IOB tagging method",
                "proposed by Ramshow",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "f-score of 80.49",
                "achieved",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Kim and Hovy   proposed a method for extracting opinion holders, topics and opinion words, in which they use semantic role labeling as an intermediate step to label opinion holders and topics.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic role labeling",
                "as an intermediate step to label opinion holders and topics",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "opinion holders and topics",
                "use semantic role labeling",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.1 Heuristic Grammar Induction Grammar based SMT models almost exclusively follow the same two-stage approach to grammar induction developed for phrase-based methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Grammar based SMT models",
                "follow the same two-stage approach",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase-based methods",
                "developed for",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "I Various models have been constructed by the IBM team  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "constructed by the IBM team",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "team",
                "IBM",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "In our case, we computed a likelihood ratio score   for all pairs of English tokens and Inuktitut substrings of length ranging from 3 to 10 characters.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "English tokens",
                "pairs of English tokens",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "length ranging from 3 to 10 characters",
                "ranging",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "See Hobbs   for explanation of this notation for events",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "notation",
                "explanation of this notation for events",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": ".1 Global linear models We follow the framework outlined in Collins  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Global linear models",
                "outline in Collins",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "framework",
                "outlined in Collins",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the WSD work involving the use of context, we can find two approaches: one that uses few strong contextual evidences for disambiguation purposes, as exemplified by  ; and the other that uses weaker evidences but considers a combination of a number of them, as exemplified by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "context",
                "strong contextual evidences",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "context",
                "weaker evidences",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank  , although there has also been much research on the use of shallow syntax   in SRL.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "systems",
                "used constituent syntax",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "research",
                "on the use of shallow syntax",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "ur approach thus provides an even more extreme version of automatic con rmation generation than that used byChu-Carroll and Carpenter   where only a small eort is required by the developer",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "even more extreme version",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "eort",
                "small",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As the third test set we selected all tokens of the Brown corpus part of the Penn Treebank  , a selected portion of the original one-million word Brown corpus  , a collection of samples of American English in many different genres, from sources printed in 1961; we refer to this test set as BROWN.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brown corpus",
                "a selected portion of the original one-million word Brown corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank",
                "a collection of samples of American English in many different genres",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In earlier IBM translation systems   each English word would be generated by, or \"\"aligned to\"\", exactly one formal language word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "English word",
                "generated by or aligned to",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "formal language word",
                "exactly one",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This formulation is similar to the energy minimization framework, which is commonly used in image analysis   and has been recently applied in natural language processing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "energy minimization framework",
                "commonly used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "image analysis and natural language processing",
                "applied in",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "After the parser produces a semantic feature structure representation of the sentence, predicate mapping rules then match against that representation in order to produce a predicate language representation in the style of Davidsonian event based semantics  , as mentioned above.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "predicate mapping rules",
                "match against that representation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "predicate language representation",
                "in the style of Davidsonian event based semantics",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "phrase-based machine translation does away with many of the problems associated with the original word-based formulation of statistical machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based machine translation",
                "does away with many of the problems",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "original word-based formulation of statistical machine translation",
                "problems associated with",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 Conclusions and Future Work The paper compares Structural Correspondence Learning   with   self-training   for the adaptation of a parse selection model to Wikipedia domains.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Structural Correspondence Learning",
                "parse selection model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "self-training",
                "adaptation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Given the probabilistic taxonomy learning model introduced by  , we leverage on the computation of logistic regression to exploit singular value decomposition   as unsupervised feature selection.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilistic taxonomy learning model",
                "introduced by",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "logistic regression",
                "to exploit singular value decomposition",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These techniques included unweighted FS morphology, conditional random fields  , synchronous parsers  , lexicalized parsers  ,22 partially supervised training `a la  ,23 and grammar induction  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "FS morphology",
                "unweighted",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parsers",
                "lexicalized",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Therefore, having correct transliterations would give only small improvements in terms of BLEU   and NIST scores.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "small improvements",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "NIST scores",
                "small improvements",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "avid Yarowsky   showed it was accurate in the word sense disambiguation",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Yarowsky",
                "accurate",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "word sense disambiguation",
                "in the",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "SO can be used to classify reviews   as positive or negative  , and applied to subjectivity analysis such as recognizing hostile messages, classifying emails, mining reviews  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SO",
                "used to classify reviews",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "SO",
                "applied to subjectivity analysis",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "More recently, Clarke and Lapata   use Centering Theory   and Lexical Chains   to identify which information to prune.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Centering Theory",
                "use",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Lexical Chains",
                "use",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In addition, we use the measure from Resnik  , which is computed using an intrinsic information content measure relying on the hierarchical structure of the category tree  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measure",
                "is computed using an intrinsic information content measure",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "category tree",
                "has a hierarchical structure",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "It has been used for diverse problems such as machine translation and sense disambiguation \\ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine translation",
                "has been used",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "sense disambiguation",
                "has been used",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Most prior work on the speci c problem of categorizing expressly opinionated text has focused on the binary distinction of positive vs. negative  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prior work",
                "focused on the binary distinction",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "binary distinction",
                "positive vs. negative",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The probabilities of derivation decisions are modelled using the neural network approximation   to a type of dynamic Bayesian Network called an Incremental Sigmoid Belief Network    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "neural network",
                "approximation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Incremental Sigmoid Belief Network",
                "type of dynamic Bayesian Network",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Different optimization techniques are available, like the Simplex algorithm or the special Minimum Error Training as described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "optimization techniques",
                "Simplex algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Minimum Error Training",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In many cases, improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "domain information",
                "taken from dictionaries or ontology",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "semi-supervised models",
                "improving",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "  approached chucking by using Transformation Based Learning .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Transformation Based Learning",
                "approached chucking by using",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Transformation Based Learning",
                "Transformation Based Learning",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For instance, Hughes and Ramage   constructed a graph which represented various types of word relations from WordNet, and compared random-walk similarity to similarity assessments from humansubject trials.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "graph",
                "constructed a graph which represented various types of word relations from WordNet",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "similarity assessments from humansubject trials",
                "compared to random-walk similarity",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Both calculate the precision of a translation by comparing it to a reference translation and incorporating a length penalty  .]",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "precision",
                "by comparing it to a reference translation and incorporating a length penalty",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "length penalty",
                "involves",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We rst recast the problem of estimating the IBM models   in a discriminative framework, which leads to an initial increase in word-alignment accuracy.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM models",
                "in a discriminative framework",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word-alignment accuracy",
                "initial increase",
                "PERFORMANCE",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "+ truecase 20.7   27.8   Table 2: Impact of truecasing on case-sensitive BLEU In a more integrated approach, factored translation models   allow us to consider grammatical coherence in form of partof-speech language models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "truecasing",
                "on case-sensitive BLEU",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "factored translation models",
                "allow considering grammatical coherence",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "To avoid this problem, we sample from a space of probable alignments, as is done in IBM models 3 and above  , and weight counts based on the likelihood of each alignment sampled under the current probability model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignments",
                "probable alignments",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "counts",
                "based on the likelihood of each alignment",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Acknowledgments I want to thank my fellow organizers of the shared task, Johan Hall, Sandra Kubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret, whoarealsoco-authorsofthelongerpaperonwhich this paper is partly based  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Johan Hall, Sandra Kubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret",
                "fellow organizers",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "longer paper",
                "partly based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Different approaches have been proposed to measure matches using words or more meaningful semantic units, for example, ROUGE  , factoid analysis  , pyramid method  , and Basic Element    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE",
                "proposed to measure matches",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Basic Element",
                "proposed to measure matches",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Although this approach can give inaccurate estimates, the counts given to the incorrect senses will disperse randomly throughout the hierarchy as noise, and by accumulating counts up the hierarchy we will tend to gather counts from the correct senses of related words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "counts",
                "will disperse randomly throughout the hierarchy as noise",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "counts",
                "tend to gather counts from the correct senses",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The mixture coefficients are trained in the usual way  , so that the additional context is exploited when it is useful and ignored when it isnt. The paper proceeds as follows.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mixture coefficients",
                "trained in the usual way",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "additional context",
                "is useful",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Three K-means algorithms using different distributional similarity or dissimilarity measures: cosine, -skew divergence   4 , and Lins similarity  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "K-means algorithms",
                "using different distributional similarity or dissimilarity measures",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Lins similarity",
                "no opinion term",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "WSD has received increasing attention in recent literature on computational linguistics  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSD",
                "received increasing attention",
                "INNOVATION",
                "neutral",
                0.7
            ],
            [
                "literature on computational linguistics",
                "recent",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "More specically, they used a parser   to determine the constituent structure of the sentences from which the grammatical function for each NP was derived.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "determine the constituent structure of the sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "NP",
                "grammatical function for each",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Alternatively, one can view   as inducing an alignment between terms in the h to the terms in the t, somewhat similar to alignment models in statistical MT  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment models",
                "somewhat similar",
                "METHODOLOGY",
                "positive",
                0.7
            ],
            [
                "terms in the h",
                "to the terms in the t",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Related Work Recently, several studies have reported about dialog systems that are capable of classifying emotions in a human-computer dialog  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dialog systems",
                "capable of classifying emotions",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "studies",
                "reported",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "otice that most in-context and dictionary translations of source words are bounded within the same category in a typical thesaurus such as the LLOCE   and CILIN  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "thesaurus",
                "within the same category",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "LLOCE and CILIN",
                "typical",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "To some extent, this can probably be explained by the strong tradition of constituent analysis in Anglo-American linguistics, but this trend has been reinforced by the fact that the major treebank of American English, the Penn Treebank  , is annotated primarily with constituent analysis.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "annotated primarily with constituent analysis",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "Anglo-American linguistics",
                "strong tradition of constituent analysis",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "ence we use a beam-search decoder during training and testing; our idea is similar to that of Collins and Roark   who used a beam-search decoder as part of a perceptron parsing model",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "beam-search decoder",
                "used during training and testing",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "idea",
                "similar to that of Collins and Roark",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In our research, 23 scores, namely BLEU   with maximum n-gram lengths of 1, 2, 3, and 4, NIST   with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM   with exponents of 1.0, 2.0, and 3.0, METEOR    , WER  , PER  , and ROUGE   with n-gram lengths of 1, 2, 3, and 4 and 4 variants  , were used to calculate each similarity S i . Therefore, the value of m in Eq.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU, NIST, GTM, METEOR, WER, PER, ROUGE",
                "with maximum n-gram lengths of 1, 2, 3, and 4",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "scores",
                "were used to calculate each similarity",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This was used, for example, by   in information extraction, and by   in POS tagging.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "information extraction",
                "in",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "POS tagging",
                "in",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our statistical tagging model is adjusted from standard bi-grams using the Viterbi-search   plus on-the-fly extra computing of lexical probabilities for unknown morphemes.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical tagging model",
                "adjusted from standard bi-grams",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "lexical probabilities",
                "on-the-fly extra computing",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "According to the Bayes Rule, the problem is transformed into the noisy channel model paradigm, where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bayes Rule",
                "maximum a posteriori solution",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "prior distribution",
                "for the channel source text",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The query tions, the syntax, semantics, and abstract knowledge representation have type declarations   which help to detect malformed representations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "type declarations",
                "help to detect malformed representations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "representations",
                "malformed",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "The CRF tagger was implemented in MALLET   using the original feature templates from  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CRF tagger",
                "implemented in MALLET",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "feature templates",
                "from ",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3 Perceptron Training The parsing problem is to find a mapping from a set of sentences x ??X to a set of parses y ??Y. We assume that the mapping F is represented through a feature vector   ??Rd and a parameter vector  ??Rd in the following way  : F  = argmax y?GEN      where GEN  denotes the set of possible parses for sentence x and     = summationtexti ii  is the inner product.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "F",
                "represented through a feature vector and a parameter vector",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "GEN",
                "set of possible parses",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "-X I-X 0 first word of a chunk of type X non-initial word in an X chunk word outside of any chunk This representation type is based on a representation proposed by Ramshaw and Marcus   for noun phrase chunks",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "representation type",
                "proposed by Ramshaw and Marcus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "noun phrase chunks",
                "chunk of type X",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The rules extracted from the training bitext have the following features: a114 P andP , the latter of which is not found in the noisy-channel model, but has been previously found to be a helpful feature  ; 210 Chiang Hierarchical Phrase-Based Translation a114 the lexical weights P w  andP w  , which estimate how well the words in  translate the words in   ; 4 a114 a penalty exp  for extracted rules, analogous to Koehns phrase penalty  , which allows the model to learn a preference for longer or shorter derivations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "P andP",
                "helpful feature",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "phrase penalty",
                "allows to learn preference for longer or shorter derivations",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Other recent work has applied M.E. to language modeling  , machine translation  , and reference resolution  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "M.E.",
                "to language modeling",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "machine translation",
                "applied",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Previous corpus-based sense disambiguation methods require substantial amounts of sense-tagged training data   or aligned bilingual corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus-based sense disambiguation methods",
                "require substantial amounts of sense-tagged training data",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpora",
                "aligned bilingual corpora",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Pearsons correlation coefficient is a standard measure of the correlation strength between two distributions; it can be calculated as follows:  = E  E E radicalbigE    2   where X =   and Y =   are vectors of numerical scores for each paraphrase provided by the humans and the competing systems, respectively, n is the number of paraphrases to score, and E  is the expectation of X. Cosine correlation coefficient is another popular alternative and was used by Nakov and Hearst  ; it can be seen as an uncentered version of Pearsons correlation coefficient:  = X.YbardblXbardblbardblYbardbl   Spearmans rank correlation coefficient is suitable for comparing rankings of sets of items; it is a special case of Pearsons correlation, derived by considering rank indices   as item scores . It is defined as follows:  = n summationtextx iyi    radicalBig nsummationtextx2i   2 radicalBig nsummationtexty2i   2   One problem with using Spearmans rank coefficient for the current task is the assumption that swapping any two ranks has the same effect.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pearsons correlation coefficient",
                "standard measure of correlation strength",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Spearmans rank correlation coefficient",
                "suitable for comparing rankings of sets of items",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In  , the use of a word graph is proposed as interface between an alignment-template SMT model and the IMT engine.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word graph",
                "proposed as interface",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "alignment-template SMT model",
                "IMT engine",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "TheData For our experiments we used a version of the British National Corpus parsed with the statistical parser of Collins  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "British National Corpus",
                "parsed with the statistical parser of Collins",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical parser of Collins",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Co-selection measures include precision and recall of co-selected sentences, relative utility  , and Kappa  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "co-selection measures",
                "precision and recall of co-selected sentences",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "co-selection measures",
                "relative utility",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Sentence Compression takes an important place for Natural Language Processing   tasks where specific constraints must be satisfied, such as length in summarization  , style in text simplification   or sentence simplification for subtitling  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tasks",
                "specific constraints",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "constraints",
                "must be satisfied",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The most commonly used automatic evaluation metrics, BLEU   and NIST  , are based on the assumption that The closer a machine translation is to a professional human translation, the better it is  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU and NIST",
                "most commonly used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "machine translation",
                "better",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "For instance, Pang and Lee   train an independent subjectivity classifier to identify and remove objective sentences from a review prior to polarity classification.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "subjectivity classifier",
                "independent",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "polarity classification",
                "prior to",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Different approaches have been proposed for modeling Pr  in Equation  : Zero-order models such as model 1, model 2,andmodel 3   and the rstorder models such as model 4, model 5  , hidden Markov model  , and model 6  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "zero-order models",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "models",
                "rstorder models",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Algorithms such as co-training     and the Yarowsky algorithm   make assumptions about the data that permit such an approach.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "make assumptions about the data",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Yarowsky algorithm",
                "such an approach",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "  and Bikel and Chiang   has demonstrated the applicability of the Collins   model for Czech and Chinese",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins model",
                "for Czech and Chinese",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "Collins model",
                "has demonstrated applicability",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A first family of libraries was based on a word alignment A, produced using the Refined method described in    : we call these the A libraries.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "A libraries",
                "based on a word alignment A",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Refined method",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Carpuat and Wu   integrated a WSD system into a phrase-based SMT system, Pharaoh  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSD system",
                "into a phrase-based SMT system",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Pharaoh",
                "phrase-based SMT system",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This is known as cost-based abduction  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "cost-based abduction",
                "is known as",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "abduction",
                "cost-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Model 1 is the word-pair translation model used in simple machine translation and understanding models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 1",
                "word-pair translation model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word-pair translation model",
                "used in simple machine translation and understanding models",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For example, the words test and exam are similar because both of them follow verbs such as administer, cancel, cheat on, conduct,  and both of them can be preceded by adjectives such as academic, comprehensive, diagnostic, difficult,  Many methods have been proposed to compute distributional similarity between words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "test",
                "follow verbs such as administer, cancel, cheat on, conduct",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "words",
                "can be preceded by adjectives such as academic, comprehensive, diagnostic, difficult",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The OP data consists of 2,452 documents from the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "OP data",
                "consists of 2,452 documents",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "Penn Treebank",
                "used",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Carletta mentions this problem, asking what the difference would be if the kappa statistic were computed across \"\"clause boundaries, transcribed word boundaries, and transcribed phoneme boundaries\"\"   rather than the sentence boundaries she suggested.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa statistic",
                "computed across clause boundaries, transcribed word boundaries, and transcribed phoneme boundaries",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence boundaries",
                "suggested",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In another generation approach, Barzilay and Lee   look for pairs of slotted word lattices that share many common slot fillers; the lattices are generated by applying a multiplesequence alignment algorithm to a corpus of multiple news articles about the same events.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lattices",
                "generated by applying a multiple-sequence alignment algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpus of multiple news articles",
                "about the same events",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Various corpus-based approaches to word sense disambiguation have been proposed  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus-based approaches",
                "have been proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word sense disambiguation",
                "proposed",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This can be the base of a principled method for detecting structural contradictions  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "base",
                "principled method",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "structural contradictions",
                "detecting",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We extract the named entities from the web pages using the Stanford Named Entity Recognizer  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stanford Named Entity Recognizer",
                "named entities from the web pages",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Stanford Named Entity Recognizer",
                "using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "studies",
                "carried out based on co-occurring frequencies",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word pairs",
                "in texts",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Decoding is carried-out using the Moses decoder  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Moses decoder",
                "is carried-out using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We use the union, re ned and intersection heuristics de ned in   which are used in conjunction with IBM Model 4 as the baseline in virtually all recent work on word alignment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "heuristics",
                "de ned in",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Model 4",
                "used as the baseline",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These rules can be learned from a parallel corpus using English parsetrees, Chinese strings, and word alignment  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsetrees",
                "English",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpus",
                "using English parsetrees, Chinese strings, and word alignment",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The Maximum Entropy Markov Model used in POS-tagging is described in detail in   and the LMR tagger here uses the same probability model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy Markov Model",
                "used in POS-tagging",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probability model",
                "same",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word contexts",
                "represent as vectors",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "similarity",
                "of the vectors",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A CYK-style decoder has to rely on binarization to preprocess the grammar as did in   to handle multi-nonterminal rules.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CYK-style decoder",
                "rely on binarization",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "grammar",
                "handle multi-nonterminal rules",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": ".1.1 Pointwise Mutual Information This measure for word similarity was first used in this context by Church and Hanks  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pointwise Mutual Information",
                "was first used in this context",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Church and Hanks",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The learning algorithm follows the global strategy introduced in   and adapted in   for partial parsing tasks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "introduced in and adapted in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "strategy",
                "global",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4 Experimental Work A part of the Wall Street Journal   which had been processed in the Penn Treebanck Project   was used in the experiments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal",
                "processed in the Penn Treebank Project",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "experiments",
                "used in",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As a sanity check, we duplicated Pang et al.s   baseline in which all unigrams that appear four or more times in the training documents are used as features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "all unigrams that appear four or more times in the training documents",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "baseline",
                "duplicated Pang et al.'s",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "791 and score the alignment template models phrases  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment template models",
                "score the",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "phrases",
                "no opinion term",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Our statistical engine 2.1 The statistical models In this study, we built an SMT engine designed to translate from French to English, following the noisy-channel paradigm flrst described by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical engine",
                "designed to translate from French to English",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "statistical models",
                "following the noisy-channel paradigm",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To search for the most probable parse, we use the heuristic search algorithm described in  , which is a form of beam search.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "heuristic search algorithm",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "beam search",
                "a form of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "urney   used collocation with excellent or poor to obtain positive and negative clues for document classification",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocation",
                "excellent or poor",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "clues",
                "positive and negative",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This method is described hereafter, while the subsequent steps, that use deeper   levels of knowledge, are implemented into the ARIOSTO_LEX lexical learning system, described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "described hereafter",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ARIOSTO_LEX lexical learning system",
                "described in ",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he later IBM models are formulated to prefer collocations  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM models",
                "prefer collocations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "collocations",
                "prefered by IBM models",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Zens and Ney   explore the re-orderings allowed by ITGs, and provide a formulation for the number of structures that can be built for a sentence pair of size n. ITGs explore almost all of permutation space when n is small, but their coverage of permutation space falls off quickly for n > 5  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITGs",
                "explore almost all of permutation space",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "ITGs",
                "coverage of permutation space falls off quickly",
                "PERFORMANCE",
                "negative",
                0.75
            ]
        ]
    },
    {
        "text": "Yarowsky   describes a 'semi-unsupervised' approach to the problem of sense disambiguation of words, also using a set of initial seeds, in this case a few high quality sense annotations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "semi-unsupervised",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "initial seeds",
                "high quality sense annotations",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Collins and Koo Discriminative Reranking for NLP Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997), or conjugate gradient methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Della Pietra, Della Pietra, and Lafferty",
                "1997",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "conjugate gradient methods",
                "or",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This step can be seen as a multi-label, multi-class call classi cation problem for customer care applications  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "call classification problem",
                "multi-label, multi-class",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "customer care applications",
                "customer care applications",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For a full derivation of the modified updates and for quite technical convergence proofs, see Collins, Schapire and Singer  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins, Schapire and Singer",
                "quite technical convergence proofs",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "modified updates",
                "full derivation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To generate word alignments we use GIZA++  , which implements both the IBM Models of Brown et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "implements both the IBM Models of Brown et al.",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "IBM Models of Brown et al.",
                "both",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A later study   found that performance increased to 87.2% when considering only those portions of the text deemed to be subjective.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "performance",
                "increased to 87.2%",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "portions of the text",
                "deemed to be subjective",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Table 2 shows results in lowercase BLEU   for both the baseline   and the improved baseline systems   on development and held151 out evaluation sets.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseline systems",
                "improved",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "results",
                "in lowercase BLEU",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Such a coding procedure covers, for example, how segmentation of a corpus is performed, if multiple tagging is allowed and if so, is it unlimited or are there just certain combinations of tags not allowed, is look ahead permitted, etc For further information on coding procedures we want to refer to \\ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "segmentation of a corpus",
                "performed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "combinations of tags",
                "not allowed",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The POS tagger uses the same contextual predicates as Ratnaparkhi  ; the supertagger adds contextual predicates corresponding to POS tags and bigram combinations of POS tags  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tagger",
                "same contextual predicates as Ratnaparkhi",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "supertagger",
                "adds contextual predicates corresponding to POS tags and bigram combinations of POS tags",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  Hindi is a verb final, flexible word order language and therefore, has frequent occurrences of non-projectivity in its dependency structures.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hindi",
                "verb final, flexible word order",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "dependency structures",
                "frequent occurrences of non-projectivity",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 The Data Our experiments on joint syntactic and semantic parsing use data that is produced automatically by merging the Penn Treebank   with PropBank    , as shown in Figure 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data",
                "produced automatically by merging the Penn Treebank and PropBank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Figure 1",
                "as shown",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In Section 3, we will present a Perceptron like algorithm   to obtain the parameters.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Perceptron like algorithm",
                "to obtain the parameters",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "like",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In the absence of an annotated corpus, dependencies can be derived by other means, e.g. part413 of-speech probabilities can be approximated from a raw corpus as in  , word-sense dependencies can be derived as definition-based similarities, etc. Label dependencies are set as weights on the arcs drawn between corresponding labels.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependencies",
                "can be derived by other means",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word-sense dependencies",
                "can be derived as definition-based similarities",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "reund and Schapire   discuss how the theory for classification problems can be extended to deal with both of these questions; Collins   describes how these results apply to NLP problems",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "theory",
                "extended to deal with both questions",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "results",
                "apply to NLP problems",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": " , extracts uninterrupted as well as interrupted collocations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocations",
                "uninterrupted as well as interrupted",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "collocations",
                "extracts",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Eisner  , Charniak  , Collins  , and many subsequent researchers1 annotated every node with lexical features passed up from its head child, in order to more precisely reflect the nodes inside contents.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexical features",
                "passed up from its head child",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "nodes",
                "more precisely reflect the nodes inside contents",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "This incremental process can be iterated to the point that the system 1 It is not just a matter of time, but also of required linguistic skills  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "incremental process",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "linguistic skills",
                "required",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "HockenmaierandSteedman showedthat a CCG corpus could be created by adapting the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CCG corpus",
                "could be created",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank",
                "adapted",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The field of statistical machine translation has been blessed with a long tradition of freely available software tools  such as GIZA++    and parallel corpora  such as the Canadian Hansards2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "software tools",
                "freely available",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parallel corpora",
                "Canadian Hansards2",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Another important direction is classifying sentences as subjective or objective, and classifying subjective sentences or clauses as positive or negative  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifying",
                "subjective or objective",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentences or clauses",
                "positive or negative",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There are many techniques for transliteration and back-transliteration, and they vary along a number of dimensions:  phoneme substitution vs. character substitution  heuristic vs. generative vs. discriminative models  manual vs. automatic knowledge acquisition We explore the third dimension, where we see several techniques in use:  Manually-constructed transliteration models, e.g.,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques",
                "various dimensions",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "techniques",
                "manual vs. automatic knowledge acquisition",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Intuitively, if we are able to find good correspondences through linking pivots, then the augmented source data should transfer better to a target domain  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "source data",
                "transfer better",
                "PERFORMANCE",
                "positive",
                0.7
            ],
            [
                "correspondences",
                "good",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Maximum Entropy Modeling As previously indicated, the weight-based scheme of L&L suggests MaxEnt modeling   as a particularly natural choice for a machine learning approach.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MaxEnt modeling",
                "a particularly natural choice",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "weight-based scheme of L&L",
                "suggests",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The proposed approach follows the same principle as  , which tried to determine the appropriate word sense according to one relevant context word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "same principle as",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "principle",
                "determine the appropriate word sense according to one relevant context word",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "his model is related to the averaged perceptron algorithm of Collins  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "related to the averaged perceptron algorithm of Collins",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "perceptron algorithm of Collins",
                "related",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Jiao et al. propose semi-supervised conditional random fields   that try to maximize the conditional log-likelihood on the training data and simultaneously minimize the conditional entropy of the class labels on the unlabeled data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "conditional random fields",
                "try to maximize the conditional log-likelihood and minimize the conditional entropy",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "class labels",
                "unlabeled data",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 Quasi-Synchronous Grammar For a formal description of QG, we recommend Smith and Eisner  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "QG",
                "formal description",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "Smith and Eisner",
                "recommend",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "imilar adaptations of the Matrix-Tree Theorem have been developed independently and simultaneouslybySmithandSmith andMcDonaldand Satta  ; see Section 5 for more discussion",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Matrix-Tree Theorem",
                "have been developed independently and simultaneously",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "more discussion",
                "for more discussion",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example,   only requires sense number and a few seeds for each sense of an ambiguous word  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sense number",
                "a few seeds",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ambiguous word",
                "only requires",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  introduced five statistical translation models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical translation models",
                "introduced five",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translation models",
                "",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Based on the data seen, a maximum entropy model   offers an expression   for the probability that there exists coreference C between a mention mi and a mention mj.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "maximum entropy model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probability",
                "offers an expression",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The other utilizes a sort of parallel texts, such as multiple translation of the same text  , corresponding articles from multiple news sources  , and bilingual corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parallel texts",
                "multiple translation of the same text, corresponding articles from multiple news sources, and bilingual corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translation",
                "multiple",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For each candidate triple, the log-likelihood   and salience   scores were calculated.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "candidate triple",
                "log-likelihood and salience scores were calculated",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "scores",
                "were calculated",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Two other groups of authors have independently and simultaneously proposed adaptations of the Matrix-Tree Theorem for structured inference on directed spanning trees  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Matrix-Tree Theorem",
                "adapations",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "authors",
                "independently and simultaneously proposed",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In the following sections, we will use 2 statistics to measure the the mutual translation likelihood  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistics",
                "2",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "translation likelihood",
                "to measure",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  and Daume III    , so in this paper we focus on the less studied, but equally important problem of annotationstyle adaptation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "problem",
                "less studied, but equally important",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "problem",
                "of annotationstyle adaptation",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Finally, Section 4 reports the results of parsing experiments using our exhaustive k-best CYK parser with the concise PCFGs induced from the Penn WSJ treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CYK parser",
                "exhaustive k-best",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Penn WSJ treebank",
                "induced concise PCFGs",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.3 CRFs and Perceptron Learning Perceptron training for conditional models   is an approximation to the SGD algorithm, using feature counts from the Viterbi label sequence in lieu of expected feature counts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CRFs and Perceptron Learning",
                "is an approximation to the SGD algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Viterbi label sequence",
                "using feature counts from",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The common types of features include contextual  , co-occurrence  , and syntactic dependency  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "contextual, co-occurrence, and syntactic dependency",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "features",
                "common types of",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "We follow the approach of bootstrapping from a model with a narrower parameter space as is done in, e.g. Och and Ney   and Fraser and Marcu  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "with a narrower parameter space",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approach",
                "done in, e.g. Och and Ney and Fraser and Marcu",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Statistical and information theoretic approaches  ,  , ,   Using lexical collocations to determine PPA with statistical techniques was first proposed by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexical collocations",
                "statistical techniques",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "statistical and information theoretic approaches",
                "first proposed",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For comparing the sentence generator sample to the English sample, we compute log-likelihood statistics   on neighboring words that at least co-occur twice.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentence generator sample",
                "log-likelihood statistics on neighboring words",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "neighboring words",
                "co-occur twice",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We can then use this newly identified set to:   use Turneys method to find the orientation for the terms and employ the terms and their scores in a classifier, and   use Turneys method to find the orientation for the terms and add the new terms as additional seed terms for a second iteration As opposed to Turney  , we do not use the web as a resource to find associations, rather we apply the method directly to in-domain data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Turneys method",
                "use to find orientation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Turneys method",
                "apply directly to in-domain data",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "IBM constraints  , the lexical word reordering model  , and inversion transduction grammar   constraints   belong to this type of approach.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM constraints",
                "belong to this type of approach",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "lexical word reordering model",
                "belong to this type of approach",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There have been many studies of zero-pronoun identification      .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "studies",
                "zero-pronoun identification",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "many",
                "many",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Related Works Some of the most common measures of unithood include pointwise mutual information     and log-likelihood ratio  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measures of unithood",
                "pointwise mutual information and log-likelihood ratio",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "measures of unithood",
                "most common",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "one-level rules",
                "are often inadequate",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "EDL",
                "linguistically motivated",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "eBonsai first performs syntactic analysis of a sentence using a parser based on GLR algorithm    , and provides candidates of its syntactic structure.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "based on GLR algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "candidates",
                "of its syntactic structure",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "These methods often involve using a statistic such as 2   or the log likelihood ratio   to create a score to measure the strength of correlation between source and target words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistic",
                "such as 2 or the log likelihood ratio",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "score",
                "to measure the strength of correlation",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "It is true that various term extraction systems have been developed, such as Xtract  , Termight  , and TERMS   among others (cf.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "term extraction systems",
                "various",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Xtract",
                "Termight",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Some researchers   have explored the use of Wikipedia information to improve the disambiguation process.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wikipedia information",
                "to improve the disambiguation process",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "disambiguation process",
                "explored",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Jiang and Zhai   then systematically explored a large space of features and evaluated the effectiveness of different feature subspaces corresponding to sequence, syntactic parse tree and dependency parse tree.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "a large space of features",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "feature subspaces",
                "different feature subspaces",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.1 Golden-standard-based criteria In the domain of machine translation systems, an increasingly accepted way to measure the quality of a system is to compare the outputs it produces with a set of reference translations, considered as an approximation of a golden standard  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reference translations",
                "considered as an approximation of a golden standard",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "quality of a system",
                "to compare with a set of reference translations",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "One way of obtaining a suitable granularity of nodes is to introduce latent classes, such as the Semi-Markov class model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "latent classes",
                "Semi-Markov class model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "nodes",
                "suitable granularity",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For details on these feature functions, please refer to  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature functions",
                "refer to",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "details",
                "please refer",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "To evaluate the performance of a parser, NP chunks can usefully be evaluated by a gold standard; many systems   use the Penn Treebank for this type of evaluation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "gold standard",
                "usefully be evaluated",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank",
                "use for evaluation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Meanwhile, some learning algorithms, like maximum likelihood for conditional log-linear models  , unsupervised models  , and models with hidden variables  , require summing over the scores of many structures to calculate marginals.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "learning algorithms",
                "maximum likelihood for conditional log-linear models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "models",
                "require summing over the scores of many structures to calculate marginals",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "This way of creating classified data is similar to that in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classified data",
                "similar to that in",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "way of creating",
                "this way",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The application of this algorithm to the basic problem using a parallel bilingual corpus aligned on the sentence level is described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "to the basic problem",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "parallel bilingual corpus aligned on the sentence level",
                "described in ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Some of the data comes from the parsed files 2-21 of the Wall Street Journal Penn Treebank corpus  , and additional parsed text was obtained by parsing the 1987 Wall Street Journal text using the parser described in Charniak et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal Penn Treebank corpus",
                "parsed files 2-21 of",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parser",
                "described in Charniak et al.",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These records are also known as field books and reference sets in literature  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "records",
                "also known as field books and reference sets",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "literature",
                "in literature",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Almost all of the work in the area of automatically trained taggers has explored Markov-model based part of speech tagging \\ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "taggers",
                "Markov-model based",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "work in the area",
                "automatically trained",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In a different work, Banerjee and Lavie   argued that the measured reliability of metrics can be due to averaging effects but might not be robust across translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "measured reliability",
                "PERFORMANCE",
                "neutral",
                0.5
            ],
            [
                "reliability",
                "not robust across translations",
                "PERFORMANCE",
                "negative",
                0.6
            ]
        ]
    },
    {
        "text": "1 word w 2 word bigram w1w2 3 single-character word w 4 a word of length l with starting character c 5 a word of length l with ending character c 6 space-separated characters c1 and c2 7 character bigram c1c2 in any word 8 the first / last characters c1 / c2 of any word 9 word w immediately before character c 10 character c immediately before word w 11 the starting characters c1 and c2 of two consecutive words 12 the ending characters c1 and c2 of two consecutive words 13 a word of length l with previous word w 14 a word of length l with next word w Table 1: Feature templates for the baseline segmentor 2 The Baseline System We built a two-stage baseline system, using the perceptron segmentation model from our previous work   and the perceptron POS tagging model from Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron segmentation model",
                "from our previous work",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "perceptron POS tagging model",
                "from Collins",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In practice, when training the parameters of an SMT system, for example using the discriminative methods of  , the cost for skips of this kind is typically set to a very high value.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "set to a very high value",
                "METHODOLOGY",
                "negative",
                0.7
            ],
            [
                "methods of  ",
                "discriminative",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  s in Equation 1 are the weights of different feature functions, learned to maximize development set BLEU scores using a method similar to  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights of different feature functions",
                "learned to maximize development set BLEU scores",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "method similar to ",
                "unknown",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A word is considered to be known when it has an ambiguous tag   attributed to it in the LEXICON, which is compiled in the same way as for the MBT-tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LEXICON",
                "compiled in the same way as for the MBT-tagger",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "tag",
                "has an ambiguous",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "task, originally introduced in Ramshaw and Marcus   and also described in  , brackets just base NP constituents5.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "task",
                "originally introduced",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "NP constituents",
                "just base",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2 The alignment Algorithm 2.1 Estimation of translation probabilities The translation probabilities are estimated using a method based on Brown et al.'s Model 2  , which is summarized in the following subsection, 2.1.1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Algorithm 2.1",
                "based on Brown et al.'s Model 2",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Brown et al.'s Model 2",
                "is summarized in the following subsection",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4.1 Data Preparation NP chunking results have been reported on two slightly different data sets: the original RM data set of Ramshaw and Marcus  , and the modi ed CoNLL-2000 version of Tjong Kim Sang and Buchholz  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "RM data set",
                "original",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "CoNLL-2000 version",
                "modified",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Let W1,W2 be the vocabulary sizes of the two languages, and N = {A1,,AN} be the set of nonterminals with indices 1,,N. Wu   also showed that ITGs can be equivalently be defined in two other ways.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "W1,W2",
                "vocabulary sizes",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "N = {A1,,AN}",
                "set of nonterminals",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Both models have been used to achieve state-of-the-art accuracy for a wide range of languages, as shown in the CoNLL shared tasks on dependency parsing  , but McDonald and Nivre   showed that a detailed error analysis reveals important differences in the distribution of errors associated with the two models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "state-of-the-art accuracy",
                "PERFORMANCE",
                "neutral",
                0.85
            ],
            [
                "models",
                "distribution of errors",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Automatic Evaluation Measures A variety of automatic evaluation methods have been recently proposed in the machine translation community  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation methods",
                "have been recently proposed",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "machine translation community",
                "various",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this work, we use the prototype lists originally defined by Haghighi and Klein     and subsequently used by Chang et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prototype lists",
                "originally defined by Haghighi and Klein",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "lists",
                "subsequently used by Chang et al.",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In order to determine inter-annotator agreement for the database of annotated texts, we computed kappa statistics  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa statistics",
                "computed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "annotator agreement",
                "for the database of annotated texts",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "145 2 The Latent Variable Architecture In this section we will begin by briefly introducing the class of graphical models we will be using, Incremental Sigmoid Belief Networks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Latent Variable Architecture",
                "Incremental Sigmoid Belief Networks",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "class of graphical models",
                "briefly introducing",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Apart from BLEU, a standard automatic measure METEOR   was used for evaluation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "METEOR",
                "standard automatic measure",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The template we use here is similar to Turney  , but we have added extra context words before the X and after the Y . Our morphological processing also differs from Turney  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "template",
                "similar to Turney",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "morphological processing",
                "differ from Turney",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There is a large number of potentially informative features that could play a role in correctly predicting the tag of an unknown word  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "potentially informative",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "unknown word",
                "predicting the tag",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.1 Training The training procedure is identical to the factored phrase-based training described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training procedure",
                "identical to the factored phrase-based training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "factored phrase-based training",
                "described in  ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Discriminative training has been used mainly for translation model combination   and with the exception of  , has not been used to directly train parameters of a translation model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training",
                "mainly for translation model combination",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parameters of a translation model",
                "has not been used to directly train",
                "METHODOLOGY",
                "negative",
                0.75
            ]
        ]
    },
    {
        "text": "Compared to the Penn Treebank  , the POS tagset of the French Treebank is smaller  : all punctuation marks are represented as the single PONCT tag, there are no separate tags for modal verbs, whwords, and possessives.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tagset",
                "smaller",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tagset",
                "single PONCT tag",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes  , and FrameNet frames  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "situation-specific events",
                "not unlike scripts, caseframes, and FrameNet frames",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "sets of situation-specific events",
                "represent",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The precision rate using the lexical statistics approach can reach around 60% if both word bi-gram extraction and n-gram extractions are taking into account  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexical statistics approach",
                "can reach around 60%",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "word bi-gram extraction and n-gram extractions",
                "are taking into account",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For many languages, large-scale syntactically annotated corpora have been built  ), and many parsing algorithms using CFGs have been proposed.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "large-scale syntactically annotated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parsing algorithms",
                "using CFGs",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The sequential classi cation approach can handle many correlated features, as demonstrated in work on maximum-entropy   and a variety of other linear classi ers, including winnow  , AdaBoost  , and support-vector machines  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classi cation approach",
                "handle many correlated features",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "linear classi ers",
                "including winnow, AdaBoost, and support-vector machines",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "The approach is evaluated by cross-validation on the WSJ treebank corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "WSJ treebank",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "evaluation",
                "by cross-validation",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "1 Introduction Since the introduction of the BLEU metric  , statistical MT systems have moved away from human evaluation of their performance and towards rapid evaluation using automatic metrics.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metric",
                "statistical MT systems have moved away from human evaluation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "automatic metrics",
                "rapid evaluation using",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We train IBM Model 4 with GIZA++   in both translation directions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 4",
                "with GIZA++",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "GIZA++",
                "in both translation directions",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Following  , we used sections 0-18 of the Wall Street Journal   corpus for training, sections 19-21 for development, and sections 22-24 for final evaluation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal corpus",
                "sections 0-18 for training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sections 19-21 for development",
                "METHODOLOGY",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Examples of such knowledge sources include stemming and TF-IDF weighting  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "stemming",
                "stemming",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "TF-IDF weighting",
                "TF-IDF weighting",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A simpler, related idea of penalizing distortion from some ideal matching pattern can be found in the statistical translation   and word alignment   models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distortion",
                "penalizing distortion",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "translation models",
                "statistical and word alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Two disjoint corpora are used in steps 2 and 5, both consisting of complete articles taken from the Wall Street Journal Treebank Corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "consisting of complete articles taken from the Wall Street Journal Treebank Corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Wall Street Journal Treebank Corpus",
                "widely used",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Clusters are created by means of distributional techniques in  , while in   low level synonim sets in WordNet are used.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distributional techniques",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "WordNet",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A description of the flat featurized dependency-style syntactic representation we use is available in  , which describes how the entire Penn Treebank   was converted to this representation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "representation",
                "flat featurized dependency-style syntactic",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank",
                "was converted",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, work which failed to detect improvements in translation quality with the integration of word sense disambiguation  , or work which attempted to integrate syntactic information but which failed to improve Bleu   may deserve a second look with a more targeted manual evaluation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "failed to detect improvements",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "work",
                "failed to improve Bleu",
                "PERFORMANCE",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "In the Penn Treebank  , null elements, or empty categories, are used to indicate non-local dependencies, discontinuous constituents, and certain missing elements.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "null elements",
                "are used to indicate non-local dependencies, discontinuous constituents, and certain missing elements",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank",
                "null elements, or empty categories",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The first work on SMT done at IBM  , used a noisy-channel model, resulting in what Brown et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work on SMT",
                "done at IBM",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "noisy-channel model",
                "resulting in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The row labelled Precision shows the precision of the extracted information   estimated by random sampling and manual evaluation of 1% of the data for each table, similar to  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Precision",
                "estimated by random sampling and manual evaluation of 1% of the data",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "table",
                "similar to ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "With automatic refinement it is harder to guarantee improved performance than with manual refinements   or with refinements based on direct lexicalization  , Collins  , Charniak  , etc.).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "refinements",
                "harder to guarantee improved performance",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "manual refinements",
                "or refinements based on direct lexicalization",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Researchers such as   and   have applied robust grammars and statistical techniques over large corpora to extract interesting noun phrases and subject-verb, verb-object pairs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "large",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "statistical techniques",
                "robust",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this paper, we adopt Stanford Maximum Entropy   implementation in our experiments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stanford Maximum Entropy implementation",
                "in our experiments",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "implementation",
                "adopted",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "On the other hand, other authors  ) do use the expression phrase-based models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "authors",
                "do use",
                "METHODOLOGY",
                "neutral",
                0.5
            ],
            [
                "phrase-based models",
                "other authors use",
                "METHODOLOGY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "he description of the minimum cut framework in Section 4.1 was inspired by Pang and Lee  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "minimum cut framework",
                "was inspired by Pang and Lee",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "description",
                "in Section 4.1",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Theyalsoappliedself-training to domain adaptation of a constituency parser  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "domain adaptation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "self-training",
                "applied to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "BLEU: Automatic evaluation by BLEU score  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "automatic evaluation",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "BLEU score",
                "by",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "We trained three Arabic-English syntax-based statistical MT systems   using max-B training  : one on a newswire development set, one on a weblog development set, and one on a combined development set containing documents from both genres.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training systems",
                "syntax-based statistical MT",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "development sets",
                "newswire, weblog, combined",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We viewed the seed word as a classified sentence, following a similar proposal in Yarowsky  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "seed word",
                "classified sentence",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "proposal",
                "in Yarowsky",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "The importance of including single nonheadwords is now also uncontroversial  , and the current paper has shown the importance of including two and more nonheadwords.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "nonheadwords",
                "uncontroversial",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "nonheadwords",
                "importance",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4.1 Overview In this work, factored models   are experimented with three factors : the surface form, the lemma and the part of speech  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "factored models",
                "three factors",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "surface form, lemma, part of speech",
                "are experimented with",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The system combination weights  one for each system, LM weight, and word and NULL insertion penalties  were tuned to maximize the BLEU   score on the tuning set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system combination weights",
                "were tuned to maximize the BLEU score",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "NULL insertion penalties",
                "were tuned to maximize the BLEU score",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Following  , the slot labels are drawn from a set of classes constructed by extending each label by three additional symbols, Beginning/Inside/Outside  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "slot labels",
                "constructed by extending each label by three additional symbols",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "classes",
                "Beginning/Inside/Outside",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In contrast to the opinion extracts produced by Pang and Lee  , our summaries are not text extracts, but rather explicitly identify and 337 characterize the relations between opinions and their sources.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pang and Lee",
                "opinion extracts",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "relations between opinions and their sources",
                "characterize",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The chunker is trained on the answer side of the Training corpus in order to learn 2 and 3word collocations, defined using the likelihood ratio of Dunning  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunker",
                "trained on the answer side of the Training corpus",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "collocations",
                "defined using the likelihood ratio of Dunning",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To use the data from NANC, we use self-training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data from NANC",
                "use self-training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "self-training",
                "self-training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  describe an approach that targets translation of French phrases of the form NOUN de NOUN  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NOUN de NOUN",
                "form",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "translation",
                "approach that targets",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To identify conjunctions, lists, and appositives, we first parsed the corpus, using an efficient statistical parser  , trMned on the Penn Wall Street Journal Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "parsed using an efficient statistical parser",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Wall Street Journal Treebank",
                "standard dataset",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "To help our model learn that it is desirable to copy answer words into the question, we add to each corpus a list of identical dictionary word pairs w iw i . For each corpus, we use GIZA  , a publicly available SMT package that implements the IBM models  , to train a QA noisy-channel model that maps flattened answer parse trees, obtained using the cut procedure described in Section 3.1, into questions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA",
                "publicly available SMT package",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM models",
                "implements",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "It will also be relevant to apply advanced statistical models that can incorporate various useful information to this task, e.g., the maximum entropy model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "advanced statistical models",
                "can incorporate various useful information",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "maximum entropy model",
                "relevant to apply",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There exists a variety of different metrics, e.g., word error rate, position-independent word error rate, BLEU score  , NIST score  , METEOR  , GTM  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "variety of different",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU score",
                "none",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4 Experiment 4.1 Evaluation Method We evaluated each sentence compression method using word F-measures, bigram F-measures, and BLEU scores  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentence compression method",
                "using word F-measures, bigram F-measures, and BLEU scores",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "word F-measures, bigram F-measures, and BLEU scores",
                "evaluation metrics",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Turney   predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic oriented method",
                "semantic oriented method",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "phrases in the review",
                "contain adjectives or adverbs",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1A Normal Form for SITGs can be defined   by analogy to the Chomsky Normal Form for Stochastic ContextFree Grammars.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SITGs",
                "by analogy to the Chomsky Normal Form for Stochastic Context-Free Grammars",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Chomsky Normal Form",
                "for Stochastic Context-Free Grammars",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "p0  is calculated by ME models as follows  : p0 = 1Y  exp braceleftBigg Hsummationdisplay h=1 hgh  bracerightBigg,   709 Language Features English Prefixes of 0 up to four characters, suffixes of 0 up to four characters, 0 contains Arabic numerals, 0 contains uppercase characters, 0 contains hyphens.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Language Features",
                "Prefixes of 0 up to four characters, suffixes of 0 up to four characters, 0 contains Arabic numerals, 0 contains uppercase characters, 0 contains hyphens",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ME models",
                "calculates p0",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "C function is a derivative of Fano's mutual information formula recently used by Church and Hanks   to compute word co-occurrence patterns in a 44 million word corpus of Associated Press news stories",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Fano's mutual information formula",
                "recently used",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "44 million word corpus of Associated Press news stories",
                "compute word co-occurrence patterns",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The task of classifying several different uses of definite descriptions   is somewhat analogous to that for bare nouns.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "definite descriptions",
                "somewhat analogous",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "bare nouns",
                "task of classifying",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Beyond WordNet  , a wide range of resources has been developed and utilized, including extensions to WordNet   and resources based on automatic distributional similarity methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "resources",
                "developed and utilized",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "automatic distributional similarity methods",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Success is indicated by the proportion of the original sentence regenerated, as measured by any string comparison method: in our case, using the BLEU metric  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metric",
                "using the BLEU metric",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "sentence regenerated",
                "proportion of the original sentence regenerated",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction The importance of learning to manipulate monolingual paraphrase relationships for applications like summarization, search, and dialog has been highlighted by a number of recent efforts  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "manipulate monolingual paraphrase relationships",
                "has been highlighted",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "recent efforts",
                "number of",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "One possible strategy is to exploit a widecoverage realizer that aims for applicability in multiple application domains  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "realizer",
                "wide-coverage",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "applicability",
                "in multiple application domains",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The complexities of 15 restricted alignment problems in two very different synchronous grammar formalisms of syntax-based machine translation, inversion transduction grammars     and a restricted form of range concatenation grammars  -BRCGs)  , are investigated.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment problems",
                "complexities of",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "syntax-based machine translation",
                "very different synchronous grammar formalisms",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The rationale for using Kappa is explained in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa",
                "explained in",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Kappa",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Finally, we would like to investigate the incorporation of unsupervised methods for WSD, such as the heuristically-based methods of   and  , and the theoretically purer bootstrapping method of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "heuristically-based methods",
                "heuristically-based",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "theoretically purer bootstrapping method",
                "theoretically purer",
                "METHODOLOGY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "4.1 Part-of-speech tagging experiments We split the Penn Treebank corpus   into training, development and test sets as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank corpus",
                "split into training, development and test sets",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank corpus",
                "as in .",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Evaluating the algorithm on the output of Charniaks parser   and the Penn treebank   shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Charniaks parser",
                "output",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "patternmatching algorithm",
                "does surprisingly well",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "However for remedy, many of the current word alignment methods combine the results of both alignment directions, via intersection or 249 grow-diag-final heuristic, to improve the alignment reliability  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment methods",
                "combine the results of both alignment directions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "alignment reliability",
                "improve",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "3For decoding, loc is averaged over the training iterations as in Collins and Roark  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "loc",
                "averaged over the training iterations",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Collins and Roark",
                "as in",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "aghighi and Klein   showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prototypes",
                "can improve tagging accuracy",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "small set of prototypes",
                "can improve",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This paper is heavily indebted to prior work on unsupervised learning of position categories such as Brown et al 1992, Schtze 1997, Higgins 2002, and others cited there.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prior work",
                "unsupervised learning of position categories",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Brown et al 1992",
                "Schtze 1997, Higgins 2002, and others",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, alignments can be used to learn translation lexicons  , transfer rules  , and classifiers to find safe sentence segmentation points  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignments",
                "to learn translation lexicons",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "safe sentence segmentation points",
                "find",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A possible solution to this problem is to directly estimate p  by applying a maximum entropy model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "maximum entropy model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "solution",
                "directly estimate p",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Given the estimated 3% error rate of the WSJ tagging  , they argue that the difference in performance is not sufficient to establish which of the two taggers is actually better.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSJ tagging",
                "estimated 3% error rate",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "taggers",
                "better",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These domains have been commonly used in prior work on summarization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "domains",
                "have been commonly used",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "prior work on summarization",
                "on summarization",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "One can also examine the distribution of character or word ngrams, e.g. Language Modeling  , phrases  , and so on.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ngrams",
                "e.g. Language Modeling, phrases, and so on",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "character or word ngrams",
                "distribution of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A variety of approaches have been investigated for speech summarization, for example, maximum entropy, conditional random fields, latent semantic analysis, support vector machines, maximum marginal relevance  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "have been investigated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "maximum entropy",
                "conditional random fields, latent semantic analysis, support vector machines, maximum marginal relevance",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As a follow-up to the work described in this paper we developed a method that utilizes the unlabeled NPs in the corpus using a structured rule learner  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "utilizes the unlabeled NPs in the corpus using a structured rule learner",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpus",
                "unlabeled NPs",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To support this claim, first, we used the  coefficient   to assess the agreement between the classification made by FLSA and the classification from the corpora  see Table 8.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "see Table 8",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "FLSA",
                "classification",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this paper, we used CTB 5.0   as our main corpus, defined the training, development and test sets according to  , and designed our experiments to explore the impact of the training corpus size on our approach.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "CTB 5.0",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "training corpus size",
                "on our approach",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We further note that our results are different from that of   as they use extensive feature engineering and weight tuning during the graph generation process that we have not been able to reproduce.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "graph generation process",
                "we have not been able to reproduce",
                "METHODOLOGY",
                "negative",
                0.7
            ],
            [
                "feature engineering and weight tuning",
                "extensive",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "To regularize the model we take as the final model the average of all weight vectors posited during training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weight vectors",
                "posited during training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "model",
                "the average of all",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "If the target CFG is purely binary branching, then the previous theoretical and linguistic analyses   suggest that much of the requisite constituent and word order transposition may be accommodated without change to the mirrored ITG.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CFG",
                "purely binary branching",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "requisite constituent and word order transposition",
                "may be accommodated without change",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The approaches proposed to the ACE RDC task such as kernel methods   and Maximum Entropy methods   required the availability of large set of human annotated corpora which are tagged with relation instances.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "required the availability of large set of human annotated corpora",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "corpora",
                "tagged with relation instances",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "32-39 Proceedings of HLT-NAACL 2003 similar distribution patterns  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Proceedings of HLT-NAACL 2003",
                "similar distribution patterns",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "similar distribution patterns",
                "similar",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "of the position infer marion of words at ltlat .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "at ltlat",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Och and Ney   proposed Model 6, a log-linear combination of IBM translation models and HMM model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 6",
                "log-linear combination of IBM translation models and HMM model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM translation models",
                "and HMM model",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The phrase bilexicon is derived from the intersection of bidirectional IBM Model 4 alignments, obtained with GIZA++  , augmented to improve recall using the grow-diag-final heuristic.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "augmented to improve recall",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "bidirectional IBM Model 4 alignments",
                "obtained with",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A few studies   addressed this defect by selecting the appropriate translation rules for an input span based on its context in the input sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation rules",
                "selecting the appropriate",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "input span",
                "based on its context",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It is possible that there is a better automated method for finding such phrases, such as the methods in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "better",
                "METHODOLOGY",
                "positive",
                0.7
            ],
            [
                "methods in  ",
                "such as",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Here, under the ITG constraint  , we need to consider just two kinds of reorderings, straight and inverted between two consecutive blocks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reorderings",
                "straight and inverted between two consecutive blocks",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ITG constraint",
                "need to consider",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "An additional translation set called the Maximum BLEU set is employed by the SMT system to train the weights associated with the components of its log-linear model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "components of its log-linear model",
                "associated with",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Maximum BLEU set",
                "employed by",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The following four metrics were used speci cally in this study: BLEU  : A weighted geometric mean of the n-gram matches between test and reference sentences multiplied by a brevity penalty that penalizes short translation sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "weighted geometric mean of the n-gram matches between test and reference sentences multiplied by a brevity penalty that penalizes short translation sentences",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "brevity penalty",
                "penalizes short translation sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, in phrase-based SMT systems  , distortion model is used, in which reordering probabilities depend on relative positions of target side phrases between adjacent blocks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distortion model",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "reordering probabilities",
                "depend on relative positions",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Also, we chose to average each individual perceptron   prior to Bayesian averaging.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron",
                "prior to Bayesian averaging",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "individual perceptron",
                "average",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We finally move on to present more complex models which attempt to model coreference as a global discourse phenomenon  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "complex",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "coreference",
                "global discourse phenomenon",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "More specialized methods also exist, for example for support vector machines   and for conditional random fields  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "for support vector machines and for conditional random fields",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "methods",
                "more specialized",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Then we compute the same ratio of machine translation sentence to source sentence, and take the output of p-norm function as a feature: ) __/__      Features based on parse score The usual practice to model the wellformedness of a sentence is to employ the n-gram language model or compute the syntactic structure similarity  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parse score",
                "output of p-norm function",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "n-gram language model",
                "usual practice",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Aligning tokens in parallel sentences using the IBM Models  ,   may require less information than full-blown translation since the task is constrained by the source and target tokens present in each sentence pair.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Models",
                "require less information",
                "METHODOLOGY",
                "positive",
                0.85
            ],
            [
                "task",
                "is constrained",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5.2 Translation In order to test the translation performance of the grammars induced by our model and the GHKM method6 we report BLEU   scores on sentences of up to twenty words in length from the MT03 NIST evaluation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU scores",
                "on sentences of up to twenty words in length",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "GHKM method",
                "our model",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The terms graph-based and transition-based were used by McDonald and Nivre   to describe the difference between MSTParser  , which is a graph-based parser with an exhaustive search decoder, and MaltParser  , which is a transition-based parser with a greedy search decoder.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MSTParser",
                "graph-based parser with an exhaustive search decoder",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "MaltParser",
                "transition-based parser with a greedy search decoder",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  94.36   Table 8: The HySOL performance with the F-score optimization technique on Chunking   experiments from unlabeled data appear different from each other.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HySOL performance",
                "appear different from each other",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "F-score optimization technique",
                "on Chunking experiments from unlabeled data",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We also record for each token its derivational root, using the CELEX  database.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CELEX database",
                "using",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "derivational root",
                "record for each token",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Table 2 shows the results for English projective dependency trees extracted from the Penn Treebank   using the rules of Yamada and Matsumoto  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "rules of Yamada and Matsumoto",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dependency trees",
                "extracted",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 Results We present results that compare our system against the baseline Pharaoh implementation   and MER training scripts provided for this workshop.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pharaoh implementation",
                "baseline",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "MER training scripts",
                "provided for this workshop",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In  , finite-state machine translation is based on   and is used for decoding the target language string.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "finite-state machine translation",
                "is based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "finite-state machine translation",
                "is used for decoding",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "It acquires a set of synchronous lexical entries by running the IBM alignment model   and learns a log-linear model to weight parses.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM alignment model",
                "running",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parses",
                "to weight",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction During the last decade, statistical machine translation   systems have evolved from the original word-based approach   into phrase-based translation systems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical machine translation systems",
                "phrase-based translation systems",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approach",
                "word-based",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "More specifically, two recent works have suggested to use statistical data on lexical relations for resolving ambiguity cases of PP-attachment   and pronoun references  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexical relations",
                "statistical data on",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "PP-attachment",
                "resolving ambiguity cases",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  found that direct annotation takes twice as long as automatic tagging plus correction, for partof-speech annotation); and the output quality reflects the difficulty of the task  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "direct annotation",
                "takes twice as long",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "output quality",
                "reflects the difficulty of the task",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "One of the earliest attempts at extracting \\interrupted collocations\"\"  , was that of Smadja  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Smadja",
                "earliest attempt",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "collocations",
                "interrupted",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Therefore, structure divergence and parse errors are two of the major issues that may largely compromise the performance of syntax-based SMT  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "structure divergence",
                "major issues",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "parse errors",
                "major issues",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "High values of  fall into the minimal entropy trap, while low values ofhave no effect on the model   for an example).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "entropy values",
                "fall into the minimal entropy trap",
                "LIMITATION",
                "negative",
                0.7
            ],
            [
                "entropy values",
                "have no effect",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We use as our English corpus the Wall Street Journal   portion of the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal",
                "portion of the Penn Treebank",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank",
                "portion",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "415-458, Wu, Dekai   Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stochastic inversion transduction grammars",
                "parallel corpora",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "bilingual parsing",
                "parallel corpora",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We also used the following resources: the Charniak parser   to carry out the syntactic analysis; the wn::similaritypackage   to compute the Jiang&Conrath   distance   needed to implement the lexical similarity siml  as defined in  ; SVM-lightTK   to encode the basic tree kernel function, KT, in SVM-light  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Charniak parser",
                "to carry out the syntactic analysis",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "wn::similaritypackage",
                "to compute the Jiang&Conrath distance",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Dependency Analyzer PP-Attachment Resolver Root-Node Finder Base NP Chunker   = SVM, = Preference Learning Figure 2: Module layers in the system That is, we use Penn Treebanks Wall Street Journal data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SVM",
                "Preference Learning",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebanks Wall Street Journal data",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Note that our use of cepts differs slightly from that of  , inasmuch cepts may not overlap, according to our definition.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "cepts",
                "may not overlap",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "definition",
                "according to our",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "To perform code generalization, Li adopted to Smadjas work   and defined the code strength using a code frequency and a standard deviation in each level of the concept hierarchy.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "code strength",
                "using a code frequency and a standard deviation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "concept hierarchy",
                "each level",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Large treebanks are available for major languages, however these are often based on a speci c text type or genre, e.g. nancial newspaper text  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treebanks",
                "are often based on a speci c text type or genre",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "text type or genre",
                "e.g. nancial newspaper text",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Ramshaw and Marcus used transformationbased learning   for developing two chunkers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunkers",
                "developing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "transformationbased learning",
                "for developing",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Finally, we use as a feature the mappings produced in   of WordNet senses to Oxford English Dictionary senses.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet senses",
                "produced in of",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Oxford English Dictionary senses",
                "used as a feature",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Thus, a lot of alignment techniques have been suggested at; the sentence  , phrase  , nomt t)hrase  , word  , collocation   and terminology level.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment techniques",
                "suggested",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "collocation",
                "level",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the context of part-of-speech tagging, Klein and Manning   argue for the same distinctions made here between discriminative models and discriminative training criteria, and come to the same conclusions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Klein and Manning",
                "come to the same conclusions",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "discriminative models",
                "same distinctions",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These constituent matching/violation counts are used as a feature in the decoders log-linear model and their weights are tuned via minimal error rate training    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoders log-linear model",
                "used as a feature",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "minimal error rate training",
                "tuned via",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "One of the most relevant work is  , which proposed to integrate various patterns in order to measure semantic similarity between words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "patterns",
                "various patterns",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "semantic similarity",
                "to measure semantic similarity",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  applied to the output of the reranking parser of Charniak and Johnson  , whereas in BE   dependencies are generated by the Minipar parser  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Charniak and Johnson",
                "reranking parser",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Minipar parser",
                "dependencies are generated",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In each case the input to the network is a sequence of tag-word pairs.5 5We used a publicly available tagger   to provide the tags.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger",
                "publicly available",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "sequence of tag-word pairs",
                "input to the network",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "With respect to already available POS tagsets, the scheme allows corresponding extensions of the supertype POSTag to, e.g., PennPOSTag  ) or GeniaPOSTag  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POSTag",
                "corresponding extensions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "supertype POSTag",
                "allows",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "1 Introduction In global linear models   for structured prediction,  ), the optimal label y for an input x is y = arg max yY  w f    where Y  is the set of possible labels for the input x; f   Rd is a feature vector that represents the pair  ; and w is a parameter vector.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "y",
                "optimal label",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "w",
                "parameter vector",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Importantly, this Bayesian approach facilitates the incorporation of sparse priors that result in a more practical distribution of tokens to lexical categories  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "priors",
                "sparse",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "distribution of tokens to lexical categories",
                "practical",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Here, following Smith and Eisner  , we use a weighted, quasi-synchronous dependency grammar. Apart from the obvious difference in application task, there are a few important differences with their model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar",
                "weighted, quasi-synchronous",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "model",
                "important differences with",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In the similaritybased approaches  , rather than a class, each word is modelled by its own set of similar words derived from statistical data collected from corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word",
                "modelled by its own set of similar words",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpora",
                "statistical data collected from",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The supervised component is Collins parser  , trained on the Wall Street Journal.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins parser",
                "trained on the Wall Street Journal",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Unfortunately, as was shown by Fraser and Marcu   AER can have weak correlation with translation performance as measured by BLEU score  , when the alignments are used to train a phrase-based translation system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "AER",
                "can have weak correlation with translation performance",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "translations system",
                "phrase-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Some examples of language reuse include collocation analysis  , the use of entire factual sentences extracted from corpora  , and summarization using sentence extraction  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocation analysis",
                "examples of language reuse include",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence extraction",
                "summarization using",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Similarly, the sense disambiguation problem is typically attacked by comparing the distribution of the neighbors of a word's occurrence to prototypical distributions associated with each of the word's senses \\ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distribution of the neighbors of a word's occurrence",
                "prototypical distributions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word's senses",
                "associated with each of the",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "32 7.3 Unknown Words and Parts of Speech When the parser encounters an unknown word, the first-best tag delivered by Ratnaparkhis   tagger is used.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ratnaparkhis tagger",
                "first-best tag delivered",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "unknown word",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A comparison of the two approaches can be found in Koehn, Och, and Marcu  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Koehn, Och, and Marcu",
                "approaches",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "approaches",
                "can be found in",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "e-ranking 1 uses the score of the rst model as a feature in addition to the non-local features as in Collins  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rst model",
                "score of the rst model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "non-local features",
                "as in Collins",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Most work on discriminative training for SMT has focussed on linear models, often with margin based algorithms  , or rescaling a product of sub-models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "linear models",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "algorithms",
                "margin based algorithms",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The class labeling system in our experiment is IOB2  , which is a variation of IOB  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "class labeling system",
                "IOB2",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IOB2",
                "variation of IOB",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "An acceptable agreement for most NLP classification tasks lies between 0.7 and 0.8  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NLP classification tasks",
                "lies between 0.7 and 0.8",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use a bidirectional search strategy  , and our algorithm is based on Perceptron learning  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "based on Perceptron learning",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "search strategy",
                "bidirectional search",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Finally, the translation model can be formalized as the following optimization problem argmax logPr  s.t. mwsummationdisplay j=1 Pr  = 1,k This optimization problem can be solved by the EM algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "optimization problem",
                "can be formalized as",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "EM algorithm",
                "can be solved by",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, in the WSJ corpus, part of the Penn Treebank 3 release  , the string in   is a variation 12-gram since off is a variation nucleus that in one corpus occurrence is tagged as a preposition  , while in another it is tagged as a particle  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSJ corpus",
                "part of the Penn Treebank 3 release",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "string",
                "a variation 12-gram since off",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The baseline hierarchical phrase-based system is trained using standard max-BLEU training   without sparse features  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseline hierarchical phrase-based system",
                "standard max-BLEU training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "training",
                "without sparse features",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Note that, since the FrameNet data does not include deep syntactic tree annotation, we processed the FrameNet data with Collins parser  , consequently, the experiments on FrameNet relate to automatic syntactic parse trees.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "FrameNet data",
                "does not include deep syntactic tree annotation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Collins parser",
                "processed the FrameNet data",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Instead of analyzing sentences directly, AUCONTRAIRE relies on the TEXTRUNNER Open Information Extraction system   to map each sentence to one or more tuples that represent the entities in the sentences and the relationships between them  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TEXTRUNNER Open Information Extraction system",
                "map each sentence to one or more tuples",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentences",
                "represent the entities and the relationships between them",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For the MUC6 data set, we extract noun phrases   automatically, but for MPQA, we assume mentions for coreference resolution are given as in Stoyanov and Cardie  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MUC6 data set",
                "automatically extract noun phrases",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "mentions for coreference resolution",
                "are given",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Concept similarity is often measured by vectors of co-occurrence with context words that are typed with dependency information  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "context words",
                "typed with dependency information",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "vectors of co-occurrence",
                "measured by",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The work reported in this paper is most closely related to work on statistical machine translation, particularly the IBM-style work on CANDIDE  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work on statistical machine translation",
                "IBM-style work on CANDIDE",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "work on CANDIDE",
                "reported in this paper",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church, and Yarowsky 1992; Church 1993; Brown et al. 1993; Dagan, Church, and Gale 1993; Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Parallel bilingual corpora",
                "rich source of constraints",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical analysis",
                "provide",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he translations were generated by the alignment template system of Och  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment template system",
                "of Och",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translations",
                "generated by",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The variance semiring is essential for many interesting training paradigms such as deterministic 40 annealing  , minimum risk  , active and semi-supervised learning  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "paradigms",
                "many interesting",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "variance semiring",
                "essential",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "It is equipped with head binarization to help improve parsing accuracy, following the traditional linguistic insight that phrases are organized around the head  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "head binarization",
                "help improve parsing accuracy",
                "METHODOLOGY",
                "positive",
                0.7
            ],
            [
                "traditional linguistic insight",
                "phrases are organized around the head",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Feature weight tuning was carried out using minimum error rate training, maximizing BLEU scores on a held-out development set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Feature weight tuning",
                "minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU scores",
                "maximizing",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "are equivalent to a maximum entropy variant of the phrase sense disambiguation approach studied by Carpuat & Wu  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase sense disambiguation approach",
                "studied by Carpuat & Wu",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "variant",
                "maximum entropy",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However, current statistical dependency parsers provide worse results if the dependency length becomes longer  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency length",
                "becomes longer",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "results",
                "worse",
                "PERFORMANCE",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction B   was one of the first automatic evaluation metrics for machine translation  , and despite being challenged by a number of alternative metrics  , it remains the standard in the statistical MTliterature.Callison-Burchetal. havesubjected B to a searching criticism, with two realworld case studies of significant failures of correlation between B and human adequacy/fluency judgments.Bothcasesinvolvecomparisonsbetween statistical MT systems and other translation methods  , and they recommend that the use of B be restrictedtocomparisonsbetweenrelatedsystemsor different versions of the same systems.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "B",
                "standard in the statistical MT literature",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "B",
                "subjected to searching criticism",
                "PERFORMANCE",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "e also experimented with a method suggested by Brent   which applies the binomial test on frame frequency data",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "applies the binomial test on frame frequency data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "method",
                "suggested by Brent",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This logistic regression is also called Maxent as it finds the distribution with maximum entropy that properly estimates the average of each feature over the training data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "logistic regression",
                "finds the distribution with maximum entropy",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "distribution",
                "properly estimates the average of each feature",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In our experiments, the class assignment is performed by maximizing the mutual information between adjacent phrases, following the line described in  , with only the modification that candidates to clustering are phrases instead of words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "class assignment",
                "maximizing mutual information between adjacent phrases",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "candidates to clustering",
                "phrases instead of words",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We use data from the CoNLL-2004 shared taskthe PropBank   annotations of the Penn Treebank  , with sections 1518 as the training set and section 20 as the development set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CoNLL-2004 shared task",
                "PropBank annotations",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Penn Treebank",
                "sections 15-18 as training set and section 20 as development set",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This contrasts with alternative alignment models such as those of Melamed   and Wu  , which impose a one-to-one constraint on alignments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment models",
                "impose a one-to-one constraint",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "alternative alignment models",
                "such as those of Melamed and Wu",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": ".1 NP Our NP chunks are very similar to the ones of Ramshaw and Marcus  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NP chunks",
                "very similar",
                "INNOVATION",
                "positive",
                0.85
            ],
            [
                "chunks of Ramshaw and Marcus",
                "",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Lastly, collocations are domain-dependent   and language-dependent.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocations",
                "domain-dependent and language-dependent",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "collocations",
                "domain-dependent",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "ALM does this by using alignment models from the statistical machine translation literature  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment models",
                "from the statistical machine translation literature",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "statistical machine translation literature",
                "widely established field",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "he SPECIALIST minimal commitment parser relies on the SPECIALIST Lexicon as well as the Xerox stochastic tagger  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SPECIALIST minimal commitment parser",
                "relies on",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Xerox stochastic tagger",
                "is well-established",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Instead of assigning HEAD and DEPREL in a single step, some systems use a two-stage approach for attaching and labeling dependencies  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "systems",
                "two-stage approach",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "dependencies",
                "attaching and labeling",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this paper we show how the extraction process can be scaled to the complete Wall Street Journal   section of the Penn-II treebank, with about 1 million words in 50,000 sentences, based on the automatic LFG f-structure annotation algorithm described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn-II treebank",
                "complete Wall Street Journal section",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "LFG f-structure annotation algorithm",
                "automatic",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We computed the LCS and WLCS-based F-measure following   using both the query pool and the sentence pool as in the previous section.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LCS",
                "following using both query pool and sentence pool",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "F-measure",
                "based LCS and WLCS",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The toolkit also implements suffixarray grammar extraction   and minimum error rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "suffixarray grammar extraction",
                "implemented",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "minimum error rate training",
                "implemented",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc. Sentiment detection is the task of determining positive or negative sentiment of words  , phrases and sentences  , or documents  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "subjectivity",
                "helps separate opinions from fact",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "sentiment detection",
                "determining positive or negative sentiment",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Feature weights were set with minimum error rate training   on a development set using BLEU   as the objective function.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "set with minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "objective function",
                "BLEU",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ahill and van Genabith   note that conditioning f-structure annotated generation rules on local features (Eqn",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "f-structure annotated generation rules",
                "conditioning on local features",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "generation rules",
                "annotated",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, Church and Hanks   describe the use of the mutual information index for this purpose (cf.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information index",
                "for this purpose",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Church and Hanks",
                "describe",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation  , classification  , clustering  , named entity classification  , and parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semi-supervised learning",
                "combines both labeled and unlabeled data",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "NLP tasks",
                "word sense disambiguation, classification, clustering, named entity classification, and parsing",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "However, we do not rely on linguistic resources   or on search engines   to determine the semantic orientation, but rather rely on econometrics for this task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linguistic resources",
                "do not rely on",
                "METHODOLOGY",
                "negative",
                0.85
            ],
            [
                "econometrics",
                "rely on",
                "METHODOLOGY",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "We used the heuristic combination described in   and extracted phrasal translation pairs from this combined alignment as described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "combination",
                "described",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translation pairs",
                "extracted",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": " , which is based on that of Och and Ney  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Och and Ney",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Och and Ney",
                "based on",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "They can be roughly divided into three categories: string-to-tree models  ), tree-to-string models  ), and tree-totree models  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "string-to-tree, tree-to-string, tree-to-tree",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "categories",
                "roughly divided into",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Most of the work focused on seeking better word alignment for consensus-based confusion network decoding   or word-level system combination  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "focused on seeking better word alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "consensus-based confusion network decoding",
                "word-level system combination",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "These were: BLEU  , NIST  , WER  , PER  , GTM  , and METEOR  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU, NIST, WER, PER, GTM, and METEOR",
                "metrics",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "metrics",
                "and",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Syntax-based MT approaches began with Wu  , who introduced the Inversion Transduction Grammars.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wu",
                "introduced",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Inversion Transduction Grammars",
                "Inversion Transduction Grammars",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Previous studies called the class of algorithms illustrated in Figure 2 cautious or sequential because in each iteration they acquire 1 or a small set of rules  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "cautious or sequential",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "rules",
                "1 or a small set",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In addition, IC is stable even for relatively low frequency words, which can be contrasted with Fano's mutual information formula recently used by Church and Hanks   to compute word cooccurrence patterns in a 44 million word corpus of Associated Press news stories.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Fano's mutual information formula",
                "recently used",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "IC",
                "stable even for relatively low frequency words",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "From a theoretical point of view, it is difficult to find motivation for the parameter estimation methods used by    see   for discussion.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameter estimation methods",
                "used by see",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parameter estimation methods",
                "difficult to find motivation",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Minimum error rate training was used to tune the model feature weights  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model feature weights",
                "was tuned using",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "minimum error rate training",
                "was used",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Tillmann and Zhang   used a different update style based on a convex loss function:  = L max parenleftBig 0, 1 parenleftBig si si  parenrightBigparenrightBig 768 Table 1: Experimental results obtained by varying normalized tokens used with surface form.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "convex loss function",
                "update style based on",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Experimental results",
                "obtained by varying normalized tokens used with surface form",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We tune all feature weights automatically   to maximize the BLEU   score on the dev set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "automatically tune to maximize BLEU score",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "BLEU score",
                "on the dev set",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Conditional probability, the log-likelihood ratio, and Resnik's   selectional association measure were also significantly correlated with plausibility ratings.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Conditional probability",
                "were also significantly correlated with",
                "PERFORMANCE",
                "neutral",
                0.85
            ],
            [
                "Resnik's selectional association measure",
                "were also significantly correlated with",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For the Transformation Based method, we have used both the PoS tag and the word itself, with the same templates as described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "templates",
                "same as described",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Transformation Based method",
                "used both PoS tag and word itself",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Previous workonsentimentanalysishascoveredawiderange of tasks, including polarity classification  , opinion extraction  , and opinion source assignment  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "has covered a wider range of tasks",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "tasks",
                "including polarity classification, opinion extraction, and opinion source assignment",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Given a set of terms with unknown sentiment orientation, Turney   then uses the PMI-IR algorithm   to issue queries to the web and determine, for each of these terms, its pointwise mutual information   with the two seed words across a large set of documents.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PMI-IR algorithm",
                "uses",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "pointwise mutual information",
                "with the two seed words",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "AL has already been applied to several NLP tasks, such as document classification  , POS tagging  , chunking  , statistical parsing  , and information extraction  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "AL",
                "applied to several NLP tasks",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "NLP tasks",
                "such as document classification, POS tagging, chunking, statistical parsing, and information extraction",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "By habit, most systems for automatic role-semantic analysis have used Pennstyle constituents   produced by Collins   or Charniaks   parsers.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "systems",
                "used Pennstyle constituents produced by Collins or Charniaks parsers",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parsers",
                "Collins or Charniaks",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "An early exception to this was   itself, where Model 2 used function tags during the training process for heuristics to identify arguments  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 2",
                "used function tags during the training process",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "heuristics",
                "identify arguments",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Existing statistical NLG   uses corpus statistics to inform heuristic decisions in what is otherwise symbolic generation  ;   applies n-gram models to select the overall most likely realisation after generation  ; or   reuses an existing parsing grammar or treebank for surface realisation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NLG",
                "uses corpus statistics",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "n-gram models",
                "select the overall most likely realisation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The L1 or L2 norm is commonly used in statistical natural language processing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "L1 or L2 norm",
                "is commonly used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical natural language processing",
                "in",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Inversion Transduction Grammar     and Syntax-Directed Translation Schema     lack both of these properties.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Inversion Transduction Grammar",
                "lack both of these properties",
                "INNOVATION",
                "negative",
                0.8
            ],
            [
                "Syntax-Directed Translation Schema",
                "lack both of these properties",
                "INNOVATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "u   introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "constraints",
                "restricted to Chomskynormal form",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignments",
                "using a probabilistic synchronous context-free grammar",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We were given around 15K sentences of labeled text from the Wall Street Journal     as well as 200K unlabeled sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "labeled text",
                "from the Wall Street Journal",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "unlabeled sentences",
                "200K",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We tuned Pharaohs four parameters using minimum error rate training   on DEV.12 We obtained an increase of 0.8 9As in the POS features, we map each phrase pair to its majority constellation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "four",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "POS features",
                "map each phrase pair to its majority constellation",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 Synchronous Binarization Optimization by Cost Reduction As discussed in Section 1, binarizing an SCFG in a fixed   way   may lead to a large number of competing edges and consequently high risk of making search errors.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SCFG",
                "fixed way",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "search errors",
                "high risk of making",
                "PERFORMANCE",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "The mutual information Ml  is defined as the following formula  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ml",
                "the following formula",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "formula",
                "defined as",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A token can be a word or a punctuation symbol, and each of these neighboring tokens must be in the same sentence as a2 . We use a sentence segmentation program   and a POS tagger   to segment the tokens surrounding a2 into sentences and assign POS tags to these tokens.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tokens",
                "word or punctuation symbol",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence segmentation program",
                "POS tagger",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Choosing the most advantageous, Hiemstra has published parts of the translational distributions of certain words, induced using both his method and Brown et al.'s   Model 1 from the same training bitext.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hiemstra",
                "published parts of the translational distributions",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "Model 1",
                "from the same training bitext",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "  then extended their method and established a sound probabilistic model series, relying on different parameters describing how words within parallel sentences are aligned to each other.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "established a sound probabilistic model series",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "parameters",
                "describing how words within parallel sentences are aligned to each other",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the first step, the scores are initialized according to the G 2 statistic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "We should note, however, that most other stochastic parsers do include counts of single nonheadwords: they appear in the backed-off statistics of these parsers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsers",
                "include counts of single nonheadwords",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parsers",
                "appear in the backed-off statistics",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Our use of Gibbs sampling follows from its increasing use in Bayesian inference problems in NLP  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Gibbs sampling",
                "increasing use",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Bayesian inference problems",
                "NLP",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.4 Learning algorithm Maximum entropy   models  , also known as log-linear and exponential learning models, has been adopted in the SC classification task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum entropy models",
                "has been adopted",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "SC classification task",
                "has been adopted",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Output sequence optimization Rather than basing classifications only on model parameters estimated from co-occurrences between input and output symbols employed for maximizing the likelihood of point-wise single-label predictions at the output level, classifier output may be augmented by an optimization over the output sequence as a whole using optimization techniques such as beam searching in the space of a conditional markov models output   or hidden markov models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model parameters",
                "estimated from co-occurrences between input and output symbols",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "classifier output",
                "augmented by optimization over the output sequence as a whole",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We removed all but the first two characters of each POS tag, resulting in a set of 57 tags which more closely resembles that of the Penn TreeBank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tag",
                "first two characters",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn TreeBank",
                "resembles",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "A variety of other measures of semantic relatedness have been proposed, including distributional similarity measures based on co-occurrence in a body of text see   for a survey.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distributional similarity measures",
                "based on co-occurrence in a body of text",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "measures of semantic relatedness",
                "have been proposed",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The polarity value proposed by   is as follows.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "polarity value",
                "proposed by",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "proposed by",
                "proposed by",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We rescore the ASR N-best lists with the standard HMM   and IBM   MT models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMM",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "MT models",
                "IBM",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Statistics on co-occurrence of words in a local context were used recently for monolingual word sense disambiguation    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "co-occurrence",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word sense disambiguation",
                "recently",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "All the enumerated segment pairs are listed in the following table: Feature x,y Feature x,y AM1+1 c1, c0 AM2+1 c2c1, c0 AM1+2 c1, c0c1 AM2+2 c2c1, c0c1 AM1+3 c1, c0c1c2 AM3+1 c3c2c1, c0 We use Dunnings method   because it does not depend on the assumption of normality and it allows comparisons to be made between the signiflcance of the occurrences of both rare and common phenomenon.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Dunnings method",
                "does not depend on the assumption of normality and allows comparisons to be made",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Dunnings method",
                "does not depend on assumption of normality",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "The translation quality is measured by three MT evaluation metrics: TER  , BLEU  , and METEOR  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT evaluation metrics",
                "TER, BLEU, and METEOR",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "translation quality",
                "measured by",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Indeed, the result of Collins   that including low support features helps a voted perceptron model but harms a maximum entropy model is undone once the weights of the maximum entropy model are regularized.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy model",
                "harms",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "weights of the maximum entropy model",
                "are regularized",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "key component of the parsing system is a Maximum Entropy CCG supertagger   which assigns lexical categories to words in a sentence",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy CCG supertagger",
                "assigns lexical categories to words in a sentence",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "lexical categories",
                "to words in a sentence",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "They are generated from the training corpus via the ?diag-and??method   and smoothed using Kneser-Ney smoothing  , ??one or several n-gram language model  trained with the SRILM toolkit  ; in the baseline experiments reported here, we used a trigram model, ??a distortion model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase, ??a word penalty.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "diag-and??method",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "distortion model",
                "assigns penalty based on the number of source words which are skipped",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "kanohara and Tsujii   generate ill-formed sentences by sampling a probabilistic language model and end up with pseudo-negative examples which resemble machine translation output more than they do learner texts",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilistic language model",
                "generate ill-formed sentences",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "pseudo-negative examples",
                "resemble machine translation output",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "Baseline Pharaoh with phrases extracted from IBM Model 4 training with maximum phrase length 7 and extraction method diag-growthfinal    Lex Phrase-decoder simulation: using only the initial lexical rules from the phrase table, all with LHS X, the Glue rule, and a binary reordering rule with its own reordering-feature  XCat All nonterminals merged into a single X nonterminal: simulation of the system Hiero  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Baseline Pharaoh",
                "with phrases extracted from IBM Model 4 training with maximum phrase length 7 and extraction method diag-growthfinal",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Glue rule",
                "with its own reordering-feature XCat",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In class-based n-gram modeling   for example, classbased n-grams are used to determine the probability of occurrence of a POS class, given its preceding classes, and the probability of a particular word, given its own POS class.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classbased n-grams",
                "used to determine probability",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "probability",
                "given its preceding classes",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5.1 Agreement between translators In an attempt to quantify the agreement between the two groups of translators, we computed the Kappa coefficient for annotation tasks, as defined by Carletta  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa coefficient",
                "as defined by Carletta",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "annotation tasks",
                "for quantifying agreement between translators",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "he measure of predictiveness we employed is log likelihood ratio with respect to the target variable  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "predictiveness",
                "log likelihood ratio",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "target variable",
                "with respect to",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5.3 Translation Results For the translation experiments on the BTEC task, we report the two accuracy measures BLEU   and NIST   as well as the two error rates: word error rate   and position-independent word error rate  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation results",
                "BLEU and NIST",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "translation results",
                "word error rate and position-independent word error rate",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A maximum entropy approach has been applied to partof-speech tagging before  , but the approach's ability to incorporate nonlocal and non-HMM-tagger-type evidence has not been fully explored.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "ability to incorporate nonlocal and non-HMM-tagger-type evidence",
                "METHODOLOGY",
                "neutral",
                0.7
            ],
            [
                "ability",
                "has not been fully explored",
                "PERFORMANCE",
                "negative",
                0.6
            ]
        ]
    },
    {
        "text": "The Penn Treebank documentation   defines a commonly used set of tags.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank documentation",
                "commonly used set of tags",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "set of tags",
                "commonly used",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora  , or monolingual corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "bilingual parallel and 334 non-parallel",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpora",
                "monolingual",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We selected four binary NLP datasets for evaluation: 20 Newsgroups1 and Reuters     and sentiment classification   and spam  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "datasets",
                "four binary NLP",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "Newsgroups1 and Reuters",
                "sentiment classification and spam",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Speaker ranking accuracy Table 2 summarizes the accuracy of our statistical ranker on the test data with different feature sets: the performance is 89.39% when using all feature sets, and reaches 90.2% after applying Gaussian smoothing and using incremental feature selection as described in   and implemented in the yasmetFS package.6 Note that restricting ourselves to only backward looking features decreases the performance significantly, as we can see in Table 2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature sets",
                "all feature sets",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "performance",
                "90.2%",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This iterative optimiser, derived from a word disambiguation technique  , finds the nearest local maximum in the lexical cooccurrence network from each concept seed.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "iterative optimiser",
                "derived from word disambiguation technique",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "lexical cooccurrence network",
                "from each concept seed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the literature approaches to construction of taxonomies of concepts have been proposed  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "taxonomies of concepts",
                "have been",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "e borrow this useful term from the Core Language Engine project  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Core Language Engine project",
                "useful",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "Core Language Engine project",
                "useful",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "1984), written discourse  , and conversational data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "written discourse",
                "written discourse",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "conversational data",
                "conversational data",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "are the labeled parsing recall and precision, respectively, as defined in    ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "labeled parsing recall and precision",
                "as defined in",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "precision",
                "respectively",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Even the recent generation of SMT models that explicitly use WSD modeling to perform lexical choice rely on sentence context rather than wider document context and translate sentences in isolation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT models",
                "rely on sentence context",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence context",
                "translate sentences in isolation",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "eNero and Klein   use a syntaxbased distance in an HMM word alignment model to favor syntax-friendly alignments",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMM word alignment model",
                "syntax-friendly alignments",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntax-based distance",
                "favors syntax-friendly",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "urney   suggested comparing the frequency of phrase co-occurrences with words predetermined by the sentiment lexicon",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentiment lexicon",
                "predetermined by",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase co-occurrences",
                "comparing the frequency of",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For METEOR, when used with its originally proposed parameter values of  , which the METEOR researchers mentioned were based on some early experimental work  , we obtain an average correlation value of 0.915, as shown in the row METEOR.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "METEOR",
                "originally proposed parameter values",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parameter values",
                "based on some early experimental work",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "When labeled training data is available, we can use the Maximum Entropy principle   to optimize the  weights.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy principle",
                "optimize the weights",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "weights",
                "optimize",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  and Turney   classified sentiment polarity of reviews at the document level",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Turney",
                "classified sentiment polarity of reviews",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "document level",
                "at the",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Phrase tables were learned from the training corpus using the diag-and method  , and using IBM model 2 to produce initial word alignments  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase tables",
                "learned using diag-and method",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM model 2",
                "produce initial word alignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1.2 Evaluation In this paper we report results using the BLEU metric  , however as the evaluation criterion in GALE is HTER  , we also report in TER  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metric",
                "report results using",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "HTER",
                "evaluation criterion in GALE",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Current work has been spurred by two papers,   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "papers",
                "spurred",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "work",
                "has been",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This result supports the intuition in   that correlation at segment level is necessary to ensure the reliability of metrics in different situations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "correlation at segment level",
                "necessary",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "metrics",
                "reliability",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 The data 3.1 The supervised data For English, we use the same data division of Penn Treebank   parsed section   as all of  ,  ,   and   do; for details, see Table 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data division",
                "same as all of",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "details",
                "see Table 1",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "his results also agree with Dunning's argument about overestimation on the infrequent occurrences in which many infrequent pairs tend to get higher estimation  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Dunning's argument",
                "about overestimation",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "many infrequent pairs",
                "tend to get higher estimation",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In addition, many more sophisticated parsing models are elaborations of such PCFG models, so understanding the properties of PCFGs is likely to be useful  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PCFG models",
                "elaborations of",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "properties of PCFGs",
                "likely to be useful",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "he input is POS-tagged using the tagger of Ratnaparkhi  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger",
                "of Ratnaparkhi",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tagger",
                "POS-tagged",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he tagging scheme is a variant of the IOB scheme originally put forward by Ramshaw and Marcus  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagging scheme",
                "variant of the IOB scheme",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "IOB scheme",
                "originally put forward by Ramshaw and Marcus",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "State-of-the-art statistical parsers trained on the Penn Treebank     proS a8a8 a8a8a8 a72a72 a72a72a72 NP-SBJ a16a16a16 a80a80a80the authority VP a16a16a16 a16a16a16a16 a0 a0a0 a64 a64a64 a80a80a80 a80a80a80a80 VBD dropped PP-TMP a8a8 a72a72IN at NP NN midnight NP-TMP NNP Tuesday PP-DIR a8a8 a72a72TO to NP QP a16a16a16 a80a80a80$ 2.80 trillion Figure 1: A sample syntactic structure with function labels.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "proS",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "statistical parsers",
                "trained on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It has been difficult to identify all and only those cases where a token functions as a discourse connective, and in many cases, the syntactic analysis in the Penn TreeBank   provides no help.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn TreeBank",
                "provides no help",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "cases",
                "difficult to identify",
                "LIMITATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Co-training   is related to self-training, in that an algorithm is trained on its own predictions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "trained on its own predictions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "predictions",
                "own",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1992) describe one application of MI to identify word collocations; Kashioka et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MI",
                "one application",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Kashioka et al",
                "describe",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars    , which are based on the decomposition of parsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models of disambiguation",
                "are a mere extension",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "dependencies of two words",
                "under the assumption of independence",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The transcription probabilities can then be easily learnt from the alignments induced by GIZA++, using a scoring function  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "induced alignments",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "scoring function",
                "",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "iezler and III   report an improvement in MT grammaticality on a very restricted test set: short sentences parsable by an LFG grammar without back-off rules",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LFG grammar",
                "without back-off rules",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "MT grammaticality",
                "improvement",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Given a contextual word cw that occurs in the paragraphs of bc, a log-likelihood ratio   test is employed  , which checks if the distribution of cw in bc is similar to the distribution of cw in rc; p  = p   .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-likelihood ratio test",
                "is employed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "distribution of cw in bc",
                "is similar to the distribution of cw in rc",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Related Work One of the first works that use statistical methods to detect implicit discourse relations is that of Marcu and Echihabi  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Marcu and Echihabi",
                "use statistical methods",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "statistical methods",
                "to detect implicit discourse relations",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction Translational equivalence is a mathematical relation that holds between linguistic expressions with the same meaning  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linguistic expressions",
                "with the same meaning",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translational equivalence",
                "is a mathematical relation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Even robust parsers using linguistically sophisticated formalisms, such as TAG  , CCG  , HPSG   and LFG  , often use training data derived from the Penn Treebank.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TAG",
                "linguistically sophisticated formalisms",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": " ), the tagger for grammatical functions works with lexical and contextual probability measures Pq .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger for grammatical functions",
                "works with lexical and contextual probability measures",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probability measures",
                "Pq",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Inter-annotator agreement was assessed mainly using f-score and percentage agreement as well as 11 Table 1: Annotation examples of superlative adjectives example sup span det num car mod comp set The third-largest thrift institution in Puerto Rico also   1717 pos sg no ord 1418 the kappa statistics  , where applicable  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "f-score",
                "used",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "kappa statistics",
                "applicable",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Many statistical taggers and parsers have been trained on it, e.g. Ramshaw and Marcus  , Srinivas   and Alshawi and Carter  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical taggers and parsers",
                "trained on",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Ramshaw and Marcus",
                "standard dataset",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": " , Wettler & Rapp   and Church & Hanks   describe algorithms which do this.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "which do this",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Church & Hanks",
                "describe",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Decoding time of our experiments    language model for rescoring  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "experiments",
                "Decoding time of our",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "language model",
                "for rescoring",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Following Church and Hanks  , they use mutual information to select significant two-word patterns, but, at the same time, a lexical inductive process is incorporated which, as they claim, can improve the collection of domain-specific terms.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information",
                "select significant two-word patterns",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "lexical inductive process",
                "can improve the collection of domain-specific terms",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "A monotonic segmentation copes with monotonic alignments, that is, j < k ??aj < ak following the notation of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "monotonic segmentation",
                "copes with monotonic alignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignments",
                "following the notation of ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This is a problem with other direct translation models, such as IBM model 1 used as a direct model rather than a channel model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM model 1",
                "used as a direct model rather than a channel model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "other direct translation models",
                "problem",
                "LIMITATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "To closely reproduce the experiment with the best performance carried out in   using SVM, we use unigram with the presence feature.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SVM",
                "with the best performance",
                "PERFORMANCE",
                "positive",
                0.75
            ],
            [
                "unigram with the presence feature",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Weights on the components were assigned using the   method for max-BLEU training on the development set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "max-BLEU training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "development set",
                "used for training",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "accuracy Training data Turney   66% unsupervised Pang & Lee   87.15% supervised Aue & Gamon   91.4% supervised SO 73.95% unsupervised SM+SO to increase seed words, then SO 74.85% weakly supervised Table 7: Classification accuracy on the movie review domain Turney   achieves 66% accuracy on the movie review domain using the PMI-IR algorithm to gather association scores from the web.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PMI-IR algorithm",
                "gather association scores from the web",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "Turney",
                "achieves 66% accuracy",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "In particular, we adopt the approach of phrase-based statistical machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based statistical machine translation",
                "approach",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase-based",
                "statistical",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Texts are represented by dependency parse trees  ) and templates by parse sub-trees.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency parse trees",
                "represented by",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parse sub-trees",
                "by",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our evaluation metric is BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "evaluation metric",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "For example, it has been observed that texts often contain multiple opinions on different topics  , which makes assignment of the overall sentiment to the whole document problematic.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "texts",
                "contain multiple opinions",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "assignment of sentiment",
                "problematic",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "One could use the estimated co-occurrences from a small sample to compute the test statistics, most commonly Pearsons chi-squared test, the likelihood ratio test, Fishers exact test, cosine similarity, or resemblance    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pearsons chi-squared test",
                "most commonly",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "test statistics",
                "computed using estimated co-occurrences",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In order to overcome this, several methods are proposed, including minimally-supervised learning methods  ), and active learning methods  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "minimally-supervised learning methods",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "methods",
                "active learning methods",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "This is well illustrated by the Collins parser  , scrutinized by Bikel  , where several transformations are applied in order to improve the analysis of noun phrases, coordination and punctuation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins parser",
                "scrutinized by Bikel",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "transformations",
                "applied to improve analysis",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The approach is able to achieve 94% precision and recall for base NPs derived from the Penn Treebank Wall Street Journal  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "achieve 94% precision and recall",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "Penn Treebank Wall Street Journal",
                "standard dataset",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "For tuning of decoder parameters, we conducted minimum error training   with respect to the BLEU score using 916 development sentence pairs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder parameters",
                "minimum error training with respect to the BLEU score",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU score",
                "used for tuning",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The last issue is how our binarization performs on a lexicalized parser, like Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "binarization",
                "on a lexicalized parser",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Collins",
                "like Collins",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "We estimated the probabilities P  and P  similarly to Resnik   by using relative frequencies from the BNC, together with WordNet   as a source of taxonomic semantic class information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BNC",
                "relative frequencies",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "WordNet",
                "source of taxonomic semantic class information",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In the proposed method, the statistical machine translation     is deeply incorporated into the question answering process, instead of using the SMT as the preprocessing before the mono-lingual QA process as in the previous work.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "statistical machine translation is deeply incorporated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "SMT",
                "used as preprocessing before mono-lingual QA process",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The refined grammar is estimated using a variant of the forward-backward algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "variant of the forward-backward algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "grammar",
                "is estimated",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "We do not consider mixed features between words and POS tags as in  , that is, a single feature consists of either words or tags.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mixed features",
                "between words and POS tags",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "single feature",
                "consists of either words or tags",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "In practice, 7-/ is very large and the model's expectation Efj cannot be computed directly, so the following approximation  is used: n E fj,~ E15 p fj  i=1 where fi  is the observed probability of the history hi in the training set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model's expectation",
                "cannot be computed directly",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approximation",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The pervading method for estimating these probabilities is a simple heuristic based on the relative frequency of the phrase pair in the multi-set of the phrase pairs extracted from the word-aligned corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase pair",
                "based on the relative frequency",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase pairs",
                "extracted from the word-aligned corpus",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "On the machine-learning side, it would be interesting to generalize the ideas of large-margin classi cation to sequence models, strengthening the results of Collins   and leading to new optimal training algorithms with stronger guarantees against over tting.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "large-margin classification",
                "generalize",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "results",
                "strengthening",
                "PERFORMANCE",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "We have investigated this and our results are in line with   showing that the translation quality does not improve if we utilize phrases beyond a certain length.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation quality",
                "does not improve",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "phrases beyond a certain length",
                "beyond a certain length",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We compare TERp with BLEU  , METEOR  , and TER  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TERp",
                "with BLEU, METEOR, and TER",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "TERp",
                "compared with",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Most of researchers focus on how to extract useful textual features   for determining the semantic orientation of the sentences using machine learning algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning algorithm",
                "using machine learning algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "textual features",
                "for determining semantic orientation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As, from a linguistic perspective, it is the modifier 2We use a mechanism similar to   but adapted to Chinese data to find lexical heads in the treebank data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mechanism",
                "similar to but adapted to Chinese data",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "treebank data",
                "no opinion term",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In  , a search stops if there is no hypothesis compatible with the gold standard in the queue of candidates.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "queue of candidates",
                "compatible with the gold standard",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "hypothesis",
                "no hypothesis",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For comparison to previous results, table 2 lists the results on the testing set for our best model   and several other statistical parsers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "best",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parsers",
                "several other",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Letter successor variety   models   use the hypothesis that there is less certainty when predicting the next character at morpheme boundaries.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "hypothesis that there is less certainty",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "predicting the next character",
                "at morpheme boundaries",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We thus introduce a multiplier  to form the actual objective function that we minimize with respect to :4 summationdisplay iL logp,i  +  Nsummationdisplay inegationslashL H    One may regard  as a Lagrange multiplier that is used to constrain the classifiers uncertainty H to be low, as presented in the work on entropy regularization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "multiplier",
                "used to constrain the classifiers uncertainty to be low",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "entropy regularization",
                "presented in the work",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Examples of such contexts are verb-object relations and noun-modifier relations, which were traditionally used in word similarity tasks from non-parallel corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "verb-object relations",
                "traditionally used",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "noun-modifier relations",
                "used in word similarity tasks",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, in the IBM Models  , each word ti independently generates 0, 1, or more 2Note that we refer to t as the target sentence, even though in the source-channel model, t is the source sentence which goes through the channel model P  to produce the observed sentence s. words in the source language.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Models",
                "IBM Models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "source-channel model",
                "target sentence",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "298 within LFG includes the XLE,3 Cahill and van Genabith  , Hogan et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "XLE",
                "within LFG",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "XLE",
                "includes",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Metrics in the Rouge family allow for skip n-grams  ; Kauchak and Barzilay   take paraphrasing into account; metrics such as METEOR   and GTM   calculate both recall and precision; METEOR is also similar to SIA   in that word class information is used.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Rouge family",
                "allow for skip n-grams",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "METEOR",
                "is similar to SIA",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Our process of extraction of rules as synchronous trees and then converting them to synchronous CFG rules is most similar to that of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rules",
                "most similar to that of  ",
                "METHODOLOGY",
                "neutral",
                0.5
            ],
            [
                "synchronous trees",
                "converting them to synchronous CFG rules",
                "METHODOLOGY",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "We made use of the same data set as introduced in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data set",
                "same as introduced",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "data set",
                "same as",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These results confirm the observed figures in the previous subsection and reinforce the sight that clustering is a worthless effort for automatic paraphrase corpora construction, contrarily to what   suggest.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "clustering",
                "worthless effort",
                "INNOVATION",
                "negative",
                0.8
            ],
            [
                "contrary to what suggest",
                "contrary",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This operation does not change the collection of phrases or rules extracted from a hypothesized alignment, see, for instance,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrases or rules",
                "extracted from a hypothesized alignment",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "operation",
                "does not change",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "  BLEU-4   is used as the evaluation metric.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU-4",
                "is used as the evaluation metric",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentences",
                "map to lambda-calculus meaning representations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "work",
                "has explored",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The discriminative training regimen is otherwise similar to  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "regimen",
                "similar to",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "training",
                "discriminative",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Background Several graph-based learning techniques have recently been developed and applied to NLP problems: minimum cuts  , random walks  , graph matching  , and label propagation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "graph-based learning techniques",
                "have recently been developed and applied to NLP problems",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "minimum cuts",
                "random walks",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The original IBM Models   learn word-to-word alignment probabilities which makes it computationally feasible to estimate model parameters from large amounts of training data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "These tags are drawn from a tagset which is constructed by extending each argument label by three additional symbols a11 a24 a35 a24a4a12, following  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagset",
                "constructed by extending each argument label by three additional symbols a11 a24 a35 a24a4a12",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tagset",
                "following",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.2 The Crossing Constraint According to  , crossing constraint can be defined in the following.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Crossing Constraint",
                "can be defined",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "constraint",
                "can be defined",
                "INNOVATION",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "We run Maximum BLEU   for 25 iterations individually for each system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "for 25 iterations individually for each system",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "system",
                "individually for each",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 Evaluation We trained our model parameters on a subset of the provided dev2006 development set, optimizing for case-insensitive IBM-style BLEU   with several iterations of minimum error rate training on n-best lists.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model parameters",
                "trained on dev2006 development set",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU",
                "IBM-style",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Our aim is not only to determine the utility of citation texts for survey creation, but also to examine the quality distinctions between this form of input and others such as abstracts and full textscomparing the results to human-generated surveys using both automatic and nugget-based pyramid evaluation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "input",
                "this form of input",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "results",
                "comparing the results",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction In the past few years, there has been an increasing interest in mining opinions from product reviews  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "interest",
                "increasing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "opinions from product reviews",
                "has been an increasing interest",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use a simple, single parameter distribution, with  = 8.0 throughout P  = P   K Word-to-Phrase Alignment Alignment is a Markov process that specifies the lengths of phrases and their alignment with source words P  = Kproductdisplay k=1 P  = Kproductdisplay k=1 p d n  The actual word-to-phrase alignment   is a firstorder Markov process, as in HMM-based word-toword alignment  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "simple, single parameter distribution",
                "distribution",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Markov process",
                "specifies lengths of phrases and alignment with source words",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A third of the corpus is syntactically parsed as part of the Penn Treebank   2This type corresponds to Princes   inferrables.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "syntactically parsed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Princes inferrables",
                "corresponds to",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": " ; later elaborations and refinements have been implemented in a number of systems, notably CHAT-80  , TEAM  , and CLE  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CHAT-80",
                "notably",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "TEAM",
                "notably",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "we use k = 50 and obtain our thesaurus using the distributional similarity metric described by Lin  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "k",
                "k = 50",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "thesaurus",
                "obtained using the distributional similarity metric",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "For automatic evaluation, we employed BLEU   by following  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "by following",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "evaluation",
                "automatic",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.4 Related Work   implemented an MEMM model for supertagging which is analogous to the POS tagging model of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MEMM model",
                "analogous to the POS tagging model",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "supertagging",
                "implemented",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We have explained elsewhere   how suitable features can be defined in terms of the a18 word, pos-tag a20 pairs in the context, and how maximum entropy techniques can be used to estimate the probabilities, following Ratnaparkhi  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "suitable features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "maximum entropy techniques",
                "can be used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linguistic constraints",
                "reduced need for data",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "models",
                "more complete",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "In addition, corpus-based stochastic modelling of lexical patterns   may provide information about word sense frequency of the kind advocated since  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus-based stochastic modelling",
                "lexical patterns",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word sense frequency",
                "advocated",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "4.5.2 BLEU on NIST MT Test Sets We use MT02 as the development set4 for minimum error rate training    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT02",
                "development set",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "minimum error rate training",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our appoach is based on Maximum Entropy   technique  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy",
                "technique",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "approach",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.1 The test environment For our experiments, we used a manually corrected version of the Air Travel Information System   spoken language corpus   annotated in the Pennsylvania Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Air Travel Information System",
                "manually corrected version",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Pennsylvania Treebank",
                "annotated",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5 Related Work As discussed in footnote 3, Collins   and McDonald et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins and McDonald et al.",
                "related work",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "footnote 3",
                "discussed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The wn::similarity package   to compute the Jiang&Conrath   distance   as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "wn::similarity package",
                "compute Jiang&Conrath distance",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Jiang&Conrath distance",
                "as in ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We use binary Synchronous ContextFree Grammar  , based on Inversion Transduction Grammar    , to define the set of eligible segmentations for an aligned sentence pair.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Synchronous ContextFree Grammar",
                "based on Inversion Transduction Grammar",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "set of eligible segmentations",
                "for an aligned sentence pair",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The translation quality is evaluated by BLEU metric  , as calculated by mteval-v11b.pl 6 with case-sensitive matching of n-grams.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metric",
                "as calculated by mteval-v11b.pl 6",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "n-grams",
                "case-sensitive matching",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Various clustering techniques have been proposed   which perform automatic word clustering optimizing a maximum-likelihood criterion with iterative clustering algorithms.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "clustering techniques",
                "optimizing a maximum-likelihood criterion",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "iterative clustering algorithms",
                "perform automatic word clustering",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This may be because their system was not tuned using minimum error rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "not tuned using minimum error rate training",
                "METHODOLOGY",
                "negative",
                0.7
            ],
            [
                "training",
                "minimum error rate",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We use SUMMA   to generate generic and query-based multi-document summaries and evaluate them using ROUGE evaluation metrics   relative to human generated summaries.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SUMMA",
                "generate generic and query-based multi-document summaries",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ROUGE evaluation metrics",
                "relative to human generated summaries",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "It has been shown that one sense per discourse property can improve the performance of bootstrapping algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bootstrapping algorithm",
                "improve the performance",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "one sense per discourse property",
                "can improve",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use the Stanford parser   with its default Chinese grammar, the GIZA++   alignment package with its default settings, and the ME tool developed by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stanford parser",
                "default Chinese grammar",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "GIZA++ alignment package",
                "default settings",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "It is shown that  -BRCGs induce inside-out alignments   and cross-serial discontinuous translation units  ; both phenomena can be shown to occur frequently in many hand-aligned parallel corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BRCGs",
                "induce inside-out alignments and cross-serial discontinuous translation units",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "hand-aligned parallel corpora",
                "occur frequently",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "We expect that the mean field approximation should demonstrate better results than feed-forward approximation on this task as it is theoretically expected and confirmed on the constituent parsing task  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mean field approximation",
                "should demonstrate better results",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "feed-forward approximation",
                "should demonstrate better results",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "It has been argued that the reliability of a coding schema can be assessed only on the basis of judgments made by naive coders  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "coding schema",
                "can be assessed",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "judgments made by naive coders",
                "only on the basis of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The examples represent seven-word windows of words and their respective   part-of-speech tags, and each example is labeled with a class using the IOB type of segmentation coding as introduced by Ramshaw and Marcus  , marking whether the middle word is inside  , outside  , or at the beginning   of a chunk.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IOB type of segmentation coding",
                "introduced by Ramshaw and Marcus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "middle word",
                "inside, outside, or at the beginning of a chunk",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The feature weights were optimized against the BLEU scores  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "optimized against the BLEU scores",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU scores",
                "standard evaluation metric",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "toilet/bathroom Since the word \"\"facility\"\" is the subject of \"\"employ\"\" and is modified by \"\"new\"\" in  , we retrieve other words that appeared in the same contexts and obtain the following two groups of selectors   of these words in the local contexts):  Subjects of \"\"employ\"\" with top-20 highest likelihood ratios: word freq, Iog,k word freq ORG\"\" 64 50.4 plant 14 31.0 company 27 28.6 operation 8 23.0 industry 9 14.6 firm 8 13.5 pirate 2 12.1 unit 9 9.32 shift 3 8.48 postal service 2 7.73 machine 3 6.56 corporation 3 6.47 manufacturer 3 6.21 insurance company 2 6.06 aerospace 2 5.81 memory device 1 5.79 department 3 5.55 foreign office 1 5.41 enterprise 2 5.39 pilot 2 537 *ORG includes all proper names recognized as organizations 18  Modifiees of \"\"new\"\" with top-20 highest likelihood ratios: word freq log,k post 432 952.9 issue 805 902.8 product 675 888.6 rule 459 875.8 law 356 541.5 technology 237 382.7 generation 150 323.2 model 207 319.3 job 260 269.2 system 318 251.8 word freq log )~ bonds 223 245.4 capital 178 241.8 order 228 236.5 version 158 223.7 position 236 207.3 high 152 201.2 contract 279 198.1 bill 208 194.9 venture 123 193.7 program 283 183.8 Since the similarity between Sense 1 of \"\"facility\"\" and the selectors is greater than that of other senses, the word \"\"facility\"\" in   is tagged \"\"Sense The key innovation of our algorithm is that a polysemous word is disambiguated with past usages of other words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word",
                "with top-20 highest likelihood ratios",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word",
                "tagged Sense 1",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We presented some theoretical arguments for not limiting extraction to minimal rules, validated them on concrete examples, and presented experiments showing that contextually richer rules provide a 3.63 BLEU point increase over the minimal rules of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rules",
                "provide a 3.63 BLEU point increase",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "rules",
                "minimal",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ut we did not use any LM estimate to achieve early stopping as suggested by Huang and Chiang  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LM estimate",
                "suggested by Huang and Chiang",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "early stopping",
                "did not use",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For each word in the LDV, we consulted three existing thesauri: Rogets Thesaurus  , Collins COBUILD Thesaurus  , and WordNet  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "thesauri",
                "Rogets Thesaurus, Collins COBUILD Thesaurus, WordNet",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "thesauri",
                "existing",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "By treating a letter/character as a word and a group of letters/characters as a phrase or token unit in SMT, one can easily apply the traditional SMT models, such as the IBM generative model   or the phrase-based translation model   to transliteration.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT models",
                "traditional",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IBM generative model",
                "traditional",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Finally, the parameters  i of the log-linear model   are learned by minimumerror-rate training  , which tries to set the parameters so as to maximize the BLEU score   of a development set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "learned by minimum-error rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU score",
                "maximize",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "A standard solution is to use a weighted linear mixture of N-gram models, 1  n  N,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "N-gram models",
                "standard solution",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "weighted linear mixture",
                "standard solution",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "First, the addition of each modification improves the F-score for both true and system mentions 9The H&K results shown here are not directly comparable with those reported in Haghighi and Klein  , since H&K evaluated their system on the ACE 2004 coreference corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "modification",
                "improves the F-score",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "H&K results",
                "not directly comparable",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  use HMM-based similarity for the same purpose.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMM-based similarity",
                "for the same purpose",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "HMM-based similarity",
                "HMM-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Standard MET   iterative parameter estimation under IBM BLEU   is performed on the corresponding development set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MET",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parameter estimation",
                "under IBM BLEU",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Table 1 shows the impact of increasing reordering window length   on translation quality for the ?dev06??data.2 Increasing the reordering window past 2 has minimal impact on translation quality, implying that most of the reordering effects across Spanish and English are well modeled at the local or phrase level.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reordering window length",
                "has minimal impact on translation quality",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "reordering effects",
                "are well modeled at the local or phrase level",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This can be done in a supervised  , a semi-supervised   or a fully unsupervised way  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "way",
                "supervised, semi-supervised, or fully unsupervised",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "way",
                "supervised, semi-supervised, or fully unsupervised",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  describes how the voted perceptron can be used to train maximum-entropy style taggers, and also gives a more thorough discussion of the theory behind the perceptron algorithm applied to ranking tasks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron",
                "used to train maximum-entropy style taggers",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "perceptron algorithm",
                "applied to ranking tasks",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A number of works in product review mining   automatically find features of the reviewed products.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "works",
                "find features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "features",
                "of reviewed products",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This is one manifestation of what is commonly referred to as the data sparseness problem, and was discussed by Rapp   as a side-effect of specificity.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data sparseness problem",
                "commonly referred to",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "specificity",
                "side-effect of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " , Turney  ), we are interested in fine-grained subjectivity analysis, which is concerned with subjectivity at the phrase or clause level.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "subjectivity analysis",
                "concerned with subjectivity at the phrase or clause level",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "subjectivity",
                "at the phrase or clause level",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These heuristics define a phrase pair to consist of a source and target ngrams of a word-aligned source-target sentence pair such that if one end of an alignment is in the one ngram, the other end is in the other ngram    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ngrams",
                "word-aligned source-target sentence pair",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase pair",
                "consist of a source and target",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Alignment, whether for training a translation model using EM or for nding the Viterbi alignment of test data, is O   , while translation   is O  using a bigram language model, and O  with trigrams.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment",
                "is O   ",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation",
                "is O  ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As a baseline, we use an IBM Model 4   system3 with a greedy decoder4  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 4 system",
                "greedy decoder",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Model 4 system",
                "greedy decoder",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S   and METEOR  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "proposed",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "ROUGE-W",
                "ROUGE-W",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 The IBM Model 4 For the work described in this paper we used a modified version of the statistical machine translation tool developed in the context of the 1999 Johns HopkinsSummer Workshop  , which implements IBM translation model 4  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 4",
                "statistical machine translation tool",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical machine translation tool",
                "developed in the context of the 1999 Johns Hopkins Summer Workshop",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Minimum error rate training   with respect to BLEU score was used to tune the decoders parameters, and performed using the technique proposed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoders parameters",
                "tune the decoders parameters",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "technique proposed in  ",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Denote the global feature vector for segmented sentence y with    Rd, where d is the total number of features in the model; then Score  is computed by the dot product of vector   and a parameter vector   Rd, where i is the weight for the ith feature: Score  =   841 Inputs: training examples   Initialization: set  = 0 Algorithm: for t = 1T, i = 1N calculate zi = argmaxyGEN    if zi negationslash= yi  =  +    Outputs:  Figure 1: the perceptron learning algorithm, adapted from Collins   The perceptron training algorithm is used to determine the weight values .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature vector",
                "total number of features in the model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "weight values",
                "perceptron learning algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 Previous Work The LEAF model is inspired by the literature on generative modeling for statistical word alignment and particularly by Model 4  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LEAF model",
                "inspired by the literature on generative modeling for statistical word alignment and particularly by Model 4",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "Model 4",
                "particularly by",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The cohesion between words has been evaluated with the mutual information measure, as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information measure",
                "has been evaluated",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "words",
                "has been evaluated with",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "hang and Clark     generated CTB 3.0 from CTB 4.0",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CTB 3.0",
                "generated from CTB 4.0",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "CTB 4.0",
                "used to generate CTB 3.0",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "sing thesaurus categories directly as a coarse sense division may seem to be a viable alternative  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "thesaurus categories",
                "coarse sense division",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "alternative",
                "seem to be",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "If we assign a probability a13a15a14a17a16 a10a12a11a5a19a18a2 a3a5a21a20 to each pair of strings a16 a10 a11a5a12a22 a2a4a3a5 a20, then according to Bayes decision rule, we have to choose the English string that maximizes the product of the English language model a13a23a14a24a16 a10 a11a5 a20 and the string translation model a13a15a14a17a16a25a2 a3a5a26a18a10a27a11a5a28a20 . Many existing systems for statistical machine translation   make use of a special way of structuring the string translation model like proposed by  : The correspondence between the words in the source and the target string is described by alignments which assign one target word position to each source word position.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "string translation model",
                "special way of structuring",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "alignments",
                "assign one target word position to each source word position",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This can be done by smoothing the observed frequencies  , or by class-based methods  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "frequencies",
                "observed frequencies",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "class-based methods",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Learning to Disambiguate Word Senses Several recent research projects have taken a corpus-based approach to lexical disambiguation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research projects",
                "corpus-based approach",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word senses",
                "several recent",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "We extracted tagged sentences from the parse trees.5 We split the data into training, development, and test sets as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parse trees",
                "tagged sentences",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "data",
                "split into training, development, and test sets",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar",
                "given by the grammar",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "parse results",
                "all parse results",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "For instance, the to-PP frame is poorly' represented in the syntactically annotated version of the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "syntactically annotated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "to-PP frame",
                "poorly represented",
                "PERFORMANCE",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "22 Table 5: Comparison with previous best results:   POS tagging F=1 Perceptron   97.11 Dep.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Perceptron",
                "POS tagging F=1",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "results",
                "previous best results",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We also test our language model using leave-one-out cross-validation on the Penn Treebank    , giving us 86.74% accuracy  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "leave-one-out cross-validation",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "accuracy",
                "86.74%",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To make this paper comparable to  , we use English-French notation in this section.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "notation",
                "English-French",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "paper",
                "comparable",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "TER-based: TER-based word alignment method   is an extension of multiple string matching algorithm based on Levenshtein edit distance  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TER-based word alignment method",
                "extension of multiple string matching algorithm based on Levenshtein edit distance",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "multiple string matching algorithm",
                "based on Levenshtein edit distance",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Evaluation 8.1 Effects of Unpublished Details In this section we present the results of effectively doing a clean-room implementation of Collins parsing model, that is, using only information available in  , as shown in Table 4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins parsing model",
                "clean-room implementation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "information available in",
                "as shown in Table 4",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A sinfilar approach has been chosen by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sinfilar approach",
                "has been chosen",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approach",
                "chosen",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As in phrasebased translation model estimation, ? also contains two lexical weights  , counters for number of target terminals generated.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexical weights",
                "contains two lexical weights",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "target terminals",
                "number of target terminals generated",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Because Daume III   views the adaptation as merely augmenting the feature space, each of his features has the same prior mean and variance, regardless of whether it is domain specific or independent.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "same prior mean and variance",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "adaptation",
                "merely augmenting the feature space",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Furthermore, we use averaged weights   in Algorithm 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Algorithm 1",
                "use",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "weights",
                "averaged",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "An alternative representation for baseNPs has been put forward by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseNPs",
                "alternative representation",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "baseNPs",
                "put forward",
                "INNOVATION",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "This leads to a good amount of work in this area   In the most basic approach, such as Ratnaparkhi et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "good amount",
                "PERFORMANCE",
                "positive",
                0.7
            ],
            [
                "approach",
                "basic",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "his model is very similar to Smith and Eisner  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "very similar to",
                "METHODOLOGY",
                "positive",
                0.75
            ],
            [
                "Smith and Eisner",
                "very similar",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To set the weight vector w, we train twenty averaged perceptrons   on different shuffles of data drawn from sections 0221 of the Penn Treebank.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptrons",
                "twenty averaged",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "data",
                "drawn from sections 0221 of the Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "These estimates are usually heuristic and inconsistent  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "estimates",
                "heuristic and inconsistent",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "estimates",
                "are usually",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "3.2 F-Structure Based NLD Recovery   presented a NLD recovery algorithm operating at LFG f-structure for treebankbased LFG approximations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NLD recovery algorithm",
                "operating at LFG f-structure",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "LFG f-structure",
                "for treebank-based LFG approximations",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Second, it can be applied to control the quality of parallel bilingual sentences mined from the Web, which are critical sources for a wide range of applications, such as statistical machine translation   and cross-lingual information retrieval  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "There are also research work on automatically classifying movie or product reviews as positive or negative  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research work",
                "automatically classifying movie or product reviews",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "movie or product reviews",
                "positive or negative",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Although the authors of   stated that they would discuss the search problem in a follow-up arti cle, so far there have no publications devoted to the decoding issue for statistical machine translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "authors",
                "would discuss",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "publications",
                "no devoted to decoding issue",
                "LIMITATION",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "Distance measures for CCG Our distance measures are related to those proposed by Goodman  , which are appropriate for binary trees  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distance measures",
                "related to those proposed by Goodman",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "distance measures",
                "are appropriate for binary trees",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A model was trained using Maximum Likelihood from the UPenn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "trained using Maximum Likelihood",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "UPenn Treebank",
                "used as dataset",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.1 Corpora set-up The above kernels were experimented over two corpora: PropBank   along with Penn TreeBank5 2   and FrameNet.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PropBank",
                "along with Penn TreeBank5 2",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "FrameNet",
                "",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Various approaches to word sense division have been proposed in the literature on WSD, including   sense numbers in every-day dictionaries  ,   automatic or hand-crafted clusters of dictionary senses (Dolan 1994; Bruce and Wiebe 1995; Luk * Department of Computer Science, National Tsing Hua University, Hsinchu 30043, Taiwan, ROC.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sense numbers",
                "in every-day dictionaries",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "automatic or hand-crafted clusters",
                "of dictionary senses",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In the following, we summarize the optimization algorithm for the unsmoothed error counts presented in   and the implementation detailed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "presented in",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "implementation",
                "detailed in",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "On the other hand, according to the data-driven approach, a frequency-based language model is acquired from corpora and has the forms of ngrams  , rules  , decision trees   or neural networks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "frequency-based language model",
                "has forms of ngrams, rules, decision trees, or neural networks",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpora",
                "acquired from",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This is similar to work by several other groups which aims to induce semantic classes through syntactic co-occurrence analysis  , although in .our case the contexts are limited to selected patterns, relevant to the scenario.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work by several other groups",
                "aims to induce semantic classes through syntactic co-occurrence analysis",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "contexts",
                "limited to selected patterns, relevant to the scenario",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our work expands on the general approach taken by   but arrives at insights similar to those of the most recent work  , albeit in a completely different manner.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "general approach",
                "taken by",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "insights",
                "similar to those of the most recent work",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Both models are based on IBM translation model 2   which has the 49 property that it generates tokens independently.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM translation model 2",
                "generates tokens independently",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "property",
                "49",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We parsed a 125-million word newspaper corpus with Minipar, 1 a descendent of Principar  , and extracted dependency relationships from the parsed corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Minipar",
                "descendent of Principar",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parsed corpus",
                "with 125-million words",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "aghighi and Klein   use a small list of labeled prototypes and no dictionary",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "labeled prototypes",
                "small list of",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "dictionary",
                "no",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Recently, methods for training binary classifiers to maximize the F 1 -score have been proposed for SVM   and LRM  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "maximize the F 1 -score",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "SVM and LRM",
                "proposed",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 Methodology Similar to  , we use comparison to human assocation datasets as a test bed for the scores produced by computational association measures.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "comparison",
                "as a test bed",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "scores produced by computational association measures",
                "produced",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The parser implementation in   was used in this experiment and it was run in a mode which emulated the Collins   parser.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser implementation",
                "was used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "mode",
                "emulated the Collins parser",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We repeat Ramshaw and Marcus Transformation Based NP chunking   algorithm by substituting supertags for POS tags in the dataset.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Transformation Based NP chunking algorithm",
                "by substituting supertags for POS tags",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dataset",
                "in the dataset",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Variational Bayes for ITG Goldwater and Griffiths   and Johnson   show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG",
                "modifying an HMM to include a sparse prior over its parameters",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Bayesian estimation",
                "leads to improved accuracy",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "  argue that precise alignment can improve transliteration effectiveness, experimenting on English-Chinese data and comparing IBM models   with phonemebased alignments using direct probabilities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment",
                "can improve transliteration effectiveness",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "IBM models",
                "using direct probabilities",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "DeNero and Klein   focus on alignment and do not present MT results, while May and Knight   takesthesyntacticre-alignmentasaninputtoanEM algorithm where the unaligned target words are insertedintothetemplatesandminimumtemplatesare combinedintobiggertemplates .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "EM algorithm",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "templates",
                "minimum templates are combined into bigger templates",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "As our basic data source, we use 500 000 sentences from the Wikipedia XML corpus  ; this is the corpus used by Akhmatova and Dras  , and related to one used in one set of experiments by Snow et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "used by Akhmatova and Dras",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "corpus",
                "used in one set of experiments by Snow et al.",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Instead of taking just the nal weight vector, the voted perceptron algorithm takes the average of the t. Collins   reported and we con rmed that this averaging reduces overtting considerably.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "voted perceptron algorithm",
                "takes the average",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "averaging",
                "reduces overtting considerably",
                "PERFORMANCE",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "We employ the loglikelihood ratio as a measure of the collocational status of the adjective-noun pair  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "loglikelihood ratio",
                "as a measure of the collocational status",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "adjective-noun pair",
                "collocational status",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The features used are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in  ; phrase translation model probabilities; and 4-gram language model probabilities logp , using Kneser-Ney smoothing as implemented in the SRILM toolkit  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kneser-Ney smoothing",
                "as implemented in the SRILM toolkit",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "phrase reordering",
                "in a",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "oodman   and Johnson   both suggest this strategy",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "strategy",
                "suggest this",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "strategy",
                "this",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For instance, the Penn Treebank policy   is to annotate the lowest node that is unfinished with an -UNF tag as in Figure 4 .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank policy",
                "annotate the lowest node that is unfinished with an -UNF tag",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "lowest node",
                "unfinished",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Another body of related work is the literature on word clustering in computational linguistics   and document clustering in information retrieval  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word clustering",
                "in computational linguistics",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "document clustering",
                "in information retrieval",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "len.: median length of sequences of co-specifying referring expressions with Cohen's n  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Cohen's n",
                "median length of sequences of co-specifying referring expressions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sequences of co-specifying referring expressions",
                "with",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Introduction Recently, there has been an increased interest in approaches to automatically learning to recognize shallow linguistic patterns in text \\[Ramshaw and Marcus, 1995, Vilain and Day, 1996, Argamon et al. , 1998, Buchholz, 1998, Cardie and Pierce, 1998, Veenstra, 1998, Daelemans et aI.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "to automatically learning to recognize shallow linguistic patterns in text",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Ramshaw and Marcus, 1995",
                "et al.",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Following extraction, O-CRF applies the RESOLVER algorithm   to find relation synonyms, the various ways in which a relation is expressed in text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "O-CRF",
                "applies the RESOLVER algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "relation synonyms",
                "various ways in which a relation is expressed in text",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In paraphrase generation, a text unit that matches a pattern P can be rewritten using the paraphrase patterns of P. Avarietyofmethodshavebeenproposedonparaphrase patterns extraction  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "paraphrase patterns",
                "have been proposed",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "methodshavebeenproposed",
                "on paraphrase patterns extraction",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, smoothing methods have played a central role in probabilistic approaches  , and yet they are not being used in current large margin training algorithms.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "smoothing methods",
                "played a central role",
                "METHODOLOGY",
                "neutral",
                0.7
            ],
            [
                "current large margin training algorithms",
                "are not being used",
                "METHODOLOGY",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "Word alignments were generated using Model 4   using the multi-threaded implementation of GIZA++  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 4",
                "multi-threaded implementation of GIZA++",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "GIZA++",
                "multi-threaded",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "P chunks   and technical terms   fall into this difficult-toassess category",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "P chunks",
                "difficult-to-assess",
                "PERFORMANCE",
                "negative",
                0.75
            ],
            [
                "technical terms",
                "difficult-to-assess",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "6 The Experiments We used the Penn Treebank   to perform empirical experiments on the proposed parsing models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "perform empirical experiments",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "proposed parsing models",
                "used for experiments",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "These problems formulations are similar to those studied in   and  , respectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "problems formulations",
                "similar",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "those studied",
                "respectively",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We report results on the Boston University   Radio Speech Corpus   and Boston Directions Corpus    , two publicly available speech corpora with manual ToBI annotations intended for experiments in automatic prosody labeling.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Boston University Radio Speech Corpus",
                "publicly available",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "manual ToBI annotations",
                "intended for experiments",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We trained a Chinese Treebank-style tokenizer and partof-speech tagger, both using a tagging model based on a perceptron learning algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tokenizer and part-of-speech tagger",
                "based on a perceptron learning algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tagging model",
                "based on a perceptron learning algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It has been noticed, for example that capitalized and hyphenated words have a different distribution from other words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "different distribution",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "words",
                "capitalized and hyphenated",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We have   Hypernym Patterns based on patterns proposed by   and  ,   Sibling Patterns which are basically conjunctions, and   Part-of Patterns based on patterns proposed by   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hypernym Patterns",
                "patterns proposed by",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Part-of Patterns",
                "patterns proposed by",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Thus, we propose a bootstrapping approach   to train the stochastic transducer iteratively as it extracts transliterations from a bitext.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "stochastic transducer",
                "iteratively as it extracts transliterations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "bootstrapping approach",
                "to train the stochastic transducer",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One approach to translate terms consists in using a domain-specific parallel corpus with standard alignment techniques   to mine new translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "domain-specific parallel corpus",
                "standard alignment techniques",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "terms",
                "new translations",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We use only the words that are content words   and not in the stopword list used in ROUGE  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "stopword list",
                "used in ROUGE",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "words that are content words",
                "not in the stopword list",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our learning method is an extension of Collinss perceptron-based method for sequence labeling  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "learning method",
                "perceptron-based method",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Collinss perceptron-based method",
                "sequence labeling",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In  , lexical 72 features were limited on each single side due to the feature space problem.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "limited",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "feature space problem",
                "problem",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsers",
                "best reported parsing results",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "Penn Treebank Wall Street Journal corpus",
                "given on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Previous authors have used numerous HMM-based models   and other types of networks including maximum entropy models  , conditional Markov models  , conditional random elds    , and cyclic dependency networks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMM-based models",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "conditional Markov models",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Tuning   was done using minimum error rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Tuning",
                "minimum error rate training",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "We also do not require a newly added feature to be either atomic or a collocation of an atomic feature with a feature already included into the model as it was proposed in    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature",
                "not require to be either atomic or a collocation of an atomic feature with a feature already included into the model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "model",
                "already included into",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Bitexts also play a role in less automated applications such as concordancing for bilingual lexicography  , computer-assisted language learning, and tools for translators (e.g.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bitexts",
                "in less automated applications",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "concordancing for bilingual lexicography",
                "computer-assisted language learning, and tools for translators",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "More recently, EM has been used to learn hidden variables in parse trees; these can be head-childannotations , latent head features  , or hierarchicallysplit nonterminal states  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parse trees",
                "hidden variables",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "hidden variables",
                "head-childannotations, latent head features, or hierarchicallysplit nonterminal states",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In order to incorporate a new dependency which contains extra information other than the bilingual sentence pair, we modify Eq.2 by adding a new variable v: Pr  = exp   Accordingly, we get a new decision rule: a = argmax a braceleftbigg Msummationdisplay m=1 mhm  bracerightbigg   Note that our log-linear models are different from Model 6 proposed by Och and Ney  , which defines the alignment problem as finding the alignment a that maximizes Pr  given e. 3 Feature Functions In this paper, we use IBM translation Model 3 as the base feature of our log-linear models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-linear models",
                "are different from Model 6",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "alignment problem",
                "maximizes Pr  given e",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The system is tested on base noun-phrase   chunking using the Wall Street Journal corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal corpus",
                "is used",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "base noun-phrase chunking",
                "is tested",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The weights are then averaged across all iterations of the perceptron, as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron",
                "iterations",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "weights",
                "averaged",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5.1 CoNLL named entities presence feature We use Stanford named entity recognizer     to identify CoNLL style NEs7 as possible answer strings in a candidate sentence for a given type of question.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stanford named entity recognizer",
                "identify CoNLL style NEs7",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "CoNLL named entities presence feature",
                "possible answer strings in a candidate sentence",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Following previous work in statistical MT  , we envision a noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "language model",
                "generates English",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation model",
                "transforms English trees into Chinese",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, in machine translation evaluation, approaches such as BLEU   use n-gram overlap comparisons with a model to judge overall goodness, with higher n-grams meant to capture fluency considerations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-gram overlap comparisons",
                "judge overall goodness",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "higher n-grams",
                "capture fluency considerations",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A statistical prediction engine provides the completions to what a human translator types  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical prediction engine",
                "provides the completions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "human translator",
                "types",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Document level sentiment classification is mostly applied to reviews, where systems assign a positive or negative sentiment for a whole review document  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "document level sentiment classification",
                "is mostly applied to reviews",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentiment",
                "positive or negative",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Several recent papers have studied the use of annotations obtained from Amazon Mechanical Turk, a marketplace for recruiting online workers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Amazon Mechanical Turk",
                "marketplace for recruiting online workers",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "annotations",
                "obtained from",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Recently, methods from nonparametric Bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks, including language modeling, word and morpheme segmentation, parsing, and machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "gaining popularity",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "variety of tasks",
                "including language modeling, word and morpheme segmentation, parsing, and machine translation",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The NIST MT03 set is used to tune model weights  ) and the scaling factor 17We have also experimented with MERT  , and found that the deterministic annealing gave results that were more consistent across runs and often better.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NIST MT03 set",
                "used to tune model weights",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "deterministic annealing",
                "gave results that were more consistent and often better",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "No pretagged text is necessary for Hidden Markov Models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hidden Markov Models",
                "No pretagged text is necessary for",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "1 Introduction Recently, extracting questions, contexts and answers from post discussions of online forums incurs increasing academic attention  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "extracting questions, contexts and answers",
                "incurs increasing academic attention",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "online forums",
                "post discussions",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this method, the decision list   learning algorithm   is used.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decision list learning algorithm",
                "is used",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "While early head-lexicalized grammars restricted the fragments to the locality of headwords  , later models showed the importance of including context from higher nodes in the tree  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "fragments",
                "including context from higher nodes in the tree",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "early head-lexicalized grammars",
                "restricted the fragments to the locality of headwords",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.1 Word Sequence Classification Similar to English text chunking  , the word sequence classification model aims to classify each word via encoding its context features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word sequence classification model",
                "encodes context features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word sequence classification",
                "similar to English text chunking",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "For evaluation we use ROUGE   SU4 recall metric1, which was among the official automatic evaluation metrics for DUC.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE SU4 recall metric",
                "was among the official automatic evaluation metrics",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "DUC",
                "official automatic evaluation metrics",
                "APPLICABILITY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "iscovering orientations of context dependent opinion comparative words is related to identifying domain opinion words  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "orientations",
                "comparative words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "domain",
                "opinion words",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The training and test set were derived by finding all instances of the confusable words in the Brown Corpus, using the Penn Treebank parts of speech and tokenization  , and then dividing this set into 80% for training and 20% for testing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brown Corpus",
                "parts of speech and tokenization",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "set",
                "80% for training and 20% for testing",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 Related Work Automatically finding sentences with the same meaning has been extensively studied in the field of automatic paraphrasing using parallel corpora and corporawith multiple descriptionsof the same events  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "parallel corpora and corporawith multiple descriptions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "field of automatic paraphrasing",
                "extensively studied",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4 Experiments Our experiments involve data from two treebanks: the Wall Street Journal Penn treebank   and the Chinese treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treebanks",
                "Wall Street Journal Penn treebank and the Chinese treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "experiments",
                "involves data from",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The statistical approach involves the following: alignment of bilingual texts at the sentence level nsing statistical techniques  , Gale and Church  , Chen  , and Kay and RSscheisen  ), statistical machine translation models (e.g. Brown, Cooke, Pietra, Pietra et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical approach",
                "alignment of bilingual texts at the sentence level",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical machine translation models",
                "e.g. Brown, Cooke, Pietra, Pietra et al.",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4 Comparison to Related Work Previous work has compared generative and discriminative models having the same structure, such as the Naive Bayes and Logistic regression models   and other models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generative and discriminative models",
                "having the same structure",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Naive Bayes and Logistic regression models",
                "other models",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Section 3 describes two standard lexicalized models  , as well as an unlexicalized baseline model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicalized models",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "baseline model",
                "unlexicalized",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.2 ITG Constraints In this section, we describe the ITG constraints  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG constraints",
                "are described",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "ITG constraints",
                "are",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "If distributional similarity is conceived of as substitutability, as Weeds and Weir   and Lee   emphasize, then asymmetries arise when one word appears in a subset of the contexts in which the other appears; for example, the adjectives that typically modify apple are a subset of those that modify fruit,sofruit substitutes for apple better than apple substitutes for fruit.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distributional similarity",
                "as substitutability",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "adjectives that typically modify apple",
                "are a subset of those that modify fruit",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "All the TB-LMs and O-RLMs were unpruned 5gram models and used Stupid-backoff smoothing   2 with the backoff parameter set to 0.4 as suggested.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TB-LMs",
                "unpruned 5gram models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Stupid-backoff smoothing",
                "with the backoff parameter set to 0.4",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The training of IBM model 4 was implemented by the GIZA++ package  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ package",
                "implemented",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "IBM model 4",
                "was trained",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "n the LFG-based generation algorithm presented by Cahill and van Genabith   complex named entities   and other multi-word units can be fragmented in the surface realization",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LFG-based generation algorithm",
                "presented by Cahill and van Genabith",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "named entities and other multi-word units",
                "can be fragmented",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "illmann and Zhang   describe a perceptron style algorithm for training millions of features",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "perceptron style",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "features",
                "millions of",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Following  , we used the version 11a NIST BLEU script with its default settings to calculate the BLEU scores   based on case-insensitive ngram matching, where n is up to 4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NIST BLEU script",
                "default settings",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU scores",
                "based on case-insensitive ngram matching up to 4",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Statistical Word Alignment Model According to the IBM models  , the statistical word alignment model can be generally represented as in equation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM models",
                "generally represented as",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "statistical word alignment model",
                "can be",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction In this paper, we study the use of so-called word trigger pairs     to improve an existing language model, which is typically a trigram model in combination with a cache component  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "trigram model",
                "typically",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "cache component",
                "in combination with",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This approach will generally take advantage of language-specific  ) and domain-specific knowledge, of any external resources  , and of any information about the entities to process, e.g. their type  , or internal structure  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "language-specific knowledge",
                "domain-specific knowledge",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "information about entities",
                "internal structure",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Previously published approaches to reducing the rule set include: enforcing a minimum span of two words per non-terminal  , which would reduce our set to 115M rules; or a minimum count   threshold  , which would reduce our set to 78M   or 57M   rules.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rule set",
                "minimum span of two words per non-terminal",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "rule set",
                "minimum count threshold",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For both experiments, we used dependency trees extracted from the Penn Treebank   using the head rules and dependency extractor from Yamada and Matsumoto  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "extracted using the head rules and dependency extractor",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Yamada and Matsumoto",
                "dependency extractor",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  and   do worse on the English test data than they do on German, Dutch, or French.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "test data",
                "do worse",
                "PERFORMANCE",
                "negative",
                0.75
            ],
            [
                "English",
                "compared to German, Dutch, or French",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.2 Training Algorithm We adopt the perceptron training algorithm of Collins   to learn a discriminative model mapping from inputs xX to outputs yY , where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron training algorithm",
                "of Collins",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "discriminative model",
                "mapping from inputs xX to outputs yY",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Not only is this beneficial in terms of parsing complexity, but smaller rules can also improve a translation models ability to generalize to new data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation models",
                "ability to generalize to new data",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "rules",
                "improve",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Introduction Many applications that process natural language can be enhanced by incorporating information about the probabilities of word strings; that is, by using statistical language model information  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical language model information",
                "using",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word strings",
                "probabilities",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A pipage approach   has been proposed for MCKP, but we do not use this algorithm, since it requires costly partial enumeration and solutions to many linear relaxation problems.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MCKP",
                "pipage approach",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "partial enumeration and solutions to many linear relaxation problems",
                "requires",
                "PERFORMANCE",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "These problems include collocation discovery  , smoothing and estimation   and question answering  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocation discovery",
                "discovery",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "smoothing and estimation",
                "estimation",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Phrase-pairs are then extracted from the word alignments  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-pairs",
                "extracted from the word alignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word alignments",
                "are then extracted",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One of our goals was to use for our study only information that could be annotated reliably  , as we believe this will make our results easier to replicate.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "information",
                "can be annotated reliably",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "results",
                "easier to replicate",
                "PERFORMANCE",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "As to analysis of NPs, there have been a lot of work on statistical techniques for lexical dependency parsing of sentences  , and these techniques potentially can be used for analysis of NPs if appropriate resources for NPs are available.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical techniques",
                "lexical dependency parsing of sentences",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "resources",
                "available",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In the context of statistical machine translation  , we may interpretE as an English sentence, F its translation in French, and A a representation of how the words correspond to each other in the two sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "E",
                "English sentence",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "A",
                "representation of how the words correspond to each other",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Note that the predicate language representation utilized by Carmel-Tools is in the style of Davidsonian event based semantics  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "predicate language representation",
                "in the style of Davidsonian event based semantics",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Carmel-Tools",
                "utilized",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  proposed a method to identify discourse relations between text segments using Nave Bayes classifiers trained on a huge corpus",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Nave Bayes classifiers",
                "trained on a huge corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "discourse relations",
                "between text segments",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The initial state contains terminal items, whose labels are the POS tags given by the tagger of Ratnaparkhi  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger of Ratnaparkhi",
                "POS tags",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "terminal items",
                "labels",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The first two phases are approached as straightforward classification in a maximum entropy framework  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "framework",
                "maximum entropy",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phases",
                "straightforward classification",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Unlike  , one interesting idea proposed by   is to cluster similar pairs of paraphrases to apply multiplesequence alignment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "paraphrases",
                "similar pairs of",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "multiple sequence alignment",
                "",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "while most current stochastic parsing models use a \"\"markov grammar\"\"  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "markov grammar",
                "most current stochastic parsing models use",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "markov grammar",
                "used by most current stochastic parsing models",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This feature, which is based on the lexical parameters of the IBM Model 1  , provides a complementary probability for each tuple in the translation table.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature",
                "based on lexical parameters of IBM Model 1",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probability",
                "complementary",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This can be seen as a simplified version of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "version",
                "simplified",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "",
                "simplified version of",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The second one needs no labeled data for the new domain  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "domain",
                "needs no labeled data",
                "APPLICABILITY",
                "positive",
                0.8
            ],
            [
                "domain",
                "new",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Specifically, three features are used to instantiate the templates:  POS tags on both sides: We assign POS tags using the MXPOST tagger   for English and Chinese, and Connexor for Spanish.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tags",
                "using the MXPOST tagger for English and Chinese, and Connexor for Spanish",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "templates",
                "instantiate",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This situation is very similar to that involved in training HMM text taggers, where joint probabilities are computed that a particular word corresponds to a particular part-ofspeech, and the rest of the words in the sentence are also generated  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMM text taggers",
                "joint probabilities are computed",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "word corresponds to a particular part-of-speech",
                "rest of the words in the sentence are also generated",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "1 Introduction Word alignment is a critical component in training statistical machine translation systems and has received a significant amount of research, for example,  , including work leveraging syntactic parse trees, e.g.,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "has received a significant amount",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "statistical machine translation systems",
                "critical component in training",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Various machine learning strategies have been proposed to address this problem, including semi-supervised learning  , domain adaptation  , multi-task learning  , self-taught learning  , etc. A commonality among these methods is that they all require the training data and test data to be in the same feature space.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning strategies",
                "proposed",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "methods",
                "require training data and test data to be in the same feature space",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The boosting approach to ranking has been applied to named entity segmentation   and natural language generation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "boosting approach",
                "to ranking",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "named entity segmentation and natural language generation",
                "has been applied",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Candidate term Segment result of GPWS for one sentence, in which term appears Table 2: Examples of candidates eliminated by GPWS 5 Relative frequency ratio against background corpus Relative frequency ratio   is a useful method to be used to discover characteristic linguistic phenomena of a corpus when compared with another  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GPWS",
                "is a useful method",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "Relative frequency ratio",
                "against background corpus",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Modulo more minor differences, these notions are close to the ideas of interpretation as abduction   and generation as abduction  , where we take abduction, in the former case for instance, to be a process returning a temporal-causal structure which can explain the utterance in context.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "abduction",
                "a process returning a temporal-causal structure",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "utterance in context",
                "explain",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Unfortunately, longer sentences  , longer phrases  , two LMs  , higher-order LMs  , multiple higher-order lexicalized re-ordering models  , etc. all contributed to increased system?s complexity, and, as a result, time limitations prevented us from performing minimum-error-rate training     for ucb3, ucb4 and ucb5.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "complexity",
                "increased",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "time limitations",
                "prevented",
                "LIMITATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "More recently, the problem has been tackled using unsupervised  ) and supervised  , Ng and Cardie  ) approaches.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "unsupervised and supervised",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "approaches",
                "Ng and Cardie",
                "INNOVATION",
                "neutral",
                0.75
            ]
        ]
    },
    {
        "text": "Again the best result was obtained with IOB1   which is an imI)rovement of the best reported F,~=1 rate for this data set  : 92.03).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IOB1",
                "improvement",
                "INNOVATION",
                "positive",
                0.9
            ],
            [
                "F,~=1 rate",
                "best reported",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "On the other hand, both BLEU   and NIST   scores are higher for the baseline system  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU scores",
                "are higher",
                "PERFORMANCE",
                "positive",
                0.75
            ],
            [
                "NIST scores",
                "are higher",
                "PERFORMANCE",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "3.2 Translation performance For the experiments reported in this section, we used feature weights trained with minimum error rate training   . Because MERT ignores the denominator in Equation 1, it is invariant with respect to the scale of the weight vector   the Moses implementation simply normalises the weight vector it finds by its lscript1-norm.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MERT",
                "ignores the denominator in Equation 1",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Moses implementation",
                "normalises the weight vector",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Following previous work  , we assume that the tag of a word is independent of the tags of all preceding words given the tags of the previous two words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tags of a word",
                "independent of the tags of all preceding words",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tags of the previous two words",
                "given",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 Implementation 3.1 Feature Structure To implement the twin model, we adopt the log linear or maximum entropy   model   for its flexibility of combining diverse sources of information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "log linear or maximum entropy",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "flexibility",
                "combining diverse sources of information",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "4 Options from the Translation Table Phrase-based statistical machine translation methods acquire their translation knowledge in form of large phrase translation tables automatically from large amounts of translated texts  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Translation Table",
                "large phrase translation tables",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase-based statistical machine translation methods",
                "acquire their translation knowledge",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.1 Conditional Maximum Entropy Model The goal of CME is to find the most uniform conditional distribution of y given observation x,  xyp, subject to constraints specified by a set of features  yxf i,, where features typically take the value of either 0 or 1  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "take the value of either 0 or 1",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "CME",
                "find the most uniform conditional distribution",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In most statistical machine translation   models  , some of measure words can be generated without modification or additional processing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measure words",
                "can be generated without modification or additional processing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical machine translation models",
                "some of measure words",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We discriminatively trained our parser in an on-line fashion using a variant of the voted perceptron  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "discriminatively trained",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "voted perceptron",
                "variant",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "While work on subjectivity analysis in other languages is growing  , Chinese data are used in  , and German data are used in  ), much of the work in subjectivity analysis has been applied to English data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "subjectivity analysis",
                "is growing",
                "INNOVATION",
                "positive",
                0.75
            ],
            [
                "English data",
                "has been applied",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "It is appreciated that multi-sense words appearing in the same document tend to be tagged with the same word sense if they belong to the same common domain in the semantic hierarchy  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "multi-sense words",
                "same word sense",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "common domain in the semantic hierarchy",
                "belong to the same",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Many statistical translation models   try to model word-toword correspondences between source and target words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical translation models",
                "try to model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word-to-word correspondences",
                "between source and target words",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We present two approaches to SMT-based query expansion, both of which are implemented in the framework of phrase-based SMT  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based SMT",
                "framework of",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "SMT-based query expansion",
                "two approaches",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement   also aim to automatically learn latent structure underlying treebanked data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar refinement",
                "aim to automatically learn latent structure",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "prior over grammars",
                "within a Bayesian framework",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We performed feature selection by incrementally growing a log-linear model with order0 features f  using a forward feature selection procedure similar to  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature selection",
                "incrementally growing a log-linear model with order0 features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "forward feature selection procedure",
                "similar to  ",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Semantic DSN: The construction of this network is inspired by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "DSN",
                "construction",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "DSN",
                "inspired by",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "They were based on mutual information  , conditional probabilities  , or on some standard statistical tests, such as the chi-square test or the loglikelihood ratio  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical tests",
                "such as the chi-square test or the loglikelihood ratio",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "conditional probabilities",
                "or",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To deal with this question, we use ATIS p-o-s trees as found in the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "found in",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "ATIS p-o-s trees",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Specifically, stochastic translation lexicons estimated using the IBM method   from a fairly large sentence-aligned Chinese-English parallel corpus are used in their approach  a considerable demand for a resourcedeficient language.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "stochastic translation lexicons",
                "estimated using the IBM method",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approach",
                "considerable demand for a resourcedeficient language",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The value of fj is calculated by Mutual Information   between xi and fj.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "fj",
                "Mutual Information between xi and fj",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "xi",
                "Mutual Information",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Step 2 involves extracting minimal xRS rules   from the set of string/tree/alignments triplets.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "xRS rules",
                "minimal",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "string/tree/alignments triplets",
                "set of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Hindle   reports interesting results of this kind based on literal collocations, where he parses the corpus   into predicate-argument structures and applies a mutual information measure   to weigh the association between the predicate and each of its arguments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "predicate-argument structures",
                "applies a mutual information measure",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "corpus",
                "into predicate-argument structures",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The data for all our experiments was extracted from the Penn Treebank II Wall Street Journal   corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "experiments",
                "was extracted from",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The automatic alignments were extracted by appending the manually aligned sentences on to the respective Europarl v3 corpora and aligning them using GIZA++   and the growfinal-diag algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Europarl v3 corpora",
                "respective",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "GIZA++ and the growfinal-diag algorithm",
                "using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  94.17 Li and Roth   93.02 94.64 Table 2: Baseline results on three shallow parsing tasks: the NP-Chunking task  ; the CoNLL-2000 Chunking task  ; and the Li & Roth task  , which is the same as CoNLL-2000 but with more training data and a different test section.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Li and Roth",
                "task",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "CoNLL-2000 Chunking task",
                "same as CoNLL-2000 but with more training data and a different test section",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A similar view underlies the class-based methods cited in Section 2.4.3  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "class-based methods",
                "cited in Section 2.4.3",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "view",
                "underlies",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "To achieve robust training, Daume III and Marcu   employed the averaged perceptron   and ALMA  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron",
                "averaged",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ALMA",
                "",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "It can be proven that the probability distribution p satisfying the above assumption is the one with the highest entropy, is unique and has the following expone ntial form  :    = = k j cajf jcZcap 1 ),  |  is a normalization factor, fj  are the values of k features of the pair   and correspond to the linguistic cues of c that are relevant to predict the outcome a. Features are extracted from the training data and define the constraints that the probabilistic model p must satisfy.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probability distribution",
                "has the highest entropy",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "features",
                "define the constraints",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The search also uses a Tag Dictionary constructed from training data, described in  , that reduces the number of actions explored by the tagging model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Tag Dictionary",
                "reduces the number of actions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tagging model",
                "number of actions explored",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "W  = summationdisplay uS,vT w  Globally optimal minimum cuts can be found in polynomial time and near-linear running time in practice, using the maximum flow algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum flow algorithm",
                "polynomial time and near-linear running time in practice",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "minimum cuts",
                "globally optimal",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We also report on applying Factored Translation Models   for English-to-Arabic translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Factored Translation Models",
                "for English-to-Arabic translation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "English-to-Arabic translation",
                "is a specific task",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Monte Carlo sampling methods and Variational Bayes are two kinds of approximate inference methods that have been applied to Bayesian inference of unsupervised HMM POS taggers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Monte Carlo sampling methods",
                "are two kinds of approximate inference methods",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Variational Bayes",
                "are two kinds of approximate inference methods",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A Broad-Coverage Word Sense Tagger Dekang Lin Department of Computer Science University of Manitoba Winnipeg, Manitoba, Canada R3T 2N2 lindek@cs.umanitoba.ca Previous corpus-based Word Sense Disambiguation   algorithms   determine the meanings of polysemous words by exploiting their local contexts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Word Sense Tagger",
                "polysemous words by exploiting their local contexts",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "corpus-based Word Sense Disambiguation algorithms",
                "Previous",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The recurrence property had been utilized to extract keywords or key-phrases from text  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "recurrence property",
                "utilized to extract keywords or key-phrases",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "text",
                "had been utilized",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "cDonald and Nivre   showed that the MSTParser and MaltParser produce different errors",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MSTParser",
                "produce different errors",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "MaltParser",
                "produce different errors",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "There has of course been a large amount of work on the more general problem of word-sense disambiguation, e.g.,    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "large amount",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "problem",
                "more general",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In batch mode, OpinionFinder parses the data again, this time to obtain constituency parse trees  , which are then converted to dependency parse trees  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "OpinionFinder",
                "parses the data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parse trees",
                "converted to dependency parse trees",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this study we have concentrated on the NPs??term extraction, which comprises the focus of interest in several studies  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NPs??term extraction",
                "comprises the focus of interest",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "several studies",
                "focus of interest",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein  , and build62 ing a syntax-based word alignment model May and Knight   with TTS templates.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMM-based word alignment model",
                "encoding syntactic structure information",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "TTS templates",
                "build[ing] syntax-based",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "We ran GIZA++   on the training corpus in both directions using its default setting, and then applied the refinement rule diagand described in   to obtain a single many-to-many word alignment for each sentence pair.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "default setting",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "refinement rule",
                "diagand described in  ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We build phrase translations by first acquiring bidirectional GIZA++   alignments, and using Moses grow-diag alignment symmetrization heuristic.1 We set the maximum phrase length to a large value  , because some segmenters described later in this paper will result in shorter 1In our experiments, this heuristic consistently performed better than the default, grow-diag-final.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "alignments",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Moses grow-diag alignment symmetrization heuristic",
                "performed better",
                "METHODOLOGY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Since that time, however, increasingly large amounts of language model training data have become available ranging from approximately one billion words   to trillions of words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training data",
                "approximately one billion words to trillions of words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "language model",
                "become available",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "To date, researchers have harvested, with varying success, several resources, including concept lists  , topic signatures  , facts  , and word similarity lists  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "resources",
                "including concept lists, topic signatures, facts, and word similarity lists",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "success",
                "varying",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "intuition comes from an observation by Yarowsky   regarding multiple tokens of words in documents",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Yarowsky",
                "observation",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "multiple tokens of words in documents",
                "regarding",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In WASP, GIZA++   is used to obtain the best alignments from the training examples.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "obtain the best alignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "training examples",
                "used to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.3 Probabilistic models for generation with HPSG Some existing studies on probabilistic models for HPSG parsing   adopted log-linear models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-linear models",
                "adopted",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "probabilistic models for HPSG parsing",
                "Some existing studies on",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Several other measures like Log-Likelihood  , Pearsons a2a4a3  , Z-Score  , Cubic Association Ratio  , etc. , have been also proposed.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measures",
                "like Log-Likelihood, Pearsons a2a4a3, Z-Score, Cubic Association Ratio, etc.",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "measures",
                "have been proposed",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Computational linguists have demonstrated that a words meaning is captured to some extent by the distribution of words and phrases with which it commonly co-occurs  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words and phrases",
                "commonly co-occurs",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "words meaning",
                "captured to some extent",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The labeled corpus is the Penn Wall Street Journal treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Wall Street Journal treebank",
                "labeled corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Wall Street Journal treebank",
                "the Penn Wall Street Journal treebank",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The features we used are as follows:  Direct and inverse IBM model;  3, 4-gram target language model;  3, 4, 5-gram POS language model  ; 96  Sentence length posterior probability  ;  N-gram posterior probabilities within the NBest list  ;  Minimum Bayes Risk probability;  Length ratio between source and target sentence; The weights are optimized via MERT algorithm.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "Direct and inverse IBM model; 3, 4-gram target language model; 3, 4, 5-gram POS language model",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "weights",
                "optimized via MERT algorithm",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  and compare with results reported by HK06   and CRR07  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "results",
                "reported",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "results",
                "reported",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "controlled NP-traces  , we follow the standard technique of marking nodes dominating the empty element up to but not including the parent of the antecedent as defective   with a gap feature  .1 Furthermore, to make antecedent co-indexation possible with many types of EEs, we generalize Collins approach by enriching the annotation of non-terminals with the type of the EE in question (eg.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "nodes dominating the empty element",
                "up to but not including the parent of the antecedent",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "annotation of non-terminals",
                "enriching with the type of the EE in question",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In Section 2, we examine aggregate Markov models, or class-based bigram models   in which the mapping from words to classes 81 is probabilistic.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Markov models",
                "probabilistic",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "class-based bigram models",
                "in which the mapping from words to classes is probabilistic",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "First, we can construct an infinite number of more specialized PCFGs by splitting or refining the PCFGs nonterminals into increasingly finer states; this leads to the iPCFG or infinite PCFG  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PCFGs",
                "into increasingly finer states",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "iPCFG",
                "infinite PCFG",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "There are two tasks  for the domain adaptation problem.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "domain adaptation problem",
                "two tasks",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "We evaluate the system generated summaries using the automatic evaluation toolkit ROUGE  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE",
                "automatic evaluation toolkit",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "summaries",
                "system generated",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Increasingly, parallel corpora are becoming available for many language pairs and SMT systems have been built for French-English, German-English, Arabic-English, Chinese-English, Hindi-English and other language pairs  ,  ,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "language pairs",
                "many",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "SMT systems",
                "built",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Word alignment is an important component of a complete statistical machine translation pipeline  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "component",
                "important",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "statistical machine translation pipeline",
                "complete",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4.3 Adaptation for unknown word2 The unknown word problem is an important issue for domain adaptation .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unknown word",
                "important issue",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "domain adaptation",
                "problem",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The phrasebased machine translation   uses the grow-diag-final heuristic to extend the word alignment to phrase alignment by using the intersection result.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrasebased machine translation",
                "uses the grow-diag-final heuristic",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "intersection result",
                "extends word alignment to phrase alignment",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We compare against several competing systems, the first of which is based on the original IBM Model 4 for machine translation   and the HMM machine translation alignment model   as implemented in the GIZA++ package  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 4",
                "original",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "GIZA++ package",
                "as implemented",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "LW was originally used to validate the quality of a phrase translation pair in MT  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LW",
                "validate the quality of a phrase translation pair",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "phrase translation pair",
                "in MT",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Dependency Treebank  , and in Figure 2, for an English sentence taken from the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Dependency Treebank",
                "English sentence taken from the Penn Treebank",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank",
                "taken from",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Either pruning   or lossy randomizing approaches   may result in a compact representation for the application run-time.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pruning",
                "may result in a compact representation",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "lossy randomizing approaches",
                "may result in a compact representation",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Assuming that the parameters P  are known, the most likely alignment is computed by a simple dynamic-programming algorithm.1 Instead of using an Expectation-Maximization algorithm to estimate these parameters, as commonly done when performing word alignment  , we directly compute these parameters by relying on the information contained within the chunks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "are known",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "simple dynamic-programming",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54   and 0.55 0.63  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa values",
                "reported rather low",
                "PERFORMANCE",
                "negative",
                0.75
            ],
            [
                "earlier works",
                "reported inter-judge agreement level",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "To measure interannotator agreement, we compute Cohens Kappa   from the two sets of annotations, obtaining a Kappa value of only 0.43.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa value",
                "only 0.43",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "Cohens Kappa",
                "compute",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": " , is not very useful for applications like statistical machine translation,  , for which an accurate word-to-word alignment between the source and the target languages is critical for high quality translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical machine translation",
                "not very useful",
                "APPLICABILITY",
                "negative",
                0.8
            ],
            [
                "word-to-word alignment",
                "critical for high quality",
                "METHODOLOGY",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "2 Latent Variable Parsing In latent variable parsing  , we learn rule probabilities on latent annotations that, when marginalized out, maximize the likelihood of the unannotated training trees.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rule probabilities",
                "maximize the likelihood",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "latent annotations",
                "marginalized out",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The sentences were processed with the Collins parser   to generate automatic parse trees.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "Collins parser",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parse trees",
                "automatic",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This approach gave an improvement of 2.7 in BLEU   score on the IWSLT05 Japanese to English evaluation corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "2.7",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "IWSLT05 Japanese to English evaluation corpus",
                "evaluation corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Alternatively we could have simply incorporated the DIVERSITY measure into the objective function or used an inference algorithm that specifically accounts for redundancy, e.g., maximal marginal relevance  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "DIVERSITY measure",
                "into the objective function",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "maximal marginal relevance",
                "specifically accounts for redundancy",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This implies that the complexity of structure divergence between two languages is higher than suggested in literature  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "4 Semi-Supervised Training for Word Alignments Intuitively, in approximate EM training for Model 4  , the E-step corresponds to calculating the probability of all alignments according to the current model estimate, while the M-step is the creation of a new model estimate given a probability distribution over alignments  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 4",
                "approximate EM training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "probability distribution over alignments",
                "creation of a new model estimate",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A number of researches which utilized distributional similarity have been conducted, including   and many others.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "researches",
                "which utilized distributional similarity",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "distributional similarity",
                "distributional similarity",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he lexicalized model proposed by Collins     was re-implemented by one of the authors",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicalized model",
                "proposed by Collins",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "model",
                "re-implemented",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The decoder is capable of producing nbest derivations and nbest lists  , which are used for Maximum Bleu training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "producing nbest derivations and nbest lists",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "nbest lists",
                "used for Maximum Bleu training",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To set the weights, m, we carried out minimum error rate training   using BLEU   as the objective function.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "m",
                "minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU",
                "as the objective function",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We utilize a maximum entropy   model   to design the basic classifier used in active learning for WSD.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy model",
                "to design the basic classifier",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "classifier",
                "used in active learning",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Since adjectives have been a focus of previous work in sentiment detection  13, we looked at the performance of using adjectives alone.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "adjectives",
                "alone",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "performance",
                "using adjectives alone",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We compare the following model types: conventional   word n-gram models; conventional IBM class n-gram models interpolated with conventional word n-gram models  ; and model M. All conventional n-gram models are smoothed with modified Kneser-Ney smoothing  , except we also evaluate word n-gram models with Katz smoothing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "conventional word n-gram models",
                "smoothed with modified Kneser-Ney smoothing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "model M",
                "conventional IBM class n-gram models interpolated with conventional word n-gram models",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Using a variant of the voted perceptron  , we discriminatively trained our parser in an on-line fashion.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "discriminatively trained",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "voted perceptron",
                "variant",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2005) applied the distributional similarity proposed by Lin   to coordination disambiguation",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distributional similarity",
                "proposed by Lin",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "coordination disambiguation",
                "applied to",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "These alignment models stem from the source-channel approach to statistical machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "source-channel approach",
                "to statistical machine translation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignment models",
                "stem from",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  use the Learning as Search Optimization framework to take into account the non-locality behavior of the coreference features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "coreference features",
                "non-locality behavior",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Learning as Search Optimization framework",
                "take into account",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3.3 Voted Perceptron Unlike other methods discussed so far, voted perceptron training   attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the best-scoring labeling of that instance according to the current model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "voted perceptron training",
                "minimize the difference",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "current model",
                "best-scoring labeling",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Several approaches have been proposed in the context of word sense disambiguation  , named entity   classification  , patternacquisitionforIE , or dimensionality reduction for text categorization    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "proposed",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "dimensionality reduction",
                "for text categorization",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "It is clear that Appendix B contains far fewer true non-compositional phrases than Appendix A. 7 Related Work There have been numerous previous research on extracting collocations from corpus, e.g.,   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Appendix B",
                "contains fewer true non-compositional phrases",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "previous research",
                "extracting collocations from corpus",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We ran the decoder with its default settings and then used Moses implementation of minimum error rate training   to tune the feature weights on the development set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "default settings",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "feature weights",
                "tune on the development set",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "An analysis of the alignments shows that smoothing the fertility probabilities significantly reduces the frequently occurring problem of rare words forming garbage collectors in that they tend to align with too many words in the other language  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignments",
                "smoothed fertility probabilities",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "rare words",
                "tend to align with too many words",
                "PERFORMANCE",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "These methods are based on IBM statistical translation Model 2  , but take advantage of certain characteristics of the segments of text that can typically be extracted from translation memories.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM statistical translation Model 2",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "segments of text",
                "can typically be extracted",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleubased minimum error rate training   to optimize the weights of their log linear models feature functions  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based statistical machine translation systems",
                "all phrase-based statistical machine translation system",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "log linear models feature functions",
                "to optimize the weights",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "If the input consists of sevWe also adopt the approximation that treats every sentence with its reference as a separate corpus   so that ngram counts are not accumulated, and parallel processing of sentences becomes possible.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "input",
                "consists of",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "approximation",
                "treats every sentence with its reference as a separate corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We use a program to label syntactic arguments with the roles they are playing  , and the rules for complement/adjunct distinction given by   to never allow deletion of the complement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "program",
                "label syntactic arguments",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "rules",
                "never allow deletion of the complement",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We then compute the weight of a context word w in context c, W , using mutual information and t-test, which were reported by Weeds and Weir   to perform the best on a pseudo-disambiguation task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "context word w",
                "perform the best",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "t-test",
                "reported by Weeds and Weir",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Following the broad shift in the field from finite state transducers to grammar transducers  , recent approaches to phrase-based alignment have used synchronous grammar formalisms permitting polynomial time inference  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar transducers",
                "polynomial time inference",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase-based alignment",
                "synchronous grammar formalisms",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "tile data put tbrward by ll,amshaw and Marcus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data",
                "put forward by ll,amshaw and Marcus",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Of particular interest are lexicalized parsing models such as the ones developed by Collins   and Carroll and Rooth  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicalized parsing models",
                "developed by Collins and Carroll and Rooth",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ones",
                "developed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "They give a probabilistic formation of paraphrasing which naturally falls out of the fact that they use techniques from phrase-based statistical machine translation: e2 = argmax e2:e2negationslash=e1 p    where p  = summationdisplay f p p     summationdisplay f p p    Phrase translation probabilities p  and p  are commonly calculated using maximum likelihood estimation  : p  = count summationtext f count    where the counts are collected by enumerating all bilingual phrase pairs that are consistent with the 197 conseguido .opportunitiesequalcreatetofailedhasprojecteuropeanthe oportunidadesdeigualdadlahanoeuropeoproyectoel Figure 1: The interaction of the phrase extraction heuristic with unaligned English words means that the Spanish phrase la igualdad aligns with equal, create equal, and to create equal.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based statistical machine translation",
                "techniques",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "maximum likelihood estimation",
                "commonly calculated using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Formal grammar used in statistical machine translation  , such as Bracketing Transduction Grammar   proposed by   and the synchronous CFG presented by  , provides a natural platform for integrating linguistic knowledge into SMT because hierarchical structures produced by the formal grammar resemble linguistic structures.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Formal grammar",
                "provides a natural platform",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "linguistic structures",
                "resemble linguistic structures",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "One kind is the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "one kind",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank",
                "Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Sum of logarithms of source-to-target lexical weighting  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexical weighting",
                "Sum of logarithms of source-to-target",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "period should therefore be interpreted as an abbreviation marker and not as a sentence boundary marker if the two tokens surrounding it can indeed be considered as a collocation according to Dunnings   original log-likelihood ratio amended with the one-sidedness constraint introduced in Section 2.2",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocation",
                "can indeed be considered as a collocation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tokens surrounding it",
                "according to Dunnings original log-likelihood ratio amended with the one-sidedness constraint",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Most of the reported work on paraphrase generation from arbitrary input sentences uses machine learning techniques trained on sentences that are known or can be inferred to be paraphrases of each other  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning techniques",
                "trained on sentences that are known or can be inferred to be paraphrases of each other",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "work on paraphrase generation",
                "uses machine learning techniques",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In cut-and-paste summarization  , sentence combination operations were implemented manually following the study of a set of professionally written abstracts; however the particular pasting operation presented here was not implemented.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentence combination operations",
                "implemented manually",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "pasting operation",
                "not implemented",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Some researchers   targeted nouns, noun phrases and verb phrases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "researchers",
                "targeted nouns, noun phrases and verb phrases",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The RST-DT consists of 385 documents from the Wall Street Journal, about 176,000 words, which overlaps with the Penn Wall St. Journal   Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "RST-DT",
                "385 documents from the Wall Street Journal, about 176,000 words",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Wall St. Journal Treebank",
                "overlaps with",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "These alignments can be obtained from single-word models   using the available public software GIZA++  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "available public software",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "single-word models",
                "using available public software",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This curve plots the average labeled attachment score over Basque, Chinese, English, and Turkish as a function of parsing time per token.4 Accuracy of only 1% below the maximum can be achieved with average processing time of 17 ms per token, or 60 tokens per second.5 We also refer the reader to   for more detailed analysis of the ISBN dependency parser results, where, among other things, it was shown that the ISBN model is especially accurate at modeling long dependencies.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing time per token",
                "average processing time of 17 ms per token",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ISBN model",
                "especially accurate at modeling long dependencies",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Perhaps the most related is 86 learning as search optimization    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "learning",
                "as search optimization",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "86",
                "most related",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "aoife.cahill@ims.uni-stuttgart.de and van Genabith  , which do not rely on handcrafted grammars and thus can easily be ported to new languages.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "handcrafted grammars",
                "do not rely on",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "languages",
                "can easily be ported to",
                "APPLICABILITY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "Other researchers  ,   use clustering techniques coupled with syntactic dependency features to identify IS-A relations in large text collections.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques",
                "clustering techniques",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "dependency features",
                "syntactic dependency features",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "While Liu and Gildea   calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency structures produced by a LexicalFunctional Grammar   parser.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-gram matches",
                "calculate",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "f-score",
                "calculate",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The annotation guidelines for the Penn Treebank flattened noun phrases to simplify annotation  , so there is no complex structure to NPs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotation guidelines",
                "flattened noun phrases",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "NPs",
                "no complex structure",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Bean and Riloff   used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic compatibility model",
                "called contextual-role knowledge",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "certain cases of easily-resolved anaphors and antecedents",
                "identifying",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "urney   used a corpus-based algorithm",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "urney",
                "corpus-based algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "corpus-based",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Previous approaches for training CRFs have either   opted for a training method that no longer maximizes the likelihood,  , Roth and Yih  ) 1, or   opted for a 1 Both McCallum and Wellner   and Roth and Yih   used the voted perceptron algorithm   to train intractable CRFs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training method",
                "no longer maximizes the likelihood",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "voted perceptron",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For instance, automatic summary can be seen as a particular paraphrasing task   with the aim of selecting the shortest paraphrase.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "task",
                "selecting the shortest paraphrase",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "paraphrasing",
                "particular",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Intuitively, if we are able to find good correspondences among features, then the augmented labeled source domain data should transfer better to a target domain    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "good",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "augmented labeled source domain data",
                "transfer better",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "2 Related Work The most commonly used similarity measures are based on the WordNet lexical database   and a number of such measures have been made publicly available  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "similarity measures",
                "based on WordNet lexical database",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "number of measures",
                "have been made publicly available",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The differences between a k-best and a beam-search parser   make a running time difference unsur17 Our score of 85.8 average labeled precision and recall for sentences less than or equal to 100 on Section 23 compares to: 86.7 in Charniak  , 86.9 in Ratnaparkhi  , 88.2 in Collins  , 89.6 in Charniak  , and 89.75 in Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "make a running time difference",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Charniak",
                "86.7",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "However, by exploiting the fact that the underlying scores assigned to competing hypotheses, w , vary linearly w.r.t. changes in the weight vector, w, Och   proposed a strategy for finding the global minimum along any given search direction.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "scores",
                "vary linearly",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "strategy",
                "finding the global minimum",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Firstly, rather than induce millions of xRS rules from parallel data, we extract phrase pairs in the standard way   and associate with each phrase-pair a set of target language syntactic structures based on supertag sequences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase pairs",
                "standard way",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "target language syntactic structures",
                "based on supertag sequences",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he models were originally introduced in Collins  ; the current article 1 gives considerably more detail about the models and discusses them in greater depth",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "gives considerably more detail",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "models",
                "discusses them in greater depth",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3 Network Evaluation We present an evaluation which has been carried out on an initial set of annotations of English articles from The Wall Street Journal  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotations",
                "of English articles from The Wall Street Journal",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "set of annotations",
                "initial",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Given two sentences X and Y, the WLCS score of X and Y can be computed using the similar dynamic programming procedure as stated in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WLCS score",
                "computed using the similar dynamic programming procedure",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "procedure",
                "stated in  ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "By comparing derivation trees for parallel sentences in two languages, instances of structural divergences   can be automatically detected.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "derivation trees",
                "can be automatically detected",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "structural divergences",
                "can be detected",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "He then goes on to adapt the conventional noisy channel MT model of   to NLU, where extracting a semantic representation from an input text corresponds to finding: argmax  {p  p }, where p  is a model for generating semantic representations, and p  is a model for the relation between semantic representations and corresponding texts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "conventional noisy channel MT model",
                "to NLU",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "model for generating semantic representations",
                "a model for generating semantic representations",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "7Our decoder lacks certain features shown to be beneficial to synchronous grammar decoding, in particular rule binarisation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "lacks certain features",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "rule binarisation",
                "shown to be beneficial",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "In this paper, two synchronous grammar formalisms are discussed, inversion transduction grammars     and two-variable binary bottom-up non-erasing range concatenation grammars  -BRCGs)  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar formalisms",
                "two synchronous",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "two-variable binary bottom-up non-erasing range concatenation grammars",
                "BRCGs",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We perform minimum-error-rate training   to tune the feature weights of the translation model to maximize the BLEU score on development set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "feature weights",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU score",
                "maximize",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.2.1 Factored Treelet Translation Labels of nodes at the t-layer are not atomic but consist of more than 20 attributes representing various linguistic features.3 We can consider the attributes as individual factors  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "labels of nodes",
                "not atomic but consist of more than 20 attributes",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "attributes",
                "various linguistic features",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Examples of such affinities include synonyms  , verb similarities   and word associations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "synonyms",
                "are",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "word associations",
                "are",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "2 Motivation Automatic subjectivity analysis methods have been used in a wide variety of text processing applications, such as tracking sentiment timelines in online forums and news  , review classification  , mining opinions from product reviews  , automatic expressive text-to-speech synthesis  , text semantic analysis  , and question answering  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Automatic subjectivity analysis methods",
                "have been used in a wide variety of text processing applications",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "text processing applications",
                "such as tracking sentiment timelines in online forums and news",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "First, we adopt an ONTOLOGICALLY PROMISCUOUS representation   that includes a wide variety of types of entities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "representation",
                "ontologically promiscuous",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "types of entities",
                "wide variety",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Opinion forecasting differs from that of opinion analysis, such as extracting opinions, evaluating sentiment, and extracting predictions  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "opinion forecasting",
                "extracting opinions, evaluating sentiment, and extracting predictions",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "opinion forecasting",
                "differs",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, the fact that the DGSSN uses a large-vocabulary tagger   as a preprocessing stage may compensate for its smaller vocabulary.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "DGSSN",
                "uses a large-vocabulary tagger as a preprocessing stage",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "vocabulary",
                "smaller",
                "METHODOLOGY",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "achine translation   but also in other applications such as word sense disanabiguation   and bilingnal lexicography  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine translation",
                "other applications",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "bilingnal lexicography",
                "other applications",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Hw6: Implement beam search and reduplicate the POS tagger described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "beam search",
                "described in POS tagger",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "POS tagger",
                "described",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "MT output was evaluated using the standard evaluation metric BLEU  .2 The parameters of the MT System were optimized for BLEU metric on NIST MTEval2002 test sets using minimum error rate training  , and the systems were tested on NIST MTEval2003 test sets for both languages.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT output",
                "evaluated using the standard evaluation metric BLEU",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "BLEU metric",
                "optimized for",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We used the WordNet::Similarity package   to compute baseline scores for several existing measures, noting that one word pair was not processed in WS-353 because one of the words was missing from WordNet.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet::Similarity package",
                "compute baseline scores",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "WordNet",
                "missing from",
                "LIMITATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "Atthefinestlevel, thisinvolvesthealignment of words and phrases within two sentences that are known to be translations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words and phrases",
                "within two sentences that are known to be translations",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignment",
                "within two sentences",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This weak supervision has been encoded using priors and initializations  , specialized models  , and implicit negative evidence  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "priors and initializations",
                "specialized",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "negative evidence",
                "implicit",
                "METHODOLOGY",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "To obtain their corresponding weights, we adapted the minimum-error-rate training algorithm   to train the outside-layer model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "minimum-error-rate training algorithm",
                "adapted to train",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "outside-layer model",
                "trained",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "EM-HMM tagger provided with good initial conditions   91.4*   Figure 1: Previous results on unsupervised POS tagging using a dictionary   on the full 45-tag set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "EM-HMM tagger",
                "good initial conditions",
                "METHODOLOGY",
                "positive",
                0.85
            ],
            [
                "Previous results",
                "on the full 45-tag set",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "By contrast, alternative approaches, like Collins  , apply an additional transformation to each tree in the tree-bank, splitting each rule into small parts, which finally results in a new grammar covering many more sentences than the explicit one.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tree-bank",
                "applying an additional transformation",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "grammar",
                "covering many more sentences",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Following   and the other literature in TM, this paper only focuses the details of TM.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TM",
                "focuses on details",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "TM",
                "only",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "c2007 Association for Computational Linguistics Structural Correspondence Learning for Dependency Parsing Nobuyuki Shimizu Information Technology Center University of Tokyo Tokyo, Japan shimizu@r.dl.itc.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center University of Tokyo Tokyo, Japan nakagawa@dl.itc.u-tokyo.ac.jp Abstract Following  , we present an application of structural correspondence learning to non-projective dependency parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "structural correspondence learning",
                "to non-projective dependency parsing",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "dependency parsing",
                "non-projective",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The probability distributions of these binary classifiers are learnt using maximum entropy model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probability distributions",
                "learnt using maximum entropy model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "maximum entropy model",
                "learnt using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "arowsky   studied a method for word sense disambiguation using unlabeled data",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "using unlabeled data",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word sense disambiguation",
                "studied",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The first LR model for each language uses maximum entropy classification   to determine possible parser actions and their probabilities4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LR model",
                "maximum entropy classification",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parser actions and their probabilities",
                "determine",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "If one reduces the problem of entity mention detection to the detection of its head, the nature of the problem changes and the annotation of data becomes at; The    This allows us to consider the problem as a tagging/chunking problem and describe each word as beginning   an entity mention, inside   an entity mention or outside   an entity mention  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "problem",
                "reduces to the detection of its head",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "annotation of data",
                "becomes at",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5 Experiments For all experiments, we trained and tested on the Penn treebank    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn treebank",
                "trained and tested on",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "experiments",
                "5 Experiments",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "See Collins   for an application of the boosting approach to named entity recognition, and Walker, Rambow, and Rogati   for the application of boosting techniques for ranking in the context of natural language generation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "boosting approach",
                "to named entity recognition",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "boosting techniques",
                "for ranking in the context of natural language generation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Similar to WSD, Carpuat and Wu   used contextual information to solve the ambiguity problem for phrases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Carpuat and Wu",
                "used contextual information",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrases",
                "solve the ambiguity problem",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This means that the 1)roblem of recognizing named entities in those cases can be solved by incorporating techniques of base noun phrase chunking  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "problem",
                "by incorporating techniques of base noun phrase chunking",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "techniques",
                "of base noun phrase chunking",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Among the four steps, the hypothesis alignment presents the biggest challenge to the method due to the varying word orders between outputs from different MT systems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT systems",
                "varying word orders",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "hypothesis alignment",
                "biggest challenge",
                "LIMITATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "The ITG we apply in our experiments has more structural labels than the primitive bracketing grammar: it has a start symbol S, a single preterminal C, and two intermediate nonterminals A and B used to ensure that only one parse can generate any given word-level alignment, as discussed by Wu   and Zens and Ney  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG",
                "more structural labels",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "primitive bracketing grammar",
                "used in comparison",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "All corpora are formatted in the IOB sequence representation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "IOB sequence representation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpora",
                "formatted",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.4 Comparison with Hybrid Model SSL based on a hybrid generative/discriminative approach proposed in   has been defined as a log-linear model that discriminatively combines several discriminative models, pDi , and generative models, pGj , such that: R  = producttext i p Di  i producttext j p Gj  j summationtext y producttext i p Di  i producttext j p Gj  j , where ={i}Ii=1, and ={{i}Ii=1,{j}I+Jj=I+1}.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Comparison With Previous Work Most of the recent corpus-based POS taggers in the literature are either statistically based, and use Markov Model  or Statistical Decision Tree   techniques, or are primarily rule based, such as Drill's Transformation Based Learner  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "previous work",
                "statistically based",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "previous work",
                "primarily rule based",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Bootstrapping a PMTG from a lower-dimensional PMTG and a word-to-word translation model is similar in spirit to the way that regular grammars can help to estimate CFGs  , and the way that simple translation models can help to bootstrap more sophisticated ones  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PMTG",
                "similar in spirit to",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "translation model",
                "help to bootstrap more sophisticated ones",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "605 ROUGE-S   Skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Skip-bigram",
                "arbitrary gaps",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Skip-bigram",
                "pair of words in their sentence order",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5 Related Work We already discussed the relation of our work to   in Section 2.4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "relation to",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "our work",
                "already discussed",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As Marcu and Echihabi   point out, WordNet does not encode antonymy across part-of-speech  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet",
                "does not encode antonymy across part-of-speech",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "antonymy across part-of-speech",
                "not encoded",
                "INNOVATION",
                "negative",
                0.9
            ]
        ]
    },
    {
        "text": "Looking rst at learning times, it is obvious that learning time depends primarily on the number of training instances, which is why we can observe a difference of several orders of magnitude in learning time between the biggest training set   and the smallest training set   14 This is shown by Nivre and Scholz   in comparison to the iterative, arc-standard algorithm of Yamada and Matsumoto   and by McDonald and Nivre   in comparison to the spanning tree algorithm of McDonald, Lerman, and Pereira  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training instances",
                "primarily on",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "arc-standard",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4 Experiments We evaluated the ISBN parser on all the languages considered in the shared task  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ISBN parser",
                "evaluated on all languages",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "shared task",
                "considered in",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For instance, instead of representing the polarity of a term using a binary value, Mullen and Collier   use Turneys   method to assign a real value to represent term polarity and introduce a variety of numerical features that are aggregate measures of the polarity values of terms selected from the document under consideration.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Mullen and Collier's method",
                "assign a real value to represent term polarity",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "numerical features",
                "aggregate measures of polarity values of terms",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These constraints tie words in such a way that the space of alignments cannot be enumerated as in IBM models 1 and 2  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignments",
                "cannot be enumerated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM models 1 and 2",
                "cannot be enumerated",
                "INNOVATION",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "Analyze resulting findings to determine a progression of competence In   we discuss the initial steps we took in this process, including the development of a list of error codes documented by a coding manual, the verification of our manual and coding scheme by testing inter-coder reliability in a subset of the corpus   of a0 a1a3a2a5a4a7a6 )2, and the subsequent tagging of the entire corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "coding manual",
                "documented by",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpus",
                "tagged",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "If e has length l and f has length m, there are possible 2lm alignments between e and f  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignments",
                "possible 2lm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "length",
                "has length",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The most relevant to our work are Kazama and Torisawa  , Toral and Muoz  , and Cucerzan  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kazama and Torisawa",
                "most relevant",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "Cucerzan",
                "most relevant",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Apart from this, the module is a straightforward implementation of  , which in turn adapts   for syntactic chunking.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "module",
                "straightforward implementation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "",
                "adapts for syntactic chunking",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2 Baseline Coreference Resolution System Our baseline coreference system implements the standard machine learning approach to coreference resolution  , Ponzetto and Strube  , Yang and Su  , for instance), which consists of probabilistic classification and clustering, as described below.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning approach",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "probabilistic classification and clustering",
                "described below",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "mith and Eisner   apply entropy regularization to dependency parsing",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency parsing",
                "apply entropy regularization to",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "entropy regularization",
                "to dependency parsing",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2 Detecting Discourse-New Definite Descriptions 2.1 Vieira and Poesio Poesio and Vieira   carried out corpus studies indicating that in corpora like the Wall Street Journal portion of the Penn Treebank  , around 52% of DDs are discourse-new  , and another 15% or so are bridging references, for a total of about 66-67% firstmention.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "like the Wall Street Journal portion of the Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "DDs",
                "are discourse-new",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Word alignment and phrase extraction We used the GIZA++ word alignment software 3 to produce initial word alignments for our miniature bilingual corpus consisting of the source French file and the English reference file, and the refined word alignment strategy of   to obtain improved word and phrase alignments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "word alignment software",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "refined word alignment strategy",
                "to obtain improved word and phrase alignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "All of the features of the ATR/Lancaster Treebank that are described below represent a radical departure from extant large-scale   treebanks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ATR/Lancaster Treebank",
                "radical departure from extant large-scale treebanks",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "features",
                "described below",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The chunking classification was made by   based on the parsing information in the WSJ corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunking classification",
                "based on parsing information in WSJ corpus",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Our method is based on the ones described in  , The objective of this paper is to dynamically rank speakers or participants in a discussion.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "based on the ones described",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "speakers or participants",
                "to dynamically rank",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Beam-search parsing using an unnormalized discriminative model, as in Collins and Roark  , requires a slightly different search strategy than the original generative model described in Roark  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "search strategy",
                "slightly different",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "model",
                "unnormalized discriminative",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The loglinear model feature weights were learned using minimum error rate training     with BLEU score   as the objective function.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "loglinear model feature weights",
                "were learned using minimum error rate training with BLEU score as the objective function",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "BLEU score",
                "as the objective function",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our approach to inducing syntactic clusters is closely related to that described in Brown, et al,   which is one of the earliest papers on the subject.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brown, et al",
                "earliest papers on the subject",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "Brown, et al",
                "described",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "s in most other statistical parsing systems we therefore use the pruning technique described in Goodman   and Collins   which assigns a score to each item in the chart equal to the product of the inside probability of the item and its prior probability",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pruning technique",
                "described in Goodman and Collins",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "score",
                "equal to the product of the inside probability and its prior probability",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ther background on this method of hypothesis testing the reader is referred to  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reader",
                "referred to",
                "APPLICABILITY",
                "neutral",
                1.0
            ],
            [
                "method of hypothesis testing",
                "background",
                "INNOVATION",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Recent lexicalized stochastic parsers such as Collins  , Charniak  , and others add additional features to each constituent, the most important being the head word of the parse constituent.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicalized stochastic parsers",
                "add additional features",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "head word of the parse constituent",
                "most important",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Although the above statement was made about translation problems faced by human translators, recent research   suggests that it also applies to problems in machine translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "problems in machine translation",
                "also applies",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "recent research",
                "suggests",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The weighting parameters of these features were optimized in terms of BLEU by the approach of minimum error rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "optimized in terms of BLEU",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "approach",
                "minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "FollowingtheworkofKooetal. andSmith and Smith  , it is possible to compute all expectations in O  through matrix inversion.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "matrix inversion",
                "compute all expectations in O",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Koo et al.",
                "work",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Here, ppicker shows the accuracy when phrases are extracted by using the N-best phrase alignment method described in Section 4.1, while growdiag-final shows the accuracy when phrases are extracted using the standard phrase extraction algorithm described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "N-best phrase alignment method",
                "described in Section 4.1",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "standard phrase extraction algorithm",
                "described in  ",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "SGD was recently used for NLP tasks including machine translation   and syntactic parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SGD",
                "was used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "NLP tasks",
                "including machine translation and syntactic parsing",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The methodology used   is based on the definition of a function Pr  that returns the probability that tI1 is a 835 source Transferir documentos explorados a otro directorio interaction-0 Move documents scanned to other directory interaction-1 Move s canned documents to other directory interaction-2 Move scanned documents to a nother directory interaction-3 Move scanned documents to another f older acceptance Move scanned documents to another folder Figure 1: Example of CAT system interactions to translate the Spanish source sentence into English.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methodology",
                "based on the definition of a function",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "CAT system interactions",
                "translate the Spanish source sentence into English",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In contrast, globally optimized clustering decisions were reported in   and  , where all clustering possibilities are considered by searching on a Bell tree representation or by using the Learning as Search Optimization   framework   respectively, but the first search is partial and driven by heuristics and the second one only looks back in text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "clustering decisions",
                "globally optimized",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "search",
                "partial and driven by heuristics",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We report case-insensitive scores for version 0.6 of METEOR   with all modules enabled, version 1.04 of IBM-style BLEU  , and version 5 of TER  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "METEOR",
                "version 0.6",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "modules",
                "all enabled",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Meanwhile, it is common for NP chunking tasks to represent a chunk   with two labels, the begin   and inside   of a chunk  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunk",
                "with two labels",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "labels",
                "begin and inside",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The progression in the probabilistic parsing literature has been to start with lexical head-head dependencies   and then add non-lexical sis2 This result generalizes to Ss, which are also flat in Negra  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "progression",
                "start with lexical head-head dependencies and then add non-lexical",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "result",
                "generalizes",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "We would expect better performance with the more accurate approximation based on variational inference proposed and evaluated in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approximation",
                "more accurate",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "proposed and evaluated",
                "in ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "6 Experiments 6.1 Data preparation Our experiments were conducted with data made available through the Penn Treebank annotation effort  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank annotation effort",
                "made available",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "data",
                "made available",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Optimal algorithms exist for minimising the size of rules in a Synchronous Context-Free Grammar    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Synchronous Context-Free Grammar",
                "minimising the size of rules",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "algorithms",
                "exist",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Recently, there have been several discriminative approaches at training large parameter sets including   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "large parameter sets",
                "including",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "discriminative approaches",
                "at training",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Clearly the present research task is quite considerably harder than the parsing and tagging tasks undertaken in  , which would seem to be the closest work to ours, and any comparison between this work and ours must be approached with extreme caution.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research task",
                "harder than parsing and tagging tasks",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "comparison",
                "must be approached with extreme caution",
                "METHODOLOGY",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "Similar observations have been made in the context of tagging problems using maximum-entropy models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum-entropy models",
                "using maximum-entropy models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tagging problems",
                "similar observations have been made",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The English experiments were performed on the Penn Treebank  , using a standard set of head-selection rules   to convert the phrase structure syntax of the Treebank to a dependency tree representation.6 We split the Treebank into a training set  , a development set  , and several test sets  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "standard set",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "head-selection rules",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3ThePOS taggers The two POS taggers used in the experiments are TNT, a publicly available Markov model tagger  , and a reimplementation of the maximum entropy   tagger MXPOST  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TNT",
                "publicly available",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "MXPOST",
                "reimplementation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4 Phrase-Based Translation In phrase-based translation, the translation process is modeled by splitting the source sentence into phrases   and translating the phrases as a unit  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation process",
                "is modeled by splitting the source sentence into phrases and translating the phrases as a unit",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrases",
                "as a unit",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Our test set is 3718 sentences from the English Penn treebank   which were translated into German.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "test set",
                "from the English Penn treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentences",
                "translated into German",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rules",
                "strictly required",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translational equivalences",
                "contiguous",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For the final ranking, we chose the log likelihood statistic outlined in Dunning  , which is based upon the co-occurrence counts of all nouns  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log likelihood statistic",
                "based upon the co-occurrence counts of all nouns",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Dunning",
                "outlined in",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Second, movie reviews are apparently harder to classify than reviews of other products  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "movie reviews",
                "are harder to classify",
                "METHODOLOGY",
                "negative",
                0.6
            ],
            [
                "reviews of other products",
                "are easier to classify",
                "METHODOLOGY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein  , including the bilingual dictionary.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "DeNero and Klein",
                "same unlabeled data set",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "bilingual dictionary",
                "including",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our experiments created translation modules for two evaluation corpora: written news stories from the Penn Treebank corpus   and spoken task-oriented dialogues from the TRAINS93 corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank corpus",
                "written news stories",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "TRAINSthe 93 corpus",
                "spoken task-oriented dialogues",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Dunning   argues for the use of G 2 rather than X 2, based on the claim that the sampling distribution of G 2 approaches the true chi-square distribution quicker than the sampling distribution of X 2 . However, Agresti   makes the opposite claim: The sampling distributions of X 2 and G 2 get closer to chi-squared as the sample size n increasesThe convergence is quicker for X 2 than G 2 . In addition, Pedersen   questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read  , who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "G 2",
                "approaches the true chi-square distribution quicker",
                "PERFORMANCE",
                "neutral",
                0.85
            ],
            [
                "X 2",
                "convergence is quicker",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval  , cross-language text classification  , lexical choice in machine translation  , induction of translation lexicons  , cross-language annotation and resource projections to a second language  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measures of cross-language relatedness",
                "useful",
                "APPLICABILITY",
                "positive",
                0.8
            ],
            [
                "cross-language annotation and resource projections to a second language",
                "resource projections to a second language",
                "LIMITATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It uses a log-linear model to define a distribution over the lexical category set for each word and the previous two categories   and the forward backward algorithm efficiently sums over all histories to give a distibution for each word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-linear model",
                "define a distribution",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "forward backward algorithm",
                "efficiently sums over all histories",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "L1 or Lasso regularization of linear models, introduced by Tibshirani  , embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Lasso regularization of linear models",
                "embeds feature selection",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Lasso regularization of linear models",
                "generated a large amount of interest",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "For evaluation, we used the BLEU metrics, which calculates the geometric mean of n-gram precision for the MT outputs found in reference translations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metrics",
                "calculates the geometric mean of n-gram precision",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "MT outputs",
                "found in reference translations",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Goodman, 2004) and lscript22 regularization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lscript22",
                "regularization",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "lscript22",
                "regularization",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In Turney  , features are selected according to part-of-speech labels.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "selected according to part-of-speech labels",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "part-of-speech labels",
                "according to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Conditional Maximum Entropy models have been used for a variety of natural language tasks, including Language Modeling  , partof-speech tagging, prepositional phrase attachment, and parsing  , word selection for machine translation  , and finding sentence boundaries  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Conditional Maximum Entropy models",
                "used for a variety of natural language tasks",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Language Modeling, partof-speech tagging, prepositional phrase attachment, parsing, word selection for machine translation, and finding sentence boundaries",
                "including",
                "METHODOLOGY",
                "neutral",
                0.98
            ]
        ]
    },
    {
        "text": "The POS disambiguation has usually been performed by statistical approaches mainly using hidden markov model   (Cutting et al. , 1992; Kupiec.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hidden markov model",
                "using hidden markov model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "statistical approaches",
                "mainly using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parse action sequences",
                "derived from the Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Wall Street Journal sentences",
                "40,000",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Consequently, the mainstream research in the literature has been focused on the modeling and utilization of local and sentential contexts, either linguistically in a rule-based framework or statistically in a searching and optimization set-up  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "in the literature",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "framework",
                "rule-based or statistically in a searching and optimization set-up",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Related work Turney   recently advocated the need for a uniform approach to corpus-based semantic tasks",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Turney",
                "advocated",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "uniform approach",
                "to corpus-based semantic tasks",
                "APPLICABILITY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "1 Introduction Sentiment detection and classification has received considerable attention recently  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentiment detection and classification",
                "has received considerable attention",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "recently",
                "no explicit sentiment",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Some studies have been done for acquiring collocation translations using parallel corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "parallel",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "studies",
                "have been done",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "1 Introduction Decoding is one of the three fundamental problems in classical SMT   as proposed by IBM in the early 1990s  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT",
                "proposed by IBM in the early 1990s",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "fundamental problems",
                "one of the three",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The HWC metrics compare dependency and constituency trees for both reference and machine translations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HWC metrics",
                "compare dependency and constituency trees",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "reference and machine translations",
                "both",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Our learning algorithm stems from Perceptron training in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Perceptron training",
                "in ",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "learning algorithm",
                "stems from",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The idea caught on very quickly: Suhm and Waibel  , Mast et aL  , Warnke et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Suhm and Waibel",
                "caught on very quickly",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "idea",
                "caught on",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This linear model is learned using a variant of the incremental perceptron algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linear model",
                "learned using a variant of the incremental perceptron algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "incremental perceptron algorithm",
                "variant",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The principle of maximum entropy states that when one searches among probability distributions that model the observed data  , the preferred one is the one that maximizes the entropy    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probability distributions",
                "maximizes the entropy",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "entropy",
                "model the observed data",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  built 5-gram LMs over web using distributed cluster of machines and queried them via network requests.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LMs",
                "built 5-gram LMs",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "machines",
                "distributed cluster",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A growing body of recent research has focused on the problems of identifying and generating paraphrases, e.g., Barzilay & McKeown  , Lin & Pantel  , Shinyama et al,  , Barzilay & Lee  , and Pang et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "growing body of recent",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "paraphrases",
                "identifying and generating",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The proposed synchronous grammar is able to cover the previous proposed grammar based on tree   and tree sequence   alignment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar",
                "cover",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "previous proposed grammar",
                "based on tree and tree sequence alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 Semantic Representation 3.1 The Need for Dependencies Perhaps the most common representation of text for assessing content is Bag-Of-Words or Bag-of-NGrams  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bag-Of-Words",
                "most common representation",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "Bag-of-NGrams",
                "most common representation",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Multiple translations of the same text  , corresponding articles from multiple news sources  , and bilingual corpus   have been utilized.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translations",
                "multiple",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpus",
                "bilingual",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In the first set of experiments, we compare two settings of our UALIGN system with other aligners, GIZA++     and LEAF    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "UALIGN system",
                "with other aligners",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "GIZA++ and LEAF",
                "compared",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "so they conform to the Penn Treebank corpus   annotation style, and then do experiments using models built with Treebank data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank corpus",
                "annotation style",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "models built with Treebank data",
                "do experiments using",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "NeATS computes the likelihood ratio   to identify key concepts in unigrams, bigrams, and trigrams and clusters these concepts in order to identify major subtopics within the main topic.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NeATS",
                "computes the likelihood ratio",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "concepts",
                "identify key",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For this study, we used the same 6 test meetings as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "test meetings",
                "same 6 test meetings",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "meetings",
                "as in ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  invented heuristic symmetriza57 FRENCH/ENGLISH ARABIC/ENGLISH SYSTEM F-MEASURE   BLEU F-MEASURE   BLEU GIZA++ 73.5 30.63 75.8 51.55   74.1 31.40 79.1 52.89 LEAF UNSUPERVISED 74.5 72.3 LEAF SEMI-SUPERVISED 76.3 31.86 84.5 54.34 Table 3: Experimental Results tion of the output of a 1-to-N model and a M-to-1 model resulting in a M-to-N alignment, this was extended in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "F-MEASURE",
                "73.5",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "BLEU",
                "30.63",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "irect feedback loops that copy a predicted output label to the input representation of the next example have been used in symbolic machine-learning architectures such as the the maximum-entropy tagger described by Ratnaparkhi   and the memory-based tagger   proposed by Daelemans et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum-entropy tagger",
                "described by Ratnaparkhi",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "memory-based tagger",
                "proposed by Daelemans et al",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The unit of utterance corresponds to the unit of segment in the original BLEU and NIST studies  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unit of utterance",
                "corresponds to the unit of segment",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU and NIST studies",
                "original",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The idea is that the translation of a sentence x into a sentence y can be performed in the following steps1:   If x is small enough, IBMs model 1   is employed for the translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "IBM's model 1",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence",
                "small enough",
                "PERFORMANCE",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "Given a collection of facts, ME chooses a model consistent with all the facts, but otherwise as uniform as possible  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ME",
                "as uniform as possible",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "model",
                "consistent with all the facts",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Using techniques described in Church and Hindle  , Church and Hanks  , and Hindle and Rooth  , Figure 4 shows some examples of the most frequent V-O pairs from the AP corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques",
                "described in Church and Hindle, Church and Hanks, and Hindle and Rooth",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "V-O pairs",
                "most frequent",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We symmetrized bidirectional alignments using the growdiag-final heuristic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "growdiag-final heuristic",
                "symmetrized using",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "bidirectional alignments",
                "symmetrized",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The notion of incrementally merging classes of lexical items is intuitively satisfying and is explored in detail in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classes of lexical items",
                "intuitively satisfying",
                "INNOVATION",
                "positive",
                0.75
            ],
            [
                "incrementally merging",
                "explored in detail",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Computational linguistics research generally attaches great value to high kappa measures  , which indicate high human agreement on a particular task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa measures",
                "indicate high human agreement",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "value",
                "great value",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "2 Discriminative Reordering Model Basic reordering models in phrase-based systems use linear distance as the cost for phrase movements  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Basic reordering models",
                "use linear distance",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "reordering models",
                "in phrase-based systems",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Then, we used the refinement technique grow-diag-final-and   to all 50  50 bidirectional alignment pairs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "refinement technique",
                "grow-diag-final-and",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "bidirectional alignment pairs",
                "all 50 50",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One is a phrase-based translation in which a phrasal unit is employed for translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrasal unit",
                "employed for translation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase-based translation",
                "one",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Related Work 2.1 Translation with Non-parallel Corpora A straightforward approach to word or phrase translation is to perform the task by using parallel bilingual corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parallel bilingual corpora",
                "straightforward approach",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translation",
                "task",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The percentage agreement for each of the features is shown in the following table: feature percent agreement form 100% intentionality 74.9% awareness 93.5% safety 90.7% As advocated by Carletta  , we have used the Kappa coefficient   as a measure of coder agreement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa coefficient",
                "as a measure of coder agreement",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "percent agreement",
                "74.9%",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Model parameters are estimated using maximum entropy  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model parameters",
                "estimated using maximum entropy",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Semantic features are used for classifying entities into semantic types such as name of person, organization, or place, while syntactic features characterize the kinds of dependency 5It is worth noting that the present approach can be recast into one based on constraint relaxation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic features",
                "for classifying entities",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntactic features",
                "characterize the kinds of dependency",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 Related Work Dolan   describes a method for clustering word senses with the use of information provided in the electronic version of LDOCE  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Dolan",
                "method for clustering word senses",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "LDOCE",
                "provided in the electronic version",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "For each span in the chart, we get a weight factor that is multiplied with the parameter-based expectations.9 4 Experiments We applied GIZA++   to word-align parts of the Europarl corpus   for English and all other 10 languages.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "applied",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Europarl corpus",
                "parts of the",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Inspired by the conjunction and appositive structures, Riloff and Shepherd  , Roark and Charniak   used cooccurrence statistics in local context to discover sibling relations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Riloff and Shepherd",
                "used cooccurrence statistics in local context",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "cooccurrence statistics",
                "in local context",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "However, they do not elaborate on how the comparisons are done, or on how effective the program is. Dolan   describes a heuristic approach to forming unlabeled clusters of closely related senses in an MRD.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "comparisons",
                "do not elaborate on",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "approach",
                "heuristic",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In order to improve translation quality, this tuning can be effectively performed by minimizing translation error over a development corpus for which manually translated references are available  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation quality",
                "minimizing translation error",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "development corpus",
                "manually translated references are available",
                "APPLICABILITY",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "Since so many concepts used in discourse are graindependent, a theory of granularity is also fundamental  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "concepts",
                "grain-dependent",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "theory of granularity",
                "fundamental",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "As an alternative to linear interpolation, we also employ a weighted product for phrase-table combination: p   productdisplay j pj j   This has the same form used for log-linear training of SMT decoders  , which allows us to treateachdistributionasafeature,andlearnthemixing weights automatically.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "product",
                "same form used for log-linear training of SMT decoders",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "weighted product",
                "allows us to treat each distribution as a feature, and learn the mixing weights automatically",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Additionally, we present results of the tagger on the NEGRA corpus   and the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NEGRA corpus",
                "results of the tagger",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank",
                "results of the tagger",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Recently several latent variable models for constituent parsing have been proposed  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "latent variable models",
                "have been proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "constituent parsing",
                "have been proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For these first SMT systems, translation-model probabilities at the sentence level were approximated from word-based translation models that were trained by using bilingual corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation-model probabilities",
                "approximated from word-based translation models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word-based translation models",
                "trained by using bilingual corpora",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "amshaw and Marcus   introduced a transformationbased learning method which considered chunking as a kind of tagging problem",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transformation-based learning method",
                "considered chunking as a kind of tagging problem",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "chunking",
                "as a kind of tagging problem",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "At last, the dependency parser presented in   is used to generate the full parse.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency parser",
                "presented in",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parse",
                "generate the full",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Models of that form include hidden Markov models   as well as discriminative tagging models based on maximum entropy classification  , conditional random fields  , and large-margin techniques  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hidden Markov models",
                "include",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "maximum entropy classification",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "conditional random fields",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "large-margin techniques",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We scored systems and our own output using case-insensitive IBM-style BLEU 1.04  , METEOR 0.6   with all modules, and TER 5  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "1.04",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "TER",
                "5",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We then built separate directed word alignments for EnglishX andXEnglish   using IBM model 4  , combined them using the intersect+grow heuristic  , and extracted phrase-level translation pairs of maximum length seven using the alignment template approach  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM model 4",
                "using IBM model 4",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignment template approach",
                "using the alignment template approach",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "So far, this approach has been taken by a lot of researchers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "researchers",
                "taken by a lot of",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "approach",
                "has been",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Therefore, estimating a natural language model based on the maximum entropy   method   has been highlighted recently.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "maximum entropy",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "natural language model",
                "has been highlighted recently",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "madja   proposed a statistical model by measuring the spread of the distribution of cooccurring pairs of words with higher strength",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "statistical",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "strength",
                "higher",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Our trees look just like syntactic constituency trees, such as those in the Penn TreeBank  , 141 ROOT PROT PROT NN PEBP2 PROT NN alpha NN A1 , , PROT NN alpha NN B1 , , CC and PROT NN alpha NN B2 NNS proteins VBD bound DT the DNA PROT NN PEBP2 NN site IN within DT the DNA NN mouse PROT NN GM-CSF NN promoter . . Figure 1: An example of our tree representation over nested named entities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "trees",
                "look just like syntactic constituency trees",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tree representation",
                "over nested named entities",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The evaluation metric is casesensitive BLEU-4  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU-4",
                "casesensitive",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "BLEU-4",
                "metric",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Accurate measurement of semantic similarity between lexical units such as words or phrases is important for numerous tasks in natural language processing such as word sense disambiguation  , synonym extraction  , and automatic thesauri generation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic similarity",
                "important",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "word sense disambiguation",
                "synonym extraction",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We also implemented an averaged perceptron system     for comparison.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron system",
                "for comparison",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "perceptron system",
                "implemented",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Movie and product reviews have been the main focus of many of the recent studies in this area  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "studies",
                "main focus",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "recent studies",
                "in this area",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Similar to bidirectional labelling in  , there are two learning tasking in this model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "learning tasking",
                "in this model",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "bidirectional labelling",
                "similar to",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We systematically explored the feature space for relation extraction   . Kernel methods allow a large set of features to be used without being explicitly extracted.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature space",
                "being used without being explicitly extracted",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "kernel methods",
                "allow a large set of features",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "These relations are then used for various tasks, ranging from the interpretation of a noun sequence   or a prepositional phrase  , to resolving structural ambiguity  , to merging dictionary senses for WSD  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "relation",
                "used for various tasks",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "dictionary senses",
                "merging for WSD",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information  , or to combine it with discriminative latent models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hiero",
                "incorporate additional syntactic information",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Hiero",
                "combine with discriminative latent models",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Ramshaw and Marcus   views chunking as a tagging problem.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chunking",
                "a tagging problem",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "tagging problem",
                "",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this paper we adopt a maximum entropy model   to estimate the local probabilities a28 a14 a1 a25 a19a1 a25a30a29 a2 a9a22a21 since it can incorporate diverse types of features with reasonable computational cost.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy model",
                "can incorporate diverse types of features with reasonable computational cost",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "features",
                "diverse types of features",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Generation of paraphrase examples was also investigated  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "urney   starts from a small   set of terms with known orientation  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "terms",
                "with known orientation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "set",
                "small",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The model employs a stochastic version of an inversion transduction grammar or ITG  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "stochastic version of an inversion transduction grammar",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ITG",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4 Experiments and Results 4.1 Set up We parsed the 3 GB AQUAINT corpus   using Minipar  , and collected verb-object and verb-subject frequencies, building an empirical MI model from this data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "AQUAINT corpus",
                "parsed using Minipar",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "MI model",
                "built from data",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Unlike Smadja  , the keword rnay be part of a Chinese word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "keword",
                "may be part of a Chinese word",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Smadja",
                "unlike",
                "INNOVATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "Some papers   based on Smadja's paradigm   learned an aided dictionary from a corpus to reduce the possibility of unknown words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Smadja's paradigm",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "aided dictionary",
                "from a corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We augment each labeled target instance xj with the label assigned by the source domain classifier  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "target instance",
                "with the label assigned by the source domain classifier",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "source domain classifier",
                "assigned by",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example,   have studied synchronous context free grammar.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar",
                "synchronous context-free",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "grammar",
                "studied",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The commonly used phrase extraction approach based on word alignment heuristics   as described in   is a special case of the algorithm, where candidate phrase pairs are restricted to those that respect word alignment boundaries.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase extraction approach",
                "based on word alignment heuristics",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "restricted to those that respect word alignment boundaries",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Even a length limit of 3, as proposed by  , would result in almost optimal translation quality.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "length limit",
                "proposed by",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation quality",
                "almost optimal",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The translations are evaluated in terms of BLEU score  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "evaluated in terms of",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "translations",
                "are",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Then, we build a classier learned by training data, using a maximum entropy model   and the features related to spelling variations in Table 3.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classier",
                "learned by training data",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "features related to spelling variations",
                "in Table 3",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The usual recall and precision metrics   require either a test corpus previously annotated with the required information, or manual evaluation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "require either a test corpus previously annotated or manual evaluation",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "evaluation",
                "manual",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We use these tuples to calculate a balanced f-score against the gold alignment tuples.4 Method Dict size f-score Gold 28 100.0 Monotone 39 68.9 IBM-1   30 80.3 IBM-4   29 86.9 IP 28 95.9 The last line shows an average f-score over the 8 tied IP solutions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "f-score",
                "100.0",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "IBM-1",
                "80.3",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As is common  , the treebank is first transformed in various ways, in order to give an accurate PCFG.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treebank",
                "transformed in various ways",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "PCFG",
                "accurate",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "hen they adapted Brown et al.'s   statistical translation Model 2 to work with this model of cooccurrence",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brown et al.'s statistical translation Model 2",
                "adapted to work with this model of cooccurrence",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "this model of cooccurrence",
                "adapted",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Thus, the WSJ+NANC model has better oracle rates than the WSJ model   for both the WSJ and BROWN domains.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSJ+NANC model",
                "better oracle rates",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "WSJ model",
                "better oracle rates",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "It is important because a wordaligned corpus is typically used as a first step in order to identify phrases or templates in phrase-based Machine Translation  ,  , (Koehn et al. , 2003, sec.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "wordaligned",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrases or templates",
                "identify in phrase-based Machine Translation",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The first is identifying words and phrases that are associated with subjectivity, for example, that think is associated with private states and that beautiful is associated with positive sentiments  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "subjectivity",
                "associated with private states",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "beautiful",
                "associated with positive sentiments",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "This method uses mutual information and loglikelihood, which Dunning   used to calculate the dependency value between words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information and loglikelihood",
                "used to calculate the dependency value between words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dependency value between words",
                "calculated using mutual information and loglikelihood",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The named-entity features are generated by the freely available Stanford NER tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stanford NER tagger",
                "freely available",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "named-entity features",
                "generated by",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Inversion Transduction Grammar   is the model of Wu  , Tree-to-String is the model of Yamada and Knight  , and Tree-to-String, Clone allows the node cloning operation described above.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Inversion Transduction Grammar",
                "model of",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Tree-to-String",
                "model of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The usefulness of prosody was found to be very limited by itself, if the effect of utterance length is not considered  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prosody",
                "usefulness",
                "PERFORMANCE",
                "neutral",
                0.75
            ],
            [
                "utterance length",
                "effect",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "ethod Correlation Edge-counting 0.664 Jiang & Conrath   0.848 Lin   0.822 Resnik   0.745 Li et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Correlation Edge-counting",
                "0.664",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Jiang & Conrath",
                "0.848",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "determining document orientation  , as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter  ; 3.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Subjective text",
                "expresses a Positive or a Negative opinion",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "document orientation",
                "deciding",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicon",
                "importance",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "lexicalized models",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We build sentencespecific zero-cutoff stupid-backoff   5-gram language models, estimated using 4.7B words of English newswire text, and apply them to rescore each 10000-best list.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "5-gram language models",
                "estimated using 4.7B words of English newswire text",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "10000-best list",
                "apply them to rescore",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Joint parsing with a simplest synchronous context-free grammar   is O  as opposed to the monolingual O  time.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar",
                "simplest synchronous context-free",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parsing",
                "O as opposed to the monolingual O time",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Other similar work includes the mention detection   task   and joint probabilistic model of coreference  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mention detection task",
                "similar work",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "joint probabilistic model of coreference",
                "similar work",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4 Related Work   and   tackle the problem of segmenting Chinese while aligning it to English.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "segmenting Chinese",
                "aligning it to English",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "problem of segmenting Chinese",
                "tackle",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In the future, we plan to explore our discriminative framework on a full distortion model   or even a hierarchical model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "discriminative framework",
                "on a full distortion model or even a hierarchical model",
                "METHODOLOGY",
                "neutral",
                0.5
            ],
            [
                "full distortion model or even a hierarchical model",
                "to explore",
                "INNOVATION",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "For example, the words corruption and abuse are similar because both of them can be subjects of verbs like arouse, become, betray, cause, continue, cost, exist, force, go on, grow, have, increase, lead to, and persist, etc, and both of them can modify nouns like accusation, act, allegation, appearance, and case, etc. Many methods have been proposed to compute distributional similarity between words, e.g.,  ,  ,   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "can be subjects of verbs like",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "compute distributional similarity between words",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "255 Meteor  , Precision and Recall  , and other such automatic metrics may also be affected to a greater or lesser degree because they are all quite rough measures of translation similarity, and have inexact models of allowable variation in translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "rough measures",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "models",
                "inexact models",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2.4 Reordering Reordering features take many forms in MT. In phrase-based systems, reordering is accomplished both within phrase pairs   as well as through distance-based distortion models   and lexicalized reordering models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reordering features",
                "many forms",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "reordering models",
                "lexicalized reordering models",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Pooling the sets to form two large CE and AE test sets, the AE system improvements are significant at a 95% level  ; the CE systems are only equivalent.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "AE system",
                "significant",
                "PERFORMANCE",
                "positive",
                0.9
            ],
            [
                "CE systems",
                "equivalent",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Most clustering schemes   use the average entropy reduction to decide when two words fall into the same cluster.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "clustering schemes",
                "use the average entropy reduction",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "words",
                "fall into the same cluster",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The hypothesis scores and tuning are identical to the setup used in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hypothesis scores",
                "are identical",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tuning",
                "are identical",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The modify features involve the dependency parse tree for the sentence, obtained by first parsing the sentence   and then converting the tree into its dependency representation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency parse tree",
                "obtained by first parsing the sentence and then converting the tree",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dependency representation",
                "into its dependency representation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based string transducer",
                "directly from aligned sentence pairs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word alignment step",
                "separate",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The tree-based reranker includes the features described in   as well as features based on non-projective edge attributes explored in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "described in as well as based on non-projective edge attributes",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "reranker",
                "includes",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96% are readily available to assign POS to unrestricted English sentences  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS taggers",
                "can achieve accuracy of 96%",
                "PERFORMANCE",
                "positive",
                0.9
            ],
            [
                "unrestricted English sentences",
                "POS to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The most obvious comparison takes on the form of a keyword analysis, which looks for the words that are significantly more frequent in the one corpus as compared to the other  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "significantly more frequent",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "keyword analysis",
                "obvious comparison",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This approach is usually referred to as the noisy sourcechannel approach in SMT  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "usually referred to as the noisy source-channel approach",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "noisy source-channel approach",
                "noisy",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.2 Rare Word Accuracy For these experiments, we use the Wall Street Journal portion of the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "portion",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Wall Street Journal",
                "portion",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Learning algorithm The translation model is a standard linear model  , which we train using MIRA  , following Watanabe et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "standard linear model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "trained using MIRA",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Unconstrained CL corresponds exactly to a conditional maximum entropy model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CL",
                "conditional maximum entropy model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "CL",
                "corresponds exactly",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This estimate could be used as a starting point for a more detailed alignment algorithm such as word_align  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "estimate",
                "used as a starting point",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word_align",
                "more detailed alignment algorithm",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "A path in a translation hypergraph induces a translation hypothesis E along with its sequence of SCFG rules D = r1,r2,,rK which, if applied to the start symbol, derives E. The sequence of SCFG rules induced by a path is also called a derivation tree for E. 3 Minimum Error Rate Training Given a set of source sentences FS1 with corresponding reference translations RS1, the objective of MERT is to find a parameter set M1 which minimizes an automated evaluation criterion under a linear model: M1 = argmin M1  SX s=1 Err`Rs, E  ff E  = argmax E  SX s=1 mhm  ff . In the context of statistical machine translation, the optimization procedure was first described in Och   for N-best lists and later extended to phrase-lattices in Macherey et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SCFG rules",
                "sequence of SCFG rules induced by a path",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parameter set",
                "minimizes an automated evaluation criterion",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The analyser--and therefore the generator-includes exception lists derived from WordNet  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "exception lists",
                "derived from WordNet",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "generator",
                "includes exception lists",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Probability Based Commensurability Charniak and Goldman   started out with a model very similar to Hobbs et al. , but became concerned with 227 the lack of theoretical grounding for Ihe number, in rules, much as we we.re.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "very similar to Hobbs et al.",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "rules",
                "lack of theoretical grounding",
                "LIMITATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "N-gram language models have also been used in Statistical Machine Translation   as proposed by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "N-gram language models",
                "have also been used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Statistical Machine Translation",
                "as proposed",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In one set of experiments, we generated lexicons for PEOPLE and ORGANIZATIONS using 2500 Wall Street Journal articles from the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "from the Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "2500 Wall Street Journal articles",
                "using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The mutual information clustering algorithm  were used for this.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "mutual information clustering",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "were used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The resulting training procedure is analogous to the one presented in   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training procedure",
                "analogous to",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "proceedure",
                "presented in",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Partitioning 2: Medium and low frequency words As noted in  , log-likelihood statistics are able to capture word bi-gram regularities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-likelihood statistics",
                "are able to capture word bi-gram regularities",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "medium and low frequency words",
                "Partitioning 2",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The third column reports the BLEU score   along with 95% confidence interval.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "along with 95% confidence interval",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "confidence interval",
                "95%",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The other recipe that is currently used on a large scale is to measure the performance of a parser on existing treebanks, such as WSJ  , and assume that the accuracy measure will carry over to the domains of interest.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "performance",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "WSJ",
                "existing treebanks",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Some of these have been previously employed for various tasks by Gabrilovich and Markovitch,  ; Overell and Ruger  , Cucerzan  , and Suchanek et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Gabrilovich and Markovitch",
                "previously employed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Suchanek et al.",
                "previously employed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, this may still be too expensive as part of an MT model that directly optimizes some performance measure, e.g., minimum error rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "performance measure",
                "minimum error rate",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "MT model",
                "directly optimizes",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "e have begun experimenting with log likelihood ratio   as a thresholding technique",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "thresholding technique",
                "as a thresholding technique",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "log likelihood ratio",
                "used as a thresholding technique",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The list is obtained by first extracting the phrases with -TMP function tags from the PennTree bank, and taking the words in these phrases  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PennTree bank",
                "phrases with -TMP function tags",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "words",
                "in these phrases",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Two block sets are derived for each of the training sets using a phrase-pair selection algorithm similar to  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-pair selection algorithm",
                "similar to ",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "training sets",
                "two block sets are derived",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Building heavily on the ideas of History-based parsing  , training the parser means essentially running the parsing algorithms in a learning mode on the data in order to gather training instances for the memory-based learner.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "running the parsing algorithms in a learning mode",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "parser",
                "gathering training instances",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "135 Considering the discourse relation annotations in the PDTB  , there can be alignment between discourse relations   and our opinion frames when the frames represent dominant relations between two clauses.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "discourse relation annotations",
                "in the PDTB",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "opinion frames",
                "represent dominant relations between two clauses",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "First, we can let the number of nonterminals grow unboundedly, as in the Infinite PCFG, where the nonterminals of the grammar can be indefinitely refined versions of a base PCFG  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "nonterminals",
                "can be indefinitely refined versions of a base PCFG",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Infinite PCFG",
                "unboundedly",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use IBMs BLEU score   to measure the performance of SMS text normalization.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "to measure the performance",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "SMS text normalization",
                "performance of",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The difference in accuracy between a SVM model applied to RRR dataset   and the same experiment applied to TB2 dataset   88.2 RRR Average human, whole sentence   93.2 RRR Maximum Likelihood-based   79.7 AP Maximum entropy, words   77.7 RRR Maximum entropy, words & classes   81.6 RRR Decision trees   77.7 RRR Transformation-Based Learning   81.8 WordNet Maximum-Likelihood based   84.5 RRR Maximum-Likelihood based   86.1 TB2 Decision trees & WSD   88.1 RRR WordNet Memory-based Learning   84.4 RRR LexSpace Maximum entropy, unsupervised   81.9 Maximum entropy, supervised   83.7 RRR Neural Nets   86.0 RRR WordNet Boosting   84.4 RRR Semi-probabilistic   84.31 RRR Maximum entropy, ensemble   85.5 RRR LSA SVM   84.8 RRR Nearest-neighbor   86.5 RRR DWS FN dataset, w/o semantic features   91.79 FN PR-WWW FN dataset, w/ semantic features   92.85 FN PR-WWW TB2 dataset, best feature set   93.62 TB2 PR-WWW Table 5: Accuracy of PP-attachment ambiguity resolution   basic experiment) is 2.9%.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SVM model",
                "applied to RRR dataset",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Decision trees & WSD",
                "applied to TB2 dataset",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "nd Semantic Knowledge Sources for Coreference Resolution Ponzetto & Strube   and Strube & Ponzetto   aimed at showing that the encyclopedia that anyone can edit can be indeed used as a semantic resource for research in NLP",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "encyclopedia",
                "can be indeed used as a semantic resource",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "Ponzetto & Strube",
                "showing that",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "The up-arrows and down-arrows are shorthand for  ) =   where ni is the c-structure node annotated with the equation.2 Treebest := argmaxTreeP    P  := productdisplay X  Y in Tree Feats = {ai|vj )ai = vj} P    The generation model of   maximises the probability of a tree given an f-structure (Eqn.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Treebest",
                "argmaxTreeP",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "generation model",
                "maximises the probability of a tree",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Previous publications on Meteor   have described the details underlying the metric and have extensively compared its performance with Bleu and several other MT evaluation metrics.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Meteor",
                "details underlying the metric",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "performance",
                "compared with Bleu and several other MT evaluation metrics",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This shows that hypothesis features are either not discriminative enough, or that the reranking model is too weak This performance gap can be mainly attributed to two problems: optimization error and modeling error  .1 Much work has focused on developing better algorithms to tackle the optimization problem  ), since MT evaluation metrics such as BLEU and PER are riddled with local minima and are difficult to differentiate with respect to re-ranker parameters.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hypothesis features",
                "not discriminative enough",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "reranking model",
                "too weak",
                "METHODOLOGY",
                "neutral",
                0.75
            ]
        ]
    },
    {
        "text": "Many statistical metrics have been proposed, including pointwise mutual information    , mean and variance, hypothesis testing  , log-likelihood ratio    , statistic language model  , and so on.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical metrics",
                "pointwise mutual information",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "statistical metrics",
                "log-likelihood ratio",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4 The Dependency Labeler 4.1 Classifier We used a maximum entropy classifier   to assign labels to the unlabeled dependencies produced by the Bayes Point Machine.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Dependency Labeler",
                "maximum entropy classifier",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Bayes Point Machine",
                "produced unlabeled dependencies",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In   and  , clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "clusters of similar words",
                "recover data items",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "input corpus",
                "removed one at a time",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Wehope the present work will, together with Talbot and Osborne  , establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bloom filter",
                "practical alternative",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "conventional associative data structures",
                "used in computational linguistics",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Drawing on Abneys   analysis of the Yarowsky algorithm, we perform bootstrapping by entropy regularization: we maximize a linear combination of conditional likelihood on labeled data and confidence   on unlabeled data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "Yarowsky algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "combination",
                "linear combination of conditional likelihood",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In  , an undirected graphical model is used for parse reranking.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "undirected graphical model",
                "used for parse reranking",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parse reranking",
                "used",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "t also contains tools for tuning these models using minimum error rate training   and evaluating the resulting translations using the BLEU score  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "using minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "translations",
                "evaluating using BLEU score",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Itowever, Harris' methodology implies also to simplify and transform each parse tree 2, so as to obtain so-called \"\"elementary sentences\"\" exhibiting the main conceptual classes for the domain   needs a six million word corpus in order to extract noun similarities from predicate-argunlent structures.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parse tree",
                "simplify and transform",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "corpus",
                "six million word",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "80 8.0% Positive child education Positive cost Negative SUBJECT increase Figure 3: An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification, ranging from the use of co-occurrence with typical positive and negative words   to bag of words   and dependency structure  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Various methods",
                "have been proposed",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "bag of words",
                "and dependency structure",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Brent   estimated the error probabilities for each SCF experimentally from the behaviour of his SCF extractor, which detected simple morpho-syntactic cues in the corpus data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SCF extractor",
                "detected simple morpho-syntactic cues",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpus data",
                "in the corpus data",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.1 The BLEU Metric The metric most often used with MERT is BLEU  , where the score of a candidate c against a reference translation r is: BLEU = BP ,len )exp , where pn is the n-gram precision2 and BP is a brevity penalty meant to penalize short outputs, to discourage improving precision at the expense of recall.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "BLEU = BP,len )exp, where pn is the n-gram precision2 and BP is a brevity penalty",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU",
                "penalize short outputs, to discourage improving precision at the expense of recall",
                "PERFORMANCE",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "3 Previous Work on Subjectivity Tagging In previous work  , a corpus of sentences from the Wall Street Journal Treebank Corpus   was manually annotated with subjectivity classi cations bymultiplejudges.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "Wall Street Journal Treebank Corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "judges",
                "multiple judges",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.1 Experimental Set-up We used two different corpora: PropBank   along with PennTree bank 2   and FrameNet.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "PropBank and PennTree bank 2 and FrameNet",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "PropBank",
                "along with PennTree bank 2 and FrameNet",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Occasionally, in 59 sentences out of 2416 on section 23 of the Wall Street Journal Penn Treebank  , the shift-reduce parser fails to attach a node to a head, producing a disconnected graph.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "shift-reduce parser",
                "fails to attach a node to a head",
                "METHODOLOGY",
                "neutral",
                0.7
            ],
            [
                "node",
                "producing a disconnected graph",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Following Ramshaw and Marcus  , the input to the NP chunker consists of the words in a sentence annotated automatically with part-of-speech   tags.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NP chunker",
                "annotated automatically with part-of-speech tags",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "part-of-speech tags",
                "annotated automatically",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "But it makes obvious that   were tackling a problem different from   given the fact that their baseline was at 59% guessing noun attachment  .3 Of course, the baseline is not a direct indicator of the difficulty of the disambiguation task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseline",
                "59% guessing noun attachment",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "difficulty of the disambiguation task",
                "not a direct indicator",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Hierarchical rules were extracted from a subset which has about 35M/41M words5, and the rest of the training data were used to extract phrasal rules as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "subset",
                "has about 35M/41M words",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrasal rules",
                "as in ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For instance  ,     all automatically acquire large TAGs for English from the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TAGs",
                "from the Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Penn Treebank",
                "large",
                "APPLICABILITY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "Rulesize and lexicalization affect parsing complexity whether the grammar is binarized explicitly   or implicitly binarized using Early-style intermediate symbols  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Rulesize",
                "affect parsing complexity",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "grammar",
                "binarized explicitly or implicitly",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "One is to find unknown words from corpora and put them into a dictionary  ), and the other is to estimate a model that can identify unknown words correctly  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "into a dictionary",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "model",
                "identify unknown words correctly",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Following Zhang and Clark  , we first generated CTB 3.0 from CTB 4.0 using sentence IDs 110364.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CTB 3.0",
                "from CTB 4.0 using sentence IDs 110364",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "CTB 4.0",
                "sentence IDs 110364",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "First, even when sentiment is the desired focus, researchers in sentiment analysis have shown that a two-stage approach is often beneficial, in which subjective instances are distinguished from objective ones, and then the subjective instances are further classified according to polarity  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "two-stage approach",
                "is often beneficial",
                "METHODOLOGY",
                "positive",
                0.75
            ],
            [
                "subjective instances",
                "are distinguished from objective ones",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In some cases, class   n-grams are used instead of word n-grams .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-grams",
                "are used instead",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "class n-grams",
                "used instead of word n-grams",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Bean and Riloff   extracts rules from non-anaphoric noun phrases and noun phrases patterns, which are then applied to test data to identify existential noun phrases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rules",
                "extracts",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "noun phrases patterns",
                "applied to test data",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In their seminal paper on SMT, Brownand his colleagues highlighted the problems weface aswe go from IBM Models 1-2 to 3-5  3: Asweprogress from Model1toModel5, evaluating the expectations that gives us counts becomes increasingly difficult.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Models 1-2 to 3-5",
                "becomes increasingly difficult",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "Model 1 to Model 5",
                "evaluating the expectations",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Additionally, some research has explored cutting and pasting segments of text from the full document to generate a summary  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "segments of text",
                "generate a summary",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "research",
                "has explored",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We use the perceptron algorithm for sequence tagging  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron algorithm",
                "for sequence tagging",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "perceptron algorithm",
                "for sequence tagging",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The other form of hybridization ??a statistical MT model that is based on a deeper analysis of the syntactic 33 structure of a sentence ??has also long been identified as a desirable objective in principle  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical MT model",
                "based on a deeper analysis of syntactic structure",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "desirable objective",
                "in principle",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "As comparison, Turney and Littman   used seed sets consisting of 7 words in their word valence annotation experiments, while Turney   used minimal seed sets consisting of only one positive and one negative word   in his experiments on review classification.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "seed sets",
                "consisting of 7 words",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "experiments",
                "on review classification",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "The observation probabilities for a given state, representing a certain word class, are determined by the relative frequencies of words belonging to that class  ); the probabilities of other words are set to a small initial value.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "relative frequencies",
                "determined by",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probabilities of other words",
                "set to a small initial value",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However, there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information, e.g., by the integration of word similarity information as in Meteor   or MaxSim  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Meteor",
                "integration of word similarity information",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "MaxSim",
                "integration of word similarity information",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The current release of PDTB2.0 contains the annotations of 1,808 Wall Street Journal articles   from the Penn TreeBank   II distribution and a total of 40,600 discourse connective tokens  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PDTB2.0",
                "contains annotations",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn TreeBank II distribution",
                "discourse connective tokens",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For each training data size, we report the size of the resulting language model, the fraction of 5-grams from the test data that is present in the language model, and the BLEU score   obtained by the machine translation system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training data size",
                "resulting language model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU score",
                "obtained by the machine translation system",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In such cases, additional information may be coded into the HMM model to achieve higher accuracy  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMM model",
                "achieve higher accuracy",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "additional information",
                "coded into",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Translation qualities are measured by uncased BLEU   with 4 reference translations, sysids: ahb, ahc, ahd, ahe.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sysids",
                "ahb, ahc, ahd, ahe",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU",
                "with 4 reference translations",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "discriminative approaches",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "tagging",
                "unsupervised and semi-supervised",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Several algorithms have been proposed in the literature that try to find the best splits, see for instance  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "have been proposed",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "splits",
                "find the best",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We should note from equation 4 that the neural network model is similar in functional form to the maximum entropy model   except that the neural network learns the feature functions by itself from the training data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "neural network model",
                "is similar in functional form",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "neural network",
                "learns the feature functions by itself",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Some o1' l;his research has treated the sentenees as unstructured word sequences to be aligned; this work has primarily involved the acquisition of bilingual lexical correspondences  , although there has also been a,n attempt to create a full MT system based on such trcat, ment  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bilingual lexical correspondences",
                "acquisition",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "full MT system",
                "create",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "The corpus lines retained are part-of-speech tagged  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus lines",
                "part-of-speech tagged",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "part-of-speech tagged",
                "retained",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "By contrast, in the training method proposed by  , the discriminative function f  is estimated to maximize the F 1 -score of training dataset D. This training method employs an approximate form of the F 1 -score obtained by using a logistic function.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "F 1 -score",
                "estimated to maximize",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "logistic function",
                "approximate form of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4.6 Weakly-constrained algorithms In evaluation with ROUGE  , summaries are truncated to a target length K. Yih et al. usedastackdecodingwithaslightmodication, which allows the last sentence in a summary to be truncated to a target length.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE",
                "evaluation with",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "stack decoding with a slight modification",
                "allows the last sentence to be truncated to a target length",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We used the procedure described in Rapp  , with the only modification being the multiplication of the loglikelihood values with a triangular function that depends on the logarithm of a words frequency.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "procedure",
                "described in Rapp",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "loglikelihood values",
                "multiplied with a triangular function",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In the field of parsing, McDonald and Nivre   compared parsing errors between graphbased and transition-based parsers.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing errors",
                "between graphbased and transition-based parsers",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "parsing",
                "compared parsing",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "But the lack of corpora has led to a situation where much of the current work on parsing is performed on a single domain using training data from that domain  the Wall Street Journal   section of the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "lack",
                "LIMITATION",
                "neutral",
                0.9
            ],
            [
                "Wall Street Journal",
                "section of the Penn Treebank",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this paper we use the so-called Model 4 from  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 4",
                "so-called",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Model 4",
                "from  ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Similarly,   propose a relative distortion model to be used with a phrase decoder.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "relative distortion model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "decoder",
                "phrase decoder",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "t has been claimed that content analysis researchers usually regard a > .8 to demonstrate good reliability and .67 < ~ < .8 alf16 lows tentative conclusions to be drawn  )",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reliability",
                "good",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "conclusions",
                "tentative",
                "PERFORMANCE",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "ang and Lee   use a graph-based technique to identify and analyze only subjective parts of texts",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "graph-based technique",
                "identify and analyze only subjective parts of texts",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "subjective parts of texts",
                "only",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "As described in Section 3 we retrieved neighbors using Lins   similarity measure on a RASP parsed   version of the BNC.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Lins similarity measure",
                "described in Section 3",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "RASP parsed version of the BNC",
                "used for neighbors retrieval",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "First, we considered single sentences as documents, and tokens as sentences   but also a cognate-based one similar to  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tokens",
                "as sentences",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "cognate-based one",
                "similar to  ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "here are several other approaches such as Ji and Ploux   and the already mentioned Rapp  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ji and Ploux",
                "approaches",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "Rapp",
                "already mentioned",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 Baseline MT System The phrase-based SMT system used in our experiments is Moses, phrase translation pro ing probabilities, and languag ties are combined in the log-linear model to obtain the best translation best e  of the source sentence f :  =  = M p | )  m mm h 1 , e e  The weights are set by a discriminative training method using a held-out data set as describ in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based SMT system",
                "Moses",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "weights",
                "set by a discriminative training method",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We applied the union, intersection and refined symmetrization metrics   to the final alignments output from training, as well as evaluating the two final alignments directly.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "union, intersection and refined symmetrization",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignments",
                "final",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he problem is due to the assumption of normality in naive frequency based statistics according to Dunning  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "assumption of normality",
                "in naive frequency based statistics",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "naive frequency based statistics",
                "according to Dunning",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "129 5 Active learning Whereas a passive supervised learning algorithm is provided with a collection of training examples that are typically drawn at random, an active learner has control over the labelled data that it obtains  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "control over labelled data",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "supervised learning",
                "passive",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "As in   fstructures are generated from the   treebank and from this data, along with the treebank trees, the PCFG-based grammar, which is used for training the generation model, is extracted.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PCFG-based grammar",
                "is used for training the generation model",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "treebank trees",
                "along with the",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Many adaptation methods operate by simple augmentations of the target feature space, as we have donehere .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "target feature space",
                "augmentations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "operate by simple",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We worked with an implementation of the log likelihood ratio   as proposed by Dunning   and two variants of the t-score, one considering all values   and one where only positive values   are kept following the results of Curran and Moens  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log likelihood ratio",
                "as proposed by Dunning",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "t-score",
                "considering all values",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "While we can only compare class models with word models on the largest training set, for this training set model M outperforms the baseline Katzsmoothed word trigram model by 1.9% absolute.6 4 Domain Adaptation In this section, we introduce another heuristic for improving exponential models and show how this heuristic can be used to motivate a regularized version of minimum discrimination information   models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model M",
                "outperforms the baseline Katzsmoothed word trigram model",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "minimum discrimination information models",
                "motivated by heuristic",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Among all possible target strings, we will choose the one with the highest probability which is given by Bayes' decision rule  :,~ = argmaxP,' } = argmax {P,' .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bayes' decision rule",
                "highest probability",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "P,'",
                "given by",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Pivots are features occurring frequently and behaving similarly in both domains  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "occurring frequently and behaving similarly",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "features",
                "behaving similarly",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Conditional Maximum Entropy   models have been widely used for a variety of tasks, including language modeling  , part-of-speech tagging, prepositional phrase attachment, and parsing  , word selection for machine translation  , and finding sentence boundaries  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Conditional Maximum Entropy models",
                "have been widely used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Conditional Maximum Entropy models",
                "for a variety of tasks",
                "INNOVATION",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "Thus, we obtain the following second-order model: a36a39a38a41a40 a17 a5a7 a42a4 a5a7 a44 a8 a5a57 a15a27a58 a7 a36a39a38a41a40 a17a20a15a59a42a17 a15a41a49 a7 a7 a60 a4 a5a7 a44 a8 ma61a63a62a65a64a33a66 a5a57 a15a27a58 a7a68a67 a40 a17 a15 a42a17 a15a50a49 a7 a15a50a49a51a48 a60 a4 a15a27a47a55a48 a15a50a49a54a48 a44 a11 A well-founded framework for directly modeling the posterior probability a67 a40 a17 a15 a42a17 a15a50a49 a7 a15a50a49a54a48 a60 a4 a15a12a47a55a48 a15a50a49a54a48 a44 is maximum entropy  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "a36a39a38a41a40",
                "second-order model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "maximum entropy",
                "maximum entropy framework",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The POS tag features were produced by rst predicting the tags with Ratnaparkhis Maximum Entropy Tagger   and then clustered by hand into a smaller number of groups based on their syntactic role.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ratnaparkhis Maximum Entropy Tagger",
                "rst predicting the tags",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "syntactic role",
                "based on their syntactic role",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This setting is reminiscent of the problem of optimizing feature weights for reranking of candidate machine translation outputs, and we employ an optimization technique similar to that used by Och   for machine translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "optimization technique",
                "similar to that used by Och",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "problem",
                "optimizing feature weights for reranking of candidate machine translation outputs",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "What, therefore, has to be explored are various similarity metrics, defining similarity in a concrete way and evaluate the results against human annotations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "similarity metrics",
                "defining similarity in a concrete way",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "results",
                "against human annotations",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "scored with lowercased, tokenized NIST BLEU, and exact match METEOR  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NIST BLEU",
                "scored with",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "METEOR",
                "exact match",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In previous work  , we developed an unsupervised learning algorithm that automatically recognizes definite NPs that are existential without syntactic modification because their meaning is universally understood.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "unsupervised learning",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "meaning",
                "universally understood",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "In order to filter the noise caused by the error alignment links, we only retain those translation pairs whose log-likelihood ratio scores   are above a threshold.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-likelihood ratio scores",
                "are above a threshold",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation pairs",
                "whose",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Given that semantically similar words can be identified automatically on the basis of distributional properties and linguistic cues  , identifying the semantic orientation of words would allow a system to further refine the retrieved semantic similarity relationships, extracting antonyms.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distributional properties",
                "identifying the semantic orientation of words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "semantic similarity relationships",
                "extracting antonyms",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Another technique used was to filter sentences of the out-of-domain corpus based on their similarity to the target domain, as predicted by a classifier  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifier",
                "predicted by",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentences of the out-of-domain corpus",
                "based on their similarity to the target domain",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However, other types of nonlocal information have also been shown to be effective   and we will examine the effectiveness of other non-local information which can be embedded into label information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "The clusters were found automatically by attempting to minimize perplexity  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perplexity",
                "minimize",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "6 Experiments We evaluated the translation quality of the system using the BLEU metric  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metric",
                "translation quality",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "translation quality",
                "evaluated",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Other authors have applied this approach to language modeling  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "to language modeling",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "authors",
                "have applied",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The basic phrase reordering model is a simple unlexicalized, context-insensitive distortion penalty model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase reordering model",
                "simple unlexicalized context-insensitive distortion penalty",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "distortion penalty model",
                "basic",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "From multilingual texts, translation lexica can be generated  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation lexica",
                "can be generated",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he linear kernel derived from the L1 distance is the same as the difference-weighted token-based similarity measure of Weeds and Weir  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "linear kernel",
                "derived from the L1 distance",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "difference-weighted token-based similarity measure",
                "of Weeds and Weir",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2.2 STT: A Statistical Tree-based Tagger The aim of statistical or probabilistic tagging   is to assign the most likely sequence of tags given the observed sequence of words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagging",
                "most likely sequence of tags",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "sequence of words",
                "observed sequence of words",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "??Initial phrase pairs are identified following the procedure typically employed in phrase based systems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase pairs",
                "typically employed in phrase based systems",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "procedure",
                "typically employed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The IBM models, together with a Hidden Markov Model  , form a class of generative models that are based on a lexical translation model P  where each word fj in the foreign sentence fm1 is generated by precisely one word ei in the sentence el1, independently of the other translation decisions  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM models",
                "are based on a lexical translation model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Hidden Markov Model",
                "form a class of generative models",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4.2 Impact of Paraphrases on Machine Translation Evaluation The standard way to analyze the performance of an evaluation metric in machine translation is to compute the Pearson correlation between the automatic metric and human scores  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation metric",
                "standard way",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Pearson correlation",
                "between automatic metric and human scores",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For instance, we may find metrics based on full constituent parsing  , and on dependency parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "based on full constituent parsing",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "metrics",
                "based on dependency parsing",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in  ; phrase translation model probabilities; and trigram language model probabilities logp , using Kneser-Ney smoothing as implemented in the SRILM toolkit  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kneser-Ney smoothing",
                "as implemented in the SRILM toolkit",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase translation model probabilities",
                "logp",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As with the graph-based parser, we use the discriminative perceptron   to train the transition-based model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transition-based model",
                "train the transition-based model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "discriminative perceptron",
                "use the discriminative perceptron",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5 Phrase Pair Induction A common approach to phrase-based translation is to extract an inventory of phrase pairs   from bitext  , For example, in the phraseextract algorithm  , a word alignment am1 is generated over the bitext, and all word subsequences ei2i1 and fj2j1 are found that satisfy : am1 : aj    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase pairs",
                "inventory of phrase pairs",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phraseextract algorithm",
                "word alignment am1",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The summaries from the above algorithm for the QF-MDS were evaluated based on ROUGE metrics  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE metrics",
                "evaluated based on",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "for the QF-MDS",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4 SMT-Based Query Expansion Our SMT-based query expansion techniques are based on a recent implementation of the phrasebased SMT framework  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT-based query expansion techniques",
                "phrase-based SMT framework",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "phrase-based SMT framework",
                "recent implementation",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Methods focussing on the use and generation of dictionaries capturing the sentiment of words have ranged from manual approaches of developing domain-dependent lexicons   to semi-automated approaches  , and even an almost fully automated approach  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dictionaries",
                "capturing the sentiment of words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approaches",
                "manual to semi-automated to almost fully automated",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The scores were then weighted by the inverse of their height in the tree and then summed together, similarly to the procedure in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "scores",
                "weighted by the inverse of their height in the tree",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "procedure",
                "similarly to the procedure in  ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4.3 Scoring All-N Rules We observed that the likelihood of nouns mentioned in a definition to be referred by the concept title depends greatly on the syntactic path connecting them  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "nouns mentioned in a definition",
                "depends greatly on the syntactic path connecting them",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "concept title",
                "referred by",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The problem itself has started to get attention only recently  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "problem",
                "has started to get attention",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "attention",
                "only recently",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Training and testing were performed using the noun phrase chunking corpus described in Ramshaw & Marcus    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "noun phrase chunking corpus",
                "described in Ramshaw & Marcus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "noun phrase chunking corpus",
                "used for training and testing",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "First, researchers are divided between a general method   and one that is applied only to a small trial selection of texts words    ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "researchers",
                "divided",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "method",
                "general and one applied only to a small trial selection of texts words",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Table 1 shows theresultsalongwithB andthethreemetricsthat achieved higher correlations than B: semantic role overlap  , ParaEval recall  , and METEOR  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metric",
                "achieved higher correlations than B",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "metric",
                "achieved higher correlations",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Performance is measured by computing the BLEU scores   of the systems translations, when compared against a single reference translation per sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU scores",
                "of the systems translations",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "reference translation per sentence",
                "single",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Recently, researchers have developed algorithms that learn to map natural language sentences to representations of their underlying meaning  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "learn to map natural language sentences to representations of their underlying meaning",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "natural language sentences",
                "underlying meaning",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The results were evaluated using the character/pinyin-based 4-gram BLEU score  , word error rate  , position independent word error rate  , and exact match  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "character/pinyin-based 4-gram",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "word error rate",
                "position independent",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Hindle and Rooth   and Church and Hanks   used partial parses generated by Fidditch to study word ~urrt.nc patterns m syntactic contexts",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Fidditch",
                "generated partial parses",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word ~urrt.nc patterns",
                "studied in syntactic contexts",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "8 Conclusions In this paper, we developed probability models for the multi-level transfer rules presented in  , showed how to acquire larger rules that crucially condition on more syntactic context, and how to pack multiple derivations, including interpretations of unaligned words, into derivation forests.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probability models",
                "developed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "rules",
                "crucially condition on more syntactic context",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "ollins   Model 3 integrates the detection and resolution of WH-traces in relative clauses into a lexicalized PCFG",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 3",
                "integrates detection and resolution of WH-traces in relative clauses",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "lexicalized PCFG",
                "into",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Word Alignment algorithm We use IBM Model 4   as a basis for our word alignment system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 4",
                "basis for our word alignment system",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word alignment system",
                "use",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  Merkel, Nilsson, & Ahrenberg   have constructed a system that uses frequency of recurrent segments to determine long phrases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "uses frequency of recurrent segments",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "long phrases",
                "to determine",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "It is today common practice to use phrases as translation units   instead of the original word-based approach.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrases",
                "as translation units",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word-based approach",
                "original",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We briefly describe the tagger   for more details), a Hidden Markov Model trained with the perceptron algorithm introduced in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger",
                "Hidden Markov Model trained with the perceptron algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "perceptron algorithm",
                "introduced in",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In the iNeast system  , the identification of relevant terms is oriented towards multi-document summarization, and they use a likelihood ratio   which favours terms which are representative of the set of documents as opposed to the full collection.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "likelihood ratio",
                "favours terms which are representative of the set of documents as opposed to the full collection",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "identification of relevant terms",
                "oriented towards multi-document summarization",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Actually, now that SMT has reached some maturity, we see several attempts to integrate more structure into these systems, ranging from simple hierarchical alignment models   to syntax-based statistical systems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT",
                "reached maturity",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "syntax-based statistical systems",
                "simple hierarchical alignment models",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "In addition to the block sampler used by Bhattacharya and Getoor  , we are investigating general-purpose splitmerge samplers   and the permutation sampler  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "block sampler",
                "used by Bhattacharya and Getoor",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "splitmerge samplers",
                "general-purpose",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Of course, many applications require smoothing of the estimated distributionsthis problem also has known solutions in MapReduce  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distribution",
                "estimated distribution",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "solutions",
                "has known solutions",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We can sum over all non-projective spanning trees by taking the determinant of the Kirchhoff matrix of the graph defined above, minus the row and column corresponding to the root node  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kirchhoff matrix",
                "defined above",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "root node",
                "corresponding to",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This approach has also been used by (Dagan and Itai, 1994; Gale et al. , 1992; Shiitze, 1992; Gale et al. , 1993; Yarowsky, 1995; Gale and Church, 1Lunar is not an unknown word in English, Yeltsin finds its translation in the 4-th candidate.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Dagan and Itai, 1994",
                "has also been used by",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Yeltsin",
                "finds its translation in the 4-th candidate",
                "LIMITATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Turney   is the first, to the best of our knowledge, to raise the issue of a unified approach.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Turney",
                "to raise the issue of a unified approach",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "knowledge",
                "best of",
                "PERFORMANCE",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "The training data is aligned using the LEAF technique  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training data",
                "aligned using the LEAF technique",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "LEAF technique",
                "used for alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To measure the coherence of sentences, we use a statistical parser Toolkit   to assign each sentence a parsers score that is the related log probability of parsing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser Toolkit",
                "assign each sentence a parser's score",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parser's score",
                "related log probability of parsing",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "From this data, we use the the GHKM minimal-rule extraction algorithm of   to yield rules like: NP-C )$x1 de x0 Though this rule can be used in either direction, here we use it right-to-left  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GHKM minimal-rule extraction algorithm",
                "yield rules like",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "rule",
                "can be used in either direction",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Two main approaches have generally been considered: rule-based   probabilistic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rule-based",
                "probabilistic",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approaches",
                "generally been considered",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In this work, we study a method for obtaining word phrases that is based on Stochastic Inversion Transduction Grammars that was proposed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stochastic Inversion Transduction Grammars",
                "was proposed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word phrases",
                "obtaining",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "One possible conclusion from the POS tagging literature is that accuracy is approaching the limit, and any remaining improvement is within the noise of the Penn Treebank training data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank training data",
                "noise",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "accuracy",
                "approaching the limit",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We used the Berkeley Parser 2 to train such grammars on sections 2-21 of the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Berkeley Parser 2",
                "train grammars",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Penn Treebank",
                "sections 2-21",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These were combined using the Grow Diag Final And symmetrization heuristic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Grow Diag Final And symmetrization heuristic",
                "combined using",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "symmetrization heuristic",
                "heuristic",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Relative frequency ratio   of terms between two different corpora can also be used to discover domain-oriented multi-word terms that are characteristic of a corpus when compared with another  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "when compared with another",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "terms",
                "characteristic of a corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "arzilay and Lee   applied multi-sequence alignment   to parallel news sentences and induced paraphrase patterns for generating new sentences  )",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "multi-sequence alignment",
                "to parallel news sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "paraphrase patterns",
                "for generating new sentences",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Again, we find the clearest patterns in the graphs for precision, where Malt has very low precision near the root but improves with increasing depth, while MST shows the opposite trend  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Malt",
                "has very low precision",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "MST",
                "shows the opposite trend",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The IBM models   search a version of permutation space with a one-to-many constraint.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM models",
                "search a version of permutation space with a one-to-many constraint",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "permutation space",
                "one-to-many constraint",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In addition, we also made a word alignment available, which was derived using a variant of the current default method for word alignment  Och and Ney  s refined method.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method for word alignment",
                "Och and Ney's refined method",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word alignment",
                "derived using a variant of the current default",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ne approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "distribution in a corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "thesauri",
                "automatic",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER  , which employs a version of edit distance for word substitution and reordering; or METEOR  , which uses stemming and WordNet synonymy.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CDER",
                "employs a version of edit distance for word substitution and reordering",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "METEOR",
                "uses stemming and WordNet synonymy",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "McDonald et al 2007; Ivan et al 2008) proposed a structured model based on CRFs for jointly classifying the sentiment of text at varying levels of granularity",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CRFs",
                "structured model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentiment of text",
                "at varying levels of granularity",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1, 2 show the examples of w~rious transliterations in KTSET 2.0 .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "KTSET 2.0",
                "wrious transliterations",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "wrious transliterations",
                "in",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "any other projects have used statistics in a way that summarizes facts about the text but does not draw any explicit conclusions from them  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "projects",
                "summarizes facts",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "conclusions",
                "does not draw",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "hat some model structures work better than others at real NLP tasks was discussed by Johnson   and Klein and Manning  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model structures",
                "work better",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "real NLP tasks",
                "discussed",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To find the optimal coefficients  for a loglinear combination of these experts, we use separate development data, using the following procedure due to Och  : 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "coefficients",
                "optimal loglinear combination of experts",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "procedure",
                "due to Och",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There are other approaches to statistical machine translation where translation is achieved through transduction of source language structure to target language structure  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "structure",
                "source language structure to target language structure",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "approaches",
                "other approaches",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "A second example of subtle language dependence comes from Dasgupta and Ng  , who present an unsupervised morphological segmentation algorithm meant to be language-independent.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "unsupervised morphological segmentation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "language-independent",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Since the DUC 2004 evaluation, Lin   has concluded that certain ROUGE metrics correlate better with human judgments than others, depending on the summarisation task being evaluated, i.e. single document, headline, or multi-document summarisation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE metrics",
                "correlate better",
                "PERFORMANCE",
                "positive",
                0.7
            ],
            [
                "ROUGE metrics",
                "depending on the summarisation task",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Early experiments with syntactically-informed phrases  , and syntactic reranking of K-best lists   produced mostly negative results.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrases",
                "produced mostly negative results",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "K-best lists",
                "syntactic reranking",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Several models were introduced for these problems, for example, the Hidden Markov Model    , Maximum Entropy Model    , and Conditional Random Fields    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hidden Markov Model",
                "example",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Maximum Entropy Model",
                "example",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Note that the results of MB-D here cannot be directly compared with those in Yarowsky  , because the data used are different.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MB-D",
                "cannot be directly compared",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "data used",
                "are different",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Gibbs sampling is not new to the natural language processing community  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Gibbs sampling",
                "is not new",
                "INNOVATION",
                "neutral",
                0.5
            ],
            [
                "natural language processing community",
                "not new",
                "APPLICABILITY",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "illmann and Zhang   use a BLEU oracle decoder for discriminative training of a local reordering model",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU oracle decoder",
                "for discriminative training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "local reordering model",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A re nement of this model is the class-based n-gram where the words are partitioned into equivalence classes  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "class-based n-gram",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "words",
                "partitioned into equivalence classes",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "c 2009 Association for Computational Linguistics Reverse Revision and Linear Tree Combination for Dependency Parsing Giuseppe Attardi Dipartimento di Informatica Universit`a di Pisa Pisa, Italy attardi@di.unipi.it Felice DellOrletta Dipartimento di Informatica Universit`a di Pisa Pisa, Italy felice.dellorletta@di.unipi.it 1 Introduction Deterministic transition-based Shift/Reduce dependency parsers make often mistakes in the analysis of long span dependencies  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency parsers",
                "make mistakes",
                "METHODOLOGY",
                "negative",
                0.75
            ],
            [
                "long span dependencies",
                "are analyzed",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For example, when applying their approach to a different domain with somewhat less rigid syntax, Zettlemoyer and Collins   need to introduce new combinators and new forms of candidate lexical entries.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "combinators",
                "new",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "lexical entries",
                "new forms of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Treebanks have been used within the field of natural language processing as a source of training data for statistical part og speech taggers   and for statistical parsers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Treebanks",
                "source of training data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "statistical part of speech taggers and parsers",
                "used within the field",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "The applications range from simple classification tasks such as text classification and history-based tagging   to more complex structured prediction tasks such as partof-speech   tagging  , syntactic parsing   and semantic role labeling  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classification tasks",
                "simple",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "structured prediction tasks",
                "more complex",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "context-free rules Charniak   Collins  , Eisner   context-free rules, headwords Charniak   context-free rules, headwords, grandparent nodes Collins   context-free rules, headwords, grandparent nodes/rules, bigrams, two-level rules, two-level bigrams, nonheadwords Bod   all fragments within parse trees Scope of Statistical Dependencies Model Figure 4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "context-free rules",
                "Charniak",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parse trees",
                "all fragments within",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We use the distributed training and application infrastructure described in   with modifications to allow the training of predictive class-based models and their application in the decoder of the machine translation system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "infrastructure",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "predictive class-based models",
                "allow training of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The parameters of the NIST systems were tuned using Ochs algorithm to maximize BLEU on the MT02 test set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NIST systems",
                "tuned using Ochs algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "MT02 test set",
                "maximize BLEU",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "As with conventional smoothing methods  , triangulation increases the robustness of phrase translation estimates.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "smoothing methods",
                "increases the robustness",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "phrase translation estimates",
                "triangulation increases",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For example, aspects of a digital camera could include picture quality, battery life, size, color, value, etc. Finding such aspects is a challenging research problem that has been addressed in a number of ways  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "picture quality",
                "is a challenging research problem",
                "METHODOLOGY",
                "neutral",
                0.7
            ],
            [
                "size",
                "is a feature",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "But in fact, the issue of editing in text summarization has usually been neglected, notable exceptions being the works by Jing and McKeown   and Mani, Gates, and Bloedorn  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "works",
                "notable exceptions",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "issue",
                "usually been neglected",
                "LIMITATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A recent trend is to store the LM in a distributed cluster of machines, which are queried via network requests  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LM",
                "in a distributed cluster of machines",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "query",
                "via network requests",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We use the semi-supervised EMD algorithm   to train the model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "EMD algorithm",
                "to train the model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "EMD algorithm",
                "semi-supervised",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "edu Abstract This paper reports on our experience hand tagging the senses of 25 of the most frequent verbs in 12,925 sentences of the Wall Street Journal Treebank corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hand tagging",
                "the senses of 25 of the most frequent verbs",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Unlike stochastic approaches to part-of-speech tagging  , up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "knowledge found in finite-state taggers",
                "handcrafted",
                "METHODOLOGY",
                "negative",
                0.7
            ],
            [
                "stochastic approaches",
                "to part-of-speech tagging",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the work of Smadja   on extracting collocations, preference was given to constructions whose constituents appear in a fixed order, a similar   version of our assumption here that asymmetric constructions are more idiomatic than symmetric ones.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "constructions",
                "appear in a fixed order",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "asymmetric constructions",
                "more idiomatic",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Furthermore, WASP1++ employs minimum error rate training   to directly optimize the evaluation metrics.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation metrics",
                "directly optimize",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "minimum error rate training",
                "employs",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": ".4 Experiment 2: Yarowskys Words We also conducted translation on seven of the twelve English words studied in Yarowsky  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Yarowsky",
                "Words",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "seven",
                "of the twelve English words",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Many statistical translation models   try to model word-to-word correspondences between source and target words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical translation models",
                "try to model word-to-word correspondences",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word-to-word correspondences",
                "between source and target words",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Co-training  , and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation  , named entity recognition  , noun phrase bracketing   and statistical parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "co-training",
                "have been applied",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "NLP problems",
                "including word sense disambiguation, named entity recognition, noun phrase bracketing, and statistical parsing",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1   Introduction In the community of sentiment analysis  , transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work, because sentiment expression often behaves with strong domain-specific nature.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentiment classifier",
                "transferring from one source domain to another target domain",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "sentiment expression",
                "behaves with strong domain-specific nature",
                "PERFORMANCE",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "The initial state contains terminal items, whose labels are the POS tags given by Ratnaparkhi  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ratnaparkhi",
                "POS tags given by",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "terminal items",
                "contain",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Like the models of Goodman  , the additional features in our model are generated probabilistically, whereas in the parser of Collins   distance measures are assumed to be a function of the already generated structure and are not generated explicitly.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "generated probabilistically",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "distance measures",
                "assumed to be a function of the already generated structure",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The order of constituents, for instance, can be used to inform prototype-driven learning strategies  , which can then be applied to raw corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "constituents",
                "inform prototype-driven learning strategies",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "prototype-driven learning strategies",
                "can be applied to raw corpora",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In particular, this method has been used for word sense disambiguation   and thesaurus construction  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "has been used for word sense disambiguation and thesaurus construction",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "method",
                "has been used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-gram models",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Hidden Markov Model-based taggers",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Semantic  : The named entity   tag of wi obtained using the Stanford CRF-based NE recognizer  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stanford CRF-based NE recognizer",
                "using the",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "named entity",
                "obtained",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Parameters were tuned with MERT algorithm   on the NIST evaluation set of 2003   for both the baseline systems and the system combination model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MERT algorithm",
                "tuned",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "NIST evaluation set of 2003",
                "used",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Since we need knowledge-poor Daille, 1996) induction, we cannot use human-suggested filtering Chi-squared   2   Z-Score   Students t-Score   n-gram list in accordance to each probabilistic algorithm.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Daille, 1996 induction",
                "knowledge-poor",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "probabilistic algorithm",
                "each probabilistic algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The idea of bidirectional parsing is related to the bidirectional sequential classification method described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bidirectional parsing",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "bidirectional sequential classification method",
                "described",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "These methods have reported performance in the range of 95-99% \"\"correct\"\" by word  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "performance",
                "95-99% correct",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "correct",
                "by word",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "c2009 Association for Computational Linguistics Improving Mid-Range Reordering using Templates of Factors Hieu Hoang School of Informatics University of Edinburgh h.hoang@sms.ed.ac.uk Philipp Koehn School of Informatics University of Edinburgh pkoehn@inf.ed.ac.uk Abstract We extend the factored translation model   to allow translations of longer phrases composed of factors such as POS and morphological tags to act as templates for the selection and reordering of surface phrase translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "factored translation model",
                "allow translations of longer phrases composed of factors such as POS and morphological tags",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "surface phrase translation",
                "as templates for the selection and reordering",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Also, the aspect of generalizing features across different products is closely related to fully supervised domain adaptation  , and we plan to combine our approach with the idea from Daume III   to gain insights into whether the composite back-off features exhibit different behavior in domain-general versus domain-specific feature sub-spaces.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "generalizing across different products",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "composite back-off features",
                "exhibit different behavior",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4.4 Text chunking Next, a rule-based text chunker   is applied on the tagged sentences to further identify phrasal units, such as base noun phrases NP and verbal units VB.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rule-based text chunker",
                "applied on the tagged sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrasal units",
                "such as base noun phrases NP and verbal units VB",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, as also pointed out by Yarowsky  , this observation does not hold uniformly over all possible co-occurrences of two words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "co-occurrences of two words",
                "does not hold uniformly",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "Yarowsky",
                "pointed out",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Furthermore, as pointed out in Dolan  , the sense division in an MRD is frequently too fine-grained for the purpose of WSD.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sense division",
                "too fine-grained",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "MRD",
                "for the purpose of WSD",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As an example, consider the fiat NP structures that are in the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "is in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "NP structures",
                "are",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Much previous work has been done on this problem and many different methods have been used: Church's PARTS   program uses a Markov model; Bourigault   uses heuristics along with a grammar; Voutilainen's NPTool   uses a lexicon combined with a constraint grammar; Juteson and Katz   use repeated phrases; Veenstra  , Argamon, Dagan & Krymolowski  and Daelemaus, van den Bosch & Zavrel   use memory-based systems; Ramshaw & Marcus   and Cardie & Pierce   use rule-based systems.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PARTS program",
                "uses a Markov model",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "memory-based systems",
                "use memory-based systems",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3.1 Evaluation Measure and MERT We evaluate our experiments using the   BLEU metric and estimate the empirical confidence using the bootstrapping method described in Koehn  .6 We report the scores obtained on the test section with model parameters tuned using the tuning section for minimum error rate training  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metric",
                "evaluation measure",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "bootstrapping method",
                "described in Koehn",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Research in the field of unsupervised and weakly supervised parsing ranges from various forms of EM training   over bootstrapping approaches like selftraining   to feature-based enhancements of discriminative reranking models   and the application of semisupervised SVMs  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "EM training",
                "various forms",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "semisupervised SVMs",
                "application",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This operation can be used in applications like Minimum Error Rate Training  , or optimizing system combination as described by Hillard et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "operation",
                "used in applications",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "system combination",
                "as described by Hillard et al.",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The results are consistent with the idea in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "results",
                "consistent with the idea",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "idea",
                "in ",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The implementation includes path-length  , information-content   and text-overlap   measures, as described in Strube & Ponzetto  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "path-length, information-content, and text-overlap measures",
                "described in Strube & Ponzetto",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "measures",
                "described in Strube & Ponzetto",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Since this trade-off is also affected by the settings of various pruning parameters, we compared decoding time and translation quality, as measured by BLEU score  , for the two models on our first test set over a broad range of settings for the decoder pruning parameters.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder pruning parameters",
                "settings for",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translation quality",
                "as measured by BLEU score",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.2 Compound Noun Interpretation The task of interpreting the semantics of noun compounds is one which has recently received considerable attention  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "noun compounds",
                "has received considerable attention",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "semantics",
                "of noun compounds",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Statistical dependency parsers of English must therefore rely on dependency structures automatically converted from a constituent corpus such as the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency parsers",
                "automatically converted from a constituent corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank",
                "constituent corpus",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The basic LCS has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LCS",
                "does not differentiate LCSes of different spatial relations",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "embedding sequences",
                "within their embedding sequences",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Gildea and Jurafsky   used a supervised learning method to learn both the identifier of the semantic roles defined in FrameNet such as theme, target, goal, and the boundaries of the roles  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "supervised learning method",
                "to learn both the identifier of the semantic roles",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "semantic roles",
                "defined in FrameNet",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he cube-pruning by Chiang   and the lazy cube-pruning of Huang and Chiang   turn the computation of beam pruning of CYK decoders into a top-k selection problem given two columns of translation hypotheses that need to be combined",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "beam pruning of CYK decoders",
                "turn into a top-k selection problem",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation hypotheses",
                "need to be combined",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Statistical parsers have been developed for TAG  , LFG  , and HPSG  , among others.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsers",
                "developed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "TAG, LFG, and HPSG",
                "others",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "potential caveat with Lins   distributional similarity measure is its reliance on syntactic information for obtaining dependency relations",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Lins distributional similarity measure",
                "reliance on syntactic information",
                "METHODOLOGY",
                "neutral",
                0.7
            ],
            [
                "dependency relations",
                "obtaining",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Section 3 describes previous work   that derives connections between boosting and maximum-entropy models for the simpler case of classification problems; this work forms the basis for the reranking methods.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "boosting",
                "the basis for reranking methods",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "maximum-entropy models",
                "the simpler case of classification problems",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The words with the highest association probabilities are chosen as acquired words for entity e. 4.1 Base Model I Using the translation model I  , where each word is equally likely to be aligned with each entity, we have p  = 1 m mproductdisplay j=1 lsummationdisplay i=0 p    where l and m are the lengths of entity and word sequences respectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model I",
                "each word is equally likely to be aligned with each entity",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Base Model I",
                "Using the translation model",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "POS tagger: The maximum entropy POS tagger developed by Ratnaparkhi   and the rule-based POS tagger developed by Brill   are trained with 1200 abstracts extracted from the GENIA corpus, which achieve accuracies of 97.97% and 98.06% respectively, when testing on the rest 800 abstract of the GENIA corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tagger",
                "developed by Ratnaparkhi and the rule-based POS tagger developed by Brill",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "accuracies",
                "97.97% and 98.06%",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "c2006 Association for Computational Linguistics Robust PCFG-Based Generation using Automatically Acquired LFG Approximations Aoife Cahill1 and Josef van Genabith1,2 1 National Centre for Language Technology   School of Computing, Dublin City University, Dublin 9, Ireland 2 Center for Advanced Studies, IBM Dublin, Ireland {acahill,josef}@computing.dcu.ie Abstract We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations   automatically extracted from treebanks, maximising the probability of a tree given an f-structure.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PCFG-based architecture",
                "novel",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "LFG approximations",
                "automatically extracted from treebanks",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Statistical Machine Translation We use a log-linear approach   in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: e = argmax e wT h    where h  is a large-dimension feature vector.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-linear approach",
                "seeking a maximum solution",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "large-dimension feature vector",
                "",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The standard method to overcome this problem to use the model in both directions   and applying heuristic-based combination techniques to produce a refined alignment  henceforth referred to as RA. Several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "use in both directions",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignment models",
                "injecting additional knowledge or combining different alignment models",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Apart from the fact that we present an alternative model, our work differs from Marcu and Echihabi   in two important ways.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "alternative",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "work",
                "differs from",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We measure translation performance by the BLEU   and METEOR   scores with multiple translation references.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU and METEOR scores",
                "translation performance",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "multiple translation references",
                "translation performance",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A novel approach was described in  , which used an unsupervised training technique, extracting relations that were explicitly and unamibiguously signalled and automatically labelling those examples as the training set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unsupervised training technique",
                "extracting relations explicitly and unambiguously signalled",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "training set",
                "automatically labelling examples",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Most work in the area of unknown words and tagging deals with predicting part-of-speech information based on word endings and affixation information, as shown by work in  ,  ,  , and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work in the area of unknown words and tagging",
                "predicting part-of-speech information",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word endings and affixation information",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Due to the importance of WN for NLP tasks, substantial research was done on direct or indirect automated extension of the English WN  ) or WN in other languages  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WN",
                "English WN or WN in other languages",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "research",
                "substantial",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Introduction Log-linear models have been applied to a number of problems in NLP, for example, POS tagging  , named entity recognition  , chunking  , and parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Log-linear models",
                "applied to a number of problems",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "POS tagging",
                "named entity recognition, chunking, and parsing",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "3 MaltParser MaltParser   is a languageindependent system for data-driven dependency parsing, based on a transition-based parsing model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MaltParser",
                "transition-based parsing model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "language-independent",
                "system",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Peter F. Brown, Vincent J. Della Pietra, Petere V. deSouza, Jenifer C. Lai, and Robert L. Mercer.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brown et al.'s work",
                "is a statistical model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical model",
                "uses a combination of n-gram and word-based models",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The f-structure annotation algorithm used for inducing LFG resources from the Penn-II treebank for English   uses configurational, categorial, function tag and trace information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "f-structure annotation algorithm",
                "uses configurational, categorial, function tag and trace information",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn-II treebank",
                "for English",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this paper, a new part-of-speech tagging method hased on neural networks   is presented and its performance is compared to that of a llMM-tagger   and a trigrambased tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "neural networks",
                "based",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "performance",
                "compared to",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For Penn Treebank II style annotation  , in which a nonterminal symbol is a category together with zero or more functional tags, we adopt the following scheme: the atomic pattern a matches any label with category a or functional tag a; moreover, we define Boolean operators^,_, and:.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank II style annotation",
                "adopts the following scheme",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Boolean operators",
                "are defined",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The latter group did an experiment early on in which they found that manual tagging took about twice as long as correcting  , with about twice the interannotator disagreement rate and an error rate that was about 50% higher  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "manual tagging",
                "took about twice as long",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "error rate",
                "about 50% higher",
                "PERFORMANCE",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "A few exceptions are the hierarchical   transduction models   and the string transduction models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transduction models",
                "hierarchical and string",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "transduction models",
                "a few exceptions",
                "INNOVATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "or cooking, which agrees with the knowledge presented in previous work  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "knowledge",
                "presented in previous work",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "previous work",
                "agrees with",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In open-domain opinion extraction, some approaches use syntactic features obtained from parsed input sentences  , as is commonly done in semantic role labeling.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic features",
                "obtained from parsed input sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "approaches",
                "commonly done in semantic role labeling",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "We compare an ordinary PCFG estimated with maximum likelihood   and the HDP-PCFG estimated using the variational inference algorithm described in Section 2.6.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PCFG",
                "estimated with maximum likelihood",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "HDP-PCFG",
                "estimated using the variational inference algorithm",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "illmann and Zhang   present a procedure to directly optimize the global scoring function used by a phrasebased decoder on the accuracy of the translations",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based decoder",
                "global scoring function",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translations",
                "accuracy of",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This view is supported by Lin  , who concludes that correlations to human judgments were increased by using multiple references but using single reference summary with enough number of samples was a valid alternative.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "references",
                "using multiple references",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "summary with enough number of samples",
                "was a valid alternative",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "During training, the baseline POS tagger stores special word-tag pairs into a tag dictionary  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseline POS tagger",
                "stores special word-tag pairs into a tag dictionary",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tag dictionary",
                "stores special word-tag pairs",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This approach was subsequently extended to other languages including German  , Chinese  ,  , Spanish  ,   and French  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "languages",
                "German, Chinese, Spanish, and French",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "extension",
                "to other languages",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Instead of using a single system output as the skeleton, we employ a minimum Bayes-risk decoder to select the best single system output from the merged N-best list by minimizing the BLEU   loss.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bayes-risk decoder",
                "minimizing the BLEU loss",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "system output",
                "best single system output",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Koehn and Hoang   propose Factored Translation Models, which extend phrase-based statistical machine translation by allowing the integration of additional morphological features at the word level.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Factored Translation Models",
                "extend phrase-based statistical machine translation",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "morphological features",
                "allowing integration at the word level",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To extract such word clusters we used suffix arrays proposed in Yamamoto and Church   and the pointwise mutual information measure, see Church and Hanks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "suffix arrays",
                "proposed in Yamamoto and Church",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "pointwise mutual information measure",
                "see Church and Hanks",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1418 examples of structures of the kind 'VB N1 PREP N2' were extracted from the Penn-TreeBank Wall Street Journal  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "structures",
                "of the kind 'VB N1 PREP N2'",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn-TreeBank Wall Street Journal",
                "was extracted from",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Then, those structurally matched parallel sentences are used as a source for acquiring lexical knowledge snch as verbal case frames  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parallel sentences",
                "source for acquiring lexical knowledge",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "verbal case frames",
                "snch",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "As discussed in  , undirected graphical models do not seem to be suitable for history-based parsing models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "graphical models",
                "not seem to be suitable",
                "METHODOLOGY",
                "negative",
                0.85
            ],
            [
                "history-based parsing models",
                "",
                "INNOVATION",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "greement among annotators was measured using the K statistic  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotators",
                "measured using the K statistic",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "K statistic",
                "measured",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The baseline score using all phrase pairs was 59.11   with a 95% confidence interval of  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase pairs",
                "was 59.11",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "confidence interval",
                "95%",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Finally, following Haghighi and Klein   and Johnson   we can instead insist that at most one HMM state can be mapped to any part-of-speech tag.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HMM state",
                "at most one",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "part-of-speech tag",
                "mapped to",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "aghighi and Klein   ask the user to suggest a few prototypes   for each class and use those as features",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prototypes",
                "as features",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "user",
                "suggest",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In particular, knowing a little about the structure of a language can help in developing annotated corpora and tools, since a little knowledge can go a long way in inducing accurate structure and annotations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora and tools",
                "inducing accurate structure and annotations",
                "INNOVATION",
                "positive",
                0.85
            ],
            [
                "little knowledge",
                "go a long way",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The inclusion of phrases longer than three words in translation resources has been avoided, as it has been shown not to have a strong impact on translation performance  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrases longer than three words",
                "not to have a strong impact",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "translation resources",
                "avoided",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For the first set of experiments, we divide all inputs based on the mean value of the average system scores as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system scores",
                "mean value of",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "inputs",
                "divide based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, Och   shows how to train a log-linear translation model not by maximizing the likelihood of training data, but maximizing the BLEU score   of the model on 53 the data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "maximizing the BLEU score of the model",
                "PERFORMANCE",
                "positive",
                0.7
            ],
            [
                "BLEU score",
                "of the model on 53 the data",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, in the coarse-grained task, the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sense inventory",
                "clustered semi-automatically",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "cluster",
                "representing an equivalence class over senses",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In particular, Hockenmaier and Steedman   report a generative model for CCG parsing roughly akin to the Collins parser   specific to CCG.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CCG parsing",
                "roughly akin to the Collins parser",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "generative model",
                "specific to CCG",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Furthermore, I plan to apply my parsers in other domains     besides treebank data, to investigate the effectiveness and generality of my approaches.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsers",
                "effectiveness and generality",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "approaches",
                "investigate the",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Note that using stems and their synonyms as used in METEOR   could also be considered for word similarity.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "METEOR",
                "used in METEOR",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word similarity",
                "could be considered",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One option would be to leverage unannotated text  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unannotated text",
                "unannotated text",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "urney   has presented an unsupervised opinion classification algorithm called SO-PMI  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SO-PMI",
                "unsupervised opinion classification algorithm",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "opinion classification algorithm",
                "called",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": ", 1989), e.g, lexicography  , information retrieval  , text input  , etc. This paper will touch on its feasibility in topic identification.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicography",
                "e.g",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "topic identification",
                "feasibility",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our work so far has focused on data in the Penn Treebank  , particularly the Brown corpus and some examples from the Wall Street Journal corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "Penn Treebank",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "Brown corpus",
                "some examples",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "One such approach is maximum entropy classification  , which we use in the form of a library implemented by Tsuruoka1 and used in his classifier-based parser  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy classification",
                "implemented by Tsuruoka1",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "classifier-based parser",
                "used in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.2 Details To learn alignments, translation probabilities, etc in the first method we used work that has been done in statistical machine translation  , where the translation process is considered to be equivalent to a corruption of the source language text to the target language text due to a noisy channel.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "statistical machine translation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translation process",
                "noisy channel",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ome of the alignment sets also have links which are not Sure links but are Possible links  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models  , heuristic-based models   and even for syntactically motivated models such as ITG  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "deeper syntax",
                "has been shown useful",
                "INNOVATION",
                "positive",
                0.85
            ],
            [
                "ITG",
                "syntactically motivated",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.1 The Likelihood Ratio We adopted a method for collocation discovery based on the likelihood ratio  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Likelihood Ratio",
                "based on the likelihood ratio",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "method",
                "for collocation discovery",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Previous research in this area includes several models which incorporate hidden variables  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "incorporate hidden variables",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "hidden variables",
                "hidden",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Wu and Weld   and Cucerzan   calculate the overlap between contexts of named entities and candidate articles from Wikipedia, using overlap ratios or similarity scores in a vector space model, respectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "named entities",
                "contexts of",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "vector space model",
                "respectively",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The second voting model, a maximum entropy model  , was built as Klein and Manning   found that it yielded higher accuracy than nave Bayes in a subsequent comparison of WSD performance.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy model",
                "yielded higher accuracy",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "Bayes",
                "lower accuracy",
                "PERFORMANCE",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "The mapping of answer terms to question terms is modeled using Black et al.s   simplest model, called IBM Model 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 1",
                "simplest model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Black et al.'s",
                "simplest model",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in  ; phrase translation model probabilities; and 4-gram language model probabilities logp , using Kneser-Ney smoothing as implemented in the SRILM toolkit.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase reordering",
                "in a",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Kneser-Ney smoothing",
                "as implemented in the SRILM toolkit",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The parse trees on the English side of the bitexts were generated using a parser   implementing the Collins parsing models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "implementing Collins parsing models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Collins parsing models",
                "parsing models",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Word alignments were first introduced as an intermediate result of statistical machine translation systems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Word alignments",
                "introduced as an intermediate result",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "statistical machine translation systems",
                "first",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Recently, Yarowsky   combined an MRD and a corpus in a bootstrapping process.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MRD",
                "combined with a corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "bootstrapping process",
                "combined",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3 Online Learning Again following  , we have used the single best MIRA  , which is a variant of the voted perceptron   for structured prediction.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MIRA",
                "variant of the voted perceptron",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "best MIRA",
                "single best",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "he above observations can be stated formally from the perspective of Brown et al.'s   Model 2",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Model 2",
                "Brown et al.'s Model 2",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "perspective",
                "from the perspective",
                "INNOVATION",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "An example set of tags can be found in the Penn Treebank project  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank project",
                "example set of tags can be found in",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "tags",
                "example set",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Snow etal   use known hypernym/hyponym pairs to generate training data for a machine-learning system, which then learns many lexico-syntactic patterns.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hypernym/hyponym pairs",
                "known",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "machine-learning system",
                "learns many lexico-syntactic patterns",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Much research has been carried out recently in this area  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "has been carried out recently",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "area",
                "in this area",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  introduce IBM Models 1-5 for alignment modelling; Vogel et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Models 1-5",
                "for alignment modelling",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Vogel et al",
                "no opinion term present",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Using an Maximum Entropy approach to POS tagging, Ratnaparkhi   reports a tagging accuracy of 96.6% on the Wall Street Journal.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy approach",
                "POS tagging",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tagging accuracy of 96.6%",
                "on the Wall Street Journal",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Coling 2008: Companion volume  Posters and Demonstrations, pages 103106 Manchester, August 2008 Range concatenation grammars for translation Anders Sgaard University of Potsdam soegaard@ling.uni-potsdam.de Abstract Positive and bottom-up non-erasing binary range concatenation grammars   with at most binary predicates  -BRCGs) is a O  time strict extension of inversion transduction grammars    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "range concatenation grammars",
                "positive and bottom-up non-erasing binary",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "BRCGs",
                "O time strict extension of inversion transduction grammars",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "And we consider that word pairs that have a small distance between vectors also have similar word neighboring characteristics    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word pairs",
                "have similar word neighboring characteristics",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "vectors",
                "small distance between",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "in   proposed a word similarity measure based on the distributio nal pattern of words which allows to construct a thesaurus using a parsed corpus",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word similarity measure",
                "based on the distributio nal pattern of words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parsed corpus",
                "used to construct a thesaurus",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Examples of such techniques are Markov Random Fields  , and boosting or perceptron approaches to reranking  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Markov Random Fields",
                "approaches to reranking",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "boosting or perceptron",
                "approaches to reranking",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "An alternative method we considered was to estimate certain conditional probabilities, similarly to the formula used in  : SW  log P  f f  = ~ log   P  f f  Here f  is   the probability that any given candidate phrase will be accepted by the spotter, and f  is the probability that this phrase is rejected, i.e., f  = l-f  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "formula",
                "similarly to the formula used in",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "probability",
                "the probability that any given candidate phrase will be accepted by the spotter",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora    , or sense-tagged seed examples  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bilingual corpora",
                "obtained from",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sense-tagged seed examples",
                "sense-tagged",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The measure simHinate is the same as the similarity measure proposed in  , except that it does not use dependency triples with negative mutual information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "simHinate",
                "same as the similarity measure",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dependency triples with negative mutual information",
                "does not use",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In addition to precision and recall, we also evaluate the Bleu score   changes before and after applying our measure word generation method to the SMT output.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bleu score",
                "changes before and after applying our measure word generation method",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "SMT output",
                "our measure word generation method",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "dwait Ratnaparkhi   estimates a probability distribution for tagging using a maximum entropy approach",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dwait Ratnaparkhi",
                "estimates a probability distribution",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "maximum entropy approach",
                "approach",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This includes the automatic generation of sense-tagged data using monosemous relatives  , automatically bootstrapped disambiguation patterns  , parallel texts as a way to point out word senses bearing different translations in a second language  , and the use of volunteer contributions over the Web  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "monosemous relatives",
                "automatic generation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "volunteer contributions",
                "over the Web",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  effectively utilized unlabeled data to improve parsing accuracy on the standard WSJ training set, but they used a two-stage parser comprised of Charniaks lexicalized probabilistic parser with n-best parsing and a discriminative reranking parser  , and thus it would be better categorized as co-training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "two-stage parser",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "parser",
                "better categorized as co-training",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "stance, the IBM models   can be improved by adding more context dependencies into the translation model using a ME framework rather than using only p   .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "adding more context dependencies",
                "METHODOLOGY",
                "positive",
                0.75
            ],
            [
                "IBM models",
                "using only p",
                "METHODOLOGY",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "For our out-of-domain training condition, the parser was trained on sections 2-21 of the Wall Street Journal   corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal corpus",
                "sections 2-21",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parser",
                "trained on",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.2 Results and Discussion The BLEU scores   for 10 direct translations and 4 sets of heuristic selections 4Admittedly, in typical instances of such chains, English would appear earlier.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU scores",
                "for 10 direct translations and 4 sets of heuristic selections",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "English",
                "would appear earlier",
                "APPLICABILITY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "6 Phrase Recognition with a Maximum Entropy Classifier For the candidates which are not filtered out in the above two phases, we perform classification with maximum entropy classifiers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Phrase Recognition",
                "with maximum entropy classifiers",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "candidates which are not filtered out",
                "above two phases",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Wu   used a binary bracketing ITG to segment a sen19 tence while simultaneously word-aligning it to its translation, but the model was trained heuristically with a fixed segmentation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "binary bracketing ITG",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "training",
                "heuristically with a fixed segmentation",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Classes can be induced directly from the corpus   or taken from a manually crafted taxonomy  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "directly induced",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "taxonomy",
                "manually crafted",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "u   provides anecdotal evidence that only incorrect alignments are eliminated by ITG constraints",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG constraints",
                "eliminate incorrect alignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "incorrect alignments",
                "are eliminated",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Although some early systems for web-page analysis induce rules at character-level   and DIPRE  ), most recent approaches for set expansion have used either tokenized and/or parsed free-text  , or have incorporated heuristics for exploiting HTML structures that are likely to encode lists and tables  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rules",
                "induce at character-level",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "approaches",
                "have incorporated heuristics",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The second alternative used BerkeleyAligner  , which shares information between the two alignment directions to improve alignment quality.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BerkeleyAligner",
                "improve alignment quality",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignment directions",
                "shares information",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ca   and Cucerzan  , in mining relationships between named entities, or in extracting useful facet terms from news articles  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "named entities",
                "mining relationships between",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "news articles",
                "extracting useful facet terms from",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This is similar to results in the literature  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "results",
                "similar to results in the literature",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The reader is referred to   for detailed information about phrase-based statistical machine translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine translation",
                "statistical",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Experimenting with other effective reranking algorithms, such as SVMs   and MaxEnt  , is also a direction of our future research.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SVMs",
                "effective reranking algorithms",
                "METHODOLOGY",
                "positive",
                0.75
            ],
            [
                "MaxEnt",
                "effective reranking algorithms",
                "METHODOLOGY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Experimental results were only reported for the METEOR metric  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "METEOR metric",
                "Experimental results were reported for",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "METEOR metric",
                "only reported",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For instance, about 38% of verbs in the training sections of the Penn Treebank     occur only once  the lexical properties of these verbs   cannot be represented accurately in a model trained exclusively on the Penn Treebank.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "verbs",
                "occur only once",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "lexical properties",
                "cannot be represented accurately",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Related Work WSD approaches can be classified as   knowledge-based approaches, which make use of linguistic knowledge, manually coded or extracted from lexical resources  ;   corpus-based approaches, which make use of shallow knowledge automatically acquired from corpus and statistical or machine learning algorithms to induce disambiguation models  ; and   hybrid approaches, which mix characteristics from the two other approaches to automatically acquire disambiguation models from corpus supported by linguistic knowledge  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "knowledge-based approaches",
                "make use of linguistic knowledge",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpus-based approaches",
                "automatically acquired from corpus and statistical or machine learning algorithms",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Binarizing the syntax trees for syntax-based machine translation is similar in spirit to generalizing parsing models via markovization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntax trees",
                "similar in spirit",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "markovization",
                "generalizing parsing models",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ollins   proposed a Perceptron like learning algorithm to solve sequence classification in the traditional left-to-right order",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Perceptron like learning algorithm",
                "solve sequence classification in the traditional left-to-right order",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "traditional left-to-right order",
                "traditional",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However, current sentence alignment models,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentence alignment models",
                "current",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "sentence alignment models",
                "current",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Adapting a vectorbased approach reported by Chu-Carroll and Carpenter  , the Task ID Frame Agent is domain-independent and automatically trained.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Task ID Frame Agent",
                "domain-independent",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "Task ID Frame Agent",
                "automatically trained",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For each feature function, there is a model parameter  i . The best word segmentation W * is determined by the decision rule as  = == M i ii W M W WSfWSScoreW 0 0 * ), ,,  Below we describe how to optimize  s. Our method is a discriminative approach inspired by the Minimum Error Rate Training method proposed in Och  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model parameter",
                "i",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Minimum Error Rate Training method",
                "proposed in Och",
                "INNOVATION",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "Finally, knowledge of polarity can be combined with corpus-based collocation extraction methods   to automatically produce entries for the lexical functions used in MeaningText Theory   for text generation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus-based collocation extraction methods",
                "automatically produce entries",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "lexical functions used in MeaningText Theory",
                "used in",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In the geometric interpolation above, the weight n controls the relative veto power of the n-gram approximation and can be tuned using MERT   or a minimum risk procedure  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weight n",
                "controls the relative veto power",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "MERT or a minimum risk procedure",
                "can be tuned",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For Japanese sentences, instead of using full parse trees, existing sentence compression methods trim dependency trees by the discriminative model   through the use of simple linear combined features  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency trees",
                "trim",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "simple linear combined features",
                "use",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": " , Johnson  --that conditioning the probabilities of structures on the context within which they appear, for example on the lexical head of a constituent  , on the label of its parent nonterrninal  , or, ideally, on both and many other things besides, leads to a much better parsing model and results in higher parsing accuracies.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "leads to a much better parsing model",
                "METHODOLOGY",
                "positive",
                0.85
            ],
            [
                "results",
                "results in higher parsing accuracies",
                "PERFORMANCE",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "We use the likelihood ratio for a binomial distribution  , which tests the hypothesis whether the term occurs independently in texts of biographical nature given a large corpus of biographical and non-biographical texts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "likelihood ratio",
                "for a binomial distribution",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "term occurs independently",
                "in texts of biographical nature",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this years shared task we evaluated a number of different automatic metrics:  Bleu  Bleu remains the de facto standard in machine translation evaluation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bleu",
                "de facto standard",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "Bleu",
                "machine translation evaluation",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "albot and Brants   used a Bloomier filter to encode a LM",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Bloomier filter",
                "encode a LM",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "LM",
                "used in encoding",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Automatic Creation of WIDL-expressions for MT. We generate WIDL-expressions from Chinese strings by exploiting a phrase-based translation table  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WIDL-expressions",
                "phrase-based translation table",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Chinese strings",
                "exploiting",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Data 2.1 The US Congressional Speech Corpus The text used in the experiments is from the United States Congressional Speech corpus  , which is an XML formatted version of the electronic United States Congressional Record from the Library of Congress1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "United States Congressional Speech corpus",
                "is an XML formatted version",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Library of Congress",
                "from the",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Intuitively speaking, the gaps on the target-side will lead to exponential complexity in decoding with integrated language models  , as well as synchronous parsing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "target-side",
                "gaps will lead to exponential complexity",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "integrated language models",
                "as well as synchronous parsing",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In  , the authors use the transcripts of debates from the US Congress to automatically classify speeches as supporting or opposing a given topic by taking advantage of the voting records of the speakers.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transcripts of debates from the US Congress",
                "to automatically classify speeches",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "voting records of the speakers",
                "supporting or opposing a given topic",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In previous alignment methods, some researchers modeled the alignments with different statistical models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "different statistical models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "alignments",
                "modeled with",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In natural language processing, recent years have seen ME techniques used for sentence boundary detection, part of speech tagging, parse selection and ambiguity resolution, and stochastic attribute-value grammars, to name just a few applications  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ME techniques",
                "used for",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ME techniques",
                "name just a few applications",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "2.3 Perceptron Learning As learning algorithm, we use Perceptron tailored for structured scenarios, proposed by Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Perceptron",
                "tailored for structured scenarios",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Perceptron",
                "proposed by Collins",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our evaluation metrics is casesensitive BLEU-4  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation metrics",
                "casesensitive BLEU-4",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "BLEU-4",
                "casesensitive",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "6.2 Experimental Settings We utilize a maximum entropy   model   to design the basic classifier for WSD and TC tasks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "maximum entropy model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "classifier",
                "basic classifier",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Starting from the list of 12 ambiguous words provided by Yarowsky   which is shown in table 2, we created a concordance for each word, with the lines in the concordances each relating to a context window of 20 words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Yarowsky",
                "provided",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "context window",
                "20 words",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We do not use particular lexicosyntactic patterns, as previous attempts have  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicosyntactic patterns",
                "previous attempts have",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "lexicosyntactic patterns",
                "do not use",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "corpus  , the Penn Treebank  , the SUSANNE corpus  , the Spoken English Corpus  , the Oxford Psycholinguistic Database  , and the \"\"Computer-Usable\"\" version of the Oxford Advanced Learner's Dictionary of Current English  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "Penn Treebank, SUSANNE corpus, Spoken English Corpus, Oxford Psycholinguistic Database, Computer-Usable version of the Oxford Advanced Learner's Dictionary of Current English",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "corpus",
                "Penn Treebank, SUSANNE corpus, Spoken English Corpus, Oxford Psycholinguistic Database, Computer-Usable version of the Oxford Advanced Learner's Dictionary of Current English",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text  , sentence   or word   level.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "at the text, sentence, or word level",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "body of research",
                "large and diverse",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The feature weights are learned by maximizing the BLEU score   on held-out data,usingminimum-error-ratetraining  as implemented by Koehn.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "learned by maximizing the BLEU score",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "minimum-error-rate training",
                "as implemented by Koehn",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Their weights are optimized w.r.t. BLEU score using the algorithm described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights",
                "optimized w.r.t. BLEU score",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "algorithm",
                "described in  ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Unlexicalized parsers, on the other hand, achieved accuracies almost equivalent to those of lexicalized parsers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsers",
                "accuracies almost equivalent",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "parsers",
                "almost equivalent",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "One is to use a stochastic gradient descent   or Perceptron like online learning algorithm to optimize the weights of these features directly for MT  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "optimize the weights",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "weights",
                "directly",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We can incorporate each model into the system in turn, and rank the results on a test corpus using BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "into the system in turn",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "results",
                "on a test corpus using BLEU",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Attempts to alleviate this tagbottleneck i~lude tmotstr~ias   and unsupervised algorith~   Dictionary-based approaches rely on linguistic knowledge sources such as ma~l~i,~e-readable dictionaries   and WordNet   and e0(ploit these for word sense disaznbiguation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Dictionary-based approaches",
                "rely on linguistic knowledge sources",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "WordNet",
                "e0(ploit these for word sense disambiguation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Probabilities based on relative frequencies, or derived fl'om the measure defined in  , for example, allow to take this fact into account.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilities",
                "allow to take this fact into account",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "measure defined",
                "derived from",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Identifying transliteration pairs is an important component in many linguistic applications which require identifying out-of-vocabulary words, such as machine translation and multilingual information retrieval  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "component",
                "important component",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "machine translation",
                "require identifying out-of-vocabulary words",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "1 Introduction Syntax-based translation models   are usually built directly from Penn Treebank     style parse trees by composing treebank grammar rules.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treebank grammar rules",
                "are usually built directly from",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank style parse trees",
                "by composing",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Therefore, we also carried out evaluations using the NIST  , METEOR  , WER  , PER   and TER   machine translation evaluation techniques.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine translation evaluation techniques",
                "NIST, METEOR, WER, PER, TER",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "machine translation evaluation techniques",
                "evaluation techniques",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Furthermore, I based training on maximizing the conditional probability of a parse tree given a sentence, unlike most previous generative models  , which focus on maximizing the joint probability of the parse tree and the sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generative models",
                "focus on maximizing joint probability",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parse tree",
                "given a sentence",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 IBM Model 4 Various statistical alignment models of the form Pr  have been introduced in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 4",
                "Various statistical alignment models",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Pr",
                "introduced in",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "It is a fundamental and often a necessary step before linguistic knowledge acquisitions, such as training a phrase translation table in phrasal machine translation   system  , or extracting hierarchial phrase rules or synchronized grammars in syntax-based translation framework.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase translation table",
                "necessary step",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "syntax-based translation framework",
                "synchronized grammars",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A variety of algorithms  , co-training  , alternating structure optimization  , etc).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "variety of",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "co-training",
                "",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alternating structure optimization",
                "",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Since one of these filters restricts the number of nonterminal symbols to two, our extracted grammar is equivalent to an inversion transduction grammar  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "nonterminal symbols",
                "restricts the number of",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "grammar",
                "equivalent to an inversion transduction grammar",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This is confirmed by a comparison between our baseline result   and some baseline results of English base-NP chunking task  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "baseline result",
                "of English base-NP chunking task",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "baseline results",
                "of English base-NP chunking task",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "larke and Lapata   included discourse level features in their framework to leverage context for enhancing coherence",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "framework",
                "leverage context for enhancing coherence",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "features",
                "discourse level",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The idea of topic signature terms was introduced by Lin and Hovy   in the context of single document summarization, and was later used in several multi-document summarization systems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "topic signature terms",
                "was introduced",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "single document summarization",
                "in the context of",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "While   showed that this technique was effective when testing on WSJ, the true distribution was closer to WSJ so it made sense to emphasize it.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "technique",
                "effective when testing on WSJ",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "true distribution",
                "closer to WSJ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Unlike previous annotations of sentiment or subjectivity  , which typically relied on binary 0/1 annotations, we decided to use a finer-grained scale, hence allowing the annotators to select different degrees of emotional load.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotations",
                "finer-grained scale",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "degrees of emotional load",
                "different",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "2 F 1 -score Maximization Training of LRM We first review the F 1 -score maximization training method for linear models using a logistic function described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LRM",
                "F 1 -score maximization training method",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "logistic function",
                "described in  ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "CRP-based samplers have served the communitywellinrelatedlanguagetasks,suchaswordsegmentation and coreference resolution  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CRP-based samplers",
                "have served the community well",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "related language tasks",
                "such as word segmentation and coreference resolution",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In showing how DLTAG and an interpretative process on its derivations operate, we must, of necessity, gloss over how inference triggered by adjacency or associated with a structural connective provides the intended relation between adjacent discourse 578 Computational Linguistics Volume 29, Number 4 units: It may be a matter simply of statistical inference, as in Marcu and Echihabi  , or of more complex inference, as in Hobbs et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "inference",
                "triggered by adjacency or associated with a structural connective",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "inference",
                "may be a matter of statistical inference or of more complex inference",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "By introducing the hidden word alignment variable a   , the optimal translation can be searched for based on the following criterion: * 1 , arg max ) M mm m ea eh = = efa               where  is a string of phrases in the target language, e f  fa    is the source language string of phrases,  he  are feature functions, weights   m m  are typically optimized to maximize the scoring function  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation",
                "optimal",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "scoring function",
                "maximize",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A reranking parser  ) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses  , and the second layer is a reranker that reorders these parses using more detailed features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "layered model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parses",
                "reorders using more detailed features",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "This process is repeated for a number of iterations in a self-training fashion  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "iterations",
                "repeated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "fashion",
                "self-training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 Maximum Entropy ME models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence, but otherwise is as uniform as possible  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ME models",
                "consistent with the set of constraints imposed by the evidence",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "models",
                "as uniform as possible",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It assumes that the distance of the positions relative to the diagonal of the   plane is the dominating factor: r  p  =  , Ei,=l r  As described in  , the EM algorithm can be used to estimate the parameters of the model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distance",
                "dominating factor",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "EM algorithm",
                "used to estimate parameters",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Many authors claim that class-based methods are more robust against data sparseness problems  ,  ,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "class-based methods",
                "more robust against data sparseness problems",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "This corpus contains annotations of semantic PASs superimposed on the Penn Treebank    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PASs",
                "superimposed on the Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpus",
                "contains annotations of semantic",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We have computed the BLEU score    , the NIST score    , the General Text Matching   F-measure    , and the METEOR measure  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "computed",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "METEOR measure",
                "computed",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Precursors to this work include  ,  ,  ,  , and   and, as applied to child language acquisition,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "precursors",
                "work",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "child language acquisition",
                "as applied",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Expectation Maximization does surprisingly well on larger data sets and is competitive with the Bayesian estimators at least in terms of cross-validation accuracy, confirming the results reported by Johnson  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Expectation Maximization",
                "does surprisingly well",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "Expectation Maximization",
                "is competitive with Bayesian estimators",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Similar to Goldwater and Griffiths   and Johnson  , Toutanova and Johnson   also use Bayesian inference for POS tagging.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Goldwater and Griffiths",
                "use Bayesian inference",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Toutanova and Johnson",
                "also use Bayesian inference",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction ROUGE   and its linguisticallymotivated descendent, Basic Elements    , evaluate a summary by computing its overlap with a set of model   summaries; ROUGE considers lexical n-grams as the unit for comparing the overlap between summaries, while Basic Elements uses larger units of comparison based on the output of syntactic parsers.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE",
                "computing its overlap",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "Basic Elements",
                "uses larger units of comparison",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  and Chiang  , in terms of what alignments they induce, has been discussed in Wu   and Wellington et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wu and Wellington et al.",
                "has been discussed in",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "alignments",
                "they induce",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Previous research in automatic acquisition focuses primarily on the use of statistical techniques, such as bilingual alignment  , or extraction of syntactic constructions from online dictionaries and corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical techniques",
                "use of",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "bilingual alignment",
                "or extraction of syntactic constructions from online dictionaries and corpora",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Minimum error-rate   training   was applied to obtain weights   for these features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "obtained weights",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "error-rate",
                "minimum",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We take the generator of   as our baseline generator.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generator",
                "our baseline generator",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "generator",
                "our",
                "INNOVATION",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "2.2 Statistical Approaches with a grmnnmr There have been nlally l)rOl)osals tbr statistical t'rameworks particularly designed tbr 1)arsers with hand-crafted grmnmars  es, 1992; Briscoe and Carroll, 1993; Abney, 1996; Inui et al. , 1!)97).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical approaches",
                "particularly designed",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "hand-crafted grammars",
                "1992; Briscoe and Carroll, 1993; Abney, 1996; Inui et al., 1997",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Unsupervised Learning: Results To test the effectiveness of the above unsupervised learning algorithm, we ran a number of experiments using two different corpora and part of speech tag sets: the Penn Treebank Wall Street Journal Corpus \\ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "Penn Treebank Wall Street Journal Corpus",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "part of speech tag sets",
                "two different",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Following Smith and Eisner  , we adopt the view that the syntactic structure of sentences paraphrasing some sentence s should be inspired by the structure of s. Because dependency syntax is still only a crude approximation to semantic structure, we augment the model with a lexical semantics component, based on WordNet  , that models how words are probabilistically altered in generating a paraphrase.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency syntax",
                "crude approximation",
                "METHODOLOGY",
                "neutral",
                0.75
            ],
            [
                "lexical semantics component",
                "probabilistically altered",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Carletta   cites the convention from the domain of content analysis indicating that .67 K K < .8 indicates marginal agreement, while K > .8 is an indication of good agreement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "K",
                "K <.8",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "K",
                "K >.8",
                "PERFORMANCE",
                "positive",
                1.0
            ]
        ]
    },
    {
        "text": "Strube and Ponzetto explored the use of Wikipedia for measuring Semantic Relatedness between two concepts  , and for Coreference Resolution  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wikipedia",
                "for measuring Semantic Relatedness and Coreference Resolution",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Semantic Relatedness",
                "between two concepts",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "For the classifier, we used the OpenNLP MaxEnt implementation   of the maximum entropy classification algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "OpenNLP MaxEnt implementation",
                "maximum entropy classification algorithm",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "maximum entropy classification algorithm",
                "implementation",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "For our studies here, the parser employed was that of Collins   applied to the sentences of the British National Corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "of Collins",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "British National Corpus",
                "sentences of",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "u   adopted chammls that eliminate syntactically unlikely alignments and Wang et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chammls",
                "eliminate syntactically unlikely alignments",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Wang et al",
                "novel approach",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "History-based models for predicting the next parser action   3.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser action",
                "predicting the next",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "history-based models",
                "for predicting",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In  , as well as other similar works  , only left-toright search was employed.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "search",
                "left-to-right",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "works",
                "similar",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "O'Hara and Wiebe   make use of Penn Treebank   and FrameNet   to classify prepositions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "use",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "FrameNet",
                "use",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Table 6: Lexicalized Features for Joint Models aging of the weights suggested by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "joint models",
                "aging of the weights",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "weights",
                "suggested by",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "One is the longest common subsequence   based approach  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "longest common subsequence based",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "approach",
                "based",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In  , a small set of sample results are presented.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sample results",
                "are presented",
                "PERFORMANCE",
                "neutral",
                0.5
            ],
            [
                "sample results",
                "small set",
                "PERFORMANCE",
                "neutral",
                0.5
            ]
        ]
    },
    {
        "text": "The notation will assume ChineseEnglish word alignment and ChineseEnglish MT. Here we adopt a notation similar to  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "notation",
                "assuming ChineseEnglish word alignment and ChineseEnglish MT",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "notation",
                "similar to ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For every class the weights of the active features are combined and the best scoring class is chosen  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights",
                "combined",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "scoring class",
                "best",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction A \"\"pain in the neck\"\"   for NLP in languages of the Indo-Aryan family   is the fact that most verbs   occur as complex predicates multi-word complexes which function as a single verbal unit in terms of argument and event structure  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NLP in languages of the Indo-Aryan family",
                "pain in the neck",
                "APPLICABILITY",
                "negative",
                0.8
            ],
            [
                "verbs",
                "occur as complex predicates multi-word complexes",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Two main extensions from that work that we are making use of are: 1) proofs falling below a user defined cost threshold halt the search 2) a simple variable typing system reduces the number of axioms written and the size of the search space  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "proofs",
                "falling below a user defined cost threshold halt the search",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "variable typing system",
                "reduces the number of axioms written and the size of the search space",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "6.3 Unsupervised sentiment classification Turney proposed the unsupervised method for sentiment classification  , and similar method is utilized by many other researchers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "unsupervised",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "method",
                "similar",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "3 Haghighi and Kleins Coreference Model To gauge the performance of our model, we compare it with a Bayesian model for unsupervised coreference resolution that was recently proposed by Haghighi and Klein  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Haghighi and Klein's Coreference Model",
                "was recently proposed",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Bayesian model",
                "for unsupervised coreference resolution",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Disperp and Distortion Corpora 2.1 Defining Disperp The ultimate reason for choosing one SCM over another will be the performance of an MT system containing it, as measured by a metric like BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SCM",
                "choosing one over another",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "performance of an MT system",
                "measured by a metric like BLEU",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Previous studies   defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "log-linear model or maximum entropy model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "HPSG",
                "as a log-linear model or maximum entropy model",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The model weights are trained using the standard ranking perceptron  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model weights",
                "standard ranking perceptron",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "standard ranking perceptron",
                "standard",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "However, in experiments in unsupervised POS tag learning using HMM structured models, Johnson   shows that VB is more effective than Gibbs sampling in approaching distributions that agree with the Zipfs law, which is prominent in natural languages.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "VB",
                "more effective",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "Gibbs sampling",
                "less effective",
                "PERFORMANCE",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "2 Related work Our approach for emotion classification is based on the idea of   and is similar to those of   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "based on the idea of and is similar to those of and  ",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "idea",
                "similar to those of and  ",
                "INNOVATION",
                "neutral",
                0.75
            ]
        ]
    },
    {
        "text": "We have implemented a parallel version of our GIS code using the MPICH library  , an open-source implementation of the Message Passing Interface   standard.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MPICH library",
                "open-source implementation of the Message Passing Interface standard",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "GIS code",
                "parallel version",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "hurch and Hanks   employed mutual information to extract both adjacent and distant bi-grams that tend to co-occur within a fixed-size window",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information",
                "extract both adjacent and distant bi-grams",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "fixed-size window",
                "within",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2.1 Data and Semantic Role Annotation Proposition Bank   adds Levins style predicate-argument annotation and indication of verbs alternations to the syntactic structures of the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "predicate-argument annotation",
                "adds Levins style",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank",
                "indication of verbs alternations to the syntactic structures",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Under a phrase based translation model  , this distinction is important and will be discussed in more detail.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase based translation model",
                "important",
                "INNOVATION",
                "positive",
                0.6
            ],
            [
                "distinction",
                "will be discussed in more detail",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The structure of the graphical model resembles IBM Model 1   in which each target   word is assigned one or more source   words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 1",
                "resembles",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "source words",
                "assigned",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "mith and Eisner   used a quasisynchronous grammar to discover the correspondence between words implied by the correspondence between the trees",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "quasisynchronous grammar",
                "discover the correspondence",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "correspondence between trees",
                "implied by",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  used patterns representing part-of-speech sequences,   recognized adjectival phrases, and   learned N-grams.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "patterns",
                "representing part-of-speech sequences",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "N-grams",
                "learned",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Related Work The issue of MWE processing has attracted much attention from the Natural Language Processing   community, including Smadja, 1993; Dagan and Church, 1994; Daille, 1995; 1995; McEnery et al. , 1997; Wu, 1997; Michiels and Dufour, 1998; Maynard and Ananiadou, 2000; Merkel and Andersson, 2000; Piao and McEnery, 2001; Sag et al. , 2001; Tanaka and Baldwin, 2003; Dias, 2003; Baldwin et al. , 2003; Nivre and Nilsson, 2004 Pereira et al,.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MWE processing",
                "has attracted much attention",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Natural Language Processing community",
                "including",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "C3BTC5 and CCCDCA were used in   and  , respectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "C3BTC5",
                "used",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "CCCDCA",
                "used",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "We automatically measure performance by comparing the produced headlines against one reference headline produced by a human using ROUGEa129  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE",
                "reference headline",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "headlines",
                "produced by a human",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "The part of speech tags for the development and test data were automatically assigned by MXPOST  , where the tagger was trained on the entire training corpus; to generate part of speech tags for the training data, we used 10-way jackknifing.8 English word clusters were derived from the BLLIP corpus  , which contains roughly 43 million words of Wall Street Journal text.9 The Czech experiments were performed on the Prague Dependency Treebank 1.0  , which is directly annotated with dependency structures.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MXPOST",
                "trained on the entire training corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLLIP corpus",
                "contains roughly 43 million words of Wall Street Journal text",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Chu-Carroll and Carpenter   describe a method of disambiguation, where disambiguation questions are dynamically constructed on the basis of an analysis of the differences among the closest routing destination vectors.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "disambiguation questions",
                "are dynamically constructed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "routing destination vectors",
                "analysis of the differences among",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The dataset is available only in English and has been widely used in previous semantic relatedness evaluations  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dataset",
                "has been widely used",
                "APPLICABILITY",
                "positive",
                0.8
            ],
            [
                "dataset",
                "available only in English",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "First, the Wikipedia gazetteer improved the accuracy as expected, i.e., it reproduced the result of Kazama and Torisawa   for Japanese NER.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wikipedia gazetteer",
                "improved the accuracy",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "Kazama and Torisawa",
                "result",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Several authors have used mutual information and similar statistics as an objective function for word clustering  , for automatic determination of phonemic baseforms  , and for language modeling for speech recognition  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information",
                "as an objective function",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word clustering",
                "automatic determination of phonemic baseforms",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Other scores for the word arc are set as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word arc",
                "set as in",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "scores",
                "other scores",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "It has been further observed that simply compressing sentences individually and concatenating the results leads to suboptimal summaries  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "summaries",
                "suboptimal",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "results",
                "leads to",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "joint likelihood   productdisplay i p parenleftBig xi,yi | vector parenrightBig conditional likelihood   productdisplay i p parenleftBig yi | xi,vector parenrightBig classification accuracy   summationdisplay i  ) expected classification accuracy   summationdisplay i p parenleftBig yi | xi,vector parenrightBig negated boosting loss    summationdisplay i p parenleftBig yi | xi,vector parenrightBig1 margin    s.t. bardbl vectorbardbl  1;i,y negationslash= yi, vector     vectorf )   expected local accuracy   productdisplay i productdisplay j p parenleftBig lscriptj  = lscriptj  | xi,vector parenrightBig Table 1: Various supervised training criteria.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "joint likelihood",
                "productdisplay",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "classification accuracy",
                "summationdisplay",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  report extracting database records by learning record field compatibility.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "database records",
                "learning record field compatibility",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "record field compatibility",
                "is used to extract",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "lmost all recent work in developing automatically trained part-of-speech taggers has been on further exploring Markovmodel based tagging  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "Markov-model based",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "work",
                "further exploring",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this paper, translation quality is evaluated according to   the BLEU metrics which calculates the geometric mean of ngram precision by the system output with respect to reference translations  , and   the METEOR metrics that calculates unigram overlaps between translations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metrics",
                "calculates geometric mean of ngram precision",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "METEOR metrics",
                "calculates unigram overlaps",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction The use of various synchronous grammar based formalisms has been a trend for statistical machine translation    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "formalisms",
                "various synchronous grammar based",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "statistical machine translation",
                "has been a trend",
                "INNOVATION",
                "neutral",
                0.75
            ]
        ]
    },
    {
        "text": "1 Introduction Sentence-aligned parallel bilingual corpora have been essential resources for statistical machine translation  , and many other multi-lingual natural language processing applications.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "essential resources",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "corpora",
                "statistical machine translation",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  and Pang and Lee   in merely using binary unigram features, corresponding to the 17,744 unstemmed word or punctuation types with count  4 in the full 2000-document corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "2000-document corpus",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "features",
                "binary unigram features",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For regularization purposes we adopt an average perceptron   which returns for each y, y = 1T summationtextTt=1 ty, the average of all weight vectors ty posited during training.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "average perceptron",
                "returns the average of all weight vectors",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "weight vectors",
                "posited during training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.1 Corpora Sentence compression systems have been tested on product review data from the Ziff-Davis   Corpus by Knight and Marcu  , general news articles by Clarke and Lapata   corpus   and biomedical articles  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Corpora",
                "from the Ziff-Davis Corpus",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "Corpora",
                "general news articles by Clarke and Lapata corpus",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "6 Related Work Several works attempt to extend WordNet with additional lexical semantic information  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet",
                "extend with additional lexical semantic information",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "several works",
                "attempt to",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This tagging scheme is the IOB scheme originally put forward by Ramshaw and Marcus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IOB scheme",
                "originally put forward by Ramshaw and Marcus",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "IOB scheme",
                "originally put forward",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "It has been used for a variety of tasks, such as wide-coverage parsing  , sentence realization  , learning semantic parsers  , dialog systems  , grammar engineering  , and modeling syntactic priming  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsing",
                "wide-coverage parsing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parsers",
                "semantic parsers",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "1.2.2 SPECIFIC SYNTACTIC AND SEMANTIC ASSUMPTIONS The basic scheme, or some not too distant relative, is the one used in many large-scale implemented systems; as examples, we can quote TEAM  , PUNDIT  , TACITUS  , MODL  , CLE  , and SNACK-85  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "basic scheme",
                "the one used in many large-scale implemented systems",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "TEAM",
                "quote as example",
                "APPLICABILITY",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "We carefully implemented the original Grammar Association system described in  , tuned empirically a couple of smoothing parameters, trained the models and, finally, obtained an a119a21a120 a100 a104a122a121 of correct translations.9 Then, we studied the impact of:   sorting, as proposed in Section 3, the set of sentences presented to ECGI;   making language models deterministic and minimum;   constraining the best translation search to those sentences whose lengths have been seen, in the training set, related to the length of the input sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Grammar Association system",
                "original",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "models",
                "deterministic and minimum",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4.2 Data The data comes from the CoNLL 2000 shared task  , which consists of sentences from the Penn Treebank Wall Street Journal corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CoNLL 2000 shared task",
                "consists of",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank Wall Street Journal corpus",
                "sentences from",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Inspired by the idea of graph based algorithms to collectively rank and select the best candidate, research efforts in the natural language community have applied graph-based approaches on keyword selection  , text summarization  , word sense disambiguation  , sentiment analysis  , and sentence retrieval for question answering  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "graph-based algorithms",
                "collectively rank and select the best candidate",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "research efforts",
                "applied graph-based approaches",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The simplest version, called Dependency Model with Valence  , has been used in isolation and in combination with other models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Dependency Model with Valence",
                "simplest version",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Dependency Model with Valence",
                "used in isolation and in combination with other models",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We thus propose to adapt the statistical machine translation model   for SMS text normalization.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical machine translation model",
                "adaptation for SMS text normalization",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "SMS text normalization",
                "proposed adaptation",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction In phrase-based statistical machine translation   phrases extracted from word-aligned parallel data are the fundamental unit of translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrases",
                "fundamental unit of translation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase-based statistical machine translation",
                "statistical machine translation",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Data Data for 64 verbs   was collected from three corpora; The British National Corpus    , the Penn Treehank parsed version of the Brown Corpus  , and the Penn Treebank Wall Street Journal corpas    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "British National Corpus, Penn Treebank parsed version of the Brown Corpus, and the Penn Treebank Wall Street Journal corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "verbs",
                "64 verbs",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Use of probability estimates is not a serious limitation of this approach because in practice candidates are normally provided by some probabilistic model and its probability estimates are used as additional features in the reranker  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "not a serious limitation",
                "LIMITATION",
                "negative",
                0.8
            ],
            [
                "probability estimates",
                "used as additional features",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Incremental Parsing This section gives a description of Collins and Roarks incremental parser   and discusses its problem.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins and Roark's incremental parser",
                "description",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "problem",
                "discusses its",
                "LIMITATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, researchers   have identified semantic correlation between words and views: positive words tend to appear more frequently in positive movie and product reviews and newswire article sentences that have a positive semantic orientation and vice versa for negative reviews or sentences with a negative semantic orientation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "positive words tend to appear more frequently",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "semantic orientation",
                "positive semantic orientation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Previous work   has shown it to be appropriate to large-scale language modeling.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "language modeling",
                "appropriate",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "language modeling",
                "large-scale",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "One other published model for grouping semantically related words  , is based on a statistical model of bigrams and trigrams and produces word groups using no linguistic knowledge, but no evaluation of the results is reported.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "based on a statistical model of bigrams and trigrams",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "results",
                "no evaluation of the results is reported",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "1 Introduction Despite a surge in research using parallel corpora for various machine translation tasks  ,(Brown et al. 1991; Gale & Church 1993; Church 1993; Dagan & Church 1994; Simard et al. 1992; Chen 1993; Melamed 1995; Wu & Xia 1994; Wu 1994; Smadja et aI.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "using parallel corpora for various machine translation tasks",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "tasks",
                "various machine translation",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction Several approaches including statistical techniques  , lexical techniques   and hybrid techniques  , have been pursued to design schemes for word alignment which aims at establishing links between words of a source language and a target language in a parallel corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "statistical techniques, lexical techniques, hybrid techniques",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word alignment",
                "establishing links between words",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Next we use the conclusions from two psycholinguistic experiments on ranking the Cf-list, the salience of discourse entities in prepended phrases   and the ordering of possessor and possessed in complex NPs  , to try to improve the performance of LRC.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "conclusions",
                "from psycholinguistic experiments",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "performance of LRC",
                "to try to improve",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "5 Evaluation 5.1 Datasets We used two datasets, customer reviews 1   and movie reviews 2   to evaluate sentiment classification of sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "datasets",
                "customer reviews and movie reviews",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentiment classification",
                "of sentences",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We automatically converted the phrase structure output of the Collins parser into the syntactic dependency representation used by our syntactic realizer, RealPro  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins parser",
                "phrase structure output",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "RealPro",
                "syntactic realizer",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "When alignment quality stops increasing on the discriminative training set, perceptron training ends.10 The weight vector returned by perceptron training is the average over the training set of all weight vectors seen during all iterations; averaging reduces overfitting on the training set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron training",
                "stops increasing on the discriminative training set",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "weight vector",
                "averaging reduces overfitting on the training set",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "n   associations are detected in a 5 window",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "associations",
                "are detected in a 5 window",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "window",
                "5",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We employ the phrase-based SMT framework  , and use the Moses toolkit  , and the SRILM language modelling toolkit  , and evaluate our decoded translations using the BLEU measure  , using a single reference translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based SMT framework",
                "METHODOLOGY",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU measure",
                "used to evaluate translations",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stanford Named Entity Recognizer",
                "named entity recognition",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "analysis of English",
                "using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We used these weights in a beam search decoder to produce translations for the test sentences, which we compared to the WMT07 gold standard using Bleu  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights",
                "in a beam search decoder",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "WMT07 gold standard",
                "using Bleu",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Based on IBM Model 1 lexical parameters , providing a complementary probability for each tuple in the translation table.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 1 lexical parameters",
                "providing a complementary probability",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation table",
                "each tuple",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use the standard minimum error-rate training   to tune the feature weights to maximize the systems BLEU score on the dev set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "to maximize the systems BLEU score",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "minimum error-rate training",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "73 2.2.4 Minimum Error Rate Training A good way of training is to minimize empirical top-1 error on training data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training",
                "minimize empirical top-1 error on training data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "error rate",
                "minimize",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 MaxEnt Model and Features 3.1 MaxEnt Model for NOR The principle of maximum entropy   model is that given a collection of facts, choose a model consistent with all the facts, but otherwise as uniform as possible  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MaxEnt Model",
                "as uniform as possible",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "model",
                "consistent with all the facts",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Recognizing this, Dolan   proposes a method for \"\"ambiguating\"\" dictionary senses by combining them to create grosser sense distinctions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dictionary senses",
                "combining to create grosser sense distinctions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "method",
                "proposes",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.1 Model-based Phrase Pair Posterior In a statistical generative word alignment model  , it is assumed that   a random variable a specifies how each target word fj is generated by   a source 1 word eaj; and   the likelihood function f  specifies a generativeprocedurefromthesourcesentencetothe target sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generative word alignment model",
                "statistical",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "likelihood function",
                "generative procedure",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Training of the phrase translation model builds on top of a standard statistical word alignment over the training corpus of parallel text   for identifying corresponding word blocks, assuming no further linguistic analysis of the source or target language.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase translation model",
                "standard statistical word alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corresponding word blocks",
                "assuming no further linguistic analysis",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Examples of monolingual parallel corpora that have been used are multiple translations of classical French novels into English, and data created for machine translation evaluation methods such as Bleu   which use multiple reference translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classical French novels",
                "multiple translations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Bleu",
                "multiple reference translations",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Various machine learning approaches have been proposed for chunking  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning approaches",
                "have been proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "chunking",
                " Various machine learning approaches",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "5.1 The Prague Dependency Tree Bank  , which has been inspired by the build-up of the Penn Treebank  , is aimed at a complex annotation of   the Czech National Corpus  , the creation of which is under progress at the Department of Czech National Corpus at the Faculty of Philosophy, Charles University  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Prague Dependency Tree Bank",
                "has been inspired by the build-up of the Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Czech National Corpus",
                "the creation of which is under progress",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In the SUMMAC experiments, the Kappa score   for interannotator agreement was reported to be 0.38  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa score",
                "was reported to be 0.38",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "interannotator agreement",
                "0.38",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "azama and Torisawa   explores the first sentence of an article and identifies the first noun phrase following the verb be as a label for the article title",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "azama and Torisawa",
                "explores the first sentence of an article and identifies the first noun phrase following the verb be as a label for the article title",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "first noun phrase",
                "following the verb be",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We experimented with two independent, arguably complementary techniques for clustering and aligning  a predicate argument based approach that extracts more general templates containing one predicate and a ROUGE   based 265 approach that can extract templates containing multiple verbs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "predicate argument based approach",
                "extracts more general templates",
                "METHODOLOGY",
                "positive",
                0.75
            ],
            [
                "ROUGE based 265 approach",
                "can extract templates containing multiple verbs",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We follow   and approximate the metrics using the sigmoid function.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "using the sigmoid function",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "sigmoid function",
                "approximate",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "But because we want the insertion state a1a16a20 to model digressions or unseen topics, we take the novel step of forcing its language model to be complementary to those of the other states by setting a2 a3a27a38 a21 a8 a8 a4 a8 a24 a26a11a28a30a29a6 a39a41a40a43a42a45a44a16a46 a1a48a47a1a50a49 a20 a2 a3 a26a17a21 a8a9a8 a4 a8 a24 a51a53a52a55a54a57a56 a21 a39a58a40a43a42a45a44a16a46 a1a59a47a1a50a49 a20 a2 a3a27a26a11a21a50a60 a4 a8 a24a30a24 a17 4Following Barzilay and Lee  , proper names, numbers and dates are   replaced with generic tokens to help ensure that clusters contain sentences describing the same event type, rather than same actual event.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "insertion state",
                "to model digressions or unseen topics",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "proper names, numbers and dates",
                "are replaced with generic tokens",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For example, the constrained optimization method of   relies on approximations of sensitivity   and specificity2  ; related techniques   rely on approximations of true positives, false positives, and false negatives, and, indirectly, recall and precision.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "relies on approximations of sensitivity and specificity",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "techniques",
                "rely on approximations of true positives, false positives, and false negatives",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "POS disambiguation has usually been performed by statistical approaches, mainly using the hidden Markov model   in English research communities  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "statistical",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "hidden Markov model",
                "in English research communities",
                "APPLICABILITY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": " , Walker  , Fink and Biermann  , Mudler and Paulus  , Carbonell and Pierrel  , Young  , and Young et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Walker",
                "et al.",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Young et al.",
                "et al.",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "From   9 Combined metric BY BP B4AC BE B7BDB5C8CABPB4AC BE C8 B7 CAB5, from  , AC BPBD.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metric",
                "Combined",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Combined",
                "metric",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our evaluation metric is BLEU-4  , as calculated by the script mteval-v11b.pl with its default setting except that we used case-sensitive matching of n-grams.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU-4",
                "as calculated by the script mteval-v11b.pl with its default setting except that we used case-sensitive matching of n-grams",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "mteval-v11b.pl",
                "with its default setting",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our scores fall within the range of previous researchers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "scores",
                "fall within the range of previous researchers",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "researchers",
                "previous",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This could, for example, aid machine-translation evaluation, where it has become common to evaluate systems by comparing their output against a bank of several reference translations for the same sentences  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reference translations",
                "common to evaluate systems",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "machine-translation evaluation",
                "aid",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "2 Models, Search Spaces, and Errors A translation model consists of two distinct elements: an unweighted ruleset, and a parameterization  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "two distinct elements",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ruleset",
                "unweighted",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Statistical data about these various cooccurrence relations is employed for a variety of applications, such as speech recognition  , language generation  , lexicography  , machine translation  , information retrieval   and various disambiguation tasks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "cooccurrence relations",
                "employed for various applications",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "disambiguation tasks",
                "various",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Their systems output was an ordered list of possible parts according to some statistical metrics  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "This differs from typical generative settings for IR and MT  , where all conditioned events are disjoint by construction.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generative settings",
                "typical",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "events",
                "disjoint by construction",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  show that this model is a member of an exponential family with one parameter for each constraint, specifically a model of the form 1 ~ I~   p  = E' in which z  = eZ, Y The parameters A1,  , An are Lagrange multipliers that impose the constraints corresponding to the chosen features fl, -,fnThe term Z  normalizes the probabilities by summing over all possible outcomes y. Berger et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "a member of an exponential family",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parameters",
                "impose the constraints",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": ".2 Themaximumentropytagger The maximum entropy model used in POStagging is described in detail in Ratnaparkhi  andthePOCtaggerhereusesthesame probability model",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy model",
                "used in POStagging",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "probability model",
                "same as POCtagger",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This also makes our grammar weakly equivalent to an inversion transduction grammar  , although the conversion would create a very large number of new nonterminal symbols.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar",
                "weakly equivalent",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "nonterminal symbols",
                "very large number",
                "LIMITATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Yarowsky   tested the claim on about 37,000 examples and found that when a polysemous word appeared more than once in a discourse, they took on the majority sense for the discourse 99.8% of the time on average.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Yarowsky",
                "tested the claim on about 37,000 examples",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "polysemous word",
                "took on the majority sense for the discourse 99.8% of the time on average",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The following treebanks were used for training the parser:  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treebanks",
                "used for training the parser",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parser",
                "training",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "7 Related Work Much work on sentiment analysis classifies documents by their overall sentiment, for example determining whether a review is positive or negative  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentiment analysis",
                "classifies documents by their overall sentiment",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "documents",
                "positive or negative",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "  describes experiments on the same named-entity dataset as in this paper, but using explicit features rather than kernels.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "named-entity dataset",
                "same as in this paper",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "features",
                "explicit rather than kernels",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A structured perceptron   learns weights for our transliteration features, which are drawn from two broad classes: indicator and hybrid generative features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron",
                "learns weights",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "transliteration features",
                "drawn from two broad classes",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For this purpose, we adopt the view of the ITG constraints as a bilingual grammar as, e.g., in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG constraints",
                "bilingual grammar",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "view",
                "as a bilingual grammar",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This method has the advantage that it is not limited to the model scaling factors as the method described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "not limited to the model scaling factors",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "method described in ",
                "method",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Such metrics have been introduced in other fields, including PARADISE   for spoken dialogue systems, BLEU   for machine translation,1 and ROUGE   for summarisation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "introduced in other fields",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "PARADISE",
                "for spoken dialogue systems",
                "INNOVATION",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "Actually, it is defined similarly to the translation model in SMT  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "similarly defined",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translation model",
                "in SMT",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "hurch and Hanks   introduced a statistical measurement called mutual information for extracting strongly associated or collocated words",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information",
                "for extracting strongly associated or collocated words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical measurement",
                "called mutual information",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As an alternative, Huang and Chiang   describes a forest-based reranking algorithm called cube growing, which also employs beam search, but focuses computation only where necessary in a top-down pass through a parse forest.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "cube growing",
                "forest-based reranking algorithm",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "parse forest",
                "top-down pass through",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Below is an example of the initial-state tagging of a sentence from the Penn Treebank \\ , where an underscore is to be read as or.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "initial-state tagging",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence",
                "from",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For instance, implementing an efficient version of the MXPOST POS tagger   will simply involve composing and configuring the appropriate text file reading component, with the sequential tagging component, the collection of feature extraction components and the maximum entropy model component.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MXPOST POS tagger",
                "efficient version",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "text file reading component",
                "composing and configuring",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We tune using Ochs algorithm   to optimize weights for the distortion model, language model, phrase translation model and word penalty over the BLEU metric  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ochs algorithm",
                "to optimize weights",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU metric",
                "over the",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "e follow Collins   and Sha and Pereira   in using section 21 as a heldout set",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "heldout set",
                "as a heldout set",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Collins and Sha and Pereira",
                "using section 21",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "While the tag features, containing WSJ paxt-ofspeech tags  , have about 45 values, the word features have more than 10,000 values.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tag features",
                "containing WSJ part-of-speech tags",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word features",
                "have more than 10,000 values",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, in the experiments described here, we focus on alignment at the level of sentences, this for a number of reasons: First, sentence alignments have so far proven their usefulness in a number of applications, e.g. bilingual lexicography  , automatic translation verification   and the automatic acquisition of knowledge about translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentence alignments",
                "have so far proven their usefulness",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "sentence alignments",
                "in a number of applications",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.2 Building a Human Performance Model We adopt the evaluation approach that a good content selection strategy should perform similarly to humans, which is the view taken by existing summarization evaluation schemes such as ROUGE   and the Pyramid method  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation approach",
                "perform similarly to humans",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "evaluation schemes",
                "taken by existing summarization evaluation schemes",
                "INNOVATION",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "ch   found that such smoothing during training gives almost identical results on translation metrics",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "smoothing",
                "gives almost identical results",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "results",
                "on translation metrics",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In our decoder, we incorporate two pruning techniques described by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "two pruning techniques described by ",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "pruning techniques",
                "described by ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "s e, the window to consider when extracting words related to word w, should span from postttuon w-5 to w+5 Maarek also defines the resolwng power of a parr m a document d as P = ~'Pd log Pc where Pd is the observed probabshty of appearance of the pan\"\" m document d, Pc the observed probabdny of the pmr recorpus, and -log Pc the quantity of mformauon assocmted to the pmr It Is easdy seen that p wall be h|gher, the higher the frequency of the pmr m the document and the lower sts frequency m the corpus, which agrees wlth the sdea presented at the begmnmg of this sectton Church and Hanks   propose the apphcatlon of the concept of mutual mformatton e  ~,  = hog2 ecx)e  51 to the retrieval, ro a corpus, of pairs of lextcally related words They alsoconslder a word span of :e5 words and observe that \"\"roterestrog\"\" pmr, s generally present a mutual mformatxon above 3 Salton and.Allan   foc~as on paragraph level Each paragraph Is represented by a weighed vector, where each element is a term (typically.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "window",
                "span from w-5 to w+5",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "resolution power",
                "P = ~'Pd log Pc",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "NER is typically viewed as a sequential prediction problem, the typical models include HMM  , CRF  , and sequential application of Perceptron or Winnow  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "typical models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Perceptron or Winnow",
                "sequential application",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Klein and Manning   argue that these results show a pattern where discriminative probability models are inferior to generative probability models, but that improvements can be achieved by keeping a generative probability model and training according to a discriminative optimization criteria.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "discriminative probability models",
                "inferior",
                "PERFORMANCE",
                "negative",
                0.8
            ],
            [
                "generative probability models",
                "improvements can be achieved",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "Independently, in artificial intelligence an effort arose to encode large amounts of commonsense knowledge  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "artificial intelligence",
                "effort",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "commonsense knowledge",
                "large amounts",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Clark and Curran   describe the supertagger, which uses log-linear models to define a distribution over the lexical category set for each local five-word context containing the target word  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "supertagger",
                "uses log-linear models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "lexical category set",
                "for each local five-word context containing the target word",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Furthermore, our model is not necessarily nativist; these biases may be innate, but they may also be the product of some other earlier learning algorithm, as the results of Ellison   and Brown et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "not necessarily nativist",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "results",
                "of Ellison and Brown et al.",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Distance from a target word is used for this purpose and it is calculated by the assumption that the target words in the context window have the same sense  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "target word",
                "have the same sense",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "context window",
                "same sense",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5.2 Bleu: Automatic Evaluation BLEU   is a system for automatic evaluation of machine translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "cCarthy et al. use a distributional similarity thesaurus acquired from corpus data using the method of Lin   for nding the predominant sense of a word where the senses are dened by WordNet",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distributional similarity thesaurus",
                "acquired from corpus data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "method of Lin",
                "for finding the predominant sense of a word",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The use of Profile HMMs for multiple sequence alignment also presents applications to the acquisition of mapping dictionaries   and sentence-level paraphrasing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Profile HMMs",
                "for multiple sequence alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "mapping dictionaries and sentence-level paraphrasing",
                "applications",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The final model V uses the weight vector w = summationtextk j=1  Tn  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "V",
                "uses the weight vector",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "weight vector",
                "summationtextk j=1  Tn  ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The model scaling factors M1 are optimized with respect to the BLEU score as described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "M1",
                "optimized with respect to the BLEU score",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU score",
                "described in",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The surface heuristic can define consistency according to any word alignment; but most often, the alignment is provided by GIZA++  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment",
                "provided by GIZA++",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "consistency",
                "define according to any word alignment",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The weights are trained using minimum error rate training   with BLEU score as the objective function.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights",
                "trained using minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BLEU score",
                "as the objective function",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Because of these kinds of results, the vast majority of statistical parsing work has focused on parsing as a supervised learning problem  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical parsing work",
                "focused on parsing as a supervised learning problem",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "results",
                "kinds of results",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The third estimates the equivalence based on word alignment composed using templates or translation probabilities derived from a set of parallel text  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "templates or translation probabilities",
                "derived from a set of parallel text",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word alignment",
                "composed using",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "e propose a method similar to Yarowsky   to generalize beyond the training set",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "similar to Yarowsky",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "generalize beyond the training set",
                "novel aspect",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "The second model is a maximum entropy model  , since Klein and Manning   found that this model yielded higher accuracy than nave Bayes in a subsequent comparison of WSD performance.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "maximum entropy model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Bayes",
                "naive Bayes",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "They use a conditional model, based on Collins  , which, as the authors acknowledge, has a number of theoretical deficiencies; thus the results of Clark et al. provide a useful baseline for the new models presented here.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "conditional model",
                "based on Collins",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "results of Clark et al.",
                "provide a useful baseline",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Wu   has been unable to find real examples of cases where hierarchical alignment would fail under these conditions, at least in fixed-word-order languages that are lightly inflected, such as English and Chinese.  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "We use the by now standard a0 statistic   to quantify the degree of above-chance agreement between multiple annotators, and the a1 statistic for analysis of sources of unreliability  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "a0 statistic",
                "by now standard",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "a1 statistic",
                "for analysis of sources of unreliability",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To tune feature weights minimum error rate training is used  , optimized against the Neva metric  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "training",
                "optimized against the Neva metric",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For related work on the voted perceptron algorithm applied to NLP problems, see   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "voted perceptron algorithm",
                "applied to NLP problems",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "related work",
                "on NLP problems",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction Most   statistical machine translation systems employ a word-based alignment model  , which treats words in a sentence as independent entities and ignores the structural relationship among them.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar  , and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "string-based systems",
                "whose input is a string to be simultaneously parsed and translated",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "tree-based systems",
                "whose input is already a parse tree to be directly converted",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The judges had an acceptable 0.74 mean  agreement   for the assignment of the primary class, but a meaningless 0.21 for the secondary class  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mean agreement",
                "acceptable 0.74",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "mean agreement",
                "meaningless 0.21",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Sentence-level approximations to B exist  , but we found it most effective to perform B computations in the context of a setOof previously-translated sentences, following Watanabe et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "B computations",
                "most effective",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "B computations",
                "in the context of a set of previously-translated sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In designing LEAF, we were also inspired by dependency-based alignment models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency-based alignment models",
                "inspired by",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "LEAF",
                "designing",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "No artificial glue-rules or rule span limits were employed.7 The parameters of the translation system were trained to maximize BLEU on the MT02 test set  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "trained to maximize BLEU",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "translation system",
                "without artificial glue-rules or rule span limits",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "his further supports the claim by Dunning   that loglikelihood ratio is much less sensitive than pmi to low counts",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "loglikelihood ratio",
                "is much less sensitive",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "pmi",
                "is sensitive to low counts",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4 Relation to Previous Work There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CRFs",
                "use of CRFs for a variety of chunking tasks",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "chunking tasks",
                "including named-entity recognition, gene prediction, shallow parsing and others",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Although few corpora annotated with semantic knowledge are available now, there are some valuable lexical databases describing the lexical semantics in dictionary form, for example English WordNet   and Chinese HowNet  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "annotated with semantic knowledge",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "lexical databases",
                "valuable",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Motivation There have been quite a number of recent papers on parallel text: Brown et al  , Chen  , Church  , Church et al  , Dagan et al  , Gale and Church  , Isabelle  , Kay and Rgsenschein  , Klavans and Tzoukermann  , Kupiec  , Matsumoto  , Ogden and Gonzales  , Shemtov  , Simard et al  , WarwickArmstrong and Russell  , Wu  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "papers",
                "recent",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Church et al",
                "quite a number of",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "For a sequential learning algorithm, we make use of the Collins Perceptron Learner  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins Perceptron Learner",
                "sequential learning algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Collins Perceptron Learner",
                "Perceptron Learner",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In addition to reducing the original sentences, Jing and McKeown   use a number of manually compiled rules to aggregate reduced sentences; for example, reduced clauses might be conjoined with and.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reduced sentences",
                "manually compiled rules",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "reduced clauses",
                "conjoined with and",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "3 The Framework 3.1 The Algorithm Our transductive learning algorithm, Algorithm 1, is inspired by the Yarowsky algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "3 Tagging 3.1 Corpus To facilitate comparison with previous results, we used the UPenn Treebank corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "UPenn Treebank corpus",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "results",
                "previous results",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This leads to 49 methods that use semi-supervised techniques on a treebank-infered grammar backbone, such as  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "use semi-supervised techniques",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "treebank-infered grammar backbone",
                " ",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "As expected, as we double the size of the data, the BLEU score   increases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data",
                "increases",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "BLEU score",
                "as expected",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "he existing work most similar to ours is Collins and Roark  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins and Roark",
                "most similar to ours",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "Collins and Roark",
                "existing work",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "5 Data-driven Dependency Parsing Models for data-driven dependency parsing can be roughly divided into two paradigms: Graph-based and transition-based models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Data-driven Dependency Parsing Models",
                "can be roughly divided into two paradigms",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "Graph-based and transition-based models",
                "paradigms",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Translation performance was measured using the BLEU score  , which measures n-gram overlap with a reference translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "measures n-gram overlap with a reference translation",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "n-gram overlap",
                "with a reference translation",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The implementation of the algorithm is one that has a core of code that can run on either the Penn Treebank   or on the Chinese Treebank.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "can run on either the Penn Treebank or on the Chinese Treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "code",
                "has a core of",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "his implementation is exactly the one proposed in Yarowsky  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "implementation",
                "proposed in Yarowsky",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Yarowsky",
                "proposed",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "2 Phrasal Inversion Transduction Grammar We use a phrasal extension of Inversion Transduction Grammar   as the generative framework.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Phrasal Inversion Transduction Grammar",
                "as the generative framework",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrasal extension",
                "of Inversion Transduction Grammar",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The accuracy of the generator outputs was evaluated by the BLEU score  , which is commonly used for the evaluation of machine translation and recently used for the evaluation of generation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "is commonly used",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "BLEU score",
                "used for the evaluation",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Our system attempts to recognize these syntactic patterus; in addition, it considers as unfamiliar some definites occurring in 4This list was developed by hand; more recently, Bean and Riloff   proposed methods for autolnatically extracting fl'om a corpus such special predicates, i.e., heads that correlate well with discourse novelty.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "system",
                "attempts to recognize syntactic patterns",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "methods for automatically extracting special predicates",
                "correlate well with discourse novelty",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Second, in keeping with ontological promiscuity  , we represent the importance of attributes by the salience of events and states in the discourse model--these states and events now have the same status in the discourse model as any other entities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "attributes",
                "importance",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "states and events",
                "same status",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "234 ADV Non-specific adverbial BNF Benefemtive CLF It-cleft CLR 'Closely related' DIR Direction DTV Dative EXT Extent HLN Headline LGS Logical subject L0C Location MNI~ Manner N0M Nominal PRD Predicate PRP Purpose PUT Locative complement of 'put' SBJ Subject TMP Temporal TPC Topic TTL Title V0C Vocative Grammatical DTV 0.48% LGS 3.0% PRD 18.% PUT 0.26% SBJ 78.% v0c 0.025% Figure 1: Penn treebank function tags 53.% Form/Function 37.% Topicalisation 2.2% 0.25% NOM 6.8% 2.5% TPC 100% 2.2% 1.5% ADV 11.% 4.2% 9.3% BN'F 0.072% 0.026% 0.13% DIR 8.3% 3.0% 41.% EXT 3.2% 1.2% 0.013% LOC 25.% 9.2% MNR 6.2% 2.3% PI~ 5.2% 1.9% 33.% 12.% Miscellaneous 9.5% CLR 94.% 8.8% CLF 0.34% 0.03% HLN 2.6% 0.25% TTL 3.1% 0.29% Figure 2: Categories of function tags and their relative frequencies one project that used them at all:   defines certain constituents as complements based on a combination of label and function tag information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "function tags",
                "their relative frequencies",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "certain constituents",
                "based on a combination of label and function tag information",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this paper, we compare the performance of this system, HybridTrim, with the Topiary system and a number of other baseline gisting systems on a collection of news documents from the DUC 2004 corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HybridTrim",
                "compared with Topiary system and other baseline gisting systems",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "DUC 2004 corpus",
                "collection of news documents",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "7.3 EM algorithm The only other application of the EM algorithm to word-sense disambiguation is described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "EM algorithm",
                "described in ",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "EM algorithm",
                "only other application",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Further enhancement of these utilities include compiling collocation statistics   and semi-automatic gloassary construction  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "utilities",
                "compiling collocation statistics and semi-automatic glossary construction",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "collocation statistics",
                "compiling",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A totally different approach to improving the accuracy of our parser is to use the idea of selftraining described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "self-training",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "idea",
                "described in ",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "opSense is tested on 20 words extensively investigated in recent WSD literature  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "opSense",
                "20 words extensively investigated in recent WSD literature",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "words",
                "extensively investigated",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We extracted 181,250 case frames from the WSJ   bracketed corpus of the Penn Tree Bank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "case frames",
                "from the WSJ bracketed corpus of the Penn Tree Bank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "WSJ bracketed corpus of the Penn Tree Bank",
                "bracketed corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We describe a new sequence alignment model based on the averaged perceptron  , which shares with the above approaches the ability to exploit arbitrary features of the input sequences, but is distinguished from them by its relative simplicity and the incremental character of its training procedure.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sequence alignment model",
                "relative simplicity",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "training procedure",
                "incremental character",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "??queries: The queries of Turney   are made up of a pair of adjectives, and in our approach the query contains the content words of the headline and an emotion.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "queries",
                "made up of a pair of adjectives",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "queries",
                "contains the content words of the headline and an emotion",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  show that treating U+ as a source for a new feature function in a loglinear model for SMT   allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum error-rate training    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "U+",
                "as a source for a new feature function",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "minimum error-rate training",
                "using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Finally, our newly constructed parser, like that of  , was based on a generative statistical model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "generative statistical model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parser",
                "like that of  ",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Methods have been proposed for automatic evaluation in MT  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "proposed",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "evaluation",
                "automatic",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Although a large number of studies have been made on learning paraphrases, for example  , there are only a few studies which address the connotational difference of paraphrases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "studies",
                "few",
                "INNOVATION",
                "negative",
                0.7
            ],
            [
                "connotational difference",
                "of paraphrases",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Due to space we do not describe step 8 in detail  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "step 8",
                "do not describe in detail",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The phrase-based decoder extracts phrases from the word alignments produced by GIZA++, and computes translation probabilities based on the frequency of one phrase being aligned with another  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based decoder",
                "extracts phrases",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "translation probabilities",
                "based on frequency of one phrase being aligned with another",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ore details on the different parameter settings and instance selection algorithms as well as trends in the performance of different settings can be found in Stoyanov and Cardie  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameter settings",
                "different",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "performance",
                "trends",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In order to build models that perform well in new   domains we usually find two settings  : In the semi-supervised setting the goal is to improve the system trained on the source domain using unlabeled data from the target domain, and the baseline is that of the system c2008.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "setting",
                "semi-supervised setting",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "baseline",
                "c2008",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We obtained word alignments of the training data by first running GIZA++   and then applying the refinement rule grow-diagfinal-and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "refinement rule",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "grow-diagfinal-and",
                "refinement rule",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This is an unsuitable measure for inferring reliability, and it was the use of this measure that prompted Carletta   to recommend chance-corrected measures.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measure",
                "unsuitable",
                "METHODOLOGY",
                "negative",
                0.8
            ],
            [
                "use of this measure",
                "prompted Carletta to recommend chance-corrected measures",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "594 2.3 Viterbi Approximation To approximate the intractable decoding problem of  , most MT systems   use a simple Viterbi approximation, y = argmax yT  pViterbi    = argmax yT  max dD  p    = Y parenleftBigg argmax dD  p  parenrightBigg   Clearly,   replaces the sum in   with a max.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Viterbi approximation",
                "replaces the sum in with a max",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "MT systems",
                "use",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction In this paper, we show how discriminative training with averaged perceptron models   can be used to substantially improve surface realization with Combinatory Categorial Grammar  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "discriminative training",
                "can be used",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "averaged perceptron models",
                "can be used",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "4.3 Baseline We use a standard log-linear phrase-based statistical machine translation system as a baseline: GIZA++ implementation of IBM word alignment model 4  ,8 the refinement and phrase-extraction heuristics described in  , minimum-error-rate training 7More specifically, we choose the first English reference from the 7 references and the Chinese sentence to construct new sentence pairs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ implementation of IBM word alignment model",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase-extraction heuristics",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.2 Wall Street Journal Our out-of-domain data is the Wall Street Journal   portion of the Penn Treebank   which consists of about 40,000 sentences   annotated with syntactic information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "portion of the Penn Treebank",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "40,000 sentences",
                "annotated with syntactic information",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In agreement with recent resuits on parsing with lexicalised probabilistic grammars  , we find that statistics over lexical, as opposed to structural, features best correspond to human intuitive.judgments and to experimental findings.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistics",
                "best correspond",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "lexical features",
                "best correspond",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The idea of threading EEs to their antecedents in a stochastic parser was proposed by Collins  , following the GPSG tradition  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "EEs",
                "to their antecedents",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "GPSG tradition",
                "following",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "To do this, we first identify initial phrase pairs using the same criterion as previous systems  : Definition 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "criterion",
                "same as previous systems",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase pairs",
                "using Definition 1",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  and Nakov and Hearst  , among others, look at using a large amount of unlabeled data to classify relations between words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unlabeled data",
                "large amount",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "relations between words",
                "classify",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "As in the work of  , each word or punctuation mark within a sentence is labeled with IOB tag together with its function type.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IOB tag",
                "labeled with",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "function type",
                "together with",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "any 412 Turney Similarity of Semantic Relations researchers have argued that metaphor is the heart of human thinking  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Turney Similarity of Semantic Relations",
                "researchers have argued",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "metaphor",
                "heart of human thinking",
                "PERFORMANCE",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "Optimization and measurement were done with the NIST implementation of case-insensitive BLEU 4n4r  .4 4.1 Baseline We compared translation by pattern matching with a conventional exact model representation using external prefix trees  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU 4n4r",
                "implementation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "exact model representation",
                "conventional",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "There has been considerable use in the NLP community of both WordNet   and LDOCE  , but no one has merged the two in order to combine their strengths.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet",
                "considerable use",
                "APPLICABILITY",
                "positive",
                0.8
            ],
            [
                "LDOCE",
                "considerable use",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "2.2.2 ENGLISH TRAINING DATA For training in the English experiments, we used WSJ  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSJ",
                "English training data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "English experiments",
                "used WSJ",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For symmetrization, we found that Och and Neys refined technique described in   produced the best AER for this data set under all experimental conditions.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Och and Neys' technique",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "AER",
                "best",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "\\ ), training on a corpus of one type and then applying the tagger to a corpus of a different type usually results in a tagger with low accuracy \\ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "one type",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "tagger",
                "low accuracy",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "Minimum-error-rate training   are conducted on dev-set to optimize feature weights maximizing the BLEU score up to 4grams, and the obtained feature weights are blindly applied on the test-set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "maximizing the BLEU score up to 4grams",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "feature weights",
                "blindly applied",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Recent work on the automatic acquisition of multilingual LFG resources from treebanks for Chinese, German and Spanish   has shown that given a suitable treebank, it is possible to automatically acquire high quality LFG resources in a very short space of time.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treebank",
                "suitable treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "LFG resources",
                "high quality",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The way a decoder constructs translation hypotheses is directly related to the weights for different model features in a SMT system, which are usually optimized for a given set of models with minimum error rate training     to achieve better translation performance.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights for different model features",
                "are usually optimized for",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "translation performance",
                "better",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "4.1 Baseline Our baseline system is a fairly typical phrasebased machine translation system   built within the framework of a feature-based exponential model containing the following features: Table 1: Language Resources Corpus Train Dev Eval NC Spanish sentences 74K 2,001 2,007 words 2,048K 49,116 56,081 vocab 61K 9,047 8,638 length 27.6 24.5 27.9 OOV    5.2 / 2.9 1.4 / 0.9 English sentences 74K 2,001 2,007 words 1,795K 46,524 49,693 vocab 47K 8,110 7,541 length 24.2 23.2 24.8 OOV    5.2 / 2.9 1.2 / 0.9 perplexity  349 / 381 348 / 458 EP Spanish sentences 1,404K 1,861 2,000 words 41,003K 50,216 61,293 vocab 170K 7,422 8,251 length 29.2 27.0 30.6 OOV    2.4 / 0.1 2.4 / 0.2 English sentences 1,404K 1,861 2,000 words 39,354K 48,663 59,145 vocab 121K 5,869 6,428 length 28.0 26.1 29.6 OOV    1.8 / 0.1 1.9 / 0.1 perplexity  210 / 72 305 / 125 Table 2: Testset 2009 Corpus Test NC Spanish sentences 3,027 words 80,591 vocab 12,616 length 26.6  Source-target phrase translation probability  Inverse phrase translation probability  Source-target lexical weighting probability  Inverse lexical weighting probability  Phrase penalty  Language model probability  Lexical reordering probability  Simple distance-based distortion model  Word penalty For the training of the statistical models, standard word alignment  ) and language modeling  ) tools were used.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based machine translation system",
                "fairly typical",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical models",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Translation scores are reported using caseinsensitive BLEU   with a single reference translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "with a single reference translation",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "Translation scores",
                "reported using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Almost all of these measures can be grouped into one of the following three categories: a0 frequency-based measures   a0 information-theoretic measures   a0 statistical measures   The corresponding metrics have been extensively discussed in the literature both in terms of their mathematical properties   and their suitability for the task of collocation extraction   and Krenn and Evert   for recent evaluations).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "corresponding metrics",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "metrics",
                "suitability for the task of collocation extraction",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The next section briefly reviews the word alignment based statistical machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word alignment",
                "statistical machine translation",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "machine translation",
                "based",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Since in these LVCs the complement is a predicative noun in stem form identical to a verb, we form development and test expressions by combining give or take with verbs from selected semantic classes of Levin  , taken from Stevenson et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LVCs",
                "complement is a predicative noun in stem form identical to a verb",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "verbs from selected semantic classes of Levin",
                "taken from Stevenson et al.",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A similar soft projection of dependencies was used in supervised machine translation by Smith and Eisner  , who used a source sentences dependency paths to bias the generation of its translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependencies",
                "soft projection",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation",
                "bias the generation",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision   for summarization through word extraction, ROUGE   for abstracts, and BLEU   for machine translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "varied among humans",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word string precision",
                "for summarization through word extraction",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "To model aspects of co-occurrence association that might be obscured by raw frequency, the log-likelihood ratio G2   was also used to transform the feature space.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "G2",
                "transform the feature space",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "feature space",
                "obscured by raw frequency",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "The definitions of part-of-speech   categories and syntactic labels follow those of the Treebank I style  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "part-of-speech categories",
                "follow those of the Treebank I style",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "syntactic labels",
                "follow those of the Treebank I style",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 RelatedWork 2.1 Sentiment Classification Most previous work on the problem of categorizing opinionated texts has focused on the binary classification of positive and negative sentiment  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "focused on the binary classification of positive and negative sentiment",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "previous work",
                "most",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "4.4 Corpora We ran the three syntactic preprocessors over a total of three corpora, of varying size: the Brown corpus   and Wall Street Journal corpus  , both derived from the Penn Treebank  , and the written component of the British National Corpus  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "Brown corpus and Wall Street Journal corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "corpora",
                "written component of the British National Corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Accordingly, in Ponzetto & Strube   we used a machine learning based coreference resolution system to provide an extrinsic evaluation of the utility of WordNet and Wikipedia relatedness measures for NLP applications.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet and Wikipedia relatedness measures",
                "utility for NLP applications",
                "APPLICABILITY",
                "positive",
                0.8
            ],
            [
                "machine learning based coreference resolution system",
                "extrinsic evaluation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In a phrase-based statistical translation  , a bilingual text is decomposed as K phrase translation pairs  ,  ,: The input foreign sentence is segmented into phrases f K1, 122 mapped into corresponding English eK1, then, reordered to form the output English sentence according to a phrase alignment index mapping a. In a hierarchical phrase-based translation  , translation is modeled after a weighted synchronous-CFG consisting of production rules whose right-hand side is paired  : X  ,, where X is a non-terminal,  and  are strings of terminals and non-terminals.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase translation pairs",
                "mapped into corresponding English",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase alignment index mapping",
                "according to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Banko and Etzioni   studied open domain relation extraction, for which they manually identified several common relation patterns.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "relation extraction",
                "common relation patterns",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "manual identification",
                "manually identified",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "By no means an exhaustive list, the most commonly cited ranking and scoring algorithms are HITS   and PageRank  , which rank hyperlinked documents using the concepts of hubs and authorities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HITS",
                "rank hyperlinked documents",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "PageRank",
                "rank hyperlinked documents",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Decoding weights are optimized using Ochs algorithm   to set weights for the four components of the loglinear model: language model, phrase translation model, distortion model, and word-length feature.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ochs algorithm",
                "optimized using",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "loglinear model",
                "four components",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2.2 A Perceptron-Based Edit Model In this section we present a general-purpose extension of perceptron training for sequence labeling, due to Collins  , to the problem of sequence alignment.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Perceptron-Based Edit Model",
                "general-purpose extension of perceptron training for sequence labeling",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "sequence alignment",
                "problem of",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For example, we can use automatically extracted hyponymy relations  , or automatically induced MN clusters  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hyponymy relations",
                "automatically extracted",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "MN clusters",
                "automatically induced",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "unning   used a likelihood ratio to test word similarity under the assumption that the words in text have a binomial distribution",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "likelihood ratio",
                "to test word similarity",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "binomial distribution",
                "assumption",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Some improvements on BOW are given by the use of dependency trees and syntactic parse trees  ,  ,  , but these, too are not adequate when dealing with complex questions whose answers are expressed by long and articulated sentences or even paragraphs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency trees and syntactic parse trees",
                "are not adequate",
                "PERFORMANCE",
                "negative",
                0.75
            ],
            [
                "complex questions whose answers are expressed by long and articulated sentences or even paragraphs",
                "are not adequate",
                "PERFORMANCE",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "To support distributed computation  , we further split the N-gram data into shards by hash values of the first bigram.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "N-gram data",
                "into shards",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "hash values of the first bigram",
                "by",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this framework, the source language, let-s say English, is assumed to be generated by a noisy probabilistic source.1 Most of the current statistical MT systems treat this source as a sequence of words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "source language",
                "English",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "statistical MT systems",
                "treat the source as a sequence of words",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The weights for these models are determined using the method described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weights",
                "are determined using the method described in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "method",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "On the positive side, recent work exploring the automaticbinarizationofsynchronousgrammars  has indicated that non-binarizable constructions seem to be relatively rare in practice.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "synchronous grammars",
                "indicated to be relatively rare",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "non-binarizable constructions",
                "seem to be relatively rare",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Statistical machine translation views the translation process as a noisy-channel signal recovery process in which one tries to recover the input signal e, from the observed output signal f.1 Early statistical machine translation systems used a purely word-based approach without taking into account any of the morphological or syntactic properties of the languages  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation process",
                "noisy-channel signal recovery process",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "statistical machine translation systems",
                "purely word-based approach",
                "METHODOLOGY",
                "negative",
                0.75
            ]
        ]
    },
    {
        "text": "The tensor has been adapted with a straightforward extension of pointwise mutual information   for three-way cooccurrences, following equation 4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tensor",
                "straightforward extension of pointwise mutual information",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "cooccurrences",
                "following equation 4",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The ve part-ofspeech   patterns from   were used for the extraction of indicators, all involving at least one adjective or adverb.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "part-ofspeech patterns",
                "involving at least one adjective or adverb",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "indicators",
                "all involving at least one adjective or adverb",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4 Training This section discusses how to extract our translation rules given a triple nullnull,null null ,nullnull . As we know, the traditional tree-to-string rules can be easily extracted from nullnull,null null ,nullnull  using the algorithm of Mi and Huang   2 . We would like  2  Mi and Huang   extend the tree-based rule extraction algorithm   to forest-based by introducing non-deterministic mechanism.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm of Mi and Huang",
                "traditional",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tree-based rule extraction algorithm",
                "extend to forest-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As in  , the parameter C8 D0 B4C4 CX B4D0D8 CX BND0DB CX B5CYC8BNC0BNDBBND8BNA1BNC4BVB5 is further smoothed as follows: C8 D0BD B4C4 CX CYC8BNC0BNDBBND8BNA1BNC4BVB5 A2 C8 D0BE B4D0D8 CX CYC8BNC0BNDBBND8BNA1BNC4BVBNC4 CX B5A2 C8 D0BF B4D0DB CX CYC8BNC0BNDBBND8BNA1BNC4BVBNC4 CX B4D0D8 CX B5B5 Note this smoothing is different from the syntactic counterpart.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "C8 D0 B4C4 CX B4D0D8 CX BND0DB CX B5CYC8BNC0BNDBBND8BNA1BNC4BVB5",
                "is further smoothed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntactic counterpart",
                "is different from",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "As a baseline model we used a maximum entropy tagger, very similar to the one described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy tagger",
                "very similar",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "maximum entropy tagger",
                "described in  ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Titov and McDonald   proposed a joint model of text and aspect ratings which utilizes a modified LDA topic model to build topics that are representative of ratable aspects, and builds a set of sentiment predictors.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "modified LDA topic model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentiment predictors",
                "set of",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "  binarize grammars into CNF normal form, while   allow only Griebach-Normal form grammars.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammars",
                "into CNF normal form",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Griebach-Normal form grammars",
                "allowed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The production weights are estimated either by heuristic counting   or using the EM algorithm.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "production weights",
                "estimated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "EM algorithm",
                "using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "It turns out that while problems of coverage and ambiguity prevent straightforward lookup, injection of gazetteer matches as features in machine-learning based approaches is critical for good performance  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "gazetteer matches",
                "critical for good performance",
                "METHODOLOGY",
                "positive",
                0.85
            ],
            [
                "problems of coverage and ambiguity",
                "prevent straightforward lookup",
                "LIMITATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.1 Results for English We used sections 0 to 12 of the WSJ part of the Penn Treebank   with a total of 24,618 sentences for our experiments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSJ part of the Penn Treebank",
                "sections 0 to 12",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank",
                "used for experiments",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This characteristic of our corpus is similar to problems with noisy and comparable corpora  , and it prevents us from using methods developed in the MT community based on clean parallel corpora, such as  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "noisy and comparable",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "methods",
                "based on clean parallel corpora",
                "METHODOLOGY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "Most of this prior work deals with supervised transfer learning, and thus requires labeled source domain data, though there are examples of unsupervised  , semi-supervised  , and transductive approaches  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prior work",
                "requires labeled source domain data",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "approaches",
                "unsupervised, semi-supervised, and transductive",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It is important to realize that the output of all mentioned processing steps is noisy and contains plenty of mistakes, since the data has huge variability in terms of quality, style, genres, domains etc., and domain adaptation for the NLP tasks involved is still an open problem  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "processing steps",
                "contains plenty of mistakes",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "data",
                "huge variability in terms of quality, style, genres, domains",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "57 Given a pair of English sentences to be compared  , we perform tokenization2, lemmatization using WordNet3, and part-of-speech   tagging with the MXPOST tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tokenization",
                "tokenization2",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "tagger",
                "MXPOST tagger",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our results on Chinese data confirm previous findings on English data shown in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data",
                "previous findings",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "results",
                "confirm",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1113: Recursive DP equations for summing over t and a. alignments are treated as a hidden variable to be marginalized out.10 Optimization problems of this form are by now widely known in NLP  , and have recently been used for machinetranslationaswell .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignments",
                "treated as a hidden variable to be marginalized out",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "optimization problems",
                "are widely known in NLP",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The parsing algorithm was CKY-style parsing with beam thresholding, which was similar to ones used in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CKY-style parsing with beam thresholding",
                "similar to ones used in",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ones used in",
                "used in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Each item is associated with a stack whose signa12Specifically a B-hypergraph, equivalent to an and-or graph   or context-free grammar  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "stack",
                "B-hypergraph, equivalent to an and-or graph or context-free grammar",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "item",
                "associated with",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Clustering can be done statistically by analyzing text corpora   and usually results in a set of words or word senses.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "text corpora",
                "usually results",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "set of words or word senses",
                "usually results",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Here, it might be useful to relax the strict linear control regime by exploring beam search strategies, e.g. along the lines of Collins and Roark  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "beam search strategies",
                "along the lines of Collins and Roark",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "strict linear control regime",
                "relaxing",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "We report case-insensitive scores on version 0.6 of METEOR   with all modules enabled, version 1.04 of IBM-style BLEU  , and version 5 of TER  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "METEOR",
                "version 0.6",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "modules",
                "all enabled",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These later inductive phases may rely on some level of a priori knowledge, like for example the naive case relations used in the ARIOSTO_LEX system  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ARIOSTO_LEX system",
                "used",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "naive case relations",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This implementation is exactly the one proposed in  , and we will denote it as MB-D hereafter.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "implementation",
                "proposed",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "MB-D",
                "denote",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "In previous research on splitting sentences, many methods have been based on word-sequence characteristics like N-gram  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "based on word-sequence characteristics",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "N-gram",
                "N-gram",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "6 Discussion Lack of interannotator agreement presents a significant problem in annotation efforts  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotation efforts",
                "presents a significant problem",
                "LIMITATION",
                "negative",
                0.8
            ],
            [
                "interannotator agreement",
                "lack of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For a detailed introduction to IBM translation models, please see  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM translation models",
                "detailed introduction",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "translation models",
                "please see",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "First, we need to determine whether or not the positive effect of SVD feature selection is preserved in more complex feature spaces such as syntactic feature spaces as those used in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SVD feature selection",
                "positive effect",
                "PERFORMANCE",
                "neutral",
                0.7
            ],
            [
                "syntactic feature spaces",
                "those used in",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Whilst, the parameters for the maximum entropy model are developed based on the minimum error rate training method  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "developed based on the minimum error rate training method",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "minimum error rate training method",
                "minimum error rate",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Once an acceptable rate of interjudge agreement was verified on the first nine clusters   of 0.68), the remaining 11 clusters were annotated by one judge each.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "clusters",
                "acceptable rate of interjudge agreement was verified",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "judge",
                "annotated",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Features that consider only target-side syntax and words without considering s can be seen as syntactic language model features  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "consider only target-side syntax and words without considering s",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "language model features",
                "syntactic language model features",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "CISSOR is implemented by augmenting Collins   head-driven parsing model II to incorporate the generation of semantic labels on internal nodes",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "head-driven parsing model II",
                "incorporate the generation of semantic labels",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "semantic labels",
                "generation",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In TAC 2008 Summarization track, all submitted runs were scored with the ROUGE   and Basic Elements   metrics  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE",
                "metrics",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "Basic Elements",
                "metrics",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "he prevalent use of this criterion despite repeated advice that it is unlikely to be suitable for all studies   is probably due to a desire for a simple system that can be easily applied to a scheme",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "criterion",
                "is unlikely to be suitable for all studies",
                "LIMITATION",
                "negative",
                0.8
            ],
            [
                "system",
                "simple",
                "METHODOLOGY",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "In this work, model fit is reported in terms of the likelihood ratio statistic, G 2, and its significance  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "likelihood ratio statistic",
                "G 2",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "significance",
                "its significance",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "For our POS tagging experiments, we use 561 MEDLINE sentences   from the Penn BioIE project  , a test set previously used by Blitzer et al. .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MEDLINE sentences",
                "from the Penn BioIE project",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "test set",
                "previously used by Blitzer et al.",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The toolkit also implements suffix-array grammar extraction   and minimum error rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "suffix-array grammar extraction",
                "is implemented",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "minimum error rate training",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The detailed algorithm can be found in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "found in ",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "algorithm",
                "detailed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the February 2004 version of the PropBank corpus, annotations are done on top of the Penn TreeBank II parse trees  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PropBank corpus",
                "February 2004 version",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn TreeBank II parse trees",
                "done on top of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The NP chunks in the shared task data are base-NP chunks  which are non-recursive NPs, a definition first proposed by Ramshaw and Marcus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NP chunks",
                "base-NP chunks",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "definition",
                "first proposed by Ramshaw and Marcus",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Prominent among these properties is the semi-free Language Size LR LP Source English 40,000 87.4% 88.1%   Chinese 3,484 69.0% 74.8%   Czech 19,000 80.0%   Table 1: Results for the Collins   model for various languages   wordorder, i.e., German wordorder is fixed in some respects, but variable in others.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Language Size LR LP Source",
                "40,000",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "wordorder",
                "fixed in some respects, but variable in others",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "In this paper it is shown that the synchronous grammars used in Wu  , Zhang et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "synchronous grammars",
                "used in Wu, Zhang et al.",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Introduction Since Eric Brill first introduced the method of Transformation-Based Learning   it has been used to learn rules for many natural language processing tasks, such as part-of-speech tagging \\ .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "Transformation-Based Learning",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "rules for many natural language processing tasks",
                "such as part-of-speech tagging",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "For our contrast submission, we rescore the first-pass translation lattices with a large zero-cutoff stupid-backoff   language model estimated over approximately five billion words of newswire text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "language model",
                "stupid-backoff",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "language model",
                "estimated over approximately five billion words of newswire text",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2.2 The Choice of Co-occurrence ~qeasure and Matrix Distance There :~:c many alternatives to measure cooccurrence between two words x and y  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Co-occurrence measure",
                "many alternatives",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "matrix distance",
                "many alternatives",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In the results we describe here, we use mutual information   as the metric for neighbourhood pruning, pruning which occurs as the network is being generated.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metric",
                "mutual information",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "pruning",
                "pruning which occurs as the network is being generated",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Alternatively, one can train them with respect to the final translation quality measured by some error criterion  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation quality",
                "measured by some error criterion",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "translation quality",
                "measured by some error criterion",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction The overall goal of the Penn Discourse Treebank   is to annotate the million word WSJ corpus in the Penn TreeBank   with a layer of discourse annotations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Discourse Treebank",
                "annotate the million word WSJ corpus",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "WSJ corpus",
                "in the Penn TreeBank",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this study, we use the Google Web 1T 5gram Corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "5gram",
                "5gram",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Baseline DP Decoder The translation model used in this paper is a phrasebased model  , where the translation units are so-called blocks: a block b is a pair consisting of a source phrase s and a target phrase t which are translations of each other.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "phrase-based model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "blocks",
                "translations of each other",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Nakagawa   and Hall   also showed the effectiveness of global features in improving the accuracy of graph-based parsing, using the approximate Gibbs sampling method and a reranking approach, respectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "global features",
                "improving the accuracy",
                "PERFORMANCE",
                "neutral",
                0.85
            ],
            [
                "Gibbs sampling method",
                "reranking approach",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5.1 Pharaoh The baseline system we used for comparison was Pharaoh  , a freely available decoder for phrase-based translation models: p  = p  pLM LM  pD D length W    We ran GIZA++   on the training corpus in both directions using its default setting, and then applied the refinement rule diagand described in   to obtain a single many-to-many word alignment for each sentence pair.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pharaoh",
                "freely available decoder for phrase-based translation models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "GIZA++",
                "default setting",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ag test data using the POS-tagger described in Ratnaparkhi  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS-tagger",
                "described in Ratnaparkhi",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "data",
                "using the POS-tagger",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The measures are: word overlap, length difference  , BLEU  , dependency relation overlap  , and dependency tree edit distance.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measures",
                "word overlap, length difference, BLEU, dependency relation overlap, and dependency tree edit distance",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "BLEU",
                "BLEU",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "One of our goals was to use for this study only information that could be annotated reliably  , as we believe this will make our results easier to replicate.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "information",
                "can be annotated reliably",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "results",
                "easier to replicate",
                "PERFORMANCE",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "The main data set consist of four sections   of the Wall Street Journal   part of the Penn Treebank   as training material and one section   as test material 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "training material",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Wall Street Journal",
                "test material",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2.1 Keywords As our starting point, we calculated the keywords of the Belgian corpus with respect to the Netherlandic corpus, both on the basis of a chi-square test     and the log-likelihood ratio  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "chi-square test",
                "log-likelihood ratio",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Netherlandic corpus",
                "with respect to",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " s approach for English resolves three LDD types in parser output trees without traces and coindexation  ), i.e. topicalisation  , wh-movement in relative clauses   and interrogatives  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LDD types",
                "resolves three",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "parser output trees",
                "without traces and coindexation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Wu   shows that parsing a binary SCFG is in O  while parsing SCFG is NP-hard in general  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SCFG",
                "O",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "SCFG",
                "NP-hard in general",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In our future work, we intend to adopt a looser filter together with an anaphoricity determination module  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "anaphoricity determination module",
                "determination",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "looser filter",
                "filter",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "c2009 Association for Computational Linguistics Structural Correspondence Learning for Parse Disambiguation Barbara Plank Alfa-informatica University of Groningen, The Netherlands b.plank@rug.nl Abstract The paper presents an application of Structural Correspondence Learning     for domain adaptation of a stochastic attribute-value grammar  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "The distinction between lexical and relational similarity for word pair comparison is recognised byTurney  , though the methods he presents focus on relational similarity.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Turney",
                "methods",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "relational similarity",
                "recognised",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT results",
                "was improved",
                "PERFORMANCE",
                "positive",
                0.7
            ],
            [
                "automatic MT evaluation measures",
                "parameter tuning",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For a second set of parsing experiments, we used the WSJ portion of the Penn Tree Bank   and Helmut Schmids enrichment program tmod  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Tree Bank",
                "portion of",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Helmut Schmid's enrichment program tmod",
                "used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "2 Literature Survey The task of sentiment analysis has evolved from document level analysis  ;  ) to sentence level analysis  ;  ;  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "task of sentiment analysis",
                "evolved",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "document level analysis",
                "to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Moreover, it was   the inference technique employed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "inference technique",
                "employed in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "inference technique",
                "employed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For this reason, each preposition and verb was assigned a weight based on the proportion of occurrences of that word in the Penn Treebank   which are labelled with a spatial meaning.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "preposition and verb",
                "based on the proportion of occurrences in the Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "960 1.2 Alignment with Mixture Distribution Several papers have discussed the first issue, especially the problem of word alignments for bilingual corpora  ,  ,  ,  ,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mixture distribution",
                "discussed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word alignments",
                "problem",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In the original work   the posterior probability p  is decomposed following a noisy-channel approach, but current stateof-the-art systems model the translation probability directly using a log-linear model : p  = exp parenleftBigsummationtextM m=1 mhm  parenrightBig summationdisplay ?eI1 exp parenleftBigsummationtextM m=1 mhm  parenrightBig,   with hm different models, m scaling factors and the denominator a normalization factor that can be ignored in the maximization process.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "posterior probability p",
                "is decomposed following a noisy-channel approach",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation probability",
                "is modeled directly using a log-linear model",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "recognition of semantic orientation and sentiment analysis",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "orientation and sentiment analysis",
                "closely related to our work",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The annotation can be considered reliable   with 95% agreement and a kappa   of.88.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa",
                "of.88",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "agreement",
                "95%",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "  first introduced an iterative method for increasing a small set of seed data used to disambiguate dual word senses by exploiting the constraint that in a segment of discourse only one sense of a word is used.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "seed data",
                "small set",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "word senses",
                "only one sense",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The coreference resolution system employs a variety of lexical, semantic, distance and syntactic features .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "lexical, semantic, distance and syntactic",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "system",
                "employs a variety of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Volume 17, Number 1 March 1991 References Lakoff, George and Johnson, Mark Metaphors We Live 8y University of Chicago Press 1980 MADCOW Committee   Multi-Site Data Collection for a Spoken Language Corpus in Proceedings Speech and Natural Language Workshop February 1992 Grice, H. P. Logic and Conversation in P. Cole and J. L. Morgan, Speech Acts, New York: Academic Press, 1975 Pustejovsky, James The Generative Lexicon Computational Linguistics Volume 17, Number 4 December 1991 Hobbs, Jerry R. and Stickel, Mark Interpretation as Abduction in Proceedings of the 26th ACL June 1988 Bobrow, R. , Ingria, R. and Stallard, D. The Mapping Unit Approach to Subcategorization in Proceedings Speech and Natural Language Workshop February 1991 Hobbs, Jerry R. , and Martin, Paul Local Pragmatics in Proceedings, 10th International Joint Conference on Artificial Intelligence  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Lakoff, George and Johnson, Mark",
                "Metaphors We Live",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "Grice, H. P.",
                "Logic and Conversation",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Experimental Comparison 4.1 Experiments on the ATIS corpus For our first comparison, we used I0 splits from the Penn ATIS corpus   into training sets of 675 sentences and test sets of 75 sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn ATIS corpus",
                "into training sets of 675 sentences and test sets of 75 sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "I0 splits",
                "from",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We evaluated annotation reliability by using the Kappa statistic  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa statistic",
                "using",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "annotation reliability",
                "evaluated",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Bilingual alignments have so far shown that they can play multiple roles in a wide range of linguistic applications, such as computer assisted translation  , terminology   lexicography  , and cross-language information retrieval  , via the Agence de la francophonie (http://~.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bilingual alignments",
                "play multiple roles",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "linguistic applications",
                "wide range",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "First, a parsing-based approach attempts to recover partial parses from the parse chart when the input cannot be parsed in its entirety due to noise, in order to construct a   semantic representation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parse chart",
                "recover partial parses",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parse chart",
                "cannot be parsed in its entirety due to noise",
                "LIMITATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "The parser has been trained, developed and tested on a large collection of syntactically analyzed sentences, the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "trained, developed and tested",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank",
                "large collection of syntactically analyzed sentences",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We perform minimum error rate training   to tune the feature weights for the log-linear modeltomaximizethesystemssBLEUscoreonthe development set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights",
                "to tune",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "system's BLEU score",
                "to maximize",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the hierarchical phrase-based model  , and an inversion transduction grammar    , the problem is resolved by restricting to a binarized form where at most two non-terminals are allowed in the righthand side.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "non-terminals",
                "at most two are allowed",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "binarized form",
                "restricted",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3 Building the CatVar The CatVar database was developed using a combination of resources and algorithms including the Lexical Conceptual Structure   Verb and Preposition Databases  , the Brown Corpus section of the Penn Treebank  , an English morphological analysis lexicon developed for PC-Kimmo    , NOMLEX  , Longman Dictionary of Contemporary English 2For a deeper discussion and classification of Porter stemmers errors, see  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CatVar database",
                "developed using a combination of resources and algorithms",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Porter stemmers",
                "errors",
                "LIMITATION",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "  propose a tree sequence-based tree to tree translation model and Zhang et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tree sequence-based tree",
                "tree to tree translation model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Zhang et al",
                "proposed",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Instead, researchers routinely use automatic metrics like Bleu   as the sole evidence of improvement to translation quality.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "sole evidence of improvement",
                "PERFORMANCE",
                "negative",
                0.7
            ],
            [
                "translation quality",
                "routine use",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "1 Introduction This paper reports on our experience hand tagging the senses of 25 of the most frequent verbs in 12,925 sentences of the Wall Street Journal Treebank corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal Treebank corpus",
                "most frequent verbs in 12,925 sentences",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "hand tagging",
                "senses of verbs",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.2  -cousin Classification The classifier for learning coordinate terms relies on the notion of distributional similarity, i.e., the idea that two words with similar meanings will be used in similar contexts  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "classifier",
                "rely on the notion of distributional similarity",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "words with similar meanings",
                "used in similar contexts",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model  , flat reordering model  , lexicalized reordering model  , to hierarchical phrase-based model   and classifier-based reordering model with linear features  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase reordering methods",
                "simple distance-based distortion model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase reordering model",
                "flat reordering model",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "he solution we employ here is the discriminative training procedure of Och  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Och's discriminative training procedure",
                "discriminative training procedure",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Och",
                "discriminative training procedure",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Thus, we can compute the source dependency LM score in the same way we compute the target side score, using a procedure described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "procedure",
                "described in ",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "source dependency LM score",
                "compute the same way",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Ramshaw and Marcus  first represented base noun phrase recognition as a machine learning problem.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Based on this assumption,   stored all bigrams of words along with their relative position.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bigrams",
                "stored along with their relative position",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "stored",
                "along with their relative position",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In comparison we introduce 28 several metrics coefficients reported in Albrecht and Hwa   including smoothed BLEU  , METEOR  , HWCM  , and the metric proposed in Albrecht and Hwa   using the full feature set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics coefficients",
                "reported in Albrecht and Hwa",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "metric",
                "proposed in Albrecht and Hwa",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "We then examined the inter-annotator reliability of the annotation by calculating the  score  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotation",
                "by calculating the score",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "score",
                "calculating the",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The second type has clear interpretation as a probability model, but no criteria to determine the number of clusters  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probability model",
                "clear interpretation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "number of clusters",
                "no criteria to determine",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We use eight similarity measures implemented within the WordNet::Similarity package5, described in  ; these include three measures derived from the paths between the synsets in WordNet: HSO  , LCH  , and WUP  ; three measures based on information content: RES  , LIN  , and JCN  ; the gloss-based Extended Lesk Measure LESK,  , and finally the gloss vector similarity measure VECTOR  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet::Similarity package",
                "described in",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Extended Lesk Measure LESK",
                "gloss-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Giving the increasing sophistication of probabilistic linguistic models   has a statistical approach to learning gap-threading rules) a probabilistic extension of our work is attractive--it will be interesting to see how far an integration of 'logical' and statistical can go.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilistic linguistic models",
                "increasing sophistication",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "integration of 'logical' and statistical",
                "will be interesting to see how far",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Note that generative hybrids are the norm in SMT, where translation scores are provided by a discriminative combination of generative models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generative hybrids",
                "the norm",
                "INNOVATION",
                "positive",
                0.85
            ],
            [
                "translation scores",
                "provided by a discriminative combination of generative models",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This negation handling is similar to that used in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "negation handling",
                "similar to that used in.",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                ".",
                "used in.",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Nonparametricmodels   may be appropriate.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "nonparametric models",
                "may be appropriate",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Assuming that the corpusbased error count for some translations eS1 is additively decomposable into the error counts of the individual sentences, i.e., ED4rS1 ,eS1D5 AG EWSs AG1 ED4rs,esD5,the MERT criterion is given as: M1 AG argmin M1 AZ S F4 sAG1 EA0rs,eD4fs;M1 D5A8 B7   AG argmin M1 AZ S F4 sAG1 K F4 kAG1 ED4rs,es,kD5A0eD4fs;M1 D5,es,kA8 B7 with e D4fs;M1 D5 AG argmaxe AZ M F4 mAG1 mhmD4e,fsD5 B7   In  , it was shown that linear models can effectively be trained under the MERT criterion using a special line optimization algorithm.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MERT criterion",
                "given as",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "linear models",
                "can effectively be trained",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Our method is based on a decision list proposed by Yarowsky  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "proposed by Yarowsky",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "decision list",
                "proposed by Yarowsky",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In Section 3 we review  s method for recovering English NLDs in treebank-based LFG approximations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Even before the 2006 shared task, the parsers of Collins   and Charniak  , originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto   and Yamada and Matsumoto   had been evaluated on both Japanese and English.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parsers",
                "originally developed for English",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parsing methodology",
                "proposed by Kudo and Matsumoto and Yamada and Matsumoto",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The model scaling factors are optimized with respect to some evaluation criterion  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model scaling factors",
                "optimized with respect to some evaluation criterion",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The data consists of sections of the Wall Street Journal part of the Penn TreeBank  , with information on predicate-argument structures extracted from the PropBank corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal",
                "part of the Penn TreeBank",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "predicate-argument structures",
                "extracted from PropBank corpus",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "For more detail, see Chen & Martin  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Chen & Martin",
                "detail",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The translation problem can be statistically formulated as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation problem",
                "statistically formulated",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "problem",
                "can be",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To reduce it we exploit the one sense per collocation property  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocation property",
                "one sense per",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "property",
                "exploit",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The production rules in ITGs are of the following form  , with a notation similar to what is typically used for SDTSs and SCFGs in the right column: A    A  B1C2,B1C2 A  BC A  B1C2,C2B1 A  e | f A  e,f A  e |  A  e, A   | f A  ,f It is important to note that RHSs of production rules have at most one source-side and one targetside terminal symbol.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "production rules",
                "form",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "RHSs of production rules",
                "have at most one source-side and one target-side terminal symbol",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Though this model uses trees in the formal sense, it does not create Penn Treebank   style linguistic trees, but uses only one non-terminal label   to create those trees using six simple rule structures.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "trees",
                "uses",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "rule structures",
                "six simple",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "See Collins   and Collins and Duffy   for applications of the perceptron algorithm.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron algorithm",
                "applications",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "Collins and Collins and Duffy",
                "for",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The COlllillOil poini;s regarding collocations appear to be, as   suggestsl: they are m'bil;rary  , th , t;hey are recurrenl; and cohesive lo~xical clusters: the presence of one of the.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocations",
                "m'bil;rary",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "collocations",
                "recurrenl; and cohesive lo~xical clusters",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To implement this method, we rst use the Stanford Named Entity Recognizer4  toidentifythesetofpersonandorganisation entities, E, from each article in the corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Stanford Named Entity Recognizer",
                "to identify the set of person and organisation entities",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "E",
                "from each article in the corpus",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As far as we know, language modeling always improves with additional training data, so we add data from the North American News Text Corpus     automatically parsed with the Charniak parser   to train our language model on up to 20 million additional words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "language modeling",
                "always improves",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "Charniak parser",
                "automatically parsed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Even for semantically predictable phrases, the fact that the words occur in fixed patterns can be very useful for the purposes of disambiguation, as demonstrated by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "occur in fixed patterns",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrases",
                "very useful for disambiguation",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "  use hand-coded slot-filling rules to determine the semantic roles of the arguments of a nominalization.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "slot-filling rules",
                "to determine the semantic roles",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "nominalization",
                "arguments",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Previous research has addressed revision in single-document summaries   and has suggested that revising summaries can make them more informative and correct errors.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "summaries",
                "make them more informative",
                "APPLICABILITY",
                "positive",
                0.8
            ],
            [
                "summaries",
                "correct errors",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Sentiment analysis have been widely conducted in several domains such as movie reviews, product reviews, news and blog reviews  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentiment analysis",
                "have been widely conducted",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "domains such as movie reviews, product reviews, news and blog reviews",
                "various",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.1 A simple solution Wu   suggests that in order to have an ITG take advantage of a known partial structure, one can simply stop the parser from using any spans that would violate the structure.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "not using spans that would violate the structure",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ITG",
                "take advantage of a known partial structure",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "2 We used the Collins parser   to generate the constituency parse and a dependency converter   to obtain the dependency parse of English sentences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins parser",
                "generate the constituency parse",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dependency converter",
                "obtain the dependency parse",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank   and Penn Chinese Treebank  , extracting wide-coverage, probabilistic LFG grammar 361 Computational Linguistics Volume 31, Number 3 approximations and lexical resources for German   and Chinese  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "unification grammar acquisition methodology",
                "general",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "LFG grammar approximations and lexical resources",
                "probabilistic",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Of the several slightly different definitions of a base NP in the literature we use for the purposes of this work the definition presented in   and used also by  and others.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "definition",
                "presented in and used also by and others",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "definition",
                "several slightly different definitions",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "A variety of synset similarity measures based on properties of WordNet itself have been proposed; nine such measures are discussed in  , including gloss-based heuristics  , information-content based measures  , and others.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "synset similarity measures",
                "based on properties of WordNet itself",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "nine such measures",
                "discussed in",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The feature weights i are trained in concert with the LM weight via minimum error rate   training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights i",
                "trained in concert with the LM weight",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "minimum error rate training",
                "used for training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the experiment, only the first 500 sentences were used to train the log-linear model weight vector, where minimum error rate   training was used  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-linear model",
                "minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "training",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5.2 Impact on translation quality As reported in Table 3, small increases in METEOR  , BLEU   and NIST scores   suggest that SMT output matches the references better after postprocessing or decoding with the suggested lemma translations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT output",
                "matches the references better",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "small increases",
                "in METEOR, BLEU, and NIST scores",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Many methods for calculating the similarity have been proposed  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "have been proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "similarity",
                "calculating",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For the constituent-based models, constituent information was obtained from the output of Collins parser   for English and Dubeys parser   for German.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins parser",
                "for English",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Dubeys parser",
                "for German",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These models include a standard unlexicalized PCFG parser, a head-lexicalized parser  , and a maximum-entropy inspired parser  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "standard unlexicalized",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parser",
                "maximum-entropy inspired",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "See   for additional work using perceptron algorithms to train tagging models, and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron algorithms",
                "to train tagging models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "theory underlying the perceptron algorithm",
                "applied to ranking problems",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Most of the previously proposed methods to extract compounds or to measure word association using mutual information   either ignore or penalize items with low co-occurrence counts  , because MI becomes unstable when the co-occurrence counts are very small.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "ignore or penalize items with low co-occurrence counts",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "MI",
                "becomes unstable when the co-occurrence counts are very small",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In particular, we use a randomly-selected corpus the first five columns as information-like. consisting of a 6.7 million word subset of the TREC Similarly, since the last four columns share databases  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "randomly-selected",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "TREC databases",
                "6.7 million word subset",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, developing the PDTB may help facilitate the production of more such corpora, through an initial pass of automatic annotation, followed by manual correction, much as was done in developing the PTB  .)",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PDTB",
                "may help facilitate",
                "APPLICABILITY",
                "positive",
                0.8
            ],
            [
                "PTB",
                "was done",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The extension of dynamic SBNs with incrementally specified model structure   was proposed and applied to constituent parsing in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model structure",
                "incrementally specified",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "constituent parsing",
                "in",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "73 ID Participant BBN-COMBO BBN system combination   CMU-COMBO Carnegie Mellon University system combination   CMU-GIMPEL Carnegie Mellon University Gimpel   CMU-SMT Carnegie Mellon University SMT   CMU-STATXFER Carnegie Mellon University Stat-XFER   CU-TECTOMT Charles University TectoMT   CU-BOJAR Charles University Bojar   CUED Cambridge University   DCU Dublin City University   LIMSI LIMSI   LIU Linkoping University   LIUM-SYSTRAN LIUM / Systran   MLOGIC Morphologic   PCT a commercial MT provider from the Czech Republic RBMT16 Babelfish, Lingenio, Lucy, OpenLogos, ProMT, SDL   SAAR University of Saarbruecken   SYSTRAN Systran   UCB University of California at Berkeley   UCL University College London   UEDIN University of Edinburgh   UEDIN-COMBO University of Edinburgh system combination   UMD University of Maryland   UPC Universitat Politecnica de Catalunya, Barcelona   UW University of Washington   XEROX Xerox Research Centre Europe   Table 2: Participants in the shared translation task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Participants",
                "in the shared translation task",
                "APPLICABILITY",
                "neutral",
                1.0
            ],
            [
                "BBN-COMBO",
                "BBN system combination",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": " ,   In addition to the usual issues involved with the complex annotation of data, we have come to terms with a number of issues that are specific to a highly inflected language with a rich history of traditional grammar.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "annotation",
                "complex",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "inflected language",
                "rich history of traditional grammar",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Someworkwithintheframework of synchronous grammars  , while others create a generative story that includes a parse tree provided for one of the sentences  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "framework",
                "of synchronous grammars",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parse tree",
                "provided for one of the sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Word alignment is newer, found only in a few places  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Word alignment",
                "newer",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "Word alignment",
                "found in a few places",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "There are rules, though rare, that cannot be binarized synchronously at all  , but can be incorporated in two-stage decoding with asynchronous binarization.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rules",
                "cannot be binarized synchronously",
                "METHODOLOGY",
                "negative",
                0.7
            ],
            [
                "decoding",
                "two-stage decoding with asynchronous binarization",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This second point is emphasized by the second paper on self-training for adaptation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "self-training for adaptation",
                "is emphasized",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "second paper",
                "is emphasized",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In Lin and Och  , we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "framework",
                "automatically evaluated",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "manual translations",
                "without further human involvement",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Suhm and Waibel   and Eckert, Gallwitz, and Niemann   each condition a recognizer LM on left-to-right DA predictions and are able to 366 Stolcke et al. Dialogue Act Modeling show reductions in word error rate of 1% on task-oriented corpora.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "recognizer LM",
                "conditioned on left-to-right DA predictions",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "word error rate",
                "reductions of 1% on task-oriented corpora",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ISBNs, originally proposed for constituent parsing in  , use vectors of binary latent variables to encode information about the parse history.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ISBNs",
                "use vectors of binary latent variables",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parse history",
                "encode information about",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Finally we use Minimum Error Training     to train log-linear scaling factors that are applied to the WFSTs in Equation 1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-linear scaling factors",
                "are applied to the WFSTs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Minimum Error Training",
                "to train",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This is most prominently evidenced by the PENN TREEBANK  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PENN TREEBANK",
                "most prominently evidenced by",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "PENN TREEBANK",
                "PENN TREEBANK",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  uses the mutual information clustering algorithm described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information clustering algorithm",
                "described in ",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "mutual information clustering algorithm",
                "described in ",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": " , Ponzetto and Strube   and the exploitation of advanced techniques that involve joint learning  ) and joint inference  ) for coreference resolution and a related extraction task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ponzetto and Strube",
                "joint learning and joint inference",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "advanced techniques",
                "involving joint learning and joint inference",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "We then ranked the collected query pairs using loglikelihoodratio  , whichmeasures the dependence between q1 and q2 within the context of web queries  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "loglikelihoodratio",
                "measures the dependence between q1 and q2",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "query pairs",
                "using loglikelihoodratio",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "This paper, however, aims at the k-best tree algorithms whose packed representations are hypergraphs    , which includes most parsers and parsing-based MT decoders.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "k-best tree algorithms",
                "packed representations are hypergraphs",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "most parsers and parsing-based MT decoders",
                "includes",
                "APPLICABILITY",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "The training is performed by a single generalized perceptron  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron",
                "generalized",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "training",
                "performed by a single",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The tagger used is thus one that does not need tagged and disambiguated material to be trained on, namely the XPOST originally constructed at Xerox Parc  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tagger",
                "does not need tagged and disambiguated material to be trained on",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "XPOST",
                "originally constructed",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Relatedness scores are computed for each pair of senses of the grammatically linked pair of words  , using the WordNet-Similarity-1.03 package and the lesk 759 option  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet-Similarity-1.03 package",
                "using",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "lesk 759 option",
                "option",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 Related Work Many methods have been developed for automatically identifying subjective   words, e.g.,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "many",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "developed for automatically identifying subjective words",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "hen and Martin   explored the use of a range of syntactic and semantic features in unsupervised clustering of documents",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "range of syntactic and semantic",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "clustering",
                "unsupervised",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We have already shown in Section 3 how to solve  ; here we avoid   by maximizing conditional likelihood, marginalizing out the hidden variable, denotedz: max vector summationdisplay x,y p log summationdisplay z pvector    This sort of conditional training with hidden variables was carried out by Koo and Collins  , for example, in reranking; it is related to the information bottleneck method   and contrastive estimation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "conditional likelihood",
                "maximizing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "hidden variables",
                "marginalizing out",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "To tune all lambda weights above, we perform minimum error rate training   on the development set described in Section 7.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lambda weights",
                "minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "development set",
                "described in Section 7",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Collocations were extracted according to the method described in   by moving a window on texts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "window",
                "moving on texts",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "method",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Mihalcea   shows that Wikipedia can indeed be used as a sense inventory for sense disambiguation",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wikipedia",
                "can indeed be used as a sense inventory",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "sense disambiguation",
                "can be used",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2.1 BLEU is essentially a precision-based metric and is currently the standard metric for automatic evaluation of MT performance.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "standard metric",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "metric",
                "precision-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Narrative retellings provide a natural, conversational speech sample that can be analyzed for many of the characteristics of speech and language that have been shown to discriminate between healthy and impaired subjects, including syntactic complexity   and mean pause duration  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sample",
                "natural, conversational",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "pause duration",
                "mean",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The results evaluated by BLEU score   is shown in Table 2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU score",
                "is evaluated by",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "results",
                "shown in Table 2",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The focus of much of the automatic sentiment analysis research is on identifying the affect bearing words   and on measurement approaches for sentiment  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "affect bearing",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "measurement approaches",
                "for sentiment",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As in tile HMM we easily can extend the dependencies in the alignment model of Model 4 easily using the word class of the previous English word E = G , or the word class of the French word F = G   .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "alignment model",
                "easily extended using word class",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dependencies",
                "extended in Model 4",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This analysis depends on the SPECIALIST Lexicon and the Xerox part-of-speech tagger   and provides simple noun phrases that are mapped to concepts in the UMLS Metathesaurus using MetaMap  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SPECIALIST Lexicon",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Xerox part-of-speech tagger",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Rapp  , Dunning  ) but using cosine rather than cityblock distance to measure profile similarity.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "profile similarity",
                "cosine rather than cityblock distance",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "distance",
                "cosine rather than cityblock",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Various learning models have been studied such as Hidden Markov models    , decision trees   and maximum entropy models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "learning models",
                "Hidden Markov models, decision trees, maximum entropy models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "learning models",
                "studied",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "6 Related works After the work of Ramshaw and Marcus  , many machine learning techniques have been applied to the basic chunking task, such as Support Vector Machines  , Hidden Markov Model , Memory Based Learning  , Conditional Random Fields  , and so on.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning techniques",
                "have been applied",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Conditional Random Fields",
                "and so on",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "These feature vectors and the associated parser actions are used to train maximum entropy models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature vectors and the associated parser actions",
                "are used to train",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "maximum entropy models",
                "",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In Yarowsky's experiment  , an average of 3936 examples were used to disambiguate between two senses.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Yarowsky's experiment",
                "an average of 3936 examples were used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "examples",
                "used",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The per-state models in this paper are log-linear models, building upon the models in   and  , though some models are in fact strictly simpler.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "log-linear models",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "models",
                "are in fact strictly simpler",
                "INNOVATION",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "  Here, the candidate generator gen  enumerates candidates of destination   strings, and the scorer P  denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model   and maximum entropy framework  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "scorer",
                "modeled by a noisy-channel model and maximum entropy framework",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "candidate generator gen",
                "enumerates candidates of destination strings",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our approach is based on earlier work on LFG semantic form extraction   and recent progress in automatically annotating the Penn-II treebank with LFG f-structures  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "LFG semantic form extraction",
                "earlier work",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Penn-II treebank with LFG f-structures",
                "recent progress",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Motivated by our goal of representing syntax, we used part-of-speech   tags as labeled by a maximum entropy tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "part-of-speech tags",
                "labeled by a maximum entropy tagger",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "maximum entropy tagger",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Proceedings of the 40th Annual Meeting of the Association for In a key step for locating important sentences, NeATS computes the likelihood ratio    to identify key concepts in unigrams, bigrams, and trigrams1, using the ontopic document collection as the relevant set and the off-topic document collection as the irrelevant set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NeATS",
                "computes likelihood ratio",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "document collection",
                "relevant and irrelevant sets",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "avigli   has induced clusters by mapping WordNet senses to a more coarse-grained lexical resource",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WordNet senses",
                "to a more coarse-grained lexical resource",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "clusters",
                "induced by mapping",
                "INNOVATION",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Dialogs Speakers Turns Words Fragments Distinct Words Distinct Words/POS Singleton Words Singleton Words/POS Intonational Phrases Speech Repairs 98 34 6163 58298 756 859 1101 252 350 10947 2396 Table 1: Size of the Trains Corpus 2.1 POS Annotations Our POS tagset is based on the Penn Treebank tagset  , but modified to include tags for discourse markers and end-of-turns, and to provide richer syntactic information  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tagset",
                "based on the Penn Treebank tagset",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "POS tagset",
                "modified to include tags for discourse markers and end-of-turns, and to provide richer syntactic information",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "BABAR uses the log-likelihood statistic   to evaluate the strength of a co-occurrence relationship.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-likelihood statistic",
                "to evaluate the strength of a co-occurrence relationship",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "co-occurrence relationship",
                "strength",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4 Experiments Our experiments were conducted on CoNLL-2007 shared task domain adaptation track   using treebanks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Experiments",
                "conducted on CoNLL-2007 shared task domain adaptation track",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "treebanks",
                "using treebanks",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We adopt an approach, similar to  , in which the meaning representation, in our case XML, is transformed into a sorted flat list of attribute-value pairs indicating the core contentful concepts of each command.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "meaning representation",
                "transformed into a sorted flat list of attribute-value pairs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "XML",
                "in our case",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The best prosodic label sequence is then, L = argmax L nproductdisplay i P    To estimate the conditional distribution P  we use the general technique of choosing the maximum entropy   distribution that estimates the average of each feature over the training data  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prosodic label sequence",
                "is the best",
                "METHODOLOGY",
                "positive",
                0.85
            ],
            [
                "maximum entropy distribution",
                "estimates the average of each feature",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Using linguistic principles to recover empty categories Richard CAMPBELL Microsoft Research One Microsoft Way Redmond, WA 98052 USA richcamp@microsoft.com Abstract This paper describes an algorithm for detecting empty nodes in the Penn Treebank  , finding their antecedents, and assigning them function tags, without access to lexical information such as valency.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "without access to lexical information",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "detecting empty nodes",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.3 Language Model We estimate P  using n-gram LMs trained on data from the Web, using Stupid Backoff  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-gram LMs",
                "trained on data from the Web",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Stupid Backoff",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This is possible because of the availability of statistical parsers, which can be trained on human-annotated treebanks   for multiple languages;   The binding theory is used as a guideline and syntactic structures are encoded as features in a maximum entropy coreference system;   The syntactic features are evaluated on three languages: Arabic, Chinese and English  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical parsers",
                "trained on human-annotated treebanks",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntactic features",
                "encoded as features in a maximum entropy coreference system",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Related to this issue, we note that the head rules, which were nearly identical to those used in  , have not been tuned at all to this task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "head rules",
                "not been tuned",
                "METHODOLOGY",
                "negative",
                0.6
            ],
            [
                "head rules",
                "nearly identical to those used in",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner  , which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bootstrapping",
                "similar motivation to the annealing approach",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "bootstrapping",
                "use of bootstrapping in general is quite widespread",
                "APPLICABILITY",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "One way around this dif culty is to stipulate that all rules must be binary from the outset, as in inversion-transduction grammar     and the binary synchronous context-free grammar   employed by the Hiero system   to model the hierarchical phrases.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rules",
                "must be binary from the outset",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Hiero system",
                "employed to model hierarchical phrases",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Using the ME principle, we can combine information from a variety of sources into the same language model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ME principle",
                "combine information from a variety of sources",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "language model",
                "same",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These are identical to prior work  , except that we add a root configuration that aligns the target parent-child pair to null and the head word of the source sentence, respectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "root configuration",
                "aligns the target parent-child pair to null and the head word of the source sentence",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "work",
                "prior",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Background 2.1 Hybrid Logic Dependency Semantics Hybrid Logic Dependency Semantics   is an ontologically promiscuous   framework for representing the propositional content   of an expression as an ontologically richly sorted, relational structure.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Hybrid Logic Dependency Semantics",
                "ontologically promiscuous framework",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "propositional content",
                "ontologically richly sorted, relational structure",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The sentences were processed using Collins parser   to generate parse-trees automatically.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "generate parse-trees automatically",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "parse-trees",
                "automatically",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The trends are the same as in  : Adding NANC data improves parsing performance on BROWN development considerably, improving the f-score from 83.9% to 86.4%.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NANC data",
                "improves parsing performance",
                "PERFORMANCE",
                "positive",
                0.9
            ],
            [
                "f-score",
                "from 83.9% to 86.4%",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "These forest rescoring algorithms have potential applications to other computationally intensive tasks involving combinations of different models, for example, head-lexicalized parsing  ; joint parsing and semantic role labeling  ; or tagging and parsing with nonlocal features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "potential applications to other computationally intensive tasks",
                "APPLICABILITY",
                "neutral",
                0.8
            ],
            [
                "combinations of different models",
                "involving head-lexicalized parsing ; joint parsing and semantic role labeling ; or tagging and parsing with nonlocal features",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "esearchers have mostly looked at representing words by their surrounding words   and by their syntactical contexts  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "words",
                "by their surrounding words and by their syntactical contexts",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "representation",
                "by their surrounding words and by their syntactical contexts",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.2 Results In line with previous work  , we first compare Naive Bayes and Logistic regression on the two NLP tasks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Naive Bayes and Logistic regression",
                "on the two NLP tasks",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "previous work",
                "in line with",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Word-to-Word Bitext Alignment We will study the problem of aligning an English sentence to a French sentence and we will use the word alignment of the IBM statistical translation models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM statistical translation models",
                "word alignment",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "problem of aligning an English sentence to a French sentence",
                "studying",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4 Towards an adequate similarity esfimatation for the building of ontologies The comparison with the similarity score of   shows that SYCLADE similarity indicator is specifically relevant for ontology bootstrap and tuning.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SYCLADE similarity indicator",
                "is specifically relevant",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "similarity score",
                "is adequate",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "A richer set of features besides n-grams should be checked, and we should not ignore the potential effectiveness of unigrams in this task  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "richer set",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "unigrams",
                "potential effectiveness",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Many studies and improvements have been conducted for  Presently with Service Media Laboratory, Corporate ResearchandDevelopmentCenter, OkiElectricIndustry Co. ,Ltd. POS tagging, and major methods of POS tagging achieve an accuracy of 9697% on the Penn Treebank WSJ corpus, but obtaining higher accuracies is difficult  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tagging",
                "major methods of POS tagging achieve an accuracy of 9697%",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "methods",
                "obtaining higher accuracies is difficult",
                "METHODOLOGY",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "The goal of each selection stage is to select the feature f that maximizes the gain of the log likelihood, where the a and gain of f are derived through following steps: Let the log likelihood of the model be  -=  yx xZysump pL,, )  = p  f  x,y  With the approximation assumption in Berger et al  s paper, the un-normalized component and the normalization factor of the model have the following recursive forms: )| | L -L  =- p    x  /Z S  ) +aE  p     The maximum approximate gain and its corresponding a are represented as: ) ,(~ a fS GfSL  =D maxarg f 3 A Fast Feature Selection Algorithm The inefficiency of the IFS algorithm is due to the following reasons.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature f",
                "maximizes the gain of the log likelihood",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IFs algorithm",
                "inefficiency due to following reasons",
                "LIMITATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "Within the NLP community, n-best list ranking has been looked at carefully in parsing, extractive summarization  , and machine translation  , to name a few.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-best list ranking",
                "has been looked at carefully",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "n-best list ranking",
                "to name a few",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Preparing an aligned abbreviation corpus, we obtain the optimal combination of the features by using the maximum entropy framework  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "optimal combination",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "maximum entropy framework",
                "using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In earlier work   only singletons were used as seed words; varying their number allows us to test whether multiple seed words have a positive effect in detection performance.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "seed words",
                "number of",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "multiple seed words",
                "have a positive effect",
                "INNOVATION",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "In fact, it has already been established that sentence level classification can improve document level analysis  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentence level classification",
                "can improve",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "document level analysis",
                "",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Reranking methods have also been proposed as a method for using syntactic information  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reranking methods",
                "using syntactic information",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Unsupervised systems   are based on generative models trained with the EM algorithm.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generative models",
                "trained with the EM algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "unsupervised systems",
                "based on generative models",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These methods have been used in machine translation  , terminology research and translation aids  , bilingual lexicography  , collocation studies  , word-sense disambiguation   and information retrieval in a multilingual environment  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "used in machine translation, terminology research, translation aids, bilingual lexicography, collocation studies, word-sense disambiguation, and information retrieval",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "multilingual environment",
                "multilingual",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Its roots are the same as computational linguistics  , but it has been largely ignored in CL until recently  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "computational linguistics",
                "largely ignored",
                "APPLICABILITY",
                "negative",
                0.8
            ],
            [
                "CL",
                "until recently",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  proposed sentence alignment techniques based on dynamic programming, using sentence length and lexical mapping information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentence alignment techniques",
                "based on dynamic programming",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence length and lexical mapping information",
                "using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The weights for the various components of the model   are set by minimum error rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model components",
                "set by minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "minimum error rate training",
                "is used to set weights",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This paper extends the IBM Machine Translation Group's concept of fertility   to the generation of clumps for natural language understanding.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "concept of fertility",
                "to the generation of clumps",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "IBM Machine Translation Group",
                "concept",
                "INNOVATION",
                "positive",
                0.7
            ]
        ]
    },
    {
        "text": "Having a single, canonical tree structure for each possible alignment can help when flattening binary trees, as it indicates arbitrary binarization decisions  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tree structure",
                "indicates arbitrary binarization decisions",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "binarization decisions",
                "can help when flattening binary trees",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This paper presents an empirical study measuring the effectiveness of our evaluation functions at selecting training sentences from the Wall Street Journal   corpus   for inducing grammars.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation functions",
                "selecting training sentences",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Wall Street Journal corpus",
                "for inducing grammars",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Moreover, under this view, SMT becomes quite similar to sequential natural language annotation problems such as part-of-speech tagging and shallow parsing, and the novel training algorithm presented in this paper is actually most similar to work on training algorithms presented for these task, e.g. the on-line training algorithm presented in   and the perceptron training algorithm presented in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training algorithm",
                "most similar to work on training algorithms",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "novel training algorithm",
                "presented in this paper",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In the literature on the kappa statistic, most authors address only category data; some can handle more general data, such as data in interval scales or ratio scales  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data",
                "category data",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "data",
                "interval scales or ratio scales",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our method is based on the Extended String Subsequence Kernel     which is a kind of convolution kernel  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Extended String Subsequence Kernel",
                "kind of convolution kernel",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Extended String Subsequence Kernel",
                "is a kind of convolution kernel",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Concrete similarity measures compare a pair of weighted context feature vectors that characterize two words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "weighted context feature vectors",
                "characterize two words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "pair",
                "compare",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  discuss three approaches: hand-crafted rules; grammatical inference of subsequential transducers; and log-linear classifiers with bigram and trigram features used as taggers  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "hand-crafted rules",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "taggers",
                "bigram and trigram features",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We combine different parametrization of   BLEU  , NIST  , and TER  , to give a total of roughly 100 features.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parametrization",
                "different",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "features",
                "roughly 100",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The model weights of the transducer are tuned based on the development set using a grid-based line search, and the translation results are evaluated based on a single Chinese reference6 using BLEU-4  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model weights",
                "tuned using a grid-based line search",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation results",
                "evaluated using BLEU-4",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In general, previous work in opinion mining includes document level sentiment classification using supervised   and unsupervised methods  , machine learning techniques and sentiment classification considering rating scales  , and scoring of features  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "supervised and unsupervised methods",
                "includes",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "machine learning techniques",
                "using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The system described in   also makes use of syntactic heuristics.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic heuristics",
                "makes use of",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "system",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "1 Introduction The Inversion Transduction Grammar or ITG formalism, which historically was developed in the context of translation and alignment, hypothesizes strong expressiveness restrictions that constrain paraphrases to vary word order only in certain allowable nested permutations of arguments  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITG formalism",
                "strong expressiveness restrictions",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "paraphrases",
                "vary word order only in certain allowable nested permutations of arguments",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "  describe an error-driven transformation-based learning   method for finding NP chunks in texts.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transformation-based learning",
                "method",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "NP chunks",
                "finding in texts",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It is based on code and ideas from the system of Ponzetto and Strube  , but also includes some ideas from GUITAR   and other coreference systems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ponzetto and Strube's system",
                "code and ideas",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "GUITAR and other coreference systems",
                "ideas",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "To train models, we used projectivized versions of the training dependency trees.2 1We are grateful to the providers of the treebanks that constituted the data for the shared task  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treebanks",
                "constituted the data for the shared task",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "training dependency trees",
                "projectivized versions of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use a standard data set   consisting of sections 15-19 of the WSJ corpus as training and section 20 as testing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WSJ corpus",
                "standard data set",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "section 20",
                "testing",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For this paper, we used POS tags that were provided either by the Treebank itself   or by the perceptron POS tagger3 presented in Collins  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tags",
                "provided by Treebank or perceptron POS tagger",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "perceptron POS tagger",
                "presented in Collins",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For instance, the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model; the tagged words in IGT can be used to label the resulting clusters produced by the word clustering approach; the frequent and unambiguous words in the target lines can serve as prototype examples in the prototype-driven approach  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "frequency",
                "collected from the data",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "IGT",
                "can be used to label the resulting clusters",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The translation and reference files are analyzed by a treebank-based, probabilistic Lexical-Functional Grammar   parser  , which produces a set of dependency triples for each input.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "treebank-based, probabilistic Lexical-Functional Grammar",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "dependency triples",
                "produces a set of",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It is also related to  linear models described in Berger, Della Pietra, and Della Pietra  , Xue  ; Och  , and Peng, Feng, and McCallum  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "described in Berger, Della Pietra, and Della Pietra ; Xue ; Och ; and Peng, Feng, and McCallum",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "models",
                "linear",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "However, in the Grammar Association context, when developing   the basic equations of the system presented in  , it is said that the reverse model for a28 a13a37a3a38a5a39a32a21a0a35a7 does not seem to admit a simple factorization which is also correct and convenient, so crude heuristics were adopted in the mathematical development of the expression to be maximized.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reverse model",
                "does not seem to admit a simple factorization",
                "METHODOLOGY",
                "neutral",
                0.7
            ],
            [
                "crude heuristics",
                "adopted",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Our model improves the baseline provided by  :   accuracy is increased by creating a lexicalised PCFG grammar and enriching conditioning context with parent f-structure features; and   coverage is increased by providing lexical smoothing and fuzzy matching techniques for rule smoothing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexicalised PCFG grammar",
                "improves accuracy",
                "METHODOLOGY",
                "positive",
                0.8
            ],
            [
                "fuzzy matching techniques",
                "increases coverage",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction Inversion transduction grammar   constraints   provide coherent structural constraints on the relationship between a sentence and its translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar",
                "provide coherent structural constraints",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "constraints",
                "on the relationship between a sentence and its translation",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Our human word alignments do not distinguish between Sure and Probable links  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "human word alignments",
                "do not distinguish",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "links",
                "between Sure and Probable",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "We compute log-likelihood significance between features and target nouns  ) and keep only the most significant 200 features per target word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "most significant 200 features per target word",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "log-likelihood significance",
                "compute",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "A detailed discussion on the use of kappa in natural language processing is presented in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa",
                "in natural language processing",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "use of kappa",
                "is presented",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "6.1 Hiero Results Using the MT 2002 test set, we ran the minimumerror rate training     with the decoder to tune the weights for each feature.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT 2002 test set",
                "tune the weights",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "decoder",
                "to",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4For justification for this kind of logical form for sentences with quantifiers and inteusional operators, see Hobbs  and Hobbs  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "quantifiers and interusal operators",
                "logical form",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "Hobbs and Hobbs",
                "justification",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In Brown et al  , the authors provide some sample subtrees resulting from such a 1,000-word clustering.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "subtrees",
                "resulting from 1,000-word clustering",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "sample",
                "some",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Thus, GCNF is a more restrictive normal form than those used by Wu   and Melamed  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GCNF",
                "more restrictive",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Wu and Melamed's normal form",
                "used",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": " ) or Wikipedia  , and the contextual role played by an NP  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NP",
                "played by an NP",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "Wikipedia",
                "contextual role",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Each dataset consisted of a collection of flat rules such as Sput!NP put NP PP extracted from the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "flat rules",
                "such as",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Other methods include rule-based systems  , maximum entropy models  , and memory-based models  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rule-based systems",
                "other methods",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "maximum entropy models",
                "other methods",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "memory-based models",
                "other methods",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Mutual Informatio n Church and Hanks   discussed the use of the mutual information statistics as a way to identify a variety of interesting linguistic phenomena, ranging from semanti c relations of the doctor/nurse type   to lexico-syntactic co-occurrence preferences between verbs and prepositions  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Church and Hanks",
                "mutual information statistics",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "linguistic phenomena",
                "interesting",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Wu  s Inversion Transduction Grammar, as well as tree-transformation models of translation such as Yamada and Knight  , Galley et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wu's Inversion Transduction Grammar",
                "as well as",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "tree-transformation models",
                "such as",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "The token precision is higher than 90% in all of the corpora, including the movie domain, which is considered to be difficult for SA  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "higher than 90%",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "movie domain",
                "difficult for SA",
                "LIMITATION",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "Our evaluation metrics are BLEU   and NIST, which are to perform caseinsensitive matching of n-grams up to n = 4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU and NIST",
                "to perform caseinsensitive matching of n-grams up to n = 4",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "n-grams",
                "up to n = 4",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.2 Mapping Mapping the identified units   to their equivalents in the other language was achieved by training a new translation model   using the EM algorithm as described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation model",
                "using the EM algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "EM algorithm",
                "as described in ",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Some are the result of inconsistency in labeling in the training data  , which usually reflects a lack of linguistic clarity or determination of the correct part of speech in context.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training data",
                "inconsistency in labeling",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "part of speech",
                "lack of linguistic clarity or determination",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We can stipulate the time line to be linearly ordered   nor in approaches employing branching futures  ), and we can stipulate it to be dense  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "time line",
                "linearly ordered",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "time line",
                "dense",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As the baseline standard, we took the ending-guessing rule set supplied with the Xerox tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Xerox tagger",
                "supplied with",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "ending-guessing rule set",
                "taken as",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Metrics based on word alignment between MT outputs and the references  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "based on word alignment between MT outputs and the references",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "word alignment",
                "between MT outputs and the references",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One heuristic approach is to adapt the self-training algorithm   to our model.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "self-training algorithm",
                "adapt to our model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "self-training algorithm",
                "adapt",
                "METHODOLOGY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Yarowsky   proposed an unsupervised method that used heuristics to obtain seed classifications and expanded the results to the other parts of the corpus, thus avoided the need to hand-annotate any examples.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "unsupervised",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "results",
                "avoided the need to hand-annotate any examples",
                "PERFORMANCE",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "Alternative Class-Based Estimation Methods The approaches used for comparison are that of Resnik  , subsequently developed by Ribas  , and that of Li and Abe  , which has been adopted by McCarthy  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "used for comparison",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Li and Abe",
                "has been adopted by McCarthy",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "FollowingJohnson ,Iusevariational Bayes EM   during the M-step for the transition distribution: l+1j|i = f f    f  = exp )   60   = braceleftBigg g  ifv> 7    1v o.w.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "transition distribution",
                "f",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "f",
                "exp",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "ore details about why heuristics are needed and the process used to map sources to NPs can be found in Stoyanov and Cardie  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "heuristics",
                "needed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "process",
                "used to map sources to NPs",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 The Effect of Training Corpus Size A number of past research work on WSD, such as  , were tested on a small number of words like \"\"line\"\" and \"\"interest\"\".",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Training Corpus Size",
                "number of past research work on WSD",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "words",
                "like 'line' and 'interest'",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "We use the GIZA++ implementation of IBM Model 4   coupled with the phrase extraction heuristics of Koehn et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "implementation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrase extraction heuristics of Koehn et al.",
                "of",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The MSLR parser   performs syntactic analysis of the sentence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MSLR parser",
                "performs syntactic analysis",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence",
                "is analyzed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "MT output is evaluated using the standard MT evaluation metric BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MT evaluation metric",
                "standard MT evaluation metric",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "BLEU",
                "standard",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "e collected training samples from the Brown Corpus distributed with the Penn Treebank  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Brown Corpus",
                "distributed with the Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank",
                "distributed with",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.7 Fertility-Based Transducer In  , three alignment models are described that include fertility models, these are IBM Models 3, 4, and 5.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Finally, in section 4 we add additional features to the maxent model, and chain these models into a conditional markov model  , as used for tagging   or earlier NER work  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maxent model",
                "additional features",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "conditional markov model",
                "used for tagging or earlier NER work",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Training Procedure Our algorithm is a modification of the perceptron ranking algorithm  , which allows for joint learning across several ranking problems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "a modification of the perceptron ranking algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ranking problems",
                "joint learning across several",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The most important tree-bank transformation in the literature is lexicalization: Each node in a tree is labeled with its head word, the most important word of the constituent under the node  , Collins  , Charniak  , Collins  , Carroll and Rooth  , etc.).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "tree-bank transformation",
                "lexicalization",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "head word",
                "most important word",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We use a standard maximum entropy classifier   implemented as part of MALLET  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy classifier",
                "implemented as part of MALLET",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "standard",
                "standard",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "A broad view of the possible scope of lexical semantics would thus be one which tries to chart out the systematic, generalizable aspects of word meanings, and of the relations between words, drawing on readily accessible sources of lexical knowledge, such as machine readable dictionaries, encyclopedias, and representative corpora, coupled with the kind of analytic apparatus that is needed to fruitfully explore such sources, for instance custom-built parsers to cope with dictionary definitions  , statistical programs to deal with the distributional properties of lexical items in large corpora   etc. At the same time this kind of massive data-acquisition should be made sensitive to the borders between perceptual experience, lexical knowledge and expert knowledge.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "lexical semantics",
                "systematic, generalizable aspects",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "massive data-acquisition",
                "sensitive to borders",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Part-of-speech tags are assigned by the MXPOST maximum-entropy based part-of-speech tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MXPOST maximum-entropy based part-of-speech tagger",
                "is assigned by",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "part-of-speech tags",
                "are assigned",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our decoder is a phrase-based multi-stack imple5 mentation of the log-linear model similar to Pharaoh  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "phrase-based multi-stack implementation of the log-linear model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "log-linear model",
                "similar to Pharaoh",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "More importantly, the ratio of binarizability, as expected, decreases on freer word-order languages  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "binarizability",
                "decreases",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "word-order languages",
                "freer",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The problem is typically presented in log-space, which simplifies computations, but otherwise does not change the problem due to the monotonicity of the log function   log p  = summationdisplay m m hm    Phrase-based models   are limited to the mapping of small contiguous chunks of text.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-space",
                "simplifies computations",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "log function",
                "does not change the problem",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For example, a perceptron algorithm is used for joint Chinese word segmentation and POS tagging  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perceptron algorithm",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Chinese word segmentation and POS tagging",
                "joint",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Also, adding a constituent size/distance effect, as described by Schubert   and as used by some researchers in parsing   and Collins  ) would almost certainly improve parsing.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Schubert and some researchers",
                "improve parsing",
                "INNOVATION",
                "positive",
                0.85
            ],
            [
                "constituent size/distance effect",
                "as described by Schubert and as used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We obtain aligned parallel sentences and the phrase table after the training of Moses, which includes running GIZA++  , grow-diagonal-final symmetrization and phrase extraction  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Moses",
                "includes running GIZA++",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase table",
                "after training",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "When we consider the frequency of discourse relations, i.e. 43% for ELABORATION, 32% for CONTRAST etc. , the weighted accuracy was 53% using only lexical information, which is comparable to the similar experiment by   of 49.7%.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "frequency of discourse relations",
                "43% for ELABORATION, 32% for CONTRAST etc.",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "weighted accuracy",
                "53%",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "These belong to two main categories based on machine learning   and language or domain specific rules  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine learning",
                "machine learning",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "language or domain specific rules",
                "language or domain specific rules",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "622 We also identified a length effect similar to that studied by   for self-training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "length effect",
                "similar to that studied by",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "self-training",
                "for self-training",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Effective training algorithm exists   once the set of features a42 a57 a16 a1a33a8 a71a54a8 a71a100a85a68a5 a53 is selected.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "once the set of features is selected",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "algorithm",
                "exists",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We used minimum error rate training   and the A* beam search decoder implemented by Koehn  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "A* beam search decoder",
                "implemented by Koehn",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "minimum error rate training",
                "used",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Note that conditioning on the rules parent is needed to disallow the structure   for further discussion.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rules parent",
                "needed to disallow",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "structure",
                "for further discussion",
                "LIMITATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": " ,   thesaurus categories  ,   translation in another language  ,   automatically induced clusters with sublexical representation  , and   hand-crafted lexicons  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "thesaurus categories",
                "hand-crafted",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation in another language",
                "automatically induced clusters with sublexical representation",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We follow IBM Model 1   and assume that each word in an utterance is generated by exactly one role in the parallel frame Using standard EM to learn the role to word mapping is only sufficient if one knows to which level in the tree the utterance should be mapped.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 1",
                "standard EM",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "utterance",
                "mapped to the level in the tree",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Schtze, 1993) is not suited to highly skewed distributions omni-present in natural language.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distributions",
                "omni-present in natural language",
                "LIMITATION",
                "neutral",
                0.9
            ],
            [
                "Schtze, 1993",
                "not suited",
                "INNOVATION",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "The first system is the Pharaoh decoder provided by   for the shared data task.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pharaoh decoder",
                "provided by",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "shared data task",
                "shared",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "This generates tens of millions features, so we prune those features that occur fewer than 10 total times, as in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "occur fewer than 10 total times",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "prune",
                "as in ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Entropy, used in some part-of-speech tagging systems  , is a measure of how much information is necessary to separate data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "entropy",
                "measure of how much information is necessary to separate data",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "part-of-speech tagging systems",
                "used in some",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Many of the current approaches of domain modeling collapse together different instances and make the decision on what information is important for a domain based on this generalized corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "domain modeling",
                "collapse together different instances",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "corpus",
                "generalized",
                "INNOVATION",
                "negative",
                0.75
            ]
        ]
    },
    {
        "text": "In   the model is optimized to produce a block orientation and the target sentence is used only for computing a sentence level BLEU.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "optimized to produce a block orientation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence level BLEU",
                "used for computing",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "It differs from the many approaches where   is defined by a stochastic synchronous grammar   and from transfer-based systems defined by context-free grammars  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approaches",
                "defined by a stochastic synchronous grammar",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "systems",
                "defined by context-free grammars",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Between these two extremes, there has been a relatively modest amount of work in sentence simplification   and document compression   in which words, phrases, and sentences are selected in an extraction process.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentence simplification",
                "modest amount of work",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "extraction process",
                "selected",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.2 Word Order Differences Another problem that has been noticed as early as 1993 with the first research on word alignment   concerns the differences in word order between source and target language.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research on word alignment",
                "1993",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "word order",
                "differences",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The domain axioms will bind the body variables to their most likely referents during unification with facts, and previously assumed and proven propositions similarly to  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "domain axioms",
                "bind the body variables to their most likely referents during unification with facts",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "propositions",
                "similarly to",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "5.2 Translation experiments with a bigram language model In this section we consider two real translation tasks, namely, translation from English to French, trained on Europarl   and translation from German to Spanish training on the NewsCommentary corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "bigram language model",
                "translation tasks",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "NewsCommentary corpus",
                "training on",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Traditionally, generative word alignment models have been trained on massive parallel corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "generative word alignment models",
                "trained on massive parallel corpora",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "massive parallel corpora",
                "massive",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Phrases of up to 10 in length on the French side were extracted from the parallel text, and minimum-error-rate training   was 8 We can train on the full training data shown if tighter constraints are placed on rule extraction for the United Nations data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "rule extraction",
                "tighter constraints are placed",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "training data",
                "minimum-error-rate",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4 Dependency Parsing: Baseline 4.1 Learning Model and Features According to  , all data-driven models for dependency parsing that have been proposed in recent years can be described as either graph-based or transition-based.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "graph-based or transition-based",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "models",
                "proposed in recent years",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this form, the distinction between our two models is sometimes referred to as \\joint versus conditional\"\"   rather than \\generative versus discriminative\"\"  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "joint versus conditional",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "models",
                "generative versus discriminative",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This alignment system is powered by the IBM translation models  , in which one sentence generates the other.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM translation models",
                "powers the alignment system",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "one sentence generates the other",
                "process description",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Originally introduced as a byproduct of training statistical translation models in  , word alignment has become the first step in training most statistical translation systems, and alignments are useful to a host of other tasks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical translation models",
                "byproduct of training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "alignments",
                "useful to a host of other tasks",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Hindle   proposed dealing with the sparseness problem by estimating the likelihood of unseen events from that of \"\"similar\"\" events that have been seen.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "events",
                "similar events that have been seen",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "unseen events",
                "from that of similar events",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "In comparison, we deployed the GIZA++ MT modeling tool kit, which is an implementation of the IBM Models 1 to 4  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ MT modeling tool kit",
                "implementation of the IBM Models 1 to 4",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM Models 1 to 4",
                "implementation",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For the maximum entropy classifier, we estimate the weights by maximizing the likelihood of a heldout set, using the standard IIS algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IIS algorithm",
                "standard",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "weights",
                "maximizing the likelihood",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The Collins   model does not use context-free rules, but generates the next category using zeroth order Markov chains  , hence no information about the previous sisters is included.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins model",
                "does not use context-free rules",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Markov chains",
                "no information about previous sisters is included",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Moses used the development data for minimum error-rate training   of its small number of parameters.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "small number of",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "development data",
                "used for minimum error-rate training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We used the Penn Treebank WSJ corpus   to perform the empirical evaluation of the considered approaches.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank WSJ corpus",
                "to perform empirical evaluation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "approaches",
                "considered",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In marked contrast to annotated training material for partof-speech tagging,   there is no coarse-level set of sense distinctions widely agreed upon  ;   sense annotation has a comparatively high error rate  ; and   no fully automatic method provides high enough quality output to support the \"\"annotate automatically, correct manually\"\" methodology used to provide high volume annotation by data providers like the Penn Treebank project  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sense distinctions",
                "widely agreed upon",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "annotation method",
                "high enough quality output",
                "METHODOLOGY",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "In addition to their use in machine translation  , translation models can be applied to machineassisted translation  , cross-lingual information retrieval  , and gisting of World Wide Web pages  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "translation models",
                "can be applied to",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "machine translation",
                "use in",
                "APPLICABILITY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The kappa statistic   has become the de facto standard to assess inter-annotator agreement.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa statistic",
                "de facto standard",
                "INNOVATION",
                "positive",
                0.85
            ],
            [
                "inter-annotator agreement",
                "assess",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In order to get a better understanding of these matters, we replicate parts of the error analysis presented by McDonald and Nivre  , where parsing errors are related to different structural properties of sentences and their dependency graphs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "error analysis",
                "presented by McDonald and Nivre",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parsing errors",
                "related to different structural properties of sentences and their dependency graphs",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": " , whose constrained optimization technique is similar to those in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "constrained optimization technique",
                "similar to those in",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "technique",
                "similar",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "The Powells algorithm used in this work is similar as the one from   but we modi ed the line optimization codes, a subroutine of Powells algorithm, with reference to  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Powells algorithm",
                "similar to the one from but",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "line optimization codes",
                "modi ed",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "To contrast,   concentrated on analyzing human-written summaries in order to determine how professionals construct summaries.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "concentrated",
                "analyzing human-written summaries",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "professionals construct summaries",
                "construct summaries",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.5 Maximum Entropy Model In order to build a unified probabilistic query alteration model, we used the maximum entropy approach of  , which Li et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum Entropy Model",
                "approach of Li et al.",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "query alteration model",
                "unified probabilistic",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Pointwise mutual information   was used to measure strength of selection restrictions for instance by Church and Hanks  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pointwise mutual information",
                "was used",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "strength of selection restrictions",
                "measure",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.1 System Tuning Minimum error training   under BLEU   was used to optimise the feature weights of the decoder with respect to the dev2006 development set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "decoder",
                "optimise the feature weights",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dev2006 development set",
                "used to",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For instance, Church and Hanks   calculated SA in terms of mutual information between two words wl and w2: N * f  I  = log2   f f  here N is the size of the corpus used in the estimation, f  is the frequency of the cooccurrence, f  and f  that of each word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "used in the estimation",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "frequency of cooccurrence",
                "is the frequency",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The performance of PB-SMT system is measured with BLEU score  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PB-SMT system",
                "measured with BLEU score",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "BLEU score",
                "is used as a metric",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "arletta   argues that the kappa statistic   should be adopted to judge annotator consistency for classification tasks in the area of discourse and dialogue analysis",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa statistic",
                "should be adopted",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "annotator consistency",
                "judge",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Given a set of evidences E over all the relevant word pairs, in  , the probabilistic taxonomy learning task is defined as the problem of finding the taxonomy hatwideT that maximizes the 67 probability of having the evidences E, i.e.: hatwideT = arg max T P  In  , this maximization problem is solved with a local search.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evidences E",
                "all the relevant word pairs",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "probabilistic taxonomy learning task",
                "maximizes the probability",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Following Collins  , we used the averaged parameters from the training algorithm in decoding heldout and test examples in our experiments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training algorithm",
                "averaged parameters",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parameters",
                "used in decoding",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "However, much recent work in machine learning and statistics has turned away from maximum-likelihood in favor of Bayesian methods, and there is increasing interest in Bayesian methods in computational linguistics as well  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "Bayesian methods",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "interest",
                "increasing interest",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Several automatic sentence alignment approaches have been proposed based on sentence length   and lexical information  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentence length",
                "sentence length and lexical information",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "automatic sentence alignment approaches",
                "proposed",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "But if one limits the information used for disambiguation of the PPattachment to include only the verb, the noun representing its object, the preposition and the main noun in the PP, the accuracy for human decision degrades from 93.2% to 88.2%   on a dataset extracted from Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "information used for disambiguation",
                "only the verb, the noun representing its object, the preposition and the main noun in the PP",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "accuracy",
                "degrades from 93.2% to 88.2%",
                "PERFORMANCE",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "The original Ramshaw and Marcus   publication evaluated their NP chunker on two data sets, the second holding a larger amount of training data   while using 00 as test data.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ramshaw and Marcus",
                "publication",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "training data",
                "larger amount of",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The traditional method of evaluating similarity in a semantic network by measuring the path length between two nodes   also captures this, albeit indirectly, when the semantic network is just an IS-A hierarchy: if the minimal path of IS-A links between two nodes is long, that means it is necessary to go high in the taxonomy, to more abstract concepts, in order to find their least upper bound.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic network",
                "just an IS-A hierarchy",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "path length",
                "long",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Turneys   work on classiflcation of reviews is perhaps the closest to ours.2 He applied a speciflc unsupervised learning technique based on the mutual information between document phrases and the words \\excellent\"\" and \\poor\"\", where the mutual information is computed using statistics gathered by a search engine.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Turneys' work",
                "closest to ours",
                "INNOVATION",
                "positive",
                0.8
            ],
            [
                "unsupervised learning technique",
                "speciflc",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The Xerox experiments   correspond to something between D1 and D2, and between TO and T1, in that there is some initial biasing of the probabilities.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Xerox experiments",
                "correspond to something between D1 and D2",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "probabilities",
                "initial biasing",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "  proposes two approximate models based on the variational approach.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "based on the variational approach",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "variational approach",
                "approximate models",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "We use GIZA++   to train generative directed alignment models: HMM and IBM Model4   from training record-text pairs.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++",
                "to train generative directed alignment models",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "HMM and IBM Model4",
                "from training record-text pairs",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Galley   used skip-chain Conditional Random Fields to model pragmatic dependencies between paired meeting utterances  , and used a combination of lexical, prosodic, structural and discourse features to rank utterances by importance.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "skip-chain Conditional Random Fields",
                "model pragmatic dependencies",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "utterances",
                "ranked by importance",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Huang and Chiang   searches with the full model, but makes assumptions about the the amount of reordering the language model can trigger in order to limit exploration.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "makes assumptions",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "reordering",
                "can trigger",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Please note that our approach is very different from other approaches to context dependent rule selection such as   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "approach",
                "very different",
                "METHODOLOGY",
                "negative",
                0.75
            ],
            [
                "rule selection",
                "such as and ",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Experiments are presented in table 1, using BLEU   and METEOR5  , and we also show the length ratio  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU and METEOR",
                "are used",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "length ratio",
                "is shown",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "  0.19-0.48 Leacock & Chodrow   0.36 Lin   0.36 Resnik   0.37 Proposed 0.504 7 Conclusion We proposed a relational model to measure the semantic similarity between two words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "relational model",
                "to measure the semantic similarity between two words",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "semantic similarity",
                "between two words",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Other recent work has incorporated constituent and dependency subtrees into the translation rules used by phrase-based systems  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based systems",
                "used by",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "translation rules",
                "incorporated constituent and dependency subtrees",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As a result, they are being used in a variety of applications, such as question answering  , speech recognition  , language modeling  , language generation   and, most notably, machine translation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "applications",
                "such as question answering, speech recognition, language modeling, language generation and, most notably, machine translation",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "machine translation",
                "most notably",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Considerations of sentence fluency are also key in sentence simplification  , sentence compression  , text re-generation for summarization   and headline generation  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentence fluency",
                "key in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence simplification",
                "sentence compression",
                "APPLICABILITY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Construct a parse chart with a CKY parser simultaneously constrained on the foreign string and English tree, similar to the bilingual parsing of Wu   1.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CKY parser",
                "simultaneously constrained on the foreign string and English tree",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "bilingual parsing of Wu",
                "similar to",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "To solve the problem, Cahill and van Genabith   apply an automatic generation grammar transformation to their training data: they automatically label CFG nodes with additional case information and the model now learns the new improved generation rules of Tables 4 and 5.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "training data",
                "automatically label CFG nodes with additional case information",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "generation rules",
                "improved generation rules",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Supervised methods include hidden Markov model  , maximum entropy, conditional random fields  , and support vector machines    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hidden Markov model",
                "Supervised methods",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "support vector machines",
                "Supervised methods",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Haghighi and Klein   develop a prototype-driven approach, which requires just a few prototype examples for each POS tag and exploits these labeled words to constrain the labels of their distributionally similar words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "prototype-driven approach",
                "requires just a few prototype examples",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "distributionally similar words",
                "exploits these labeled words to constrain the labels",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.1 Data and Experimental Setup The data set by Pang and Lee   consists of 2000 movie reviews   from the IMDb review archive.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data set",
                "consists of 2000 movie reviews",
                "PERFORMANCE",
                "neutral",
                1.0
            ],
            [
                "IMDb review archive",
                "from the",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "ing and McKeown   and Jing   propose a cut-and-paste strategy as a computational process of automatic abstracting and a sentence reduction strategy to produce concise sentences",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "cut-and-paste strategy",
                "computational process",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentence reduction strategy",
                "produce concise sentences",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The pchemtb-closed shared task   is used to illustrate our models.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "pchemtb-closed shared task",
                "is used",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "models",
                "our",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "3.2 Translation Scores The translation scores for four different systems are reported in Table 1.5 Baseline: In this system, we use the GIZA++ toolkit  , a suffix-array architecture  , the SRILM toolkit  , and minimum error rate training   to obtain word-alignments, a translation model, language models, and the optimal weights for combining these models, respectively.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GIZA++ toolkit",
                "a suffix-array architecture",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "SRILM toolkit",
                "minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4 Using vector-based models of semantic representation to account for the systematic variances in neural activity 4.1 Lexical Semantic Representation Computational linguists have demonstrated that a words meaning is captured to some extent by the distribution of words and phrases with which it commonly co-occurs  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "vector-based models of semantic representation",
                "account for systematic variances",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "words and phrases with which it commonly co-occurs",
                "captures the meaning",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The data was segmented into baseNP parts and nonbaseNP parts in a similar fashion as the data used by  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data",
                "similar fashion",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "parts",
                "baseNP and nonbaseNP",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Recently, some generic methods were proposed to handle context-sensitive inference  , but these usually treat only a single aspect of context matching  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "treat only a single aspect of context matching",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "methods",
                "usually",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Movie-review dataset consists of positive and negative reviews from the Internet Movie Database   archive  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Internet Movie Database",
                "archive",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "reviews",
                "positive and negative",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Machine translation based on a deeper analysis of the syntactic structure of a sentence has long been identified as a desirable objective in principle  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntactic structure of a sentence",
                "deeper analysis",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "desirable objective",
                "in principle",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "5.3 Performance of Taxonomy Induction In this section, we compare the following automatic taxonomy induction systems: HE, the system by Hearst   with 6 hypernym patterns; GI, the system by Girju et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HE",
                "with 6 hypernym patterns",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "GI",
                "by Girju et al.",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "However, most of them do not build a NEs resource but exploit external gazetteers  ,  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "NEs resource",
                "do not build",
                "INNOVATION",
                "negative",
                0.7
            ],
            [
                "gazetteers",
                "exploit external",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The difficulty of this task is that the standard method for converting NER to a sequence tagging problem with BIOencoding  , where each 1http://www.nist.gov/speech/tests/ace/ index.htm token is assigned a tag to indicate whether it is at the beginning  , inside  , or outside   of an entity, is not directly applicable when tokens belong to more than one entity.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "standard method",
                "BIOencoding",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tokens belong to more than one entity",
                "not directly applicable",
                "METHODOLOGY",
                "negative",
                0.85
            ]
        ]
    },
    {
        "text": "4 Features Features used in our experiments are inspired by previous work on corpus-based approaches for discourse analysis  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Features",
                "inspired by previous work",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpus-based approaches",
                "for discourse analysis",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "While these are based on a relatively few number of items, and while we have not performed any tests to determine whether the differences in ? are statistically significant, the results 7The Czech-English conditions were excluded since there were so few systems 146 are nevertheless interesting, since three metrics have higher correlation than Bleu: ??Semantic role overlap  , which makes its debut in the proceedings of this workshop ??ParaEval measuring recall  , which has a model of allowable variation in translation that uses automatically generated paraphrases   ??Meteor   which also allows variation by introducing synonyms and by flexibly matches words using stemming.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "metrics",
                "have higher correlation than Bleu",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "results",
                "are nevertheless interesting",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Then, by using evaluations similar to those described in   and by Rapp  , we show that the best distance-based measures correlate better overall with human association scores than do the best window based configurations  , and that they also serve as better predictors of the strongest human associations  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "distance-based measures",
                "correlate better overall",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "distance-based measures",
                "serve as better predictors",
                "PERFORMANCE",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "Secondly, we used the Kappa coefficient  , which has become the standard evaluation metric and the score obtained was 0.905.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa coefficient",
                "has become the standard evaluation metric",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "score obtained",
                "0.905",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This fact is being seriously challenged by current research  , and might not be true in the near future  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "challenging",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "fact",
                "not being true",
                "LIMITATION",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "Such coarse-grained inventories can be produced manually from scratch   or by automatically relating   or clustering   existing word senses.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "inventories",
                "can be produced",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "word senses",
                "existing",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "164 and Itai, 1990; Dagan et al. , 1995; Kennedy and Boguraev, 1996a; Kennedy and Boguraev, 1996b).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "papers",
                "164 and Itai, 1990; Dagan et al., 1995; Kennedy and Boguraev, 1996a; Kennedy and Boguraev, 1996b",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "references",
                "164 and Itai, 1990; Dagan et al., 1995; Kennedy and Boguraev, 1996a; Kennedy and Boguraev, 1996b",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For the MER training  , we modified Koehns MER trainer   for our tree sequence-based system.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Koehns MER trainer",
                "modified",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "tree sequence-based system",
                "our",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Class-based n-gram models have also been shown to benefit from their reduced number of parameters when scaling to higher-order n-grams  , and even despite the increasing size and decreasing sparsity of language model training corpora  , class-based n-gram models might lead to improvements when increasing the n-gram order.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-gram models",
                "reduced number of parameters",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "n-gram models",
                "might lead to improvements",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "We have found, however, that collocational evidence can be employed to suggest which noun compounds reflect taxonomic relationships, using a strategy similar to that employed by Hindle   for detecting synonyms.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "collocational evidence",
                "can be employed to suggest",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Hindle's strategy",
                "similar to that employed by",
                "METHODOLOGY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "The parameters of the models are estimated by iterative maximum-likelihood training on a large parallel corpus of natural language texts using the EM algorithm  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parameters",
                "estimated by iterative maximum-likelihood training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "EM algorithm",
                "used for training",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Usually the IBM Model 1, developed in the statistical machine translation field  , is used to construct translation models for retrieval purposes in practice.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM Model 1",
                "used to construct translation models",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Model 1",
                "developed in the statistical machine translation field",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Between them, the phrase-based approach   allows local reordering and contiguous phrase translation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase-based approach",
                "allows local reordering and contiguous phrase translation",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "local reordering",
                "contiguous phrase translation",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models   or a high order polynomial  )1 for a sub-class of weighted synchronous context free grammars  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "inference procedure",
                "non-trivial",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase based models",
                "NP-complete",
                "METHODOLOGY",
                "negative",
                0.8
            ]
        ]
    },
    {
        "text": "2 Related Work A number of researchers   have described approaches that preprocess the source language input in SMT systems.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SMT systems",
                "preprocess the source language input",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "approaches",
                "described",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "For handling word identities, one could follow the approach used for handling the POS tags   and view the POS tags and word identities as two separate sources of information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tags",
                "two separate sources of information",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word identities",
                "approach used for handling the POS tags",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This fact, along with the observation that machine translation quality improves as the amount of monolingual training material increases, has lead to the introduction of randomised techniques for representing large LMs in small space  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "machine translation quality",
                "improves",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "LMs",
                "representing large in small space",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Works on word similarity and word sense disambiguation are generally based on statistical methods designed for large or even very large corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "methods",
                "statistical methods",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "corpora",
                "large or very large",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Much of this work has been fueled by the availability of large corpora annotated with syntactic structures, especially the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpora",
                "annotated with syntactic structures",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank",
                "especially",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "In the field of statistical analysis of natural language data, it is common to use measures of lexical association, such as the informationtheoretic measure of mutual information, to extract useful relationships between words  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measures of lexical association",
                "informationtheoretic measure of mutual information",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "mutual information",
                "to extract useful relationships between words",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "Models of this type include:  , which use semantic word clustering, and  , which uses variablelength context.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "semantic word clustering",
                "semantic word clustering",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "variable-length context",
                "uses variable-length context",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Recently, specic probabilistic tree-based models have been proposed not only for machine translation  , but also for summarization  , paraphrasing  , natural language generation  , parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein  Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "probabilistic tree-based models",
                "proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "natural language generation",
                "not only for",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "n alternative to linear models is the log-linear models suggested by Och  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "log-linear models",
                "suggested by Och",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "log-linear models",
                "alternative to linear models",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The Bridge system uses the XLE   parser to produce syntactic structures and then the XLE ordered rewrite system to produce linguistic semantics   and abstract knowledge representations.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "XLE parser",
                "produce syntactic structures",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "XLE ordered rewrite system",
                "produce linguistic semantics and abstract knowledge representations",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Moreover, for reasons discussed by Wu  , ITGs possess an interesting intrinsic combinatorial property of permitting roughly up to four arguments of any frame to be transposed freely, but not more.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ITGs",
                "possess an interesting intrinsic combinatorial property",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "arguments",
                "to be transposed freely",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PropBank",
                "argument structure annotation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Penn Treebank",
                "syntactic structures",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2.1.3 Correlation analysis As a correlation measure between terms, we use mutual information  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information",
                "as a correlation measure",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "terms",
                "between terms",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "4.2 Binarization Schemes Besides the baseline   and iterative cost reduction binarization methods, we also perform right-heavy and random synchronous binarizations for comparison.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "binarization schemes",
                "right-heavy and random synchronous",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "binarization methods",
                "iterative cost reduction",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Errors from the sentence boundary detector in GATE   were especially problematic because they caused the Collins parser to fail, resulting in no dependency tree information.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "GATE",
                "sentence boundary detector",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Collins parser",
                "fail",
                "METHODOLOGY",
                "negative",
                0.7
            ]
        ]
    },
    {
        "text": "The separation of these two requirements 7 A more precise account of what it means to be able to identify an object is beyond the scope of this paper; for further details, see the discussions by Hobbs  , Appelt  , Kronfeld  , and Morgenstern  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "requirements",
                "separation of these two",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "account",
                "be able to identify an object",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "he fact that different authors use different versions of the same gold standard to evaluate similar experiments   versus Johnson  ) supports this claim",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "gold standard",
                "different versions",
                "INNOVATION",
                "neutral",
                0.9
            ],
            [
                "authors",
                "use",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "6 Related Work A large body of previous work exists on extending WORDNET with additional concepts and instances  ; these methods do not address attributes directly.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "previous work",
                "extending WORDNET with additional concepts and instances",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "methods",
                "do not address attributes directly",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "Classes were identified using a POS tagger   trained on the tagged Switchboard corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS tagger",
                "trained on the tagged Switchboard corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Switchboard corpus",
                "tagged",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Work focusses on analysing subjective features of text or speech, such as sentiment, opinion, emotion or point of view  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "subjective features",
                "of text or speech",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "sentiment",
                "point of view",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "IIowever,   have shown that knowledge of target-text length is not crucial to the model's i)ertbrmanee.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "target-text length",
                "is not crucial",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "model's performance",
                "is not crucial",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Our starting point is the work done by Zettlemoyer and Collins on parsing using relaxed CCG grammars    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Zettlemoyer and Collins",
                "work done on parsing using relaxed CCG grammars",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "relaxed CCG grammars",
                "used in parsing",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For instance, the HALOGEN statistical realizer  , converting them into its input formalism, and then producing output strings.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "HALOGEN statistical realizer",
                "converting into its input formalism",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "output strings",
                "producing",
                "PERFORMANCE",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "he model of Haghighi and Klein   incorporated a latent variable for named entity class",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "incorporated a latent variable for named entity class",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "latent variable",
                "for named entity class",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Previous approaches to the problem   have all been learning-based; the primary difference between the present algorithm and earlier ones is that it is not learned, but explicitly incorporates principles of GovernmentBinding theory  , since that theory underlies the annotation.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "not learned, but explicitly incorporates principles of GovernmentBinding theory",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "GovernmentBinding theory",
                "underlies the annotation",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "he principal training method is an adaptation of averaged perceptron learning as described by Collins  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "The unlabeled data for English we use is the union of the Penn Treebank tagged WSJ data   and the BLLIP corpus.5 For the rest of the languages we use only the text of George Orwells novel 1984, which is provided in morphologically disambiguated form as part of MultextEast  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank tagged WSJ data",
                "is used",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "George Orwell's novel 1984",
                "is provided",
                "METHODOLOGY",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Part-ofspeech taggers are used in a few applications, such as speech synthesis   and question answering  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "part-ofspeech taggers",
                "in a few applications",
                "APPLICABILITY",
                "neutral",
                0.9
            ],
            [
                "speech synthesis and question answering",
                "such as",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We build a subset S C ~\"\" incrementally by iterating to adjoin a feature f E ~\"\" which maximizes loglikelihood of the model to S. This algorithm is called the Basic Feature Selection  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature",
                "maximizes loglikelihood of the model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Basic Feature Selection",
                "algorithm",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": " , and component weights are adjusted by minimum error rate training  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "component weights",
                "adjusted by minimum error rate training",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "component weights",
                "minimum error rate",
                "PERFORMANCE",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "  Whether this is a useful perspective for machine translation is debatable  --however, it is a dead-on description of transliteration.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "perspective",
                "useful",
                "INNOVATION",
                "neutral",
                0.75
            ],
            [
                "description",
                "dead-on",
                "APPLICABILITY",
                "positive",
                0.9
            ]
        ]
    },
    {
        "text": "In fact, the WtoP model is a segmental Hidden Markov Model  , in which states emit observation sequences.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "WtoP model",
                "segmental Hidden Markov Model",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "states emit observation sequences",
                "observation sequences",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Although to a lesser extent, measures of word relatedness have also been applied on other languages, including German  , Chinese  , Dutch   and others.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "measures of word relatedness",
                "have also been applied",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "others",
                "others",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "synchronous grammar rules",
                "extracted from word aligned sentence pairs",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "syntactic parse",
                "annotated with",
                "METHODOLOGY",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "In NLP community, it has been shown that having more data results in better performance  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "data",
                "more",
                "PERFORMANCE",
                "positive",
                0.8
            ],
            [
                "performance",
                "better",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "We follow the work of   and choose the hypothesis that best agrees with other hypotheses on average as the backbone by applying Minimum Bayes Risk   decoding  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hypothesis",
                "that best agrees with other hypotheses on average",
                "INNOVATION",
                "neutral",
                0.85
            ],
            [
                "Minimum Bayes Risk decoding",
                "applying",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The translation quality is evaluated by BLEU metric  , as calculated by mtevalv11b.pl with case-insensitive matching of n-grams, where n =4.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU metric",
                "as calculated by mtevalv11b.pl with case-insensitive matching of n-grams",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "mtevalv11b.pl",
                "with case-insensitive matching of n-grams",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "5.2 A data recovery task In the second evaluation, the estimation method had to distinguish between members of two sets of 8It should be emphasized that the TWS method uses only a monolingual target corpus, and not a bilingual corpus as in other methods  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "method",
                "uses a monolingual target corpus",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "method",
                "distinguish between members of two sets of 8",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "PMI   between two phrases is de ned as: log2 prob prob   prob  PMI is positive when two phrases tend to co-occur and negative when they tend to be in a complementary distribution.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PMI",
                "is positive when two phrases tend to co-occur",
                "INNOVATION",
                "positive",
                0.75
            ],
            [
                "PMI",
                "is negative when they tend to be in a complementary distribution",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Grefenstette   studied two context delineation methods of English nouns: the window-based and the syntactic, whereby all the different types of syntactic dependencies of the nouns were used in the same feature space.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "context delineation methods",
                "window-based and syntactic",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntactic dependencies",
                "used in the same feature space",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Finally, our modeling approaches follow the recent work on both local classifier-based modeling of complex learning problems  , as well as global discriminative approaches based on CRFs  , SVM  , and the Perceptron algorithm   that we used in our experiments.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Perceptron algorithm",
                "used in our experiments",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "CRFs, SVM",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "here are two necessary ingredients to implement Ochs   training procedure",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Ochs training procedure",
                "necessary ingredients",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "necessary",
                "training procedure",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "It has been shown that phrasal machine translation systems are not affected by the quality of the input word alignments  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrasal machine translation systems",
                "not affected by the quality of the input word alignments",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "quality of the input word alignments",
                "not affected",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Modeling reordering as the inversion in order of two adjacent blocks is similar to the approach taken by the Inverse Transduction Model    , except that here we are not limited to a binary tree.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "reordering",
                "the inversion in order of two adjacent blocks",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "approach",
                "taken by the Inverse Transduction Model",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Intuitively, if we allow any Source words to be aligned to any Target words, the best alignment that we can come up with is the one in Figure 1.c. Sentence pair   offers strong evidence that b c in language S means the same thing as x in language T. On the basis of this evidence, we expect the system to also learn from sentence pair   that a in language S means the same thing as y in language T. Unfortunately, if one works with translation models that do not allow Target words to be aligned to more than one Source word  as it is the case in the IBM models    it is impossible to learn that the phrase b c in language S means the same thing as word x in language T. The IBM Model 4  , for example, converges to the word alignments shown in Figure 1.b and learns the translation probabilities shown in Figure 1.a.2 Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure 2To train the IBM-4 model, we used Giza  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM models",
                "do not allow Target words to be aligned to more than one Source word",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "IBM Model 4",
                "converges to the word alignments shown in Figure 1.b and learns the translation probabilities shown in Figure 1.a",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "In the probabilistic LR model, probabilities are assigned to tree 696 Precision Recall F-score Time   Best-First Classifier-Based   88.1 87.8 87.9 17 Deterministic     85.4 84.8 85.1 < 1 Charniak & Johnson   91.3 90.6 91.0 Unk Bod   90.8 90.7 90.7 145* Charniak   89.5 89.6 89.5 23 Collins   88.3 88.1 88.2 39 Ratnaparkhi   87.5 86.3 86.9 Unk Tsuruoka & Tsujii  : deterministic 86.5 81.2 83.8 < 1* Tsuruoka & Tsujii  : search 86.8 85.0 85.9 2* Sagae & Lavie   86.0 86.1 86.0 11* Table 1: Summary of results on labeled precision and recall of constituents, and time required to parse the test set.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Charniak & Johnson",
                "91.3 90.6 91.0",
                "PERFORMANCE",
                "positive",
                0.85
            ],
            [
                "Charniak",
                "89.5 89.6 89.5",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In this paper, we implement the SDB model in a state-of-the-art phrase-based system which adapts a binary bracketing transduction grammar     to phrase translation and reordering, described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SDB model",
                "phrase-based system",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "phrase translation and reordering",
                "binary bracketing transduction grammar",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3.1 Agreement for Emotion Classes The kappa coefficient of agreement is a statistic adopted by the Computational Linguistics community as a standard measure for this purpose  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "kappa coefficient of agreement",
                "a standard measure",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "Computational Linguistics community",
                "adopted as a standard",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "The disambiguation model of Enju is based on a feature forest model  , which is a log-linear model   on packed forest structure.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature forest model",
                "log-linear model on packed forest structure",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "disambiguation model",
                "based on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Recently there have been some works on using multiple treebanks for domain adaptation of parsers, where these treebanks have the same grammar formalism  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "treebanks",
                "have same grammar formalism",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "works",
                "on using multiple treebanks for domain adaptation of parsers",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "One prominent constraint of the IBM word alignment models   is functional alignment, that is each target word is mapped onto at most one source word.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM word alignment models",
                "functional alignment",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "target word",
                "mapped onto at most one source word",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Our approach is to use maximum entropy models   to learn a suitable mapping from features derived from the words in the ASR output to semantic frames.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "maximum entropy models",
                "to learn a suitable mapping",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "semantic frames",
                "derived from words in the ASR output",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "We split the returned documents into classes encompassing n-grams  , adjectives  ) and noun phrases  ).",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "n-grams",
                "encompassing",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "noun phrases",
                "encompassing",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4.2 A ROUGE Based Approach ROUGE   is the standard automatic evaluation metric in the Summarization community.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE",
                "standard automatic evaluation metric",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "Summarization community",
                "ROUGE is in",
                "APPLICABILITY",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "In the tagging domain, Collins   compared log-linear and perceptron training for HMM-style tagging based on dynamic programming.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Collins",
                "compared log-linear and perceptron training",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "HMM-style tagging",
                "based on dynamic programming",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Johnson   reports results for different numbers of hidden states but it is unclear how to make this choice a priori, while Goldwater & Griffiths   leave this question as future work.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "hidden states",
                " unclear how to make this choice a priori",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "Goldwater & Griffiths",
                "leave this question as future work",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "  was an implicit or selforganizing syntax model as it did not use a Treebank.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntax model",
                "did not use a Treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "syntax model",
                "selforganizing",
                "INNOVATION",
                "positive",
                0.75
            ]
        ]
    },
    {
        "text": "In the following, ROUGE-SN denotes ROUGE-S with maximum skip distance N. ROUGE-SU   This measure is an extension of ROUGE-S; it adds a unigram as a counting unit.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "ROUGE-SN",
                "with maximum skip distance N",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "ROUGE-SU",
                "adds a unigram as a counting unit",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "One approach here is that of Wu  , in which word-movement is modeled by rotations at unlabeled, binary-branching nodes.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "word-movement",
                "modeled by rotations at unlabeled, binary-branching nodes",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Wu's approach",
                "one approach",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "2 Data and annotation Yahoo!s image query API was used to obtain a corpus of pairs of semantically ambiguous images, in thumbnail and true size, and their corresponding web sites for three ambiguous keywords inspired by  : BASS, CRANE, and SQUASH.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Yahoo!'s image query API",
                "used to obtain a corpus",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "ambiguous keywords",
                "inspired by BASS, CRANE, and SQUASH",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "1 Introduction Previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "assumption",
                "single score can express polarity of an opinion text",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "work on sentiment categorization",
                "makes an implicit assumption",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Like  , we used mutual information to measure the cohesion between two words.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "mutual information",
                "measure the cohesion between two words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "cohesion",
                "between two words",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "itation texts have also been used to create summaries of single scientific articles in Qazvinian and Radev   and Mei and Zhai  ",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Qazvinian and Radev",
                "create summaries of single scientific articles",
                "APPLICABILITY",
                "neutral",
                0.95
            ],
            [
                "Mei and Zhai",
                "create summaries of single scientific articles",
                "APPLICABILITY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The tags sets we shall examine are the set used in the Penn Tree Bank     and the C5 tag-set used by the CLAWS part-of-speech tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Tree Bank",
                "set",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "C5 tag-set",
                "used by CLAWS part-of-speech tagger",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Background The natural language generator used in our experiments is the WSJ-trained system described in Cahill and van Genabith   and Hogan et al",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "natural language generator",
                "WSJ-trained system",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "WSJ-trained system",
                "described in Cahill and van Genabith and Hogan et al",
                "INNOVATION",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Similar to, e.g.,  , we use a Naive Bayes algorithm trained on word features cooccurring with the subjective and the objective classifications.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Naive Bayes algorithm",
                "trained on word features cooccurring with subjective and objective classifications",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "word features",
                "cooccurring with subjective and objective classifications",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "The term global feature vector is used by Collins   to distinguish between feature count vectors for whole sequences and the local feature vectors in ME tagging models, which are Boolean valued vectors containing the indicator features for one element in the sequence.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "global feature vector",
                "used by Collins",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "local feature vectors",
                "Boolean valued vectors containing indicator features for one element in the sequence",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "2 Related Work Previous work on polarity disambiguation has used contextual clues and reversal words  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "work",
                "has used contextual clues and reversal words",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "work",
                "has used",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Dependency representation has been used for language modeling, textual entailment and machine translation  , to name a few tasks.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "dependency representation",
                "used for language modeling, textual entailment and machine translation",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "dependency representation",
                "to name a few tasks",
                "INNOVATION",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "To estimate combination weights, we extend the F 1 -score maximization training algorithm for LRM described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "F 1 -score maximization training algorithm",
                "described in  ",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "combination weights",
                "estimated using",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "ENGLISH GERMAN CHINESE       TrainSet Section 2-21 Sentences 1-18,602 Articles 26-270 DevSet Section 22 18,603-19,602 Articles 1-25 TestSet Section 23 19,603-20,602 Articles 271-300 Table 3: Experimental setup.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "TrainSet",
                "Section 2-21 Sentences 1-18,602 Articles 26-270",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "Experimental setup",
                "Table 3",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "On the other hand, models that deal with structures or phrases instead of single words have also been proposed: the syntax translation models are described in  , alignment templates are used in  , and the alignment template approach is re-framed into the so-called phrase based translation   in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "syntax translation models",
                "are described in",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "alignment templates",
                "are used in",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "2 Related Work Given its potential usefulness in coreference resolution, anaphoricity determination has been studied fairly extensively in the literature and can be classified into three categories: heuristic rule-based  , statistics-based   and learning-based  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "anaphoricity determination",
                "has been studied fairly extensively",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "anaphoricity determination",
                "can be classified",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Some studies exploit topically related articles derived from multiple news sources  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "studies",
                "exploit topically related articles",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "articles",
                "derived from multiple news sources",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Moreover, as stated in  , we assume that the alleged predicate is existentially opaque in its second argument.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "predicate",
                "existentially opaque",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "second argument",
                "alleged",
                "APPLICABILITY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "The factored translation model combines features in a log-linear fashion  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "features",
                "in a log-linear fashion",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "As far as the log-linear combination of float features is concerned, similar training procedures have been proposed in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "float features",
                "similar training procedures",
                "METHODOLOGY",
                "neutral",
                0.85
            ],
            [
                "training procedures",
                "have been proposed",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Table 3 compares precision, recall, and F scores for our system with CoNLL-2001 results training on sections 15-18 of the Penn Treebank and testing on section 21  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CoNLL-2001",
                "results",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "sections 15-18 of the Penn Treebank",
                "testing on",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Unfortunately, as shown in  , with the represetation of sentences that we use, linear classifiers cannot discriminate real sentences from sentences sampled from a trigram, which is the model we use as a baseline, so here we resort to a non-linear large-margin classifier  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentences",
                "cannot discriminate",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "classifier",
                "non-linear large-margin",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "So far, most of the statistical machine translation systems are based on the single-word alignment models as described in   as well as the Hidden Markov alignment model  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "statistical machine translation systems",
                "based on single-word alignment models",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "single-word alignment models",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "4 Methodology 4.1 Data In order to be able to compare our results with the results obtained by other researchers, we worked with the same data sets already used by   for NP and SV detection.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "One of the main directions is sentiment classification, which classifies the whole opinion document   as positive or negative  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sentiment classification",
                "classifies the whole opinion document as positive or negative",
                "PERFORMANCE",
                "neutral",
                0.9
            ],
            [
                "opinion document",
                "positive or negative",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "3 Previous Work on Subjectivity Tagging In previous work  , a corpus of sentences from the Wall Street Journal Treebank Corpus   was manually anno- tated with subjectivity classifications by multiple judges.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Wall Street Journal Treebank Corpus",
                "was manually annotated with subjectivity classifications",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "multiple judges",
                "by multiple judges",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "For testing purposes, we used the Wall Street Journal part of the Penn Treebank corpus  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank corpus",
                "part of the Wall Street Journal",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "Wall Street Journal",
                "part of the Penn Treebank corpus",
                "METHODOLOGY",
                "neutral",
                0.8
            ]
        ]
    },
    {
        "text": "As a common strategy, POS guessers examine the endings of unknown words   along with their capitalization, or consider the distribution of unknown words over specific parts-of-speech  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "POS guessers",
                "examine the endings of unknown words along with their capitalization",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "distribution of unknown words over specific parts-of-speech",
                "consider",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "7 Discussion As we mentioned, there are some algorithms similar to ours  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithms",
                "similar to ours",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "algorithms",
                "similar",
                "INNOVATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "It is known that PMI gives undue importance to low frequency events  , therefore the evaluation considers only pairs of genes that occur at least 5 times in the whole corpus.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "PMI",
                "gives undue importance",
                "METHODOLOGY",
                "negative",
                0.85
            ],
            [
                "evaluation",
                "considers pairs of genes",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "8 An alternative formula for G 2 is given in Dunning  , but the two are equivalent.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "G 2",
                "are equivalent",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "formula",
                "alternative",
                "INNOVATION",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "This direction has been forming the mainstream of research on opinion-sensitive text processing  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "research",
                "mainstream",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "opinion-sensitive text processing",
                "has been forming the direction",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "olan   described a heuristic approach to forming unlabeled clusters of closely related senses in a MRD",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": []
    },
    {
        "text": "Maximum entropy   models have been used in bilingual sense disambiguation, word reordering, and sentence segmentation  , parsing, POS tagging and PP attachment  , machine translation  , and FrameNet classification  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Maximum entropy models",
                "used in bilingual sense disambiguation, word reordering, and sentence segmentation, parsing, POS tagging and PP attachment, machine translation, and FrameNet classification",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "machine translation",
                "used in",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "When we run a phrase-based system, Pharaoh  , on this sentence  , we get the following phrases with translations:     [dipl.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Pharaoh",
                "phrase-based system",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "phrases with translations",
                "dipl.",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In   it was observed that a significant percent of the queries made by a user in a search engine are associated to a repeated search.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "queries",
                "made by a user in a search engine",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "queries",
                "are associated to a repeated search",
                "PERFORMANCE",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "Detailed description of those models can be found in  ,   and  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "models",
                "Detailed description can be found",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "models",
                "in  and ",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "We perform a statistical analysis that provides information that complements the information provided by Cohen's Kappa  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Cohen's Kappa",
                "provides information",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "information",
                "complements",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Translation quality is reported using case-insensitive BLEU  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "BLEU",
                "case-insensitive",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "Translation quality",
                "reported",
                "PERFORMANCE",
                "neutral",
                1.0
            ]
        ]
    },
    {
        "text": "As discussed in  , the direct translation model represents the probability of target sentence English e = e1eI being the translation for a source sentence French f = f1 fJ through an exponential, or log-linear model p  = exp )summationtext eprimeE exp )   where e is a single candidate translation for f from the set of all English translations E,  is the parameter vector for the model, and each hk is a feature function of e and f. In practice, we restrict E to the set Gen  which is a set of highly likely translations discovered by a decoder  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "model",
                "exponential or log-linear model",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "decoder",
                "discovered highly likely translations",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "5 The SemCor collection   is a subset of the Brown Corpus and consists of 352 news articles distributed into three sets in which the nouns, verbs, adverbs, and adjectives have been manually tagged with their corresponding WordNet senses and part-of-speech tags using Brills tagger  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "SemCor collection",
                "is a subset of the Brown Corpus",
                "INNOVATION",
                "neutral",
                0.95
            ],
            [
                "Brill's tagger",
                "using",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG  but Duchi et al.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "String alignment",
                "quite expensive",
                "PERFORMANCE",
                "negative",
                0.85
            ],
            [
                "ITG",
                "simple synchronous formalisms",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Several techniques and results have been reported on learning subcategorization frames   from text corpora  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "techniques",
                "reported",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "results",
                "have been reported",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For this experiment, we used sections 02 21 of the Penn Treebank     as the training data and section 23   for evaluation, as is now standard.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Penn Treebank",
                "as the training data",
                "METHODOLOGY",
                "neutral",
                1.0
            ],
            [
                "sections 02 21",
                "now standard",
                "APPLICABILITY",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "More specifically, the work on optimizing preference factors and semantic collocations was done as part of a project on spoken language translation in which the CLE was used for analysis and generation of both English and Swedish  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "CLE",
                "used for analysis and generation",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "spoken language translation project",
                "part of a project",
                "INNOVATION",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "138 2 Rule Generation We start with phrase translations on the parallel training data using the techniques and implementation described in  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrase translations",
                "using the techniques and implementation described in",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "techniques and implementation",
                "described in",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "For this purpose, we present a data-driven beam search algorithm similar to the one used in speech recognition search algorithms  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "algorithm",
                "similar to the one used in speech recognition search algorithms",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "data-driven beam search algorithm",
                "present",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "To tackle this problem, we defined 2The best results of Collins and Roark     are achieved when the parser utilizes the information about the final punctuation and the look-ahead.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "parser",
                "utilizes the information about final punctuation and look-ahead",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "results",
                "are achieved",
                "PERFORMANCE",
                "positive",
                0.85
            ]
        ]
    },
    {
        "text": "In the second pass, 5-gram and 6-gram zero-cutoff stupid-backoff   language models estimated using 4.7 billion words of English newswire text are used to generate lattices for phrasal segmentation model rescoring.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "language models",
                "5-gram and 6-gram zero-cutoff stupid-backoff",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "lattices for phrasal segmentation model rescoring",
                "generate",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "Feature weights vector are trained discriminatively in concert with the language model weight to maximize the BLEU   automatic evaluation metric via Minimum Error Rate Training    .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "feature weights vector",
                "trained discriminatively",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "BLEU automatic evaluation metric",
                "via Minimum Error Rate Training",
                "PERFORMANCE",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "In this sense, instead of measuring only the categorial agreement between annotators with the kappa statistic   or the performance of a system in terms of precision/recall, we could take into account the hierarchical organization of the categories or concepts by making use of measures considering the hierarchical distance between two concepts such as proposed by   or  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "categorial agreement",
                "measuring with kappa statistic",
                "PERFORMANCE",
                "neutral",
                0.8
            ],
            [
                "measures",
                "considering hierarchical distance between two concepts",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "  used transformation based learning using a large annotated corpus for English.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "corpus",
                "large annotated corpus",
                "METHODOLOGY",
                "neutral",
                0.95
            ],
            [
                "learning",
                "transformation based learning",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Unlexicalized methods refine the grammar in a more conservative fashion, splitting each non-terminal or pre-terminal symbol into a much smaller number of subsymbols  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "grammar",
                "refine the grammar in a more conservative fashion",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "subsymbols",
                "much smaller number of",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "1 Introduction Nowadays, statistical machine translation is mainly based on phrases  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "phrases",
                "statistical machine translation is mainly based on",
                "INNOVATION",
                "neutral",
                0.8
            ],
            [
                "statistical machine translation",
                "mainly based on",
                "METHODOLOGY",
                "neutral",
                0.85
            ]
        ]
    },
    {
        "text": "The model was trained on sections 221 from the English Penn Treebank  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "sections 221",
                "from the English Penn Treebank",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "Penn Treebank",
                "English",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Measurement of B.eliability The Kappa Statistic Following Jean Carletta  , we use the kappa statistic   to measure degree of agreement among subjects.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "Kappa Statistic",
                "to measure degree of agreement among subjects",
                "PERFORMANCE",
                "neutral",
                0.95
            ],
            [
                "Jean Carletta",
                "Following",
                "INNOVATION",
                "positive",
                0.8
            ]
        ]
    },
    {
        "text": "Further details are in the original paper  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "details",
                "in the original paper",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "original paper",
                "has further details",
                "LIMITATION",
                "neutral",
                0.7
            ]
        ]
    },
    {
        "text": "Following the evaluation methodology of Wong and Mooney  , we performed 4 runs of the standard 10-fold cross validation and report the averaged performance in this section using the standard automatic evaluation metric BLEU   and NIST  2.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "evaluation methodology",
                "of Wong and Mooney",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "performance",
                "in this section using the standard automatic evaluation metric BLEU and NIST 2",
                "PERFORMANCE",
                "neutral",
                0.9
            ]
        ]
    },
    {
        "text": "In our experiments these were obtained automatically using MXPOST   and BBNs Identifinder  .",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "MXPOST",
                "was used",
                "METHODOLOGY",
                "neutral",
                0.9
            ],
            [
                "BBNs Identifinder",
                "was used",
                "METHODOLOGY",
                "neutral",
                0.95
            ]
        ]
    },
    {
        "text": "Several teams had approaches that relied   on an IBM model of statistical machine translation  , with different improvements brought by different teams, consisting of new submodels, improvements in the HMM model, model combination for optimal alignment, etc. Se-veral teams used symmetrization metrics, as introduced in    , most of the times applied on the alignments produced for the two directions sourcetarget and targetsource, but also as a way to combine different word alignment systems.",
        "overall_sentiment": "neutral",
        "sentiment_quadruples": [
            [
                "IBM model of statistical machine translation",
                "different improvements brought",
                "METHODOLOGY",
                "neutral",
                0.8
            ],
            [
                "symmetrization metrics",
                "as a way to combine different word alignment systems",
                "METHODOLOGY",
                "neutral",
                0.9
            ]
        ]
    }
]