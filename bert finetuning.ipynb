{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51d007ce6fde2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from huggingface_hub import notebook_login\n",
    "import wandb\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a143281a31e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina' # 主题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f791f3cd024f5ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "plt.rcParams['figure.figsize'] = 12, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e923f934775c6f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.__version__)\n",
    "# print(torch.backends.cudnn.version())\n",
    "# print(torchvision.__version__)\n",
    "# print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef261e25aba5ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark     = False  # 启用Cudnn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b8fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型相关参数\n",
    "PRETRAIN = False\n",
    "base_model_name = 'roberta-llama3.1405B-twitter-sentiment'\n",
    "TEST_DATASET_SIZE = 0.4\n",
    "NUM_LABELS = 3\n",
    "DATA_PATH = './data/citation_sentiment_corpus_new.csv'\n",
    "if PRETRAIN:\n",
    "    BASE_MODEL_PATH = f'./citation_finetuned_models/{base_model_name}_itft'\n",
    "else:\n",
    "    BASE_MODEL_PATH = f'./pretrain_models/{base_model_name}'\n",
    "\n",
    "# 训练相关参数\n",
    "BATCH_SIZE = 32\n",
    "loss_type='focal_loss' # 自定义参数 focal_loss dsc_loss ce_loss asymmetric_loss\n",
    "weight_decay = 0.05  # 0.01\n",
    "lr = 2.0e-5  # 2e-5学习率，bs设置较大 5e-6\n",
    "num_epochs = 5\n",
    "warmup_ratio = 0.1\n",
    "warmup_steps = 100\n",
    "label_smoothing_factor = 0.1\n",
    "\n",
    "# notebook_login()\n",
    "os.environ[\"WANDB_PROJECT\"] = base_model_name\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = 'true' # save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_WATCH\"] = 'false' # turn off watch to log faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d0abcac06be516",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "texts = df['Citation_Text'].tolist()\n",
    "labels = df['Sentiment'].tolist()\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f24b1533bef739c",
   "metadata": {},
   "source": [
    "## 使用Trainer类微调citation的情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61bed57f56ae333",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label={0:\"Neutral\", 1:\"Positive\", 2:\"Negative\"}\n",
    "label2id={\"Neutral\":0, \"Positive\":1, \"Negative\":2}\n",
    "\n",
    "# 加载基座模型和分词器\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL_PATH, num_labels=NUM_LABELS, id2label=id2label, label2id=label2id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f5d22992aa877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    重构数据集类，使其能够返回字典格式的数据，有标签\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(texts, labels, test_size=TEST_DATASET_SIZE, stratify=labels, random_state=seed)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, stratify=temp_labels, random_state=seed)\n",
    "\n",
    "train_dataset = MyDataset(tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt', max_length=512), train_labels)\n",
    "test_dataset = MyDataset(tokenizer(test_texts, truncation=True, padding=True, return_tensors='pt', max_length=512), test_labels)\n",
    "val_dataset = MyDataset(tokenizer(val_texts, truncation=True, padding=True, return_tensors='pt', max_length=512), val_labels)\n",
    "\n",
    "# 将数据转换为 HuggingFace 的 Dataset 格式\n",
    "train_data = datasets.Dataset.from_dict({'text': train_texts, 'label': train_labels})\n",
    "val_data = datasets.Dataset.from_dict({'text': val_texts, 'label': val_labels})\n",
    "test_data = datasets.Dataset.from_dict({'text': test_texts, 'label': test_labels})\n",
    "emotion_data = datasets.DatasetDict({'train': train_data, 'validation': val_data, 'test': test_data})\n",
    "tokenized_data = emotion_data.map(lambda f: tokenizer(f['text'], padding=True, truncation=True, return_tensors='pt', max_length=512), batch_size=16, batched=True)  # WordPiece分词器\n",
    "# tokenized_data = tokenized_data.remove_columns(['text'])\n",
    "# train_dataset = tokenized_data['train']\n",
    "# val_dataset = tokenized_data['validation']\n",
    "# test_dataset = tokenized_data['test']\n",
    "\n",
    "print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
    "print(f\"Test Dataset Size: {len(test_dataset)}\")\n",
    "print(f\"Val Dataset Size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f20932b8f137b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Focal loss作为损失函数解决样本不均衡问题\n",
    "class MultiFocalLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Focal_Loss= -1*alpha*((1-pt)**gamma)*log(pt)\n",
    "    Args:\n",
    "        num_class: number of classes\n",
    "        alpha: class balance factor shape=[num_class, ]\n",
    "        gamma: hyperparameter\n",
    "        reduction: reduction type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_class, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(MultiFocalLoss, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.smooth = 1e-4\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        if alpha is None:\n",
    "            self.alpha = torch.ones(num_class, ) - 0.5\n",
    "        elif isinstance(alpha, (int, float)):\n",
    "            self.alpha = torch.as_tensor([alpha] * num_class)\n",
    "        elif isinstance(alpha, (list, np.ndarray)):\n",
    "            self.alpha = torch.as_tensor(alpha)\n",
    "        if self.alpha.shape[0] != num_class:\n",
    "            raise RuntimeError('the length not equal to number of class')\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        # assert isinstance(self.alpha,torch.Tensor)\\\n",
    "        alpha = self.alpha.to(logit.device)\n",
    "        prob = F.softmax(logit, dim=1)\n",
    "\n",
    "        if prob.dim() > 2:\n",
    "            # used for 3d-conv:  N,C,d1,d2 -> N,C,m (m=d1*d2*...)\n",
    "            N, C = logit.shape[:2]\n",
    "            prob = prob.view(N, C, -1)\n",
    "            prob = prob.transpose(1, 2).contiguous()  # [N,C,d1*d2..] -> [N,d1*d2..,C]\n",
    "            prob = prob.view(-1, prob.size(-1))  # [N,d1*d2..,C]-> [N*d1*d2..,C]\n",
    "\n",
    "        ori_shp = target.shape\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        prob = prob.gather(1, target).view(-1) + self.smooth  # avoid nan\n",
    "        logpt = torch.log(prob)\n",
    "        # alpha_class = alpha.gather(0, target.squeeze(-1))\n",
    "        alpha_weight = alpha[target.squeeze().long()]\n",
    "        loss = -alpha_weight * torch.pow(torch.sub(1.0, prob), self.gamma) * logpt\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'none':\n",
    "            loss = loss.view(ori_shp)\n",
    "\n",
    "        return loss\n",
    "\n",
    "# DSELoss\n",
    "class MultiDSCLoss(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Creates a criterion that optimizes a multi-class Self-adjusting Dice Loss\n",
    "    (\"Dice Loss for Data-imbalanced NLP Tasks\" paper)\n",
    "\n",
    "    Args:\n",
    "        alpha (float): a factor to push down the weight of easy examples\n",
    "        gamma (float): a factor added to both the nominator and the denominator for smoothing purposes\n",
    "        reduction (string): Specifies the reduction to apply to the output:\n",
    "            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
    "            ``'mean'``: the sum of the output will be divided by the number of\n",
    "            elements in the output, ``'sum'``: the output will be summed.\n",
    "\n",
    "    Shape:\n",
    "        - logits: `(N, C)` where `N` is the batch size and `C` is the number of classes.\n",
    "        - targets: `(N)` where each value is in [0, C - 1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1.0, smooth=1.0, reduction=\"mean\"):\n",
    "        super(MultiDSCLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.smooth = smooth\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        probs = torch.gather(probs, dim=1, index=targets.unsqueeze(1))\n",
    "\n",
    "        probs_with_factor = ((1 - probs) ** self.alpha) * probs\n",
    "        loss = 1 - (2 * probs_with_factor + self.smooth) / (probs_with_factor + 1 + self.smooth)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        elif self.reduction == \"none\" or self.reduction is None:\n",
    "            return loss\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Reduction `{self.reduction}` is not supported.\")\n",
    "\n",
    "# Asymmetric Loss\n",
    "class AsymmetricLoss(torch.nn.Module):\n",
    "    \"\"\" \n",
    "    Notice - optimized version, minimizes memory allocation and gpu uploading,\n",
    "    favors inplace operations\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=False):\n",
    "        super(AsymmetricLoss, self).__init__()\n",
    "\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
    "        self.eps = eps\n",
    "\n",
    "        # prevent memory allocation and gpu uploading every iteration, and encourages inplace operations\n",
    "        self.targets = self.anti_targets = self.xs_pos = self.xs_neg = self.asymmetric_w = self.loss = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: input logits\n",
    "        y: targets (multi-label binarized vector)\n",
    "        \"\"\"\n",
    "\n",
    "        self.targets = y\n",
    "        self.anti_targets = 1 - y\n",
    "\n",
    "        # Calculating Probabilities\n",
    "        self.xs_pos = torch.sigmoid(x)\n",
    "        self.xs_neg = 1.0 - self.xs_pos\n",
    "\n",
    "        # Asymmetric Clipping\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            self.xs_neg.add_(self.clip).clamp_(max=1)\n",
    "\n",
    "        # Basic CE calculation\n",
    "        self.loss = self.targets * torch.log(self.xs_pos.clamp(min=self.eps))\n",
    "        self.loss.add_(self.anti_targets * torch.log(self.xs_neg.clamp(min=self.eps)))\n",
    "\n",
    "        # Asymmetric Focusing\n",
    "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(False)\n",
    "            self.xs_pos = self.xs_pos * self.targets\n",
    "            self.xs_neg = self.xs_neg * self.anti_targets\n",
    "            self.asymmetric_w = torch.pow(1 - self.xs_pos - self.xs_neg,\n",
    "                                          self.gamma_pos * self.targets + self.gamma_neg * self.anti_targets)\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(True)\n",
    "            self.loss *= self.asymmetric_w\n",
    "\n",
    "        return -self.loss.sum()\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, loss_type='focal_loss', *args, **kwargs):\n",
    "        super(CustomTrainer, self).__init__(*args, **kwargs)\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        if self.loss_type == 'focal_loss':\n",
    "            loss_fct = MultiFocalLoss(num_class=3, alpha=0.8, gamma=2.0)\n",
    "        elif self.loss_type == 'dsc_loss':\n",
    "            loss_fct = MultiDSCLoss(alpha=1.0, smooth=1.0)\n",
    "        elif self.loss_type == 'asymmetric_loss':\n",
    "            loss_fct = AsymmetricLoss(gamma_pos=0.5, gamma_neg=3.0)\n",
    "        elif self.loss_type == 'ce_loss':\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30290e3f281f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    class_weights = np.bincount(labels) / len(labels)\n",
    "    sample_weights = class_weights[labels]\n",
    "    weighted_accuracy = accuracy_score(labels, preds, sample_weight=sample_weights)\n",
    "\n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'Weighted Accuracy': weighted_accuracy,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    }\n",
    "\n",
    "class LossRecorderCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.eval_losses = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if 'loss' in logs:\n",
    "            self.train_losses.append(logs['loss'])\n",
    "        if 'eval_loss' in logs:\n",
    "            self.eval_losses.append(logs['eval_loss'])\n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.train_losses, label='Training Loss')\n",
    "        plt.plot(self.eval_losses, label='Validation Loss')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Loss over Time')\n",
    "        plt.show()\n",
    "\n",
    "loss_recorder = LossRecorderCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52511ce3a6032aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定义训练参数\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=f'./results/{base_model_name}',          # 输出结果目录\n",
    "#     report_to='none',\n",
    "#     num_train_epochs=num_epochs,\n",
    "#     learning_rate=lr,\n",
    "#     per_device_train_batch_size=BATCH_SIZE,\n",
    "#     per_device_eval_batch_size=BATCH_SIZE,\n",
    "#     weight_decay=weight_decay,\n",
    "#     warmup_ratio=warmup_ratio,\n",
    "#     # warmup_steps=warmup_steps,\n",
    "#     logging_strategy='steps',\n",
    "#     logging_dir=f'./logs/{base_model_name}',            # 日志目录\n",
    "#     logging_steps=50,\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=50,\n",
    "#     disable_tqdm=False,\n",
    "#     save_strategy=\"steps\",\n",
    "#     fp16= torch.cuda.is_available(), # faster and use less memory\n",
    "#     metric_for_best_model='F1',\n",
    "#     load_best_model_at_end=True,\n",
    "#     greater_is_better=True,\n",
    "#     # push_to_hub=True,\n",
    "#     # label_smoothing_factor=label_smoothing_factor\n",
    "# )\n",
    "# \n",
    "# trainer = CustomTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     loss_type=loss_type,  # 自定义参数 focal_loss dsc_loss\n",
    "#     callbacks=[loss_recorder]\n",
    "# )\n",
    "# \n",
    "# start = time.time()\n",
    "# trainer.train()\n",
    "# train_time = time.time() - start\n",
    "# print(f\"Training took: {train_time:.2f} seconds\")\n",
    "# eval_result = trainer.evaluate()\n",
    "# \n",
    "# loss_recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea91f1cbdeeb00f",
   "metadata": {},
   "source": [
    "# k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7502e7db63db83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# k折交叉验证\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.1, stratify=labels, random_state=seed)\n",
    "results = []\n",
    "fold_counter = 1\n",
    "for train_index, val_index in kf.split(train_texts):\n",
    "    print(f\"Training fold {fold_counter}/{k}\")\n",
    "    fold_train_texts = [train_texts[i] for i in train_index]\n",
    "    fold_val_texts = [train_texts[i] for i in val_index]\n",
    "    fold_train_labels = [train_labels[i] for i in train_index]\n",
    "    fold_val_labels = [train_labels[i] for i in val_index]\n",
    "\n",
    "    train_dataset = MyDataset(tokenizer(fold_train_texts, truncation=True, padding=True, return_tensors='pt', max_length=512), fold_train_labels)\n",
    "    val_dataset = MyDataset(tokenizer(fold_val_texts, truncation=True, padding=True, return_tensors='pt', max_length=512), fold_val_labels)\n",
    "\n",
    "    # 独立的k次训练\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL_PATH, num_labels=NUM_LABELS, id2label=id2label, label2id=label2id).to(device)\n",
    "\n",
    "    # 定义训练参数\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results/{base_model_name}',          # 输出结果目录\n",
    "        report_to='none',\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        # warmup_steps=warmup_steps,\n",
    "        logging_strategy='steps',\n",
    "        logging_dir=f'./logs/{base_model_name}',            # 日志目录\n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        disable_tqdm=False,\n",
    "        save_strategy=\"steps\",\n",
    "        fp16= torch.cuda.is_available(), # faster and use less memory\n",
    "        metric_for_best_model='F1',\n",
    "        load_best_model_at_end=True,\n",
    "        greater_is_better=True,\n",
    "        # push_to_hub=True,\n",
    "        # label_smoothing_factor=label_smoothing_factor\n",
    "    )\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        loss_type=loss_type,  # 自定义参数 focal_loss dsc_loss\n",
    "        callbacks=[loss_recorder]\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    trainer.train()\n",
    "    train_time = time.time() - start\n",
    "    print(f\"Training took: {train_time:.2f} seconds\")\n",
    "    eval_result = trainer.evaluate()\n",
    "    results.append(eval_result)\n",
    "    \n",
    "    # 对测试集进行评估\n",
    "    test_dataset = MyDataset(tokenizer(test_texts, truncation=True, padding=True, return_tensors='pt', max_length=512), test_labels)\n",
    "    test_results = trainer.evaluate(test_dataset)\n",
    "    print(f\"Test results for fold {fold_counter}: {test_results}\")\n",
    "\n",
    "    loss_recorder.plot_losses()\n",
    "    fold_counter += 1\n",
    "\n",
    "# 计算平均结果\n",
    "avg_results = {key: np.mean([result[key] for result in results]) for key in results[0]}\n",
    "print(\"Average results:\", avg_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa074b0b7d5a6d",
   "metadata": {},
   "source": [
    "## 评估模型:ROC曲线"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d391fe2ca46bab",
   "metadata": {},
   "source": [
    "accuracy、precision(预测为1/真实为1)、recall(真实为1/预测为1)和F1-score\n",
    "\n",
    "F1-score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "Micro-F1: 将所有类别的TP、FP、FN分别累加，然后计算F1-score\n",
    "\n",
    "Macro-F1: 分别计算每个类别的F1-score，然后求平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9467da68ae478128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "import json\n",
    "\n",
    "test_dataset = MyDataset(tokenizer(test_texts, truncation=True, padding=True, return_tensors='pt', max_length=512), test_labels)\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "results_json = {\n",
    "    \"model_name\": base_model_name,\n",
    "    \"avg_results\": avg_results,\n",
    "    \"test_results\": test_results\n",
    "}\n",
    "\n",
    "results_path = f'./output/bert_output.json'\n",
    "\n",
    "if not os.path.exists(results_path):\n",
    "    with open(results_path, 'w') as json_file:\n",
    "        json.dump({}, json_file)\n",
    "\n",
    "with open(results_path, 'w') as json_file:\n",
    "    json.dump(results_json, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3498e1f1e99673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(labels, probs):\n",
    "    # 计算ROC曲线\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    \n",
    "    # 将标签转换为One-hot编码\n",
    "    test_labels_one_hot = np.eye(3)[labels]\n",
    "    for i in range(3):\n",
    "        fpr[i], tpr[i], _ = roc_curve(test_labels_one_hot[:, i], probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # 绘制ROC曲线\n",
    "    plt.figure()\n",
    "    colors = ['aqua', 'darkorange', 'cornflowerblue']\n",
    "    for i, color in zip(range(3), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2, label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                                           ''.format(i, roc_auc[i]))\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic to multi-class')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1).numpy()\n",
    "plot_roc_curve(test_labels, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee062a3b681608",
   "metadata": {},
   "source": [
    "## PR曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a6c80872cd10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "def plot_pr_curve(labels, probs):\n",
    "    \n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    average_precision = {}\n",
    "    test_labels_one_hot = np.eye(3)[labels]\n",
    "    \n",
    "    for i in range(3):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(test_labels_one_hot[:, i], probs[:, i])\n",
    "        average_precision[i] = average_precision_score(test_labels_one_hot[:, i], probs[:, i])\n",
    "\n",
    "    plt.figure()\n",
    "    colors = ['aqua', 'darkorange', 'cornflowerblue']\n",
    "    for i, color in zip(range(3), colors):\n",
    "        plt.plot(recall[i], precision[i], color=color, lw=2, label='PR curve of class {0} (area = {1:0.2f})'\n",
    "                                                           ''.format(i, average_precision[i]))\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall curve to multi-class')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "plot_pr_curve(test_labels, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeaf09062ac411e",
   "metadata": {},
   "source": [
    "## 混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23a1be483510288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "plot_confusion_matrix(test_labels, predicted_labels, list(label2id.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ef29836fe7fae9",
   "metadata": {},
   "source": [
    "## 误差分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b26cc59b560073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_with_label(batch):\n",
    "    # Place all input tensors on the same device as the model\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        if loss_type == 'dsc_loss':\n",
    "            loss_fct = MultiDSCLoss(alpha=1.0, smooth=1.0, reduction='none')\n",
    "        elif loss_type == 'focal_loss':\n",
    "            loss_fct = MultiFocalLoss(num_class=3, alpha=0.8, gamma=2.0, reduction='none')\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        loss = loss_fct(output.logits, batch[\"label\"].to(device))\n",
    "\n",
    "    # Place outputs on CPU for compatibility with other dataset columns   \n",
    "    return {\"loss\": loss.cpu().numpy(), \n",
    "            \"predicted_label\": pred_label.cpu().numpy()}\n",
    "\n",
    "tokenized_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_data[\"validation\"] = tokenized_data[\"validation\"].map(forward_pass_with_label, batched=True, batch_size=16)\n",
    "tokenized_data.set_format(\"pandas\")\n",
    "cols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\n",
    "df_test = tokenized_data[\"validation\"][:][cols]\n",
    "df_test[\"label\"] = df_test[\"label\"].map(id2label)\n",
    "df_test[\"predicted_label\"] = (df_test[\"predicted_label\"].map(id2label))\n",
    "df_test.sort_values(\"loss\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5bc2f2559586de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.sort_values(\"loss\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bda2611270b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "best_model_dir = f'./citation_finetuned_models/{base_model_name}'\n",
    "trainer.save_model(best_model_dir)\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained(best_model_dir).to(device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
