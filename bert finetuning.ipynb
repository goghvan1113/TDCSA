{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T07:09:05.483524Z",
     "start_time": "2024-09-13T07:09:03.662591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from huggingface_hub import notebook_login\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ],
   "id": "3a6a143281a31e46",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T07:09:05.515448Z",
     "start_time": "2024-09-13T07:09:05.485545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark     = False  # 启用Cudnn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "id": "2ef261e25aba5ee0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "68b8fc19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T07:09:05.593398Z",
     "start_time": "2024-09-13T07:09:05.589395Z"
    }
   },
   "source": [
    "# 模型相关参数\n",
    "base_model_name = 'xlm-roberta-large'\n",
    "TEST_DATASET_SIZE = 0.4\n",
    "NUM_LABELS = 3\n",
    "DATA_PATH = './data/citation_sentiment_corpus_new.csv'\n",
    "BASE_MODEL_PATH = f'./pretrain_models/{base_model_name}'\n",
    "\n",
    "# 训练相关参数\n",
    "BATCH_SIZE = 32\n",
    "loss_type='dsc_loss' # 自定义参数 focal_loss dsc_loss ce_loss asymmetric_loss\n",
    "weight_decay = 0.01\n",
    "lr = 5e-6\n",
    "num_epochs = 5\n",
    "warmup_steps = 100\n",
    "label_smoothing_factor = 0.1\n",
    "\n",
    "# notebook_login()\n",
    "os.environ[\"WANDB_PROJECT\"] = base_model_name\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = 'true' # save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_WATCH\"] = 'false' # turn off watch to log faster"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "c5d0abcac06be516",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T07:09:05.639735Z",
     "start_time": "2024-09-13T07:09:05.609726Z"
    }
   },
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "print(df.shape)\n",
    "df.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8699, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  Source_Paper_ID Target_Paper_ID  Sentiment  \\\n",
       "0        A00-1043        A00-2024          0   \n",
       "1        H05-1033        A00-2024          0   \n",
       "2        I05-2009        A00-2024          0   \n",
       "3        I05-2009        A00-2024          0   \n",
       "4        I05-2009        A00-2024          0   \n",
       "\n",
       "                                       Citation_Text  Sentence_Length  \n",
       "0  We analyzed a set of articles and identified s...              486  \n",
       "1  Table 3: Example compressions Compression AvgL...              349  \n",
       "2  5.3 Related works and discussion Our two-step ...              159  \n",
       "3  (1999) proposed a summarization system based o...              368  \n",
       "4  We found that the deletion of lead parts did n...              125  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source_Paper_ID</th>\n",
       "      <th>Target_Paper_ID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Citation_Text</th>\n",
       "      <th>Sentence_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00-1043</td>\n",
       "      <td>A00-2024</td>\n",
       "      <td>0</td>\n",
       "      <td>We analyzed a set of articles and identified s...</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H05-1033</td>\n",
       "      <td>A00-2024</td>\n",
       "      <td>0</td>\n",
       "      <td>Table 3: Example compressions Compression AvgL...</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I05-2009</td>\n",
       "      <td>A00-2024</td>\n",
       "      <td>0</td>\n",
       "      <td>5.3 Related works and discussion Our two-step ...</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I05-2009</td>\n",
       "      <td>A00-2024</td>\n",
       "      <td>0</td>\n",
       "      <td>(1999) proposed a summarization system based o...</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I05-2009</td>\n",
       "      <td>A00-2024</td>\n",
       "      <td>0</td>\n",
       "      <td>We found that the deletion of lead parts did n...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "7f24b1533bef739c",
   "metadata": {},
   "source": "## 使用Trainer类微调citation的情感分析"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T07:09:07.066291Z",
     "start_time": "2024-09-13T07:09:05.687654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "id2label={0:\"Neutral\", 1:\"Positive\", 2:\"Negative\"}\n",
    "label2id={\"Neutral\":0, \"Positive\":1, \"Negative\":2}\n",
    "\n",
    "# 加载基座模型和分词器\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL_PATH, num_labels=NUM_LABELS, id2label=id2label, label2id=label2id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)"
   ],
   "id": "d61bed57f56ae333",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ./pretrain_models/xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "c4f5d22992aa877e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T07:09:08.567532Z",
     "start_time": "2024-09-13T07:09:07.098411Z"
    }
   },
   "source": [
    "import datasets\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    重构数据集类，使其能够返回字典格式的数据，有标签\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    df['Citation_Text'].tolist(), df['Sentiment'].tolist(), test_size=TEST_DATASET_SIZE, stratify=df['Sentiment'], random_state=seed)\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, stratify=temp_labels, random_state=seed)\n",
    "\n",
    "train_dataset = MyDataset(tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt', max_length=512), train_labels)\n",
    "test_dataset = MyDataset(tokenizer(test_texts, truncation=True, padding=True, return_tensors='pt', max_length=512), test_labels)\n",
    "val_dataset = MyDataset(tokenizer(val_texts, truncation=True, padding=True, return_tensors='pt', max_length=512), val_labels)\n",
    "\n",
    "# 将数据转换为 HuggingFace 的 Dataset 格式\n",
    "train_data = datasets.Dataset.from_dict({'text': train_texts, 'label': train_labels})\n",
    "val_data = datasets.Dataset.from_dict({'text': val_texts, 'label': val_labels})\n",
    "test_data = datasets.Dataset.from_dict({'text': test_texts, 'label': test_labels})\n",
    "emotion_data = datasets.DatasetDict({'train': train_data, 'validation': val_data, 'test': test_data})\n",
    "tokenized_data = emotion_data.map(lambda f: tokenizer(f['text'], padding=True, truncation=True, return_tensors='pt', max_length=512), batch_size=16, batched=True)  # WordPiece分词器\n",
    "# tokenized_data = tokenized_data.remove_columns(['text'])\n",
    "# train_dataset = tokenized_data['train']\n",
    "# val_dataset = tokenized_data['validation']\n",
    "# test_dataset = tokenized_data['test']\n",
    "\n",
    "print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
    "print(f\"Test Dataset Size: {len(test_dataset)}\")\n",
    "print(f\"Val Dataset Size: {len(val_dataset)}\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5219 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "133df2b4eeaf4f9eb0168840abd03c2a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1740 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "618b7b11dc4a4868826f7de181a5c9d6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1740 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "703467b1c039486b9e948bbea2c22682"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Size: 5219\n",
      "Test Dataset Size: 1740\n",
      "Val Dataset Size: 1740\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "3b0522f0a2c14728",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T07:09:08.645282Z",
     "start_time": "2024-09-13T07:09:08.586399Z"
    }
   },
   "source": [
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./results/{base_model_name}',          # 输出结果目录\n",
    "    report_to='none',\n",
    "    num_train_epochs=num_epochs,\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    weight_decay=weight_decay,\n",
    "    warmup_steps=warmup_steps,\n",
    "    logging_strategy='steps',\n",
    "    logging_dir=f'./logs/{base_model_name}',            # 日志目录\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    disable_tqdm=False,\n",
    "    save_strategy=\"steps\",\n",
    "    fp16= torch.cuda.is_available(), # faster and use less memory\n",
    "    # metric_for_best_model='F1',\n",
    "    # load_best_model_at_end=True,\n",
    "    # push_to_hub=True,\n",
    "    # label_smoothing_factor=label_smoothing_factor\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "2f20932b8f137b17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T07:09:08.691518Z",
     "start_time": "2024-09-13T07:09:08.676671Z"
    }
   },
   "source": [
    "# 使用Focal loss作为损失函数解决样本不均衡问题\n",
    "class MultiFocalLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Focal_Loss= -1*alpha*((1-pt)**gamma)*log(pt)\n",
    "    Args:\n",
    "        num_class: number of classes\n",
    "        alpha: class balance factor shape=[num_class, ]\n",
    "        gamma: hyperparameter\n",
    "        reduction: reduction type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_class, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(MultiFocalLoss, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.smooth = 1e-4\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        if alpha is None:\n",
    "            self.alpha = torch.ones(num_class, ) - 0.5\n",
    "        elif isinstance(alpha, (int, float)):\n",
    "            self.alpha = torch.as_tensor([alpha] * num_class)\n",
    "        elif isinstance(alpha, (list, np.ndarray)):\n",
    "            self.alpha = torch.as_tensor(alpha)\n",
    "        if self.alpha.shape[0] != num_class:\n",
    "            raise RuntimeError('the length not equal to number of class')\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        # assert isinstance(self.alpha,torch.Tensor)\\\n",
    "        alpha = self.alpha.to(logit.device)\n",
    "        prob = F.softmax(logit, dim=1)\n",
    "\n",
    "        if prob.dim() > 2:\n",
    "            # used for 3d-conv:  N,C,d1,d2 -> N,C,m (m=d1*d2*...)\n",
    "            N, C = logit.shape[:2]\n",
    "            prob = prob.view(N, C, -1)\n",
    "            prob = prob.transpose(1, 2).contiguous()  # [N,C,d1*d2..] -> [N,d1*d2..,C]\n",
    "            prob = prob.view(-1, prob.size(-1))  # [N,d1*d2..,C]-> [N*d1*d2..,C]\n",
    "\n",
    "        ori_shp = target.shape\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        prob = prob.gather(1, target).view(-1) + self.smooth  # avoid nan\n",
    "        logpt = torch.log(prob)\n",
    "        # alpha_class = alpha.gather(0, target.squeeze(-1))\n",
    "        alpha_weight = alpha[target.squeeze().long()]\n",
    "        loss = -alpha_weight * torch.pow(torch.sub(1.0, prob), self.gamma) * logpt\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'none':\n",
    "            loss = loss.view(ori_shp)\n",
    "\n",
    "        return loss\n",
    "\n",
    "# DSELoss\n",
    "class MultiDSCLoss(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Creates a criterion that optimizes a multi-class Self-adjusting Dice Loss\n",
    "    (\"Dice Loss for Data-imbalanced NLP Tasks\" paper)\n",
    "\n",
    "    Args:\n",
    "        alpha (float): a factor to push down the weight of easy examples\n",
    "        gamma (float): a factor added to both the nominator and the denominator for smoothing purposes\n",
    "        reduction (string): Specifies the reduction to apply to the output:\n",
    "            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
    "            ``'mean'``: the sum of the output will be divided by the number of\n",
    "            elements in the output, ``'sum'``: the output will be summed.\n",
    "\n",
    "    Shape:\n",
    "        - logits: `(N, C)` where `N` is the batch size and `C` is the number of classes.\n",
    "        - targets: `(N)` where each value is in [0, C - 1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1.0, smooth=1.0, reduction=\"mean\"):\n",
    "        super(MultiDSCLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.smooth = smooth\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        probs = torch.gather(probs, dim=1, index=targets.unsqueeze(1))\n",
    "\n",
    "        probs_with_factor = ((1 - probs) ** self.alpha) * probs\n",
    "        loss = 1 - (2 * probs_with_factor + self.smooth) / (probs_with_factor + 1 + self.smooth)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        elif self.reduction == \"none\" or self.reduction is None:\n",
    "            return loss\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Reduction `{self.reduction}` is not supported.\")\n",
    "\n",
    "# Asymmetric Loss\n",
    "class AsymmetricLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma_pos=0.5, gamma_neg=3.0, eps=0.1, reduction='mean'):\n",
    "        super(AsymmetricLoss, self).__init__()\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.eps = eps\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        pos_weight = targets * (1 - self.gamma_pos) + self.gamma_pos\n",
    "        neg_weight = (1 - targets) * (1 - self.gamma_neg) + self.gamma_neg\n",
    "        loss = -pos_weight * targets * torch.log(inputs + self.eps) - neg_weight * (1 - targets) * torch.log(\n",
    "            1 - inputs + self.eps)\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(loss)\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, loss_type='focal_loss', *args, **kwargs):\n",
    "        super(CustomTrainer, self).__init__(*args, **kwargs)\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        if self.loss_type == 'focal_loss':\n",
    "            loss_fct = MultiFocalLoss(num_class=3, alpha=0.8, gamma=2.0)\n",
    "        elif self.loss_type == 'dsc_loss':\n",
    "            loss_fct = MultiDSCLoss(alpha=1.0, smooth=1.0)\n",
    "        elif self.loss_type == 'asymmetric_loss':\n",
    "            loss_fct = AsymmetricLoss(gamma_pos=0.5, gamma_neg=3.0)\n",
    "        elif self.loss_type == 'ce_loss':\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "8a7502e7db63db83",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-13T07:09:08.722736Z"
    }
   },
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    }\n",
    "\n",
    "class LossRecorderCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.eval_losses = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if 'loss' in logs:\n",
    "            self.train_losses.append(logs['loss'])\n",
    "        if 'eval_loss' in logs:\n",
    "            self.eval_losses.append(logs['eval_loss'])\n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.train_losses, label='Training Loss')\n",
    "        plt.plot(self.eval_losses, label='Validation Loss')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Loss over Time')\n",
    "        plt.show()\n",
    "\n",
    "loss_recorder = LossRecorderCallback()\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    loss_type=loss_type,  # 自定义参数 focal_loss dsc_loss\n",
    "    callbacks=[loss_recorder]\n",
    ")\n",
    "\n",
    "results = trainer.train()\n",
    "loss_recorder.plot_losses()\n",
    "# wandb.finish()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  4/820 01:07 < 7:40:03, 0.03 it/s, Epoch 0.02/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 50\u001B[0m\n\u001B[0;32m     37\u001B[0m loss_recorder \u001B[38;5;241m=\u001B[39m LossRecorderCallback()\n\u001B[0;32m     39\u001B[0m trainer \u001B[38;5;241m=\u001B[39m CustomTrainer(\n\u001B[0;32m     40\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     41\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     47\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39m[loss_recorder]\n\u001B[0;32m     48\u001B[0m )\n\u001B[1;32m---> 50\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m loss_recorder\u001B[38;5;241m.\u001B[39mplot_losses()\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:1885\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1883\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   1884\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1885\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1886\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1887\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1888\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1889\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1890\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:2216\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2213\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[0;32m   2215\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[1;32m-> 2216\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2218\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2219\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   2220\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m   2221\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   2222\u001B[0m ):\n\u001B[0;32m   2223\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   2224\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:3250\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m   3248\u001B[0m         scaled_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m   3249\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 3250\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3252\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mdetach() \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\accelerate\\accelerator.py:2147\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[1;34m(self, loss, **kwargs)\u001B[0m\n\u001B[0;32m   2145\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m   2146\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 2147\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaler\u001B[38;5;241m.\u001B[39mscale(loss)\u001B[38;5;241m.\u001B[39mbackward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2148\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m learning_rate \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhas_lomo_optimizer:\n\u001B[0;32m   2149\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlomo_backward(loss, learning_rate)\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "22aa074b0b7d5a6d",
   "metadata": {},
   "source": [
    "## 评估模型:ROC曲线"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d391fe2ca46bab",
   "metadata": {},
   "source": [
    "accuracy、precision(预测为1/真实为1)、recall(真实为1/预测为1)和F1-score\n",
    "\n",
    "F1-score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "Micro-F1: 将所有类别的TP、FP、FN分别累加，然后计算F1-score\n",
    "\n",
    "Macro-F1: 分别计算每个类别的F1-score，然后求平均"
   ]
  },
  {
   "cell_type": "code",
   "id": "8f3498e1f1e99673",
   "metadata": {},
   "source": [
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "import numpy as np\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1).numpy()\n",
    "\n",
    "def plot_roc_curve(labels, probs):\n",
    "    # 计算ROC曲线\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    \n",
    "    # 将标签转换为One-hot编码\n",
    "    test_labels_one_hot = np.eye(3)[labels]\n",
    "    for i in range(3):\n",
    "        fpr[i], tpr[i], _ = roc_curve(test_labels_one_hot[:, i], probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # 绘制ROC曲线\n",
    "    plt.figure()\n",
    "    colors = ['aqua', 'darkorange', 'cornflowerblue']\n",
    "    for i, color in zip(range(3), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2, label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                                           ''.format(i, roc_auc[i]))\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic to multi-class')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_curve(test_labels, probs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "88ee062a3b681608",
   "metadata": {},
   "source": [
    "## PR曲线"
   ]
  },
  {
   "cell_type": "code",
   "id": "430a6c80872cd10e",
   "metadata": {},
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "def plot_pr_curve(labels, probs):\n",
    "    \n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    average_precision = {}\n",
    "    test_labels_one_hot = np.eye(3)[labels]\n",
    "    \n",
    "    for i in range(3):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(test_labels_one_hot[:, i], probs[:, i])\n",
    "        average_precision[i] = average_precision_score(test_labels_one_hot[:, i], probs[:, i])\n",
    "\n",
    "    plt.figure()\n",
    "    colors = ['aqua', 'darkorange', 'cornflowerblue']\n",
    "    for i, color in zip(range(3), colors):\n",
    "        plt.plot(recall[i], precision[i], color=color, lw=2, label='PR curve of class {0} (area = {1:0.2f})'\n",
    "                                                           ''.format(i, average_precision[i]))\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall curve to multi-class')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "plot_pr_curve(test_labels, probs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6eeaf09062ac411e",
   "metadata": {},
   "source": [
    "## 混淆矩阵"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "plot_confusion_matrix(test_labels, predicted_labels, list(label2id.keys()))"
   ],
   "id": "f23a1be483510288",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 误差分析",
   "id": "b4ef29836fe7fae9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def forward_pass_with_label(batch):\n",
    "    # Place all input tensors on the same device as the model\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        if loss_type == 'dsc_loss':\n",
    "            loss_fct = MultiDSCLoss(alpha=1.0, smooth=1.0, reduction='none')\n",
    "        elif loss_type == 'focal_loss':\n",
    "            loss_fct = MultiFocalLoss(num_class=3, alpha=0.8, gamma=2.0, reduction='none')\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        loss = loss_fct(output.logits, batch[\"label\"].to(device))\n",
    "\n",
    "    # Place outputs on CPU for compatibility with other dataset columns   \n",
    "    return {\"loss\": loss.cpu().numpy(), \n",
    "            \"predicted_label\": pred_label.cpu().numpy()}\n",
    "\n",
    "tokenized_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_data[\"validation\"] = tokenized_data[\"validation\"].map(forward_pass_with_label, batched=True, batch_size=16)\n",
    "tokenized_data.set_format(\"pandas\")\n",
    "cols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\n",
    "df_test = tokenized_data[\"validation\"][:][cols]\n",
    "df_test[\"label\"] = df_test[\"label\"].map(id2label)\n",
    "df_test[\"predicted_label\"] = (df_test[\"predicted_label\"].map(id2label))\n",
    "df_test.sort_values(\"loss\", ascending=False).head(10)"
   ],
   "id": "e0b26cc59b560073",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_test.sort_values(\"loss\", ascending=True).head(10)",
   "id": "4d5bc2f2559586de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "best_model_dir = f'./citation_finetuned_models/{base_model_name}'\n",
    "trainer.save_model(best_model_dir)\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained(best_model_dir).to(device)\n",
    "\n",
    "\n",
    "# def predict_sentiment_score(model, dataset, batched=True, BATCH_SIZE=32, weights=[-0.5, 1, -1], shuffle=False):\n",
    "#     '''\n",
    "#     预测句子的情感\n",
    "#     '''\n",
    "#     results = []\n",
    "#     sentiment_scores = []\n",
    "#     model.eval()  # 将模型设置为评估模式\n",
    "# \n",
    "#     dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=shuffle)\n",
    "# \n",
    "#     if batched:\n",
    "#         with torch.no_grad():  # 不计算梯度，以加速和节省内存\n",
    "#             for batch in dataloader:\n",
    "#                 # 将批次数据移动到cuda\n",
    "#                 batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#                 outputs = model(**batch)\n",
    "#                 logits = outputs.logits\n",
    "#                 probabilities = F.softmax(logits, dim=-1)\n",
    "#                 \n",
    "#                 logits_list = logits.tolist()\n",
    "#                 probabilities_list = probabilities.tolist()\n",
    "#                 results.extend(zip(logits_list, probabilities_list))\n",
    "#     else:\n",
    "#         with torch.no_grad():\n",
    "#             inputs = tokenizer(dataset, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "#             outputs = model(**inputs)\n",
    "#             logits = outputs.logits\n",
    "#             probabilities = F.softmax(logits, dim=-1)\n",
    "#             logits_list = logits.tolist()\n",
    "#             probabilities_list = probabilities.tolist()\n",
    "#             results.extend(zip(logits_list, probabilities_list))\n",
    "# \n",
    "#     for _, softmax_probs in results:\n",
    "#         softmax_probs_array = np.array(softmax_probs)\n",
    "#         weights_array = np.array(weights)\n",
    "#         score = np.sum(softmax_probs_array * weights_array)\n",
    "#         score = max(min(score, 1), -1)\n",
    "#         sentiment_scores.append(score)\n",
    "#     return sentiment_scores\n",
    "# \n",
    "# weights=[-0.1, 1, -1]\n",
    "# all_dataset = ConcatDataset([train_dataset, test_dataset, val_dataset])\n",
    "# sentiment_scores = predict_sentiment_score(best_model, all_dataset, weights=weights, batched=True, BATCH_SIZE=1, shuffle=True) # 句子维度不一样就把BATCH_SIZE设为1\n"
   ],
   "id": "a8bda2611270b290",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
