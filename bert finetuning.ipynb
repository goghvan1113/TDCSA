{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from huggingface_hub import notebook_login\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ],
   "id": "3a6a143281a31e46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "id": "2ef261e25aba5ee0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "68b8fc19",
   "metadata": {},
   "source": [
    "# 模型相关参数\n",
    "base_model_name = 'distilbert-base-uncased'\n",
    "TEST_DATASET_SIZE = 0.4\n",
    "NUM_LABELS = 3\n",
    "DATA_PATH = './data/citation_sentiment_corpus_new.csv'\n",
    "SST2_FINETUNED_MODEL_PATH = './pretrain_models/distilbert-base-uncased-finetuned-sst-2-english'\n",
    "BASE_MODEL_PATH = f'./pretrain_models/{base_model_name}'\n",
    "\n",
    "# 训练相关参数\n",
    "USE_BASE_MODEL = True\n",
    "BATCH_SIZE = 32\n",
    "loss_type='dsc_loss' # 自定义参数 focal_loss dsc_loss ce_loss\n",
    "weight_decay = 0.01\n",
    "lr = 5e-6\n",
    "num_epochs = 3\n",
    "warmup_steps = 100\n",
    "label_smoothing_factor = 0.0\n",
    "\n",
    "# notebook_login()\n",
    "os.environ[\"WANDB_PROJECT\"]=base_model_name\n",
    "os.environ[\"WANDB_LOG_MODEL\"]='true' # save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_WATCH\"]='false' # turn off watch to log faster\n",
    "os.environ[\"WANDB_DISABLED\"]='true'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5d0abcac06be516",
   "metadata": {},
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "print(df.shape)\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7f24b1533bef739c",
   "metadata": {},
   "source": "## 使用Trainer类微调citation的情感分析"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "id2label={0:\"Neutral\", 1:\"Positive\", 2:\"Negative\"}\n",
    "label2id={\"Neutral\":0, \"Positive\":1, \"Negative\":2}\n",
    "\n",
    "if USE_BASE_MODEL:\n",
    "    # 加载DistilBERT模型\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL_PATH, num_labels=NUM_LABELS, id2label=id2label, label2id=label2id).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "else:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(SST2_FINETUNED_MODEL_PATH).to(device)\n",
    "    model.classifier = torch.nn.Linear(model.classifier.in_features, NUM_LABELS) # 修改模型的分类头，使其适应三分类任务\n",
    "    tokenizer = AutoTokenizer.from_pretrained(SST2_FINETUNED_MODEL_PATH)"
   ],
   "id": "d61bed57f56ae333",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4f5d22992aa877e",
   "metadata": {},
   "source": [
    "import datasets\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    重构数据集类，使其能够返回字典格式的数据，有标签\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    df['Citation_Text'].tolist(), df['Sentiment'].tolist(), test_size=TEST_DATASET_SIZE, stratify=df['Sentiment'], random_state=seed)\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, stratify=temp_labels, random_state=seed)\n",
    "\n",
    "train_dataset = MyDataset(tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt', max_length=512), train_labels)\n",
    "test_dataset = MyDataset(tokenizer(test_texts, truncation=True, padding=True, return_tensors='pt', max_length=512), test_labels)\n",
    "val_dataset = MyDataset(tokenizer(val_texts, truncation=True, padding=True, return_tensors='pt', max_length=512), val_labels)\n",
    "\n",
    "# 将数据转换为 HuggingFace 的 Dataset 格式\n",
    "train_data = datasets.Dataset.from_dict({'text': train_texts, 'label': train_labels})\n",
    "val_data = datasets.Dataset.from_dict({'text': val_texts, 'label': val_labels})\n",
    "test_data = datasets.Dataset.from_dict({'text': test_texts, 'label': test_labels})\n",
    "emotion_data = datasets.DatasetDict({'train': train_data, 'validation': val_data, 'test': test_data})\n",
    "tokenized_data = emotion_data.map(lambda f: tokenizer(f['text'], padding=True, truncation=True, return_tensors='pt', max_length=512), batch_size=16, batched=True)  # WordPiece分词器\n",
    "tokenized_data = tokenized_data.remove_columns(['text'])\n",
    "# train_dataset = tokenized_data['train']\n",
    "# val_dataset = tokenized_data['validation']\n",
    "# test_dataset = tokenized_data['test']\n",
    "\n",
    "print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
    "print(f\"Test Dataset Size: {len(test_dataset)}\")\n",
    "print(f\"Val Dataset Size: {len(val_dataset)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3b0522f0a2c14728",
   "metadata": {},
   "source": [
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./results/{base_model_name}',          # 输出结果目录\n",
    "    # report_to=\"wandb\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    weight_decay=weight_decay,\n",
    "    warmup_steps=warmup_steps,\n",
    "    logging_strategy='steps',\n",
    "    logging_dir=f'./logs/{base_model_name}',            # 日志目录\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    disable_tqdm=False,\n",
    "    save_strategy=\"steps\",\n",
    "    fp16= torch.cuda.is_available(), # faster and use less memory\n",
    "    # metric_for_best_model='F1',\n",
    "    # load_best_model_at_end=True,\n",
    "    # push_to_hub=True,\n",
    "    label_smoothing_factor=label_smoothing_factor\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f20932b8f137b17",
   "metadata": {},
   "source": [
    "# 使用Focal loss作为损失函数解决样本不均衡问题\n",
    "def py_sigmoid_focal_loss(pred, target, gamma=2.0, alpha=0.9, reduction='mean'):\n",
    "    pred_sigmoid = pred.sigmoid()\n",
    "    target = torch.nn.functional.one_hot(target, num_classes=pred.shape[1]).type_as(pred)  # 转换为one-hot编码\n",
    "    pt = (1 - pred_sigmoid) * target + pred_sigmoid * (1 - target)\n",
    "    focal_weight = (alpha * target + (1 - alpha) * (1 - target)) * pt.pow(gamma)\n",
    "    loss = F.binary_cross_entropy_with_logits(pred, target, reduction='none') * focal_weight\n",
    "    return loss.mean() if reduction == 'mean' else loss.sum()\n",
    "\n",
    "class MultiFocalLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Focal_Loss= -1*alpha*((1-pt)**gamma)*log(pt)\n",
    "    Args:\n",
    "        num_class: number of classes\n",
    "        alpha: class balance factor shape=[num_class, ]\n",
    "        gamma: hyper-parameter\n",
    "        reduction: reduction type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_class, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(MultiFocalLoss, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.smooth = 1e-4\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        if alpha is None:\n",
    "            self.alpha = torch.ones(num_class, ) - 0.5\n",
    "        elif isinstance(alpha, (int, float)):\n",
    "            self.alpha = torch.as_tensor([alpha] * num_class)\n",
    "        elif isinstance(alpha, (list, np.ndarray)):\n",
    "            self.alpha = torch.as_tensor(alpha)\n",
    "        if self.alpha.shape[0] != num_class:\n",
    "            raise RuntimeError('the length not equal to number of class')\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        # assert isinstance(self.alpha,torch.Tensor)\\\n",
    "        alpha = self.alpha.to(logit.device)\n",
    "        prob = F.softmax(logit, dim=1)\n",
    "\n",
    "        if prob.dim() > 2:\n",
    "            # used for 3d-conv:  N,C,d1,d2 -> N,C,m (m=d1*d2*...)\n",
    "            N, C = logit.shape[:2]\n",
    "            prob = prob.view(N, C, -1)\n",
    "            prob = prob.transpose(1, 2).contiguous()  # [N,C,d1*d2..] -> [N,d1*d2..,C]\n",
    "            prob = prob.view(-1, prob.size(-1))  # [N,d1*d2..,C]-> [N*d1*d2..,C]\n",
    "\n",
    "        ori_shp = target.shape\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        prob = prob.gather(1, target).view(-1) + self.smooth  # avoid nan\n",
    "        logpt = torch.log(prob)\n",
    "        # alpha_class = alpha.gather(0, target.squeeze(-1))\n",
    "        alpha_weight = alpha[target.squeeze().long()]\n",
    "        loss = -alpha_weight * torch.pow(torch.sub(1.0, prob), self.gamma) * logpt\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'none':\n",
    "            loss = loss.view(ori_shp)\n",
    "\n",
    "        return loss\n",
    "# DSELoss\n",
    "class MultiDSCLoss(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Creates a criterion that optimizes a multi-class Self-adjusting Dice Loss\n",
    "    (\"Dice Loss for Data-imbalanced NLP Tasks\" paper)\n",
    "\n",
    "    Args:\n",
    "        alpha (float): a factor to push down the weight of easy examples\n",
    "        gamma (float): a factor added to both the nominator and the denominator for smoothing purposes\n",
    "        reduction (string): Specifies the reduction to apply to the output:\n",
    "            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
    "            ``'mean'``: the sum of the output will be divided by the number of\n",
    "            elements in the output, ``'sum'``: the output will be summed.\n",
    "\n",
    "    Shape:\n",
    "        - logits: `(N, C)` where `N` is the batch size and `C` is the number of classes.\n",
    "        - targets: `(N)` where each value is in [0, C - 1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1.0, smooth=1.0, reduction=\"mean\"):\n",
    "        super(MultiDSCLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.smooth = smooth\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        probs = torch.gather(probs, dim=1, index=targets.unsqueeze(1))\n",
    "\n",
    "        probs_with_factor = ((1 - probs) ** self.alpha) * probs\n",
    "        loss = 1 - (2 * probs_with_factor + self.smooth) / (probs_with_factor + 1 + self.smooth)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        elif self.reduction == \"none\" or self.reduction is None:\n",
    "            return loss\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Reduction `{self.reduction}` is not supported.\")\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, loss_type='focal_loss', *args, **kwargs):\n",
    "        super(CustomTrainer, self).__init__(*args, **kwargs)\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        if self.loss_type == 'focal_loss':\n",
    "            loss_fct = MultiFocalLoss(num_class=3, alpha=0.8, gamma=2.0)\n",
    "        elif self.loss_type == 'dsc_loss':\n",
    "            loss_fct = MultiDSCLoss(alpha=1.0, smooth=1.0)\n",
    "        elif self.loss_type == 'ce_loss':\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8a7502e7db63db83",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    " \n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    }\n",
    "\n",
    "# 创建Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    loss_type=loss_type # 自定义参数 focal_loss dsc_loss\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "946b7340ecbce02d",
   "metadata": {},
   "source": [
    "results = trainer.train() \n",
    "# wandb.finish()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "22aa074b0b7d5a6d",
   "metadata": {},
   "source": [
    "## 评估模型:ROC曲线"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d391fe2ca46bab",
   "metadata": {},
   "source": [
    "accuracy、precision(预测为1/真实为1)、recall(真实为1/预测为1)和F1-score\n",
    "\n",
    "F1-score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "Micro-F1: 将所有类别的TP、FP、FN分别累加，然后计算F1-score\n",
    "\n",
    "Macro-F1: 分别计算每个类别的F1-score，然后求平均"
   ]
  },
  {
   "cell_type": "code",
   "id": "8f3498e1f1e99673",
   "metadata": {},
   "source": [
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "import numpy as np\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1).numpy()\n",
    "\n",
    "def plot_roc_curve(labels, probs):\n",
    "    # 计算ROC曲线\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    \n",
    "    # 将标签转换为One-hot编码\n",
    "    test_labels_one_hot = np.eye(3)[labels]\n",
    "    for i in range(3):\n",
    "        fpr[i], tpr[i], _ = roc_curve(test_labels_one_hot[:, i], probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # 绘制ROC曲线\n",
    "    plt.figure()\n",
    "    colors = ['aqua', 'darkorange', 'cornflowerblue']\n",
    "    for i, color in zip(range(3), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2, label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                                           ''.format(i, roc_auc[i]))\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic to multi-class')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_curve(test_labels, probs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "88ee062a3b681608",
   "metadata": {},
   "source": [
    "## PR曲线"
   ]
  },
  {
   "cell_type": "code",
   "id": "430a6c80872cd10e",
   "metadata": {},
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "def plot_pr_curve(labels, probs):\n",
    "    \n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    average_precision = {}\n",
    "    test_labels_one_hot = np.eye(3)[labels]\n",
    "    \n",
    "    for i in range(3):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(test_labels_one_hot[:, i], probs[:, i])\n",
    "        average_precision[i] = average_precision_score(test_labels_one_hot[:, i], probs[:, i])\n",
    "\n",
    "    plt.figure()\n",
    "    colors = ['aqua', 'darkorange', 'cornflowerblue']\n",
    "    for i, color in zip(range(3), colors):\n",
    "        plt.plot(recall[i], precision[i], color=color, lw=2, label='PR curve of class {0} (area = {1:0.2f})'\n",
    "                                                           ''.format(i, average_precision[i]))\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall curve to multi-class')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "plot_pr_curve(test_labels, probs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6eeaf09062ac411e",
   "metadata": {},
   "source": [
    "## 混淆矩阵"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "plot_confusion_matrix(test_labels, predicted_labels, list(label2id.keys()))"
   ],
   "id": "f23a1be483510288",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 误差分析",
   "id": "b4ef29836fe7fae9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def forward_pass_with_label(batch):\n",
    "#     # Place all input tensors on the same device as the model\n",
    "#     inputs = {k:v.to(device) for k,v in batch.items() \n",
    "#               if k in tokenizer.model_input_names}\n",
    "# \n",
    "#     with torch.no_grad():\n",
    "#         output = model(**inputs)\n",
    "#         pred_label = torch.argmax(output.logits, axis=-1)\n",
    "#         if loss_type == 'dsc_loss':\n",
    "#             loss_fct = MultiDSCLoss(alpha=1.0, smooth=1.0, reduction='none')\n",
    "#         elif loss_type == 'focal_loss':\n",
    "#             loss_fct = MultiFocalLoss(num_class=3, alpha=0.8, gamma=2.0, reduction='none')\n",
    "#         else:\n",
    "#             loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "#         loss = loss_fct(output.logits, batch[\"label\"].to(device))\n",
    "#     \n",
    "#     # Place outputs on CPU for compatibility with other dataset columns   \n",
    "#     return {\"loss\": loss.cpu().numpy(), \n",
    "#             \"predicted_label\": pred_label.cpu().numpy()}\n",
    "# \n",
    "# tokenized_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "# tokenized_data[\"validation\"] = tokenized_data[\"validation\"].map(forward_pass_with_label, batched=True, batch_size=16)\n",
    "# tokenized_data.set_format(\"pandas\")\n",
    "# cols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\n",
    "# df_test = tokenized_data[\"validation\"][:][cols]\n",
    "# df_test[\"label\"] = df_test[\"label\"].map(id2label)\n",
    "# df_test[\"predicted_label\"] = (df_test[\"predicted_label\"].map(id2label))\n",
    "df_test.sort_values(\"loss\", ascending=False).head(10)"
   ],
   "id": "e0b26cc59b560073",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# df_test.sort_values(\"loss\", ascending=True).head(10)",
   "id": "4d5bc2f2559586de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "# best_model_dir = './model/distilbert-base-uncased-finetuned-citation'\n",
    "# trainer.save_model(best_model_dir)\n",
    "# best_model = AutoModelForSequenceClassification.from_pretrained(best_model_dir).to(device)\n",
    "\n",
    "\n",
    "# def predict_sentiment_score(model, dataset, batched=True, BATCH_SIZE=32, weights=[-0.5, 1, -1], shuffle=False):\n",
    "#     '''\n",
    "#     预测句子的情感\n",
    "#     '''\n",
    "#     results = []\n",
    "#     sentiment_scores = []\n",
    "#     model.eval()  # 将模型设置为评估模式\n",
    "\n",
    "#     dataloader = DataLoader(dataset, BATCH_SIZE=BATCH_SIZE, shuffle=shuffle)\n",
    "\n",
    "#     if batched:\n",
    "#         with torch.no_grad():  # 不计算梯度，以加速和节省内存\n",
    "#             for batch in dataloader:\n",
    "#                 # 将批次数据移动到cuda\n",
    "#                 batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#                 outputs = model(**batch)\n",
    "#                 logits = outputs.logits\n",
    "#                 probabilities = F.softmax(logits, dim=-1)\n",
    "                \n",
    "#                 logits_list = logits.tolist()\n",
    "#                 probabilities_list = probabilities.tolist()\n",
    "#                 results.extend(zip(logits_list, probabilities_list))\n",
    "#     else:\n",
    "#         with torch.no_grad():\n",
    "#             inputs = tokenizer(dataset, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "#             outputs = model(**inputs)\n",
    "#             logits = outputs.logits\n",
    "#             probabilities = F.softmax(logits, dim=-1)\n",
    "#             logits_list = logits.tolist()\n",
    "#             probabilities_list = probabilities.tolist()\n",
    "#             results.extend(zip(logits_list, probabilities_list))\n",
    "\n",
    "#     for _, softmax_probs in results:\n",
    "#         softmax_probs_array = np.array(softmax_probs)\n",
    "#         weights_array = np.array(weights)\n",
    "#         score = np.sum(softmax_probs_array * weights_array)\n",
    "#         score = max(min(score, 1), -1)\n",
    "#         sentiment_scores.append(score)\n",
    "#     return sentiment_scores\n",
    "\n",
    "# weights=[-0.1, 1, -1]\n",
    "# all_dataset = ConcatDataset([train_dataset, test_dataset, val_dataset])\n",
    "# sentiment_scores = predict_sentiment_score(best_model, all_dataset, weights=weights, batched=True, BATCH_SIZE=1, shuffle=True) # 句子维度不一样就把BATCH_SIZE设为1\n"
   ],
   "id": "a8bda2611270b290",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
